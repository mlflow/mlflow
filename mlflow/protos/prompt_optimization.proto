syntax = "proto2";

package mlflow;

import "scalapb/scalapb.proto";

option java_package = "org.mlflow.api.proto";
option py_generic_services = true;
option (scalapb.options) = {flat_package: true};

// Status of an optimization job.
enum OptimizationJobStatus {
  OPTIMIZATION_JOB_STATUS_UNSPECIFIED = 0;

  OPTIMIZATION_JOB_STATUS_PENDING = 1;

  OPTIMIZATION_JOB_STATUS_IN_PROGRESS = 2;

  OPTIMIZATION_JOB_STATUS_COMPLETED = 3;

  OPTIMIZATION_JOB_STATUS_FAILED = 4;

  OPTIMIZATION_JOB_STATUS_CANCELED = 5;
}

// Type of optimizer algorithm to use.
enum OptimizerType {
  OPTIMIZER_TYPE_UNSPECIFIED = 0;

  // GEPA (Guided Evolution of Prompt Attributes) optimizer.
  OPTIMIZER_TYPE_GEPA = 1;
}

// Tag for an optimization job.
message OptimizationJobTag {
  optional string key = 1;

  optional string value = 2;
}

// Model configuration for a prompt.
// Maps to the PromptModelConfig Python class.
message PromptModelConfig {
  // The model provider (e.g., "openai", "anthropic", "databricks").
  optional string provider = 1;

  // The name or identifier of the model (e.g., "gpt-4", "claude-3-opus").
  optional string model_name = 2;

  // Sampling temperature for controlling randomness (typically 0.0-2.0).
  optional double temperature = 3;

  // Maximum number of tokens to generate in the response.
  optional int32 max_tokens = 4;

  // Nucleus sampling parameter (typically 0.0-1.0).
  optional double top_p = 5;

  // Top-k sampling parameter.
  optional int32 top_k = 6;

  // Penalty for token frequency (typically -2.0 to 2.0).
  optional double frequency_penalty = 7;

  // Penalty for token presence (typically -2.0 to 2.0).
  optional double presence_penalty = 8;

  // List of sequences that will cause the model to stop generating.
  repeated string stop_sequences = 9;

  // JSON-serialized additional model-specific parameters.
  // This allows for flexibility with provider-specific parameters.
  optional string extra_params_json = 10;
}

// Tag for a prompt version.
message PromptVersionTag {
  // Tag key.
  optional string key = 1;

  // Tag value.
  optional string value = 2;
}

// Represents a prompt version entity.
// Maps to the PromptVersion Python class.
message PromptVersion {
  // The name of the prompt.
  optional string name = 1;

  // The version number of the prompt.
  optional int32 version = 2;

  // The template content of the prompt.
  // For text prompts: a string with {{variable}} placeholders.
  // For chat prompts: JSON-serialized list of message dicts with 'role' and 'content'.
  optional string template = 3;

  // The type of prompt: "text" or "chat".
  optional string prompt_type = 4;

  // Model configuration for this prompt.
  optional PromptModelConfig model_config = 5;

  // JSON-serialized response format schema (optional).
  optional string response_format_json = 6;

  // Commit message / description for this version.
  optional string commit_message = 7;

  // Timestamp when the prompt was created (milliseconds since epoch).
  optional int64 creation_timestamp_ms = 8;

  // Timestamp when the prompt was last updated (milliseconds since epoch).
  optional int64 last_updated_timestamp_ms = 9;

  // User ID that created this prompt version.
  optional string user_id = 10;

  // Tags associated with this prompt version.
  repeated PromptVersionTag tags = 11;

  // Aliases for this prompt version.
  repeated string aliases = 12;
}

// Configuration for a prompt optimization job.
// Stored as run parameters in the underlying MLflow run.
message OptimizationJobConfig {
  // The target prompt to optimize.
  optional PromptVersion target_prompt = 1;

  // The optimizer type to use.
  optional OptimizerType optimizer_type = 2;

  // JSON-serialized optimizer-specific configuration.
  // Different optimizers accept different parameters:
  // - GEPA: {"reflection_model": "openai:/gpt-5", "max_metric_calls": 300}
  optional string optimizer_config_json = 3;
}

// Represents a prompt optimization job entity.
// This is a view constructed from the underlying MLflow run.
message OptimizationJob {
  // Unique identifier for the optimization job.
  // This is the same as the MLflow run_id.
  optional string job_id = 1;

  // Current status of the job.
  optional OptimizationJobStatus status = 2;

  // Timestamp when the job was created (milliseconds since epoch).
  optional int64 creation_timestamp_ms = 3;

  // Timestamp when the job completed (milliseconds since epoch).
  // Only set if status is COMPLETED, FAILED, or CANCELED.
  optional int64 completion_timestamp_ms = 4;

  // ID of the MLflow experiment where this optimization job is tracked.
  optional string experiment_id = 5;

  // ID of the MLflow run associated with this optimization job.
  // Same as job_id for OSS MLflow.
  optional string run_id = 6;

  // Configuration for the optimization job.
  optional OptimizationJobConfig config = 7;

  // Tags associated with this job.
  repeated OptimizationJobTag tags = 8;

  // The source prompt that optimization started from.
  optional PromptVersion source_prompt = 9;

  // The optimized prompt (if optimization completed successfully).
  // This is a new version created under the same prompt name.
  optional PromptVersion optimized_prompt = 10;

  // Error message if the job failed.
  optional string error_message = 11;
}
