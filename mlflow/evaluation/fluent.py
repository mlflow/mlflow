import time
import uuid
from typing import Any, Dict, List, Optional, Set, Union

import pandas as pd

from mlflow.entities import Assessment as AssessmentEntity
from mlflow.entities import Evaluation as EvaluationEntity
from mlflow.entities import EvaluationTag, Metric
from mlflow.evaluation.evaluation import Assessment, Evaluation
from mlflow.evaluation.utils import (
    append_to_assessments_dataframe,
    append_to_tags_dataframe,
    compute_assessment_stats_by_source,
    dataframes_to_evaluations,
    evaluations_to_dataframes,
    get_empty_tags_dataframe,
    read_assessments_dataframe,
    read_evaluations_dataframe,
    read_metrics_dataframe,
    read_tags_dataframe,
    verify_assessments_have_same_value_type,
)
from mlflow.exceptions import MlflowException
from mlflow.protos.databricks_pb2 import (
    INTERNAL_ERROR,
    INVALID_PARAMETER_VALUE,
    RESOURCE_DOES_NOT_EXIST,
)
from mlflow.tracking.client import MlflowClient
from mlflow.tracking.fluent import _get_or_start_run


def log_evaluation(
    *,
    inputs: Dict[str, Any],
    outputs: Optional[Dict[str, Any]] = None,
    inputs_id: Optional[str] = None,
    request_id: Optional[str] = None,
    targets: Optional[Dict[str, Any]] = None,
    error_code: Optional[str] = None,
    error_message: Optional[str] = None,
    assessments: Optional[Union[List[Assessment], List[Dict[str, Any]]]] = None,
    metrics: Optional[Union[List[Metric], Dict[str, float]]] = None,
    tags: Optional[Dict[str, Any]] = None,
    run_id: Optional[str] = None,
) -> EvaluationEntity:
    """
    Logs an evaluation to an MLflow Run.

    Args:
      inputs (Dict[str, Any]): Input fields used by the model to compute outputs.
      outputs (Dict[str, Any]): Outputs computed by the model.
      inputs_id (Optional[str]): Unique identifier for the evaluation `inputs`. If not specified,
          a unique identifier is generated by hashing the inputs.
      request_id (Optional[str]): ID of an MLflow Trace corresponding to the inputs and outputs.
          If specified, displayed in the MLflow UI to help with root causing issues and identifying
          more granular areas for improvement when reviewing the evaluation and adding assessments.
      targets (Optional[Dict[str, Any]]): Targets (ground truths) corresponding to one or more of
          the evaluation `outputs`. Helps root cause issues when reviewing the evaluation and adding
          assessments.
      error_code (Optional[str]): An error code representing any issues encountered during
          the evaluation.
      error_message (Optional[str]): A descriptive error message representing any issues encountered
          during the evaluation.
      assessments (Optional[Union[List[Assessment], List[Dict[str, Any]]]]): Assessment of the
          evaluation, e.g., relevance of documents retrieved by a RAG model to a user input query,
          as assessed by an LLM Judge.
      metrics (Optional[Union[List[Metric], Dict[str, float]]]): Numerical metrics for the
          evaluation, e.g., "number of input tokens", "number of output tokens".
      tags (Optional[Dict[str, Any]]): Tags to log with the evaluation, e.g.
          {"topic": "vacation policy"}
      run_id (Optional[str]): ID of the MLflow Run to log the evaluation. If unspecified, the
          current active run is used, or a new run is started.

    Returns:
       EvaluationEntity: The logged Evaluation object.
    """
    if assessments and isinstance(assessments[0], dict):
        if not all(isinstance(assess, dict) for assess in assessments):
            raise MlflowException(
                "If `assessments` contains a dictionary, all elements must be dictionaries.",
                error_code=INVALID_PARAMETER_VALUE,
            )
        assessments = [Assessment.from_dictionary(assess) for assess in assessments]
    verify_assessments_have_same_value_type(assessments)

    if metrics and isinstance(metrics, dict):
        metrics = [
            Metric(key=k, value=v, timestamp=int(time.time() * 1000), step=0)
            for k, v in metrics.items()
        ]

    if tags and isinstance(tags, dict):
        tags = [EvaluationTag(key=key, value=value) for key, value in tags.items()]

    evaluation = Evaluation(
        inputs=inputs,
        outputs=outputs,
        inputs_id=inputs_id,
        request_id=request_id,
        targets=targets,
        assessments=assessments,
        metrics=metrics,
        tags=tags,
        error_code=error_code,
        error_message=error_message,
    )

    return log_evaluations(evaluations=[evaluation], run_id=run_id)[0]


def log_evaluations(
    *, evaluations: List[Evaluation], run_id: Optional[str] = None
) -> List[EvaluationEntity]:
    """
    Logs one or more evaluations to an MLflow Run.

    Args:
      evaluations (List[Evaluation]): List of one or more MLflow Evaluation objects.
      run_id (Optional[str]): ID of the MLflow Run to log the evaluation. If unspecified, the
          current active run is used, or a new run is started.

    Returns:
      List[EvaluationEntity]: The logged Evaluation objects.
    """
    run_id = run_id if run_id is not None else _get_or_start_run().info.run_id
    client = MlflowClient()
    evaluation_entities = [
        evaluation._to_entity(run_id=run_id, evaluation_id=uuid.uuid4().hex)
        for evaluation in evaluations
    ]
    evaluations_df, metrics_df, assessments_df, tags_df = evaluations_to_dataframes(
        evaluation_entities
    )
    client.log_table(
        run_id=run_id, data=evaluations_df, artifact_file="_evaluations.json", set_tag=True
    )
    client.log_table(run_id=run_id, data=metrics_df, artifact_file="_metrics.json", set_tag=True)
    client.log_table(
        run_id=run_id, data=assessments_df, artifact_file="_assessments.json", set_tag=True
    )
    client.log_table(run_id=run_id, data=tags_df, artifact_file="_tags.json", set_tag=True)

    # TODO (dbczumar): Uncomment this line once we've made adjustments to the metric names and
    #                  determined which metrics we want to log.
    # _update_assessments_stats(
    #     run_id=run_id,
    #     assessments_df=assessments_df,
    #     assessment_names=assessments_df["name"].unique(),
    # )
    return evaluation_entities


def log_evaluations_df(
    *,
    evaluations_df: pd.DataFrame,
    input_cols: List[str],
    output_cols: List[str],
    inputs_id_col: Optional[str] = None,
    target_cols: Optional[List[str]] = None,
    run_id: Optional[str] = None,
) -> pd.DataFrame:
    """
    Logs one or more evaluations from a DataFrame to an MLflow Run.

    Args:
      evaluations_df (pd.DataFrame): Pandas DataFrame containing the evaluations to log.
          Must contain the columns specified in `input_cols`, `output_cols`, and
          `target_cols`.
          Additionally, evaluation information will be read from the following optional columns,
          if specified (see documentation for the log_evaluations() API):
              - "request_id": ID of an MLflow trace corresponding to the evaluation inputs and
                  outputs.
              - "metrics": Numerical evaluation metrics, represented as a list of MLflow Metric
                  objects or as a dictionary.
              - "error_code": An error code representing any issues encountered during the
                  evaluation.'
              - "error_message": A descriptive error message representing any issues encountered
                  during the evaluation.
      input_cols (List[str]): Names of columns containing input fields for evaluation.
      output_cols (List[str]): Names of columns containing output fields for evaluation.
      inputs_id_col (Optional[str]): Name of the column containing unique identifiers for the
          inputs. If not specified, a unique identifier is generated by hashing the inputs.
      target_cols (Optional[List[str]]): Names of columns containing targets (ground truths) for
          evaluation.
      run_id (Optional[str]): ID of the MLflow Run to log the evaluation. If unspecified, the
          current active run is used, or a new run is started.

    Returns:
      pd.DataFrame: The specified evaluations DataFrame, with an additional "evaluation_id" column
          containing the IDs of the logged evaluations.
    """
    if not all(input_col in evaluations_df.columns for input_col in input_cols):
        raise MlflowException(
            "All specified input columns must be present in the DataFrame.",
            error_code=INVALID_PARAMETER_VALUE,
        )

    if not all(output_col in evaluations_df.columns for output_col in output_cols):
        raise MlflowException(
            "All specified output columns must be present in the DataFrame.",
            error_code=INVALID_PARAMETER_VALUE,
        )

    if target_cols and not all(target_col in evaluations_df.columns for target_col in target_cols):
        raise MlflowException(
            "All specified target columns must be present in the DataFrame.",
            error_code=INVALID_PARAMETER_VALUE,
        )

    if inputs_id_col and inputs_id_col not in evaluations_df.columns:
        raise MlflowException(
            "The specified inputs ID column must be present in the DataFrame.",
            error_code=INVALID_PARAMETER_VALUE,
        )

    evaluations_df = evaluations_df.copy()

    # Extract columns for Evaluation objects
    target_data = evaluations_df[target_cols] if target_cols else None

    # Create a list of Evaluation objects
    evaluations = []
    for _, row in evaluations_df.iterrows():
        inputs = row[input_cols].to_dict()
        outputs = row[output_cols].to_dict()
        targets = row[target_cols].to_dict() if target_data is not None else None
        inputs_id = row[inputs_id_col] if inputs_id_col else None
        metrics = row["metrics"] if "metrics" in evaluations_df.columns else None
        if isinstance(metrics, dict):
            metrics = [
                Metric(key=k, value=v, timestamp=int(time.time() * 1000), step=0)
                for k, v in metrics.items()
            ]
        elif metrics is not None and not isinstance(metrics, list):
            raise MlflowException(
                "The 'metrics' column must contain a list of Metric objects or a dictionary.",
                error_code=INVALID_PARAMETER_VALUE,
            )
        evaluations.append(
            Evaluation(
                inputs=inputs,
                outputs=outputs,
                inputs_id=inputs_id,
                targets=targets,
                request_id=row.get("request_id"),
                metrics=metrics,
                error_code=row.get("error_code"),
                error_message=row.get("error_message"),
            )
        )

    # Log evaluations
    evaluation_entities = log_evaluations(evaluations=evaluations, run_id=run_id)

    # Add evaluation_id column to main DataFrame for the result
    evaluations_df["evaluation_id"] = [
        eval_entity.evaluation_id for eval_entity in evaluation_entities
    ]

    return evaluations_df


def log_assessments(
    *,
    evaluation_id: str,
    assessments: Union[Assessment, List[Assessment], Dict[str, Any], List[Dict[str, Any]]],
    run_id: Optional[str] = None,
):
    """
    Logs assessments to an existing Evaluation.

    Args:
        evaluation_id (str): The ID of the evaluation.
        assessments (Union[Assessment, List[Assessment], Dict[str, Any], List[Dict[str, Any]]]):
            An MLflow Assessment object, a dictionary representation of MLflow Assessment objects,
            or a list of these objects / dictionaries.
        run_id (Optional[str]): ID of the MLflow Run containing the evaluation to which to log the
            assessments. If unspecified, the current active run is used.
    """
    run_id = run_id if run_id is not None else _get_or_start_run().info.run_id
    # Fetch the evaluation from the run to verify that it exists
    get_evaluation(run_id=run_id, evaluation_id=evaluation_id)
    client = MlflowClient()

    if isinstance(assessments, dict):
        assessments = [Assessment.from_dictionary(assessments)]
    elif isinstance(assessments, list):
        if any(isinstance(assess, dict) for assess in assessments):
            if not all(isinstance(assess, dict) for assess in assessments):
                raise ValueError(
                    "If `assessments` contains a dictionary, all elements must be dictionaries."
                )
            assessments = [Assessment.from_dictionary(assess) for assess in assessments]
    else:
        assessments = [assessments]
    assessments = [assess._to_entity(evaluation_id=evaluation_id) for assess in assessments]

    assessments_file = client.download_artifacts(run_id=run_id, path="_assessments.json")
    assessments_df = read_assessments_dataframe(assessments_file)
    for assessment in assessments:
        assessments_df = _add_assessment_to_df(
            assessments_df=assessments_df, assessment=assessment, evaluation_id=evaluation_id
        )

    _update_assessments_stats(
        run_id=run_id,
        assessments_df=assessments_df,
        assessment_names={assess.name for assess in assessments},
    )

    with client._log_artifact_helper(run_id, "_assessments.json") as tmp_path:
        assessments_df.to_json(tmp_path, orient="split")


def set_evaluation_tags(*, evaluation_id: str, tags: Dict[str, str], run_id: Optional[str] = None):
    """
    Sets key-value tags on an existing Evaluation.

    Args:
        evaluation_id (str): The ID of the evaluation.
        tags (Dict[str, str]): Tags to set on the evaluation.
        run_id (Optional[str]): ID of the MLflow Run containing the evaluation on which to set the
            tags. If unspecified, the current active run is used.
    """
    run_id = run_id if run_id is not None else _get_or_start_run().info.run_id
    # Fetch the evaluation from the run to verify that it exists
    get_evaluation(run_id=run_id, evaluation_id=evaluation_id)
    client = MlflowClient()

    tags = [EvaluationTag(key=key, value=value) for key, value in tags.items()]
    tags_df = _get_tags_dataframe(client=client, run_id=run_id)

    for tag in tags:
        tags_df = _add_tag_to_df(tags_df=tags_df, tag=tag, evaluation_id=evaluation_id)

    with client._log_artifact_helper(run_id, "_tags.json") as tmp_path:
        tags_df.to_json(tmp_path, orient="split")


def _add_assessment_to_df(
    assessments_df: pd.DataFrame, assessment: AssessmentEntity, evaluation_id: str
) -> pd.DataFrame:
    """
    Adds or updates an assessment in the assessments DataFrame.

    Args:
        assessments_df (pd.DataFrame): The DataFrame containing existing assessments.
        assessment (Assessment): The new assessment to add or update.
        evaluation_id (str): The ID of the evaluation.

    Returns:
        pd.DataFrame: The updated DataFrame with the new or updated assessment.
    """
    # Get assessments with the same name and verify that the type is the same (boolean,
    # numeric, or string)
    existing_assessments_matching_name_df = assessments_df[
        (assessments_df["evaluation_id"] == evaluation_id)
        & (assessments_df["name"] == assessment.name)
    ]
    existing_assessments_matching_name = [
        AssessmentEntity.from_dictionary(assess)
        for assess in existing_assessments_matching_name_df.to_dict(orient="records")
    ]
    if existing_assessments_matching_name:
        # Verify that the value type of the new assessment matches the value type of existing
        # assessments, excluding existing assessments that do not have values
        # (i.e. with value type `None`)
        existing_assessment_value_types = [
            existing_assessment.get_value_type()
            for existing_assessment in existing_assessments_matching_name
            if existing_assessment.get_value_type() is not None
        ]
        if not all(
            assessment.get_value_type() == existing_value_type
            for existing_value_type in existing_assessment_value_types
        ):
            raise MlflowException(
                f"Assessment with name '{assessment.name}' has value type "
                f"'{assessment.get_value_type()}' that does not match the value type "
                f"'{existing_assessment_value_types[0]}' of existing assessments with "
                f" the same name.",
                error_code=INVALID_PARAMETER_VALUE,
            )

    # Check if assessment with the same name and source already exists
    matching_idx = None
    for idx, row in assessments_df.iterrows():
        if (
            row["evaluation_id"] == evaluation_id
            and row["name"] == assessment.name
            and row["source"] == assessment.source.to_dictionary()
            and row["metadata"] == assessment.metadata
        ):
            matching_idx = idx
            break

    if matching_idx is not None:
        # Update existing assessment
        assessments_df.iloc[matching_idx] = assessment.to_dictionary()
    else:
        # Append new assessment
        assessments_df = append_to_assessments_dataframe(assessments_df, [assessment])

    return assessments_df


def _add_tag_to_df(tags_df: pd.DataFrame, tag: EvaluationTag, evaluation_id: str) -> pd.DataFrame:
    """
    Adds or updates a tag in the tags DataFrame.
    """
    # Check if tag with the same name and source already exists
    matching_idx = None
    for idx, row in tags_df.iterrows():
        if row["evaluation_id"] == evaluation_id and row["key"] == tag.key:
            matching_idx = idx
            break

    if matching_idx is not None:
        # Update existing assessment
        tags_df.iloc[matching_idx] = {"evaluation_id": evaluation_id, **tag.to_dictionary()}
    else:
        # Append new assessment
        tags_df = append_to_tags_dataframe(tags_df=tags_df, evaluation_id=evaluation_id, tags=[tag])

    return tags_df


def _update_assessments_stats(
    run_id: str, assessments_df: pd.DataFrame, assessment_names: Set[str]
):
    """
    Updates the specified MLflow Run by logging MLflow Metrics with statistics for the
    specified assessment names, aggregated by source.

    Args:
        run_id (str): ID of the MLflow Run to update.
        assessments_df (pd.DataFrame): DataFrame containing the assessments.
        assessment_names (Set[str]): Names of the assessments for which to update statistics.
    """
    client = MlflowClient()
    for assessment_name in assessment_names:
        assessment_stats_by_source = compute_assessment_stats_by_source(
            assessments_df=assessments_df, assessment_name=assessment_name
        )
        for stats in assessment_stats_by_source.values():
            client.log_batch(run_id=run_id, metrics=stats.to_metrics())


def get_evaluation(*, run_id: str, evaluation_id: str) -> EvaluationEntity:
    """
    Retrieves an Evaluation object from an MLflow Run.

    Args:
        run_id (str): ID of the MLflow Run containing the evaluation.
        evaluation_id (str): The ID of the evaluation.

    Returns:
        Evaluation: The Evaluation object.
    """
    client = MlflowClient()
    if not _contains_evaluation_artifacts(client=client, run_id=run_id):
        raise MlflowException(
            "The specified run does not contain any evaluations. "
            "Please log evaluations to the run before retrieving them.",
            error_code=RESOURCE_DOES_NOT_EXIST,
        )

    evaluations_file = client.download_artifacts(run_id=run_id, path="_evaluations.json")
    evaluations_df = read_evaluations_dataframe(evaluations_file)

    assessments_file = client.download_artifacts(run_id=run_id, path="_assessments.json")
    assessments_df = read_assessments_dataframe(assessments_file)

    metrics_file = client.download_artifacts(run_id=run_id, path="_metrics.json")
    metrics_df = read_metrics_dataframe(metrics_file)

    tags_df = _get_tags_dataframe(client=client, run_id=run_id)

    return _get_evaluation_from_dataframes(
        run_id=run_id,
        evaluation_id=evaluation_id,
        evaluations_df=evaluations_df,
        metrics_df=metrics_df,
        assessments_df=assessments_df,
        tags_df=tags_df,
    )


def search_evaluations(*, run_ids: List[str]) -> List[EvaluationEntity]:
    """
    Retrieves all evaluations from the specified MLflow Runs.

    Args:
        run_ids (List[str]): IDs of the MLflow Runs containing the evaluations to retrieve.
    """
    client = MlflowClient()
    evaluations = []
    for run_id in run_ids:
        if not _contains_evaluation_artifacts(client=client, run_id=run_id):
            continue

        evaluations_file = client.download_artifacts(run_id=run_id, path="_evaluations.json")
        evaluations_df = read_evaluations_dataframe(evaluations_file)

        assessments_file = client.download_artifacts(run_id=run_id, path="_assessments.json")
        assessments_df = read_assessments_dataframe(assessments_file)

        metrics_file = client.download_artifacts(run_id=run_id, path="_metrics.json")
        metrics_df = read_metrics_dataframe(metrics_file)

        tags_df = _get_tags_dataframe(client=client, run_id=run_id)

        for _, row in evaluations_df.iterrows():
            evaluation_id = row["evaluation_id"]
            evaluation = _get_evaluation_from_dataframes(
                run_id=run_id,
                evaluation_id=evaluation_id,
                evaluations_df=evaluations_df,
                metrics_df=metrics_df,
                assessments_df=assessments_df,
                tags_df=tags_df,
            )
            evaluations.append(evaluation)

    return evaluations


def _get_evaluation_from_dataframes(
    *,
    run_id: str,
    evaluation_id: str,
    evaluations_df: pd.DataFrame,
    metrics_df: pd.DataFrame,
    assessments_df: pd.DataFrame,
    tags_df: pd.DataFrame,
) -> EvaluationEntity:
    """
    Parses an Evaluation object with the specified evaluation ID from the specified DataFrames.
    """
    evaluation_row = evaluations_df[evaluations_df["evaluation_id"] == evaluation_id]
    if evaluation_row.empty:
        raise MlflowException(
            f"The specified evaluation ID '{evaluation_id}' does not exist in the run '{run_id}'.",
            error_code=RESOURCE_DOES_NOT_EXIST,
        )

    evaluations: List[Evaluation] = dataframes_to_evaluations(
        evaluations_df=evaluation_row,
        metrics_df=metrics_df,
        assessments_df=assessments_df,
        tags_df=tags_df,
    )
    if len(evaluations) != 1:
        raise MlflowException(
            f"Expected to find a single evaluation with ID '{evaluation_id}', but found "
            f"{len(evaluations)} evaluations.",
            error_code=INTERNAL_ERROR,
        )

    return evaluations[0]


def _contains_evaluation_artifacts(*, client: MlflowClient, run_id: str) -> bool:
    return (
        any(file.path == "_evaluations.json" for file in client.list_artifacts(run_id))
        and any(file.path == "_metrics.json" for file in client.list_artifacts(run_id))
        and any(file.path == "_assessments.json" for file in client.list_artifacts(run_id))
    )


def _get_tags_dataframe(*, client: MlflowClient, run_id: str) -> pd.DataFrame:
    if "_tags.json" in {file.path for file in client.list_artifacts(run_id)}:
        tags_file = client.download_artifacts(run_id=run_id, path="_tags.json")
        return read_tags_dataframe(tags_file)
    else:
        return get_empty_tags_dataframe()
