sklearn:
  package_info:
    pip_release: "scikit-learn"
    install_dev: |
      pip install git+https://github.com/scikit-learn/scikit-learn.git

  models:
    minimum: "0.24.1"
    maximum: "1.5.0"
    requirements:
      "< 1.0": ["scipy==1.7.3"]
    python:
      "== dev": "3.9"
    run: |
      pytest tests/sklearn/test_sklearn_model_export.py

  autologging:
    minimum: "0.24.1"
    maximum: "1.5.0"
    python:
      "== dev": "3.9"
    requirements:
      ">= 0.0.0": ["matplotlib"]
      "< 1.0": ["scipy==1.7.3"]
    run: |
      pytest tests/sklearn/test_sklearn_autolog.py

      # Ensure sklearn autologging works without matplotlib
      pip uninstall -q -y matplotlib
      pytest tests/sklearn/test_sklearn_autolog_without_matplotlib.py

pytorch:
  package_info:
    pip_release: "torch"
    install_dev: |
      pip install --upgrade --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html

  models:
    minimum: "1.9.0"
    maximum: "2.3.1"
    requirements:
      ">= 0.0.0": ["torchvision", "scikit-learn"]
      ">= 1.8": ["transformers"]
      "< 1.8": ["transformers<4.32.0"]
    run: |
      pytest tests/pytorch/test_pytorch_model_export.py tests/pytorch/test_pytorch_metric_value_conversion_utils.py

  autologging:
    minimum: "1.9.0"
    maximum: "2.3.1"
    requirements:
      ">= 0.0.0": ["tensorboard"]
    run: |
      pytest tests/pytorch/test_tensorboard_autolog.py

pytorch-lightning:
  package_info:
    pip_release: "pytorch-lightning"
    install_dev: |
      export PACKAGE_NAME=pytorch
      pip install git+https://github.com/PytorchLightning/pytorch-lightning.git

  autologging:
    minimum: "1.9.0"
    maximum: "2.3.0"
    requirements:
      ">= 0.0.0": ["scikit-learn", "torchvision", "protobuf<4.0.0", "tensorboard", "greenlet<3"]
      # torchmetrics==0.8.0 released a breaking change for versions 1.3 and 1.4 where
      # torchmetrics dependency was defined as non-upper-bounded in its requirements.
      # see: https://github.com/PyTorchLightning/metrics/commit/c537c9b3e8e801772a7e5670ff273321ed26e77b
      # for the removed function that causes pytorch-lightning imports to fail with torchmetrics==0.8.0
      ">= 1.3.0, < 1.5.0": ["torchmetrics<0.8.0"]
      # torchmetrics==1.0.0 released a breaking change in version 1.0.0 that is incompatible with
      # the usage of a private utility within torchmetrics for pytorch-lightning~=1.7 that was
      # modified in pytorch-lightning>=1.8.
      # The problematic commit: https://github.com/Lightning-AI/torchmetrics/commit/3527e17516902eb5265cd2a4f76ad6b0f9f77692
      "< 1.8.0": ["torchmetrics<1.0.0"]
    run: |
      pytest tests/pytorch/test_pytorch_autolog.py

keras:
  package_info:
    pip_release: "keras"
    install_dev: |
      pip install --upgrade git+https://github.com/keras-team/keras.git

  models:
    minimum: "3.0.2"
    maximum: "3.3.3"
    python:
      "== dev": "3.9"
    requirements:
      ">= 3.0.0": ["jax[cpu]>0.4"]
    run: |
      export KERAS_BACKEND=jax
      pytest tests/keras/test_callback.py

  autologging:
    minimum: "3.0.2"
    maximum: "3.3.3"
    python:
      "== dev": "3.9"
    requirements:
      ">= 3.0.0": ["jax[cpu]>0.4"]
    run: |
      export KERAS_BACKEND=jax
      pytest tests/keras/test_autolog.py

tensorflow:
  package_info:
    pip_release: "tensorflow"
    install_dev: |
      pip install --pre tf-nightly

  models:
    minimum: "2.7.4"
    maximum: "2.16.1"
    python:
      "== dev": "3.9"
    requirements:
      # Requirements to run tests for keras
      ">= 0.0.0": ["scikit-learn", "pyspark", "pyarrow", "transformers!=4.38.0,!=4.38.1"]
      # TensorFlow 2.12.1 and 2.13.1 requires typing_extensions < 4.6.0, which is
      # incompatible with SQLAlchemy 2.0.25 with error
      # `cannot import name 'TypeAliasType' from 'typing_extensions'`
      ">= 2.12.1, <= 2.13.1": ["sqlalchemy<2.0.25"]
      "== 2.15.1": ["transformers<4.39.0"]
    run: |
      pytest \
        tests/tensorflow/test_load_saved_tensorflow_estimator.py \
        tests/tensorflow/test_tensorflow2_core_model_export.py \
        tests/tensorflow/test_tensorflow2_metric_value_conversion_utils.py \
        tests/tensorflow/test_keras_model_export.py \
        tests/tensorflow/test_keras_pyfunc_model_works_with_all_input_types.py

  autologging:
    minimum: "2.7.4"
    maximum: "2.16.1"
    python:
      "== dev": "3.9"
    requirements:
      "== dev": ["scikit-learn"]
      # TensorFlow 2.12.1 and 2.13.1 requires typing_extensions < 4.6.0, which is
      # incompatible with SQLAlchemy 2.0.25 with error
      # `cannot import name 'TypeAliasType' from 'typing_extensions'`
      ">= 2.12.1, <= 2.13.1": ["sqlalchemy<2.0.25"]
    run: |
      pytest tests/tensorflow/test_tensorflow2_autolog.py

xgboost:
  package_info:
    pip_release: "xgboost"
    install_dev: |
      pip install git+https://github.com/dmlc/xgboost.git#subdirectory=python-package

  models:
    minimum: "1.4.2"
    maximum: "2.0.3"
    requirements:
      ">= 0.0.0": ["scikit-learn"]
      "< 1.6": ["pandas<2"]
    run: |
      pytest tests/xgboost/test_xgboost_model_export.py

  autologging:
    minimum: "1.4.2"
    maximum: "2.0.3"
    requirements:
      ">= 0.0.0": ["scikit-learn", "matplotlib"]
      "< 1.6": ["pandas<2"]
    run: |
      pytest tests/xgboost/test_xgboost_autolog.py

lightgbm:
  package_info:
    pip_release: "lightgbm"
    install_dev: |
      git clone --recursive https://github.com/microsoft/LightGBM --depth=1 --branch master /tmp/LightGBM
      cd /tmp/LightGBM
      bash build-python.sh install

  models:
    minimum: "3.1.1"
    maximum: "4.4.0"
    requirements:
      ">= 0.0.0": ["scikit-learn"]
    run: |
      pytest tests/lightgbm/test_lightgbm_model_export.py

  autologging:
    minimum: "3.1.1"
    maximum: "4.4.0"
    requirements:
      ">= 0.0.0": ["scikit-learn", "matplotlib"]
    run: |
      pytest tests/lightgbm/test_lightgbm_autolog.py

catboost:
  package_info:
    pip_release: "catboost"
    install_dev: |
      # TODO: Remove the commit once https://github.com/catboost/catboost/issues/2242 is fixed
      pip install \
        git+https://github.com/catboost/catboost.git@f8921d839eccf11e533a46347dde5b5b25b17190#subdirectory=catboost/python-package

  models:
    minimum: "1.0.0"
    maximum: "1.2.5"
    requirements:
      ">= 0.0.0": ["scikit-learn"]
      "< 1.1": ["pandas<2"]
    run: |
      pytest tests/catboost/test_catboost_model_export.py

gluon:
  package_info:
    pip_release: "mxnet"
    install_dev: |
      pip install --pre mxnet -f https://dist.mxnet.io/python/cpu

  models:
    minimum: "1.5.1"
    maximum: "1.9.1"
    requirements:
      # TODO: Adjust the numpy requirement once https://github.com/apache/mxnet/issues/21165 is fixed,
      # which is unlikely to happen given the fact it's been open for 4 months.
      ">= 0.0.0": ["numpy<1.24.0"]
    unsupported: ["1.8.0"] # MXNet 1.8.0 is a flawed release that we don't expect to work with
    run: |
      # Install libopenblas-dev for mxnet 1.8.0.post0
      sudo apt-get update -y
      sudo apt-get install libopenblas-dev -y
      pytest tests/gluon/test_gluon_model_export.py

  autologging:
    minimum: "1.5.1"
    maximum: "1.9.1"
    requirements:
      # TODO: Adjust the numpy requirement once https://github.com/apache/mxnet/issues/21165 is fixed,
      # which is unlikely to happen given the fact it's been open for 4 months.
      ">= 0.0.0": ["numpy<1.24.0"]
    unsupported: ["1.8.0"] # MXNet 1.8.0 is a flawed release that we don't expect to work with
    run: |
      pytest tests/gluon/test_gluon_autolog.py

fastai:
  package_info:
    pip_release: "fastai"

  models:
    minimum: "2.4.1"
    maximum: "2.7.15"
    requirements:
      "< 2.8.0": ["torch<1.13.0", "torchvision<0.14.0"]
      ">= 2.8.0": ["torch", "torchvision"]
    run: |
      pytest tests/fastai/test_fastai_model_export.py

  autologging:
    minimum: "2.4.1"
    maximum: "2.7.15"
    requirements:
      "< 2.8.0": ["torch<1.13.0", "torchvision<0.14.0"]
      ">= 2.8.0": ["torch", "torchvision"]
    run: |
      pytest tests/fastai/test_fastai_autolog.py

onnx:
  package_info:
    pip_release: "onnx"
    install_dev: |
      # This workflow describes how to build a wheel for Linux:
      # https://github.com/onnx/onnx/blob/51a7d932356cbb1205341660a4a52f8c121d8f4b/.github/workflows/release_linux_x86_64.yml

      auth_header="$(git config --local --get http.https://github.com/.extraheader)"
      tmp_dir=$(mktemp -d)
      git clone https://github.com/onnx/onnx.git $tmp_dir
      cd $tmp_dir
      git submodule sync --recursive
      git -c "http.extraheader=$auth_header" -c protocol.version=2 submodule update --init --force --recursive --depth=1

      # Build wheel
      python_version=$(python -c 'import sys; print(".".join(map(str, sys.version_info[:2])))')
      docker run --rm -v $(pwd):/github/workspace --workdir /github/workspace --entrypoint bash \
        quay.io/pypa/manylinux2014_x86_64 .github/workflows/manylinux/entrypoint.sh $python_version manylinux2014_x86_64 pull_request

      # Install wheel
      pip install dist/*manylinux2014_x86_64.whl

  models:
    minimum: "1.13.1"
    maximum: "1.16.1"
    requirements:
      ">= 0.0.0": ["onnxruntime", "torch", "scikit-learn", "protobuf<4.0.0"]
      "< 1.11.0": ["numpy<1.24.0"]
    run: |
      pytest tests/onnx/test_onnx_model_export.py

spacy:
  package_info:
    pip_release: "spacy"
    install_dev: |
      pip install git+https://github.com/explosion/spaCy.git

  models:
    minimum: "2.2.4"
    maximum: "3.7.5"
    requirements:
      ">= 0.0.0": ["scikit-learn", "click<8.1.0"]
      "<= 3.0.9": ["flask<2.1.0", "werkzeug<3"]
      ">= 3.0.9, < 3.4.0": ["typing_extensions<4.6.0", "sqlalchemy<2.0.25"]
    run: |
      pytest tests/spacy/test_spacy_model_export.py

statsmodels:
  package_info:
    pip_release: "statsmodels"
    install_dev: |
      pip install git+https://github.com/statsmodels/statsmodels.git

  models:
    minimum: "0.11.1"
    maximum: "0.14.2"
    python:
      "== dev": "3.9"
    requirements:
      "< 0.13.0": ["pandas==1.3.5", "scipy==1.7.3"]
    run: |
      pytest tests/statsmodels/test_statsmodels_model_export.py

  autologging:
    minimum: "0.11.1"
    maximum: "0.14.2"
    python:
      "== dev": "3.9"
    requirements:
      "< 0.13.0": ["pandas==1.3.5", "scipy==1.7.3"]
    run: |
      pytest tests/statsmodels/test_statsmodels_autolog.py

spark:
  package_info:
    pip_release: "pyspark"
    install_dev: |
      temp_dir=$(mktemp -d)
      git clone --depth 1 https://github.com/apache/spark.git $temp_dir
      cd $temp_dir
      # -Pconnect -Phive is required by spark connect
      ./build/sbt -Pconnect -Phive clean package connect/assembly
      cd python/packaging/classic
      pip install '.[connect]'
      pyspark --version

  models:
    minimum: "3.1.2"
    maximum: "3.5.1"
    python:
      "== dev": "3.9"
    java:
      "> 3.4.1": "17"
    # NB: Allow unreleased maximum versions for the pyspark package to support models and
    # autologging use cases in environments where newer versions of pyspark are available
    # prior to their release on PyPI (e.g. Databricks)
    allow_unreleased_max_version: True
    requirements:
      ">= 0.0.0": ["boto3", "scikit-learn", "pyarrow"]
      "< 3.4.0": ["pandas<2"]
      ">= 3.5.0": ["torch", "scikit-learn", "torcheval"]
      ">= 3.5.0, < dev": ["pyspark[connect]"]
    run: |
      SAGEMAKER_OUT=$(mktemp)
      if mlflow sagemaker build-and-push-container --no-push --mlflow-home . > $SAGEMAKER_OUT 2>&1; then
        echo "Sagemaker container build succeeded.";
      else
        echo "Sagemaker container build failed, output:";
        cat $SAGEMAKER_OUT;
        exit 1
      fi
      pytest tests/spark --ignore tests/spark/autologging --ignore tests/spark/test_spark_connect_model_export.py

      if [[ $(python -c "from packaging.version import Version;import pyspark;print(Version(pyspark.__version__) >= Version('3.5'))") = "True" ]]; then
        pytest tests/spark/test_spark_connect_model_export.py;
      fi

  autologging:
    minimum: "3.1.2"
    maximum: "3.5.1"
    python:
      "== dev": "3.9"
    java:
      ">= 3.4.1": "17"
    # NB: Allow unreleased maximum versions for the pyspark package to support models and
    # autologging use cases in environments where newer versions of pyspark are available
    # prior to their release on PyPI (e.g. Databricks)
    allow_unreleased_max_version: True
    requirements:
      ">= 0.0.0": ["scikit-learn"]
      "< 3.4.0": ["pandas<2"]
    run: |
      # Build Java package
      # Pyspark 4.0 ('dev' version) exclusively supports Scala 2.13
      if [[ $(python -c "import pyspark;print(pyspark.__version__)") = "4.0.0.dev0" ]]; then
        python dev/set-mlflow-spark-scala-version.py --scala-version 2.13.10
      fi
      pushd mlflow/java/spark
      mvn package -DskipTests -q
      popd

      # `test_spark_disable_autologging.py` is flaky if it runs with other tests
      # so run it individually
      pytest tests/spark/autologging/datasource/test_spark_disable_autologging.py

      # NB: Running each .py file individually is important (instead of
      # 'pytest tests/spark/autologging') not to share SparkSession across tests
      # `test_spark_datasource_autologging_crossframework.py` fails when run with other tests
      pytest tests/spark/autologging/datasource/test_spark_datasource_autologging_crossframework.py

      find tests/spark/autologging -name 'test*.py' | grep -v crossframework | grep -v test_spark_disable_autologging | xargs -L 1 pytest

mleap:
  package_info:
    pip_release: "mleap"

  models:
    minimum: "0.18.0"
    maximum: "0.23.1"
    requirements:
      ">= 0.0.0": ["pandas<2"]
      "< 0.22.0": ["numpy<1.24.0"]
      "> 0.17.0, < 0.21.0": ["pyspark==3.0.2"]
      ">= 0.21.0": ["pyspark<3.4"]
    run: |
      SAGEMAKER_OUT=$(mktemp)
      if mlflow sagemaker build-and-push-container --no-push --mlflow-home . > $SAGEMAKER_OUT 2>&1; then
        echo "Sagemaker container build succeeded.";
      else
        echo "Sagemaker container build failed, output:";
        cat $SAGEMAKER_OUT;
        exit 1
      fi
      pytest tests/mleap/test_mleap_model_export.py

prophet:
  package_info:
    pip_release: "prophet"
    install_dev: |
      sudo apt-get -y install libtbb2
      pip install git+https://github.com/facebook/prophet.git#subdirectory=python

  models:
    minimum: "1.1.1"
    maximum: "1.1.5"
    requirements:
      # Avoid holidays 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200 on
      # older version of prophet. Compatibility updates have been released as part of the 1.1.4
      # release of prophet.
      "<= 1.1.3": ["holidays<=0.24"]
    run: |
      pytest tests/prophet/test_prophet_model_export.py

pmdarima:
  package_info:
    pip_release: "pmdarima"
    install_dev: |
      pip install Cython
      pip install git+https://github.com/alkaline-ml/pmdarima.git

  models:
    minimum: "1.8.0"
    maximum: "2.0.4"
    requirements:
      "< 1.9": ["scikit-learn<1.3"]
      # Avoid holidays 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200
      ">= 0.0.0": ["prophet", "holidays!=0.25"]
    run: |
      pytest tests/pmdarima/test_pmdarima_model_export.py

diviner:
  package_info:
    pip_release: "diviner"
    install_dev: |
      pip install git+https://github.com/databricks/diviner.git

  models:
    minimum: "0.1.0"
    maximum: "0.1.1"
    requirements:
      # https://github.com/alan-turing-institute/sktime/issues/2898
      # Avoid holidays 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200
      ">= 0.0": ["cmdstanpy!=1.0.2", "holidays!=0.25"]
      "<= 0.1.0": ["pmdarima<2.0.0"]
      # Diviner <= 0.1.1 isn't compatible with pandas>=2
      "<= 0.1.1": ["pandas<2"]
      # As of Jun 7, 2023, Diviner dev isn't compatible with pandas>=2
      "== dev": ["pandas<2"]
    run: |
      pytest tests/diviner/test_diviner_model_export.py

h2o:
  package_info:
    pip_release: "h2o"
  models:
    minimum: "3.40.0.1"
    maximum: "3.46.0.3"
    run: |
      pytest tests/h2o

shap:
  package_info:
    pip_release: "shap"
  models:
    minimum: "0.41.0"
    maximum: "0.45.1"
    requirements:
      "<= 0.41.0": ["numpy<1.24.0"]
    run: |
      pytest tests/shap

paddle:
  package_info:
    pip_release: "paddlepaddle"
  models:
    minimum: "2.4.1"
    maximum: "2.6.1"
    requirements:
      "< 2.5.0": ["google-cloud-storage==2.14.0"]
    run: |
      pytest tests/paddle/test_paddle_model_export.py
  autolog:
    minimum: "2.4.1"
    maximum: "2.4.1"
    run: |
      pytest tests/paddle/test_paddle_autolog.py

transformers:
  package_info:
    pip_release: "transformers"
    install_dev: |
      pip install git+https://github.com/huggingface/transformers
  models:
    minimum: "4.25.1"
    maximum: "4.41.2"
    unsupported: [
        # Avoid this patch: https://github.com/huggingface/transformers/pull/29032
        "4.38.0",
        "4.38.1",
        "4.38.2",
      ]
    requirements:
      ">= 0.0.0": [
          "datasets",
          # 0.22.0 is broken: https://github.com/huggingface/huggingface_hub/issues/2157
          "huggingface_hub!=0.22.0",
          "hf_transfer",
          "torch",
          "torchvision",
          "tensorflow",
          "peft", # optimized fine-tuning library developed by Hugging Face
          "accelerate", # required for large torch models where weights will not fit in RAM
          "librosa", # required for transformers audio pipelines for bitrate conversion
          "ffmpeg", # required for transformers audio pipelines for audio byte to numpy conversion
          "sentencepiece", # required for transformers text2text generation pipeline
          "protobuf<=3.20.3", # fix error `Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto`
          "typing_extensions>=4.6.0", # fix error for SqlAlchemy == 2.0.25 cannot import name 'TypeAliasType' from 'typing_extensions'
        ]
      # tensorflow 2.13.0 made all Keras private functions inaccessible. transformers versions
      # prior to 4.30.0 are incompatible with this breaking change in Keras.
      # See: https://github.com/huggingface/transformers/issues/23352 for discussion
      "< 4.30.0": ["tensorflow<2.13.0"]
    pre_test: |
      export PYTHONPATH=./
      python tests/transformers/helper.py
    run: |
      # Run all Transformers tests except autologging
      pytest tests/transformers --ignore tests/transformers/test_transformers_autolog.py
      pip uninstall -y accelerate
      pytest tests/transformers/test_transformers_model_export.py -k "test_transformers_tf_model_save_without_conda_env_uses_default_env_with_expected_dependencies or test_transformers_pt_model_save_dependencies_without_accelerate"
  autologging:
    minimum: "4.25.1"
    maximum: "4.41.2"
    unsupported: [
        # Avoid this patch: https://github.com/huggingface/transformers/pull/29032
        "4.38.0",
        "4.38.1",
        "4.38.2",
      ]
    requirements:
      ">= 0.0.0": [
          "datasets",
          # 0.22.0 is broken: https://github.com/huggingface/huggingface_hub/issues/2157
          "huggingface_hub!=0.22.0",
          "hf_transfer",
          "torch",
          "torchvision",
          "tensorflow",
          "accelerate",
          "setfit",
          "optuna",
        ]
      # sentence-transformers >= 2.4.0 relies on `transformers.utils.import_utils.is_nltk_available`
      # to check nltk availability. This function is only available in transformers>=4.34.0.
      "< 4.34": ["sentence-transformers<2.4.0"]
      # tensorflow 2.13.0 made all Keras private functions inaccessible. transformers versions
      # prior to 4.30.0 are incompatible with this breaking change in Keras.
      # See: https://github.com/huggingface/transformers/issues/23352 for discussion
      "< 4.30.0": ["tensorflow<2.13.0"]
    run: |
      pytest tests/transformers/test_transformers_autolog.py

openai:
  package_info:
    pip_release: "openai"
    install_dev: |
      pip install git+https://github.com/openai/openai-python
  models:
    minimum: "0.27.2"
    maximum: "1.34.0"
    requirements:
      ">= 0.0.0": ["pyspark", "tiktoken", "aiohttp", "tenacity"]
    run: |
      pytest tests/openai/test_openai_model_export.py
  autologging:
    minimum: "1.17.0"
    maximum: "1.34.0"
    requirements:
      ">= 0.0.0": ["pyspark", "tiktoken", "aiohttp", "tenacity"]
    run: |
      pytest tests/openai/test_openai_autolog.py

langchain:
  package_info:
    pip_release: "langchain"
    install_dev: |
      pip install git+https://github.com/hwchase17/langchain#subdirectory=libs/langchain
  models:
    minimum: "0.0.244"
    maximum: "0.2.5"
    requirements:
      ">= 0.0.0": [
          "pyspark",
          "transformers",
          "tensorflow<2.16",
          "openai",
          "google-search-results",
          "psutil",
          "faiss-cpu",
          "langchain-experimental",
          "numexpr",
          "hf_transfer",
          "langchain-core!=0.1.39", # https://github.com/langchain-ai/langchain/issues/19947
        ]
      ">= 0.2": ["langchain-huggingface"]
    run: |
      LANGCHAIN_NEW=$(python -c "from packaging.version import Version;import langchain;print(Version(langchain.__version__) >= Version('0.0.267'))")
      # Run all langchain tests except autologging
      pytest tests/langchain --ignore tests/langchain/test_langchain_tracer.py --ignore tests/langchain/test_langchain_autolog.py
      if [[ $LANGCHAIN_NEW = "True" ]]; then
        pytest tests/langchain/test_langchain_tracer.py
      fi
      # test both pydantic v1 and v2
      PYDANTIC_VERSION=$(python -c "import pydantic; print(pydantic.__version__)")
      if [[ $PYDANTIC_VERSION == 1.* && $LANGCHAIN_NEW = "True" ]]; then
        pip install 'pydantic>2'
        echo "Testing langchain model export with pydantic v2"
        pytest tests/langchain --ignore tests/langchain/test_langchain_autolog.py
      fi
  autologging:
    minimum: "0.1.0"
    maximum: "0.2.5"
    requirements:
      ">= 0.0.0": [
          "pyspark",
          "transformers",
          "tensorflow",
          "openai",
          "google-search-results",
          "psutil",
          "faiss-cpu",
          "langchain-experimental",
          "numexpr",
          "spacy",
          "langchain-core!=0.1.39", # https://github.com/langchain-ai/langchain/issues/19947
          "pytest-asyncio", # for testing runnable async functions patches
        ]
    run: |
      # For MlflowCallback text analysis
      python -m spacy download en_core_web_sm
      pytest tests/langchain/test_langchain_autolog.py
      # test both pydantic v1 and v2
      PYDANTIC_VERSION=$(python -c "import pydantic; print(pydantic.__version__)")
      if [[ $PYDANTIC_VERSION == 1.* ]]; then
        pip install 'pydantic>2'
        sleep 10
        echo "Testing langchain autolog with pydantic v2"
        pytest tests/langchain/test_langchain_autolog.py
      fi

sentence_transformers:
  package_info:
    pip_release: "sentence-transformers"
    install_dev: |
      pip install git+https://github.com/UKPLab/sentence-transformers#egg=sentence-transformers
  models:
    minimum: "2.2.2"
    maximum: "3.0.1"
    requirements:
      ">= 0.0.0": [
          "pyspark",
          "torch>1.6",
          "transformers>4.25",
          "hf_transfer",
          # Required by a test model (nomic-ai/nomic-embed-text-v1.5)
          "einops",
        ]
    run: |
      pytest tests/sentence_transformers/test_sentence_transformers_model_export.py

johnsnowlabs:
  package_info:
    pip_release: "johnsnowlabs"
  models:
    minimum: "4.4.6"
    maximum: "5.3.6"
    requirements:
      ">= 0.0.0": ["pandas<=1.5.3"]
      "< 5.2.8": ["pyspark<3.4"]
    run: |
      if [ ! -z "$JOHNSNOWLABS_LICENSE_JSON" ]; then
        pytest tests/johnsnowlabs/test_johnsnowlabs_model_export.py
      else
        echo "Skipping tests due to missing license key"
      fi

promptflow:
  package_info:
    pip_release: "promptflow"
    # Install dev subpackages to ensure test running on the latest code
    install_dev: |
      pip install git+https://github.com/microsoft/promptflow#subdirectory=src/promptflow
  models:
    minimum: "1.3.0"
    maximum: "1.12.0"
    requirements:
      # Requirements to run sparkudf predict test
      ">= 0.0.0": ["pyspark", "jinja2"]
      ">= 1.11.0": ["openai>=1"]
    run: |
      pytest tests/promptflow/test_promptflow_model_export.py
