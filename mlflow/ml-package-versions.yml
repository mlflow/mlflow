sklearn:
  package_info:
    pip_release: "scikit-learn"
    install_dev: |
      pip install git+https://github.com/scikit-learn/scikit-learn.git

  models:
    minimum: "0.20.3"
    maximum: "0.24.2"
    run: |
      pytest tests/sklearn/test_sklearn_model_export.py --large

  autologging:
    minimum: "0.20.3"
    maximum: "0.24.2"
    requirements: ["matplotlib"]
    run: |
      pytest tests/sklearn/test_sklearn_autolog.py --large

pytorch:
  package_info:
    pip_release: "torch"
    install_dev: |
      pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html

  models:
    minimum: "1.4.0"
    maximum: "1.8.1"
    requirements: ["torchvision", "scikit-learn"]
    run: |
      pytest tests/pytorch/test_pytorch_model_export.py --large

pytorch-lightning:
  package_info:
    pip_release: "pytorch-lightning"
    install_dev: |
      # This installs pytorch-lightning from the default branch
      pip install git+https://github.com/PytorchLightning/pytorch-lightning.git

  autologging:
    minimum: "1.0.5"
    maximum: "1.3.0"
    requirements: ["torchvision", "scikit-learn"]
    run: |
      pytest tests/pytorch/test_pytorch_autolog.py --large

tensorflow:
  package_info:
    pip_release: "tensorflow"
    install_dev: |
      pip install tf-nightly

  models:
    minimum: "1.15.4"
    maximum: "2.4.1"
    requirements:
      "< 2.2": ["h5py<3.0", "scikit-learn"]
    run: |
      python_code="
      import tensorflow as tf
      major_ver = tf.__version__.split('.')[0]
      assert major_ver in ['1', '2']
      print(major_ver)
      "
      tf_major_version=$(python -c "$python_code")

      if [ "$tf_major_version" == "1" ]; then
        pytest tests/tensorflow/test_tensorflow_model_export.py --large
      else
        pytest tests/tensorflow/test_tensorflow2_model_export.py --large
      fi

  autologging:
    minimum: "1.15.4"
    maximum: "2.4.1"
    requirements:
      "< 2.2": ["h5py<3.0"]
    run: |
      python_code="
      import tensorflow as tf
      major_ver = tf.__version__.split('.')[0]
      assert major_ver in ['1', '2']
      print(major_ver)
      "
      tf_major_version=$(python -c "$python_code")

      if [ "$tf_major_version" == "1" ]; then
        pytest tests/tensorflow_autolog/test_tensorflow_autolog.py --large
      else
        pytest tests/tensorflow_autolog/test_tensorflow2_autolog.py --large
      fi

keras:
  package_info:
    pip_release: "keras"

  models:
    minimum: "2.2.4"
    maximum: "2.4.3"
    requirements:
      "< 2.3.1": ["tensorflow==1.15.4", "h5py<3.0", "scikit-learn", "pyspark", "pyarrow"]
      "== 2.3.1": ["tensorflow==2.2.1", "scikit-learn", "pyspark", "pyarrow"]
      "> 2.3.1": ["tensorflow<2.5.0", "scikit-learn", "pyspark", "pyarrow"]
    run: |
      pytest tests/keras/test_keras_model_export.py --large

  autologging:
    minimum: "2.2.4"
    maximum: "2.4.3"
    requirements:
      "< 2.3.1": ["tensorflow==1.15.4", "h5py<3.0"]
      # keras 2.3.1 is incompatible with tensorflow > 2.2.1 and causes the issue reported here:
      # https://github.com/tensorflow/tensorflow/issues/38589
      "== 2.3.1": ["tensorflow==2.2.1"]
      "> 2.3.1": ["tensorflow<2.5.0"]
    run: |
      pytest tests/keras_autolog/test_keras_autolog.py --large

xgboost:
  package_info:
    pip_release: "xgboost"
    install_dev: |
      temp_dir=$(mktemp -d)
      git clone --recursive https://github.com/dmlc/xgboost $temp_dir
      cd $temp_dir/python-package
      python setup.py install

  models:
    minimum: "0.90"
    maximum: "1.4.1"
    requirements: ["scikit-learn"]
    run: |
      pytest tests/xgboost/test_xgboost_model_export.py --large

  autologging:
    minimum: "0.90"
    maximum: "1.4.1"
    requirements: ["scikit-learn", "matplotlib"]
    run: |
      pytest tests/xgboost/test_xgboost_autolog.py --large

lightgbm:
  package_info:
    pip_release: "lightgbm"
    install_dev: |
      temp_dir=$(mktemp -d)
      git clone --recursive https://github.com/microsoft/LightGBM.git $temp_dir
      cd $temp_dir/python-package
      python setup.py install

  models:
    minimum: "2.3.1"
    maximum: "3.2.1"
    requirements: ["scikit-learn"]
    run: |
      pytest tests/lightgbm/test_lightgbm_model_export.py --large

  autologging:
    minimum: "2.3.1"
    maximum: "3.2.1"
    requirements: ["scikit-learn", "matplotlib"]
    run: |
      pytest tests/lightgbm/test_lightgbm_autolog.py --large

catboost:
  package_info:
    pip_release: "catboost"
    install_dev: |
      # The cross-version-tests workflow runs this command with the environment variable `CACHE_DIR`
      if [ -d "$CACHE_DIR" ] && [ ! -z $(find $CACHE_DIR -type f -name "catboost-*.whl") ]; then
        pip install $(find $CACHE_DIR -type f -name "catboost-*.whl")
      else
        # Build wheel from source
        temp_dir=$(mktemp -d)
        git clone --depth 1 https://github.com/catboost/catboost $temp_dir
        cd $temp_dir/catboost/python-package
        python setup.py bdist_wheel

        # Copy wheel in cache directory
        wheel_path=$(find dist -type f -name "*.whl")
        mkdir -p $CACHE_DIR
        cp $wheel_path $CACHE_DIR

        # Install wheel
        pip install $wheel_path
      fi

  models:
    minimum: "0.23.1"
    maximum: "0.25.1"
    requirements: ["scikit-learn"]
    run: |
      pytest tests/catboost/test_catboost_model_export.py --large

gluon:
  package_info:
    pip_release: "mxnet"
    install_dev: |
      pip install --pre mxnet -f https://dist.mxnet.io/python/cpu

  models:
    minimum: "1.5.1"
    maximum: "1.8.0.post0"
    unsupported: ["1.8.0"] # MXNet 1.8.0 is a flawed release that we don't expect to work with
    run: |
      pytest tests/gluon/test_gluon_model_export.py --large

  autologging:
    minimum: "1.5.1"
    maximum: "1.8.0.post0"
    unsupported: ["1.8.0"] # MXNet 1.8.0 is a flawed release that we don't expect to work with
    run: |
      pytest tests/gluon_autolog/test_gluon_autolog.py --large

fastai-1.x:
  package_info:
    pip_release: "fastai"

  models:
    # Avoid updating the maximum version to 2.x which isn't supported yet
    pin_maximum: True
    minimum: "1.0.60"
    maximum: "1.0.61"
    requirements: ["scikit-learn"]
    run: |
      pytest tests/fastai/test_fastai_model_export.py --large

  autologging:
    pin_maximum: True
    minimum: "1.0.60"
    maximum: "1.0.61"
    requirements: ["scikit-learn"]
    run: |
      pytest tests/fastai/test_fastai_autolog.py --large

onnx:
  package_info:
    pip_release: "onnx"
    install_dev: |
      sudo apt-get install protobuf-compiler libprotoc-dev
      temp_dir=$(mktemp -d)
      git clone https://github.com/onnx/onnx.git $temp_dir
      cd $temp_dir
      git submodule update --init --recursive
      python setup.py install

  models:
    minimum: "1.5.0"
    maximum: "1.9.0"
    requirements: ["onnxruntime", "torch", "scikit-learn"]
    run: |
      pytest tests/onnx/test_onnx_model_export.py --large

spacy:
  package_info:
    pip_release: "spacy"
    install_dev: |
      pip install git+https://github.com/explosion/spaCy

  models:
    minimum: "2.2.4"
    maximum: "3.0.6"
    requirements: ["scikit-learn"]
    run: |
      pytest tests/spacy/test_spacy_model_export.py --large

statsmodels:
  package_info:
    pip_release: "statsmodels"
    install_dev: |
      pip install git+https://github.com/statsmodels/statsmodels

  models:
    minimum: "0.11.1"
    maximum: "0.12.2"
    run: |
        pytest tests/statsmodels/test_statsmodels_model_export.py --large

  autologging:
    minimum: "0.11.1"
    maximum: "0.12.2"
    run: |
      pytest tests/statsmodels/test_statsmodels_autolog.py --large

spark:
  package_info:
    pip_release: "pyspark"
    install_dev: |
      # The cross-version-tests workflow runs this command with the environment variable `CACHE_DIR`
      if [ -d "$CACHE_DIR" ] && [ ! -z $(find $CACHE_DIR -type f -name "pyspark-*.whl") ]; then
        pip install $(find $CACHE_DIR -type f -name "pyspark-*.whl")
      else
        # Build wheel from source
        temp_dir=$(mktemp -d)
        git clone --depth 1 https://github.com/apache/spark $temp_dir
        cd $temp_dir
        export MAVEN_OPTS="-Xss256m -Xmx2g -XX:ReservedCodeCacheSize=1g"
        ./build/mvn -DskipTests --no-transfer-progress clean package
        cd python
        python setup.py bdist_wheel

        # Copy wheel in cache directory
        wheel_path=$(find dist -type f -name "*.whl")
        mkdir -p $CACHE_DIR
        cp $wheel_path $CACHE_DIR

        # Install wheel
        pip install $wheel_path
      fi

  models:
    minimum: "3.0.0"
    maximum: "3.1.1"
    requirements: ["boto3", "scikit-learn", "pyarrow"]
    run: |
      SAGEMAKER_OUT=$(mktemp)
      if mlflow sagemaker build-and-push-container --no-push --mlflow-home . > $SAGEMAKER_OUT 2>&1; then
        echo "Sagemaker container build succeeded.";
      else
        echo "Sagemaker container build failed, output:";
        cat $SAGEMAKER_OUT;
        exit 1
      fi
      pytest tests/spark --large --ignore tests/spark/test_mleap_model_export.py

  autologging:
    minimum: "3.0.0"
    maximum: "3.1.1"
    run: |
      # TODO: Add datasource autologging tests
      find tests/spark_autologging/ml -name 'test*.py' | xargs -L 1 pytest --large
