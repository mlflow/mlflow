sklearn:
  package_info:
    pip_release: "scikit-learn"
    install_dev: |
      pip install git+https://github.com/scikit-learn/scikit-learn.git

  models:
    minimum: "1.3.2"
    maximum: "1.7.2"
    run: |
      pytest tests/sklearn/test_sklearn_model_export.py

  autologging:
    minimum: "1.3.2"
    maximum: "1.7.2"
    requirements:
      ">= 0.0.0": ["matplotlib"]
    run: |
      pytest tests/sklearn/test_sklearn_autolog.py

      # Ensure sklearn autologging works without matplotlib
      pip uninstall -q -y matplotlib
      pytest tests/sklearn/test_sklearn_autolog_without_matplotlib.py

pytorch:
  package_info:
    pip_release: "torch"
    module_name: "torch"
    install_dev: |
      pip install --upgrade --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html

  models:
    minimum: "2.1.1"
    maximum: "2.8.0"
    requirements:
      ">= 0.0.0": ["torchvision", "scikit-learn"]
      ">= 1.8": ["transformers"]
      "< 2.3": ["numpy<2", "transformers<=4.49.0", "safetensors<0.6.0"]
    run: |
      pytest tests/pytorch/test_pytorch_model_export.py tests/pytorch/test_pytorch_metric_value_conversion_utils.py

  autologging:
    minimum: "2.1.1"
    maximum: "2.8.0"
    requirements:
      ">= 0.0.0": ["tensorboard"]
    run: |
      pytest tests/pytorch/test_tensorboard_autolog.py

pytorch-lightning:
  package_info:
    pip_release: "pytorch-lightning"
    module_name: "lightning"
    install_dev: |
      export PACKAGE_NAME=pytorch
      pip install git+https://github.com/PytorchLightning/pytorch-lightning.git

  autologging:
    minimum: "2.1.1"
    maximum: "2.5.5"
    requirements:
      ">= 0.0.0":
        [
          "scikit-learn",
          "torchvision",
          "protobuf<4.0.0",
          "tensorboard",
          "greenlet<3",
          "pytorch-forecasting",
        ]
    run: |
      pytest tests/pytorch/test_pytorch_autolog.py

  models:
    minimum: "2.1.1"
    maximum: "2.5.5"
    requirements:
      ">= 0.0.0": ["pytorch-forecasting"]
    run: |
      pytest tests/pytorch/test_forecasting_model.py

keras:
  package_info:
    pip_release: "keras"
    install_dev: |
      pip install --upgrade git+https://github.com/keras-team/keras.git

  models:
    minimum: "3.0.2"
    maximum: "3.11.3"
    requirements:
      ">= 3.0.0": ["jax[cpu]>0.4"]
      "< 3.10.0": ["jax[cpu]<0.6"]
    run: |
      export KERAS_BACKEND=jax
      pytest tests/keras/test_callback.py

  autologging:
    minimum: "3.0.2"
    maximum: "3.11.3"
    requirements:
      ">= 3.0.0": ["jax[cpu]>0.4"]
      "< 3.10.0": ["jax[cpu]<0.6"]
    run: |
      export KERAS_BACKEND=jax
      pytest tests/keras/test_autolog.py

tensorflow:
  package_info:
    pip_release: "tensorflow"
    install_dev: |
      pip install --pre tf-nightly

  models:
    minimum: "2.14.1"
    maximum: "2.20.0"
    requirements:
      # Requirements to run tests for keras
      ">= 0.0.0": ["scikit-learn", "pyspark", "pyarrow", "transformers!=4.38.0,!=4.38.1"]
      "== 2.15.1": ["transformers<4.39.0"]
    run: |
      pytest \
        tests/tensorflow/test_tensorflow2_core_model_export.py \
        tests/tensorflow/test_tensorflow2_metric_value_conversion_utils.py \
        tests/tensorflow/test_keras_model_export.py \
        tests/tensorflow/test_keras_pyfunc_model_works_with_all_input_types.py

  autologging:
    minimum: "2.14.1"
    maximum: "2.20.0"
    requirements:
      "== dev": ["scikit-learn"]
    run: |
      pytest tests/tensorflow/test_tensorflow2_autolog.py

xgboost:
  package_info:
    pip_release: "xgboost"
    install_dev: |
      pip install git+https://github.com/dmlc/xgboost.git#subdirectory=python-package

  models:
    minimum: "2.0.1"
    maximum: "3.0.5"
    requirements:
      ">= 0.0.0": ["scikit-learn"]
    run: |
      pytest tests/xgboost/test_xgboost_model_export.py

  autologging:
    minimum: "2.0.1"
    maximum: "3.0.5"
    requirements:
      ">= 0.0.0": ["scikit-learn", "matplotlib"]
    run: |
      pytest tests/xgboost/test_xgboost_autolog.py

lightgbm:
  package_info:
    pip_release: "lightgbm"
    install_dev: |
      git clone --recursive https://github.com/microsoft/LightGBM --depth=1 --branch master /tmp/LightGBM
      cd /tmp/LightGBM
      bash build-python.sh install

  models:
    minimum: "4.2.0"
    maximum: "4.6.0"
    requirements:
      ">= 0.0.0": ["scikit-learn"]
      "< 4.4.0": ["numpy<2"]
    run: |
      pytest tests/lightgbm/test_lightgbm_model_export.py

  autologging:
    minimum: "4.2.0"
    maximum: "4.6.0"
    requirements:
      ">= 0.0.0": ["scikit-learn", "matplotlib"]
      "< 4.4.0": ["numpy<2"]
    run: |
      pytest tests/lightgbm/test_lightgbm_autolog.py

catboost:
  package_info:
    pip_release: "catboost"

  models:
    minimum: "1.2.3"
    maximum: "1.2.8"
    requirements:
      ">= 0.0.0": ["scikit-learn"]
      "< 1.3": ["numpy<2"]
    run: |
      pytest tests/catboost/test_catboost_model_export.py

onnx:
  package_info:
    pip_release: "onnx"
    install_dev: |
      # This workflow describes how to build a wheel for Linux:
      # https://github.com/onnx/onnx/blob/51a7d932356cbb1205341660a4a52f8c121d8f4b/.github/workflows/release_linux_x86_64.yml

      auth_header="$(git config --local --get http.https://github.com/.extraheader)"
      tmp_dir=$(mktemp -d)
      git clone https://github.com/onnx/onnx.git $tmp_dir
      cd $tmp_dir
      git submodule sync --recursive
      git -c "http.extraheader=$auth_header" -c protocol.version=2 submodule update --init --force --recursive --depth=1

      # Build wheel
      python_version=$(python -c 'import sys; print(".".join(map(str, sys.version_info[:2])))')
      docker run --rm -v $(pwd):/github/workspace --workdir /github/workspace --entrypoint bash \
        quay.io/pypa/manylinux2014_x86_64 .github/workflows/manylinux/entrypoint.sh $python_version manylinux2014_x86_64 pull_request

      # Install wheel
      pip install dist/*manylinux*.whl

  models:
    minimum: "1.17.0"
    maximum: "1.19.1"
    requirements:
      ">= 0.0.0": ["onnxruntime", "onnxscript", "torch", "scikit-learn"]
    run: |
      pytest tests/onnx/test_onnx_model_export.py

semantic_kernel:
  package_info:
    genai: true
    pip_release: "semantic-kernel"
    install_dev: |
      pip install git+https://github.com/microsoft/semantic-kernel.git@main#subdirectory=python

  autologging:
    minimum: "1.34.0"
    maximum: "1.37.0"
    requirements:
      ">= 1.34.0": [
          "pydantic>=2.0,<2.12",
          # 1.99.2 ~ 1.99.8 have unintended breaking changes:
          # https://github.com/openai/openai-python/issues/2525
          "openai>=1.99.9",
        ]
    run: |
      pytest tests/semantic_kernel/test_semantic_kernel_autolog.py

spacy:
  package_info:
    pip_release: "spacy"
    install_dev: |
      pip install git+https://github.com/explosion/spaCy.git

  models:
    minimum: "3.7.4"
    maximum: "3.8.7"
    requirements:
      "< 3.8": ["numpy<2"]
    run: |
      pytest tests/spacy/test_spacy_model_export.py

statsmodels:
  package_info:
    pip_release: "statsmodels"
    install_dev: |
      pip install git+https://github.com/statsmodels/statsmodels.git

  models:
    minimum: "0.14.1"
    maximum: "0.14.5"
    run: |
      pytest tests/statsmodels/test_statsmodels_model_export.py

  autologging:
    minimum: "0.14.1"
    maximum: "0.14.5"
    run: |
      pytest tests/statsmodels/test_statsmodels_autolog.py

spark:
  package_info:
    pip_release: "pyspark"
    module_name: "pyspark"
    install_dev: |
      temp_dir=$(mktemp -d)
      git clone --depth 1 https://github.com/apache/spark.git $temp_dir
      cd $temp_dir
      # -Pconnect -Phive is required by spark connect
      ./build/sbt -Pconnect -Phive clean package connect/assembly
      cd python/packaging/classic
      pip install '.[connect]'
      pyspark --version

  models:
    minimum: "3.2.1"
    maximum: "4.0.1"
    java:
      "<= 3.4.1": "11"
    # NB: Allow unreleased maximum versions for the pyspark package to support models and
    # autologging use cases in environments where newer versions of pyspark are available
    # prior to their release on PyPI (e.g. Databricks)
    allow_unreleased_max_version: True
    requirements:
      ">= 0.0.0": ["boto3", "scikit-learn", "pyarrow", "numpy<2"]
      "< 3.4.0": ["pandas<2"]
      ">= 3.5.0": ["torch<2.6.0", "scikit-learn", "torcheval"]
      ">= 3.5.0, < dev": ["pyspark[connect]"]
    run: |
      SAGEMAKER_OUT=$(mktemp)
      if mlflow sagemaker build-and-push-container --no-push --mlflow-home . > $SAGEMAKER_OUT 2>&1; then
        echo "Sagemaker container build succeeded.";
      else
        echo "Sagemaker container build failed, output:";
        cat $SAGEMAKER_OUT;
        exit 1
      fi
      pytest tests/spark --ignore tests/spark/autologging --ignore tests/spark/test_spark_connect_model_export.py

      if [[ $(python -c "from packaging.version import Version;import pyspark;print(Version(pyspark.__version__) >= Version('3.5'))") = "True" ]]; then
        pytest tests/spark/test_spark_connect_model_export.py;
      fi

  autologging:
    minimum: "3.3.0"
    maximum: "4.0.1"
    java:
      "<= 3.4.1": "11"
    # NB: Allow unreleased maximum versions for the pyspark package to support models and
    # autologging use cases in environments where newer versions of pyspark are available
    # prior to their release on PyPI (e.g. Databricks)
    allow_unreleased_max_version: True
    requirements:
      ">= 0.0.0": ["scikit-learn", "numpy<2"]
      "< 3.4.0": ["pandas<2"]
    run: |
      # Build Java package
      # Pyspark 4.0 ('dev' version) exclusively supports Scala 2.13
      if [[ $(python -c "import pyspark;from packaging.version import Version;print(Version(pyspark.__version__).major >= 4)") = "True" ]]; then
        pushd mlflow/java/spark_2.13
      else
        pushd mlflow/java/spark_2.12
      fi
      mvn package -DskipTests -q
      popd

      # `test_spark_disable_autologging.py` is flaky if it runs with other tests
      # so run it individually
      pytest tests/spark/autologging/datasource/test_spark_disable_autologging.py

      # NB: Running each .py file individually is important (instead of
      # 'pytest tests/spark/autologging') not to share SparkSession across tests
      # `test_spark_datasource_autologging_crossframework.py` fails when run with other tests
      pytest tests/spark/autologging/datasource/test_spark_datasource_autologging_crossframework.py

      find tests/spark/autologging -name 'test*.py' | grep -v crossframework | grep -v test_spark_disable_autologging | xargs -L 1 pytest

prophet:
  package_info:
    pip_release: "prophet"

  models:
    minimum: "1.1.6"
    maximum: "1.1.7"
    requirements:
      # Prophet does not handle numpy 2 yet. https://github.com/facebook/prophet/issues/2595
      ">= 0.0.0": ["numpy<2"]
      # cmdstanpy>=1.3.0 is not compatible with prophet<=1.2.0: https://github.com/facebook/prophet/issues/2697
      "<= 1.2.0": ["cmdstanpy<1.3.0"]
      # Avoid holidays 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200 on
      # older version of prophet. Compatibility updates have been released as part of the 1.1.4
      # release of prophet.
    run: |
      pytest tests/prophet/test_prophet_model_export.py

pmdarima:
  package_info:
    pip_release: "pmdarima"
    # TODO: Uncomment once https://github.com/alkaline-ml/pmdarima/issues/582 is fixed.
    # install_dev: |
    #   # pip install Cython
    #   # pip install git+https://github.com/alkaline-ml/pmdarima.git

  models:
    minimum: "2.0.4"
    maximum: "2.0.4"
    requirements:
      ">= 0.0.0": [
          "prophet",
          # Avoid holidays 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200
          "holidays!=0.25",
          "numpy<2",
          # TODO: Try the latest version of moto (https://github.com/getmoto/moto/issues/8498) and remove this pin
          "boto3<1.36",
        ]
    run: |
      pytest tests/pmdarima/test_pmdarima_model_export.py

diviner:
  package_info:
    pip_release: "diviner"
    install_dev: |
      pip install git+https://github.com/databricks/diviner.git

  models:
    minimum: "0.1.1"
    maximum: "0.1.1"
    requirements:
      # https://github.com/alan-turing-institute/sktime/issues/2898
      # Avoid holidays 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200
      ">= 0.0": [
          "cmdstanpy!=1.0.2",
          "holidays!=0.25",
          "numpy<2",
          # TODO: Try the latest version of moto (https://github.com/getmoto/moto/issues/8498) and remove this pin
          "boto3<1.36",
        ]
      # Diviner <= 0.1.1 isn't compatible with pandas>=2
      "<= 0.1.1": ["pandas<2"]
      # As of Jun 7, 2023, Diviner dev isn't compatible with pandas>=2
      "== dev": ["pandas<2"]
    run: |
      pytest tests/diviner/test_diviner_model_export.py

h2o:
  package_info:
    pip_release: "h2o"
  models:
    minimum: "3.44.0.2"
    maximum: "3.46.0.8"
    run: |
      pytest tests/h2o

shap:
  package_info:
    pip_release: "shap"
  models:
    minimum: "0.44.0"
    maximum: "0.49.1"
    requirements:
      "< 0.46.0": ["numpy<1.24.0"]
    run: |
      pytest tests/shap

paddle:
  package_info:
    pip_release: "paddlepaddle"
  models:
    minimum: "2.6.2"
    maximum: "3.2.0"
    requirements:
    run: |
      pytest tests/paddle/test_paddle_model_export.py
  autologging:
    minimum: "2.6.2"
    maximum: "3.2.0"
    requirements:
    run: |
      pytest tests/paddle/test_paddle_autolog.py

transformers:
  package_info:
    pip_release: "transformers"
    install_dev: |
      pip install git+https://github.com/huggingface/transformers
  models:
    minimum: "4.38.2"
    maximum: "4.57.1"
    test_every_n_versions: 4
    unsupported: [
        # Avoid this patch: https://github.com/huggingface/transformers/pull/29032
        ">=4.38.0,<4.39.0",
        # https://github.com/huggingface/transformers/issues/38269
        "== 4.52.1",
        "== 4.52.2",
      ]
    requirements:
      ">= 0.0.0": [
          "datasets",
          # 0.22.0 is broken: https://github.com/huggingface/huggingface_hub/issues/2157
          "huggingface_hub!=0.22.0",
          "hf_transfer",
          "torch",
          "torchvision",
          "tensorflow",
          "peft", # optimized fine-tuning library developed by Hugging Face
          "accelerate", # required for large torch models where weights will not fit in RAM
          "librosa", # required for transformers audio pipelines for bitrate conversion
          "ffmpeg", # required for transformers audio pipelines for audio byte to numpy conversion
          "sentencepiece", # required for transformers text2text generation pipeline
          "protobuf<=3.20.3", # fix error `Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto`
          "typing_extensions>=4.6.0", # fix error for SqlAlchemy == 2.0.25 cannot import name 'TypeAliasType' from 'typing_extensions'
          # TODO: Try the latest version of moto (https://github.com/getmoto/moto/issues/8498) and remove this pin
          "boto3<1.36",
        ]
      # peft >= 0.14 relies on classes from newer versions of transformers
      "< 4.45": ["peft<0.14.0"]
      ">= 4.38.2": ["tf-keras"] # Transformers doesn't support Keras 3.0. tf-keras needs to be installed.
      "< 4.52.0.dev0": ["accelerate<1"]
    pre_test: |
      sudo apt-get update -y
      sudo apt-get install -y ffmpeg
      export PYTHONPATH=./
      python tests/transformers/helper.py
    run: |
      # Run all Transformers tests except autologging
      pytest tests/transformers --ignore tests/transformers/test_transformers_autolog.py
      pip uninstall -y accelerate
      pytest tests/transformers/test_transformers_model_export.py -k "test_transformers_pt_model_save_dependencies_without_accelerate"
  autologging:
    minimum: "4.38.2"
    maximum: "4.57.1"
    test_every_n_versions: 4
    unsupported: [
        # https://github.com/huggingface/transformers/issues/38269
        "== 4.52.1",
        "== 4.52.2",
      ]
    requirements:
      ">= 0.0.0": [
          "datasets",
          # 0.22.0 is broken: https://github.com/huggingface/huggingface_hub/issues/2157
          "huggingface_hub!=0.22.0",
          "hf_transfer",
          "torch",
          "torchvision",
          "tensorflow",
          "setfit",
          "optuna",
          # TODO: Try the latest version of moto (https://github.com/getmoto/moto/issues/8498) and remove this pin
          "boto3<1.36",
        ]
      ">= 4.38.2": ["tf-keras"] # Transformers doesn't support Keras 3.0. tf-keras needs to be installed.
      # hf_hub>=0.24.0 is broken with setfit<=1.0.3. the incompatibility was fixed in setfit>=1.1, but
      # that version of setfit requires tranformers>=4.41
      # datasets > 2.4.0 is incompatible with huggingface_hub<0.24.0
      # evaluate >= 0.4.3 is incompatible with datasets<=2.4.0
      # pyarrow >= 21.0.0 is incompatible with datasets<=2.4.0
      "< 4.41": ["huggingface_hub<0.24.0", "datasets<=2.4.0", "evaluate<0.4.3", "pyarrow<21.0.0"]
      "< 4.52.0.dev0": ["accelerate<1"]
    run: |
      pytest tests/transformers/test_transformers_autolog.py

openai:
  package_info:
    genai: true
    pip_release: "openai"
    install_dev: |
      pip install git+https://github.com/openai/openai-python
  models:
    minimum: "1.52.0"
    maximum: "2.3.0"
    requirements:
      ">= 0.0.0": [
          "pyspark",
          "tiktoken",
          "aiohttp",
          "tenacity",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
      "< 1.55.3": ["httpx<0.28.0"]
    run: |
      pytest tests/openai --ignore-glob="*autolog.py"
    # Our CI runs tests for every minor version of the library by default, which results in
    # many test tasks for openai. Reducing the number of testing only for every 10 version.
    test_every_n_versions: 10
  autologging:
    minimum: "1.52.0"
    maximum: "2.3.0"
    requirements:
      ">= 0.0.0": [
          "tiktoken",
          "tenacity",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
      "< 1.55.3": ["httpx<0.28.0"]
      # minimum version supported by openai-agents>=0.2.0
      ">= 1.93.1": ["openai-agents"]
    run: |
      pytest tests/openai/*_autolog.py
    test_every_n_versions: 10
    test_tracing_sdk: true

dspy:
  package_info:
    genai: true
    pip_release: "dspy"
    install_dev: |
      pip install git+https://github.com/stanfordnlp/dspy.git
  models:
    minimum: "2.5.17"
    maximum: "3.0.3"
    requirements:
      ">= 0.0.0": ["openai"]
    run: |
      pytest tests/dspy --ignore tests/dspy/test_dspy_autolog.py
  autologging:
    minimum: "2.5.17"
    maximum: "3.0.3"
    requirements:
      ">= 0.0.0": ["openai"]
    run: |
      pytest tests/dspy/test_dspy_autolog.py
    test_tracing_sdk: true

langchain:
  package_info:
    genai: true
    pip_release: "langchain"
    install_dev: |
      pip install git+https://github.com/langchain-ai/langchain#subdirectory=libs/langchain
  models:
    # Where the large package update was made (langchain-core, community, ...)
    minimum: "0.3.4"
    maximum: "0.3.27"
    requirements:
      ">= 0.0.0": [
          "pyspark",
          "transformers",
          "torch",
          "torchvision",
          "openai",
          "google-search-results",
          "psutil",
          "faiss-cpu",
          "langchain-experimental",
          "numexpr",
          "hf_transfer",
          "langchain-core!=0.1.39", # https://github.com/langchain-ai/langchain/issues/19947
          "langchain-openai",
          "langgraph",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
          # Required for testing Databricks dependency extraction
          "databricks-vectorsearch",
        ]
      ">= 0.2": [
          "langchain-huggingface",
          # Required for testing uc tools
          "unitycatalog-langchain",
        ]
      ">=0.3": ["databricks-langchain"]
    pre_test: |
      # Installing both pyspark and databricks-connect causes a conflict
      pip uninstall -y databricks-connect
      pip install --no-deps --force-reinstall pyspark
    run: |
      # Some dependencies above includes 'mlflow-skinny' but not full 'mlflow', such as databricks-vectorsearch.
      # This causes installation of mlflow-skinny from stable version, while 'mlflow' points to the dev version,
      # resulting in a weird module conflict issue.
      pip install './libs/skinny'
      # Run all langchain tests except autologging
      pytest tests/langchain --ignore tests/langchain/test_langchain_autolog.py
  autologging:
    minimum: "0.3.4"
    maximum: "0.3.27"
    requirements:
      ">= 0.0.0": [
          "openai",
          "google-search-results",
          "faiss-cpu",
          "langchain-core!=0.1.39", # https://github.com/langchain-ai/langchain/issues/19947
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
          # Some model logging/loading requires langchain community
          "langchain-community",
        ]
      ">= 0.3.0": [
          # Need to bump FasAPI version to support pydantic v2
          "fastapi>=0.100.0",
          "langchain-openai>=0.2.0",
        ]
    pre_test: |
      # Installing both pyspark and databricks-connect causes a conflict
      pip uninstall -y databricks-connect
      pip install --no-deps --force-reinstall pyspark
    run: |
      pytest tests/langchain/test_langchain_autolog.py

      echo "Testing langchain autolog and evaluation with langchain-community"
      # Install with 'langchain' to ensure the compatible version is installed
      pip install langchain langchain-community

      pytest tests/langchain/test_langchain_autolog.py
    test_tracing_sdk: true

langgraph:
  package_info:
    genai: true
    pip_release: "langgraph"
    install_dev: |
      pip install git+https://github.com/langchain-ai/langgraph#subdirectory=libs/langgraph
      # Force-reinstall prebuilt to ensure dev version compatibility and prevent pipeline failures
      pip install --force-reinstall --no-deps git+https://github.com/langchain-ai/langgraph#subdirectory=libs/prebuilt

  models:
    minimum: "0.2.39"
    maximum: "0.6.10"
    requirements:
      ">= 0.0.0": [
          "langchain",
          "langchain_openai",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
      ">= 0.3.0": ["langgraph-prebuilt"]
      # `langgraph == 0.4.*` is incompatible with `langgraph-prebuilt >= 0.5`
      "== 0.4.*": ["langgraph-prebuilt<0.5"]
    run: |
      pytest tests/langgraph --ignore tests/langgraph/test_langgraph_autolog.py

  autologging:
    minimum: "0.2.39"
    maximum: "0.6.10"
    requirements:
      ">= 0.0.0": [
          "langchain",
          "langchain_openai",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
      ">= 0.3.0": ["langgraph-prebuilt"]
      # `langgraph == 0.4.*` is incompatible with `langgraph-prebuilt >= 0.5`
      "== 0.4.*": ["langgraph-prebuilt<0.5"]
    run: |
      pytest tests/langgraph/test_langgraph_autolog.py
    test_tracing_sdk: true

llama_index:
  package_info:
    genai: true
    pip_release: "llama-index"
    module_name: "llama_index.core"
    install_dev: |
      pip install git+https://github.com/run-llama/llama_index.git
  models:
    # New event/span framework is fully implemented in 0.10.44
    minimum: "0.11.19"
    maximum: "0.14.4"
    requirements:
      ">= 0.0.0": [
          # Versions 1.99.2 - 1.99.8 contain regressions:
          # https://github.com/openai/openai-python/issues/2525
          "openai>=1.99.9",
          # Required to test Databricks integration
          "llama-index-llms-databricks",
          "llama-index-embeddings-databricks",
          # Required to test external vector stores
          "llama-index-vector-stores-qdrant",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
    run: pytest tests/llama_index --ignore tests/llama_index/test_llama_index_autolog.py --ignore tests/llama_index/test_llama_index_tracer.py
  autologging:
    minimum: "0.11.19"
    maximum: "0.14.4"
    requirements:
      ">= 0.0.0": [
          # Versions 1.99.2 - 1.99.8 contain regressions:
          # https://github.com/openai/openai-python/issues/2525
          "openai>=1.99.9",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
    run: pytest tests/llama_index/test_llama_index_autolog.py tests/llama_index/test_llama_index_tracer.py
    test_tracing_sdk: true

ag2:
  package_info:
    genai: true
    pip_release: "ag2"
    module_name: "autogen"
  autologging:
    minimum: "0.7.0"
    maximum: "0.9.10"
    requirements:
      ">= 0.0.0": [
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
          "openai",
          "numpy<2",
        ]
      # see https://github.com/ag2ai/ag2/issues/2046, this release
      # used simple string comparison for packages and does not
      # recognize that version 1.100 is greater than 1.66
      "== 0.8.7": ["openai<1.99.9"]
    run: pytest tests/ag2

autogen:
  package_info:
    genai: true
    pip_release: "autogen-agentchat"
    module_name: "autogen_agentchat"
  autologging:
    minimum: "0.4.9"
    maximum: "0.7.5"
    requirements:
      ">= 0.0.0": [
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
          "autogen_ext",
          # TODO: remove this pin after https://github.com/pydantic/pydantic/issues/12348 is resolved
          "pydantic<2.12.0",
        ]
    run: pytest tests/autogen
    test_tracing_sdk: true

gemini:
  package_info:
    genai: true
    pip_release: "google-genai"
    module_name: "google.genai"
    install_dev: |
      pip install git+https://github.com/googleapis/python-genai
  autologging:
    minimum: "1.0.0"
    maximum: "1.44.0"
    requirements:
    run: |
      # Install legacy gemini SDK to ensure the integration works for the legacy SDK
      pip install google-generativeai
      pytest tests/gemini
    test_tracing_sdk: true
    test_every_n_versions: 10

anthropic:
  package_info:
    genai: true
    pip_release: "anthropic"
    install_dev: |
      pip install git+https://github.com/anthropics/anthropic-sdk-python
  autologging:
    minimum: "0.36.2"
    maximum: "0.69.0"
    requirements:
      "< 0.40.0": ["httpx<0.28.0"]
    run: pytest tests/anthropic
    # Our CI runs tests for every minor version of the library by default, which results in
    # many test tasks for anthropic. Reducing the number of testing only for every 5 version.
    test_every_n_versions: 10
    test_tracing_sdk: true

crewai:
  package_info:
    genai: true
    pip_release: "crewai"
    module_name: "crewai"
    install_dev: |
      pip install git+https://github.com/crewAIInc/crewAI#subdirectory=lib/crewai
  autologging:
    minimum: "0.80.0"
    maximum: "0.203.1"
    unsupported: ["==0.114.0"]
    requirements:
    run: pytest tests/crewai
    test_tracing_sdk: true
    test_every_n_versions: 20

agno:
  package_info:
    genai: true
    pip_release: "agno"
    module_name: "agno"
    install_dev: |
      pip install git+https://github.com/agno-agi/agno.git#subdirectory=libs/agno
  autologging:
    minimum: "1.7.0"
    maximum: "2.1.4"
    requirements:
      ">= 0.0.0": ["anthropic", "yfinance"]
    run: pytest tests/agno
    test_tracing_sdk: true

pydantic_ai:
  package_info:
    genai: true
    pip_release: "pydantic-ai"
    module_name: "pydantic_ai"

    install_dev: |
      # Install the stable version first to install dependencies
      pip install pydantic-ai
      # We need to install deno to run test suite for MCPServer as deno is used to start the MCPServer
      curl -fsSL https://deno.land/install.sh | sh -s -- -y --no-modify-path
      # Update key libraries from source w/ no-deps
      pip install --no-deps \
      # We need to install pydantic_graph and pydantic_ai_slim as pydantic_ai is dependent on both of them
      # and we need to install them separately. Without pinning them separately would result in dependency
      # conflict errors and the installation of pydantic_ai would get failed.
      # Ref: https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pyproject.toml#L45-L52
        "git+https://github.com/pydantic/pydantic-ai.git#egg=pydantic-graph&subdirectory=pydantic_graph" \
        "git+https://github.com/pydantic/pydantic-ai.git#egg=pydantic-ai-slim&subdirectory=pydantic_ai_slim" \
        "git+https://github.com/pydantic/pydantic-ai.git#egg=pydantic-ai"
  autologging:
    minimum: "0.1.9"
    maximum: "1.0.18"
    requirements:
      ">= 0.0.0": ["mcp"]
    run: pytest tests/pydantic_ai
    test_tracing_sdk: true

smolagents:
  package_info:
    genai: true
    pip_release: "smolagents"
    module_name: "smolagents"
    install_dev: |
      pip install "git+https://github.com/huggingface/smolagents"
  autologging:
    minimum: "1.14.0"
    maximum: "1.22.0"
    requirements:
      "< 1.21.0": ["duckduckgo-search"]
      # `duckduckgo_search` has been renamed to `ddgs`:
      # https://github.com/huggingface/smolagents/pull/1593
      ">= 1.21.0": ["ddgs"]
    run: pytest tests/smolagents
    test_tracing_sdk: true

strands:
  package_info:
    genai: true
    pip_release: "strands-agents"
    module_name: "strands"
    install_dev: |
      pip install "git+https://github.com/strands-agents/sdk-python"
  autologging:
    minimum: "1.4.0"
    maximum: "1.12.0"
    run: pytest tests/strands
    test_tracing_sdk: true

haystack:
  package_info:
    pip_release: "haystack-ai"
    module_name: "haystack"
    install_dev: |
      pip install git+https://github.com/deepset-ai/haystack
  autologging:
    minimum: "2.0.0"
    maximum: "2.18.1"
    requirements:
    run: pytest tests/haystack
    test_tracing_sdk: true
    test_every_n_versions: 10

mistral:
  package_info:
    genai: true
    pip_release: "mistralai"
    module_name: "mistralai"
    install_dev: |
      TMP_DIR=$(mktemp -d)
      git clone --depth 1 https://github.com/mistralai/client-python $TMP_DIR
      cd $TMP_DIR
      python scripts/prepare_readme.py
      pip install .
      rm -rf $TMP_DIR
  autologging:
    minimum: "1.2.0"
    maximum: "1.9.11"
    requirements:
    run: pytest tests/mistral
    test_tracing_sdk: true
    test_every_n_versions: 10

sentence_transformers:
  package_info:
    pip_release: "sentence-transformers"
    install_dev: |
      pip install git+https://github.com/UKPLab/sentence-transformers#egg=sentence-transformers
  models:
    minimum: "2.3.1"
    maximum: "5.1.1"
    requirements:
      ">= 0.0.0": [
          "pyspark",
          "torch>1.6",
          "transformers>4.25",
          "hf_transfer",
          # Required by a test model (nomic-ai/nomic-embed-text-v1.5)
          "einops",
          # TODO: Try the latest version of moto (https://github.com/getmoto/moto/issues/8498) and remove this pin
          "boto3<1.36",
          # Fix for https://github.com/huggingface/transformers/issues/37326
          "accelerate",
        ]
    run: |
      pytest tests/sentence_transformers/test_sentence_transformers_model_export.py

johnsnowlabs:
  package_info:
    pip_release: "johnsnowlabs"
  models:
    minimum: "5.1.7"
    maximum: "6.1.1"
    requirements:
      ">= 0.0.0": ["pandas<=1.5.3"]
      "< 5.2.8": ["pyspark<3.4"]
    run: |
      if [ ! -z "$JOHNSNOWLABS_LICENSE_JSON" ]; then
        pytest tests/johnsnowlabs/test_johnsnowlabs_model_export.py
      else
        echo "Skipping tests due to missing license key"
      fi

promptflow:
  package_info:
    pip_release: "promptflow"
    # Install dev subpackages to ensure test running on the latest code
    install_dev: |
      pip install git+https://github.com/microsoft/promptflow#subdirectory=src/promptflow
  models:
    minimum: "1.3.0"
    maximum: "1.18.1"
    requirements:
      # Requirements to run sparkudf predict test
      # marshmallow 3.24.0 has breaking change causes `from marshmallow.fields import _T` error
      # inside promptflow
      ">= 0.0.0": ["pyspark", "jinja2", "marshmallow<3.24.0"]
      ">= 1.11.0": ["openai>=1"]
      "< 1.18": ["numpy<2"]
    run: |
      pytest tests/promptflow/test_promptflow_model_export.py
    test_every_n_versions: 10

litellm:
  package_info:
    genai: true
    pip_release: "litellm"
    install_dev: |
      pip install git+https://github.com/BerriAI/litellm.git
  autologging:
    minimum: "1.63.14"
    maximum: "1.74.9"
    requirements:
      ">= 0.0.0": [
          "openai",
          # Required to run tests/openai/mock_openai.py
          "fastapi",
          "uvicorn",
        ]
    run: pytest tests/litellm
    # NB: LiteLLM tracing callback still depends on MLflowClient, so we need to migrate it before
    # enabling tracing SDK tests. There are other issues like the one linked above, too.
    test_tracing_sdk: false

groq:
  package_info:
    genai: true
    pip_release: "groq"
    install_dev: |
      pip install git+https://github.com/groq/groq-python
  autologging:
    minimum: "0.13.0"
    maximum: "0.32.0"
    requirements:
    run: pytest tests/groq
    test_tracing_sdk: true
    test_every_n_versions: 10

bedrock:
  package_info:
    genai: true
    pip_release: "boto3"
    module_name: "boto3"
  autologging:
    # BedrockRuntime client is added in boto3 1.33
    minimum: "1.35.42"
    maximum: "1.40.52"
    run: pytest tests/bedrock
    test_tracing_sdk: true
