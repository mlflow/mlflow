"""
Entities for Gateway Usage Tracking.

These entities represent gateway invocations and provider LLM calls for usage tracking,
cost analysis, and metrics visualization.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum

from mlflow.entities._mlflow_object import _MlflowObject


class InvocationStatus(str, Enum):
    """Status of a gateway invocation."""

    SUCCESS = "SUCCESS"
    ERROR = "ERROR"
    PARTIAL = "PARTIAL"  # Some providers succeeded (in fallback scenarios)


class ProviderCallStatus(str, Enum):
    """Status of an individual provider call."""

    SUCCESS = "SUCCESS"
    ERROR = "ERROR"


@dataclass
class ProviderCallInput:
    """
    Input data for creating a provider call record.

    This is used when logging gateway invocations. Unlike GatewayProviderCall,
    this class doesn't require provider_call_id or invocation_id as these
    are auto-generated by the store.

    Args:
        provider: LLM provider name (e.g., "openai", "anthropic").
        model_name: Provider-specific model identifier (e.g., "gpt-4o", "claude-3-5-sonnet").
        attempt_number: The attempt number within the invocation (1-indexed).
        status: Status of this provider call (SUCCESS, ERROR).
        latency_ms: Time taken for this provider call in milliseconds.
        error_message: Error message if the call failed.
        prompt_tokens: Number of tokens in the prompt.
        completion_tokens: Number of tokens in the completion.
        total_tokens: Total number of tokens used.
        prompt_cost: Cost for prompt tokens in USD.
        completion_cost: Cost for completion tokens in USD.
        total_cost: Total cost in USD.
    """

    provider: str
    model_name: str
    attempt_number: int
    status: ProviderCallStatus
    latency_ms: int = 0
    error_message: str | None = None
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    prompt_cost: float = 0.0
    completion_cost: float = 0.0
    total_cost: float = 0.0


@dataclass
class GatewayProviderCall(_MlflowObject):
    """
    Represents an individual LLM provider call within a gateway invocation.

    In fallback scenarios, a single gateway invocation may result in multiple
    provider calls as the system tries different providers until one succeeds.

    Args:
        provider_call_id: Unique identifier for this provider call.
        invocation_id: ID of the parent gateway invocation.
        provider: LLM provider name (e.g., "openai", "anthropic").
        model_name: Provider-specific model identifier (e.g., "gpt-4o", "claude-3-5-sonnet").
        attempt_number: The attempt number within the invocation (1-indexed).
        status: Status of this provider call (SUCCESS, ERROR).
        error_message: Error message if the call failed.
        prompt_tokens: Number of tokens in the prompt.
        completion_tokens: Number of tokens in the completion.
        total_tokens: Total number of tokens used.
        prompt_cost: Cost for prompt tokens in USD.
        completion_cost: Cost for completion tokens in USD.
        total_cost: Total cost in USD.
        latency_ms: Time taken for this provider call in milliseconds.
        created_at: Timestamp (milliseconds) when the call was made.
    """

    provider_call_id: str
    invocation_id: str
    provider: str
    model_name: str
    attempt_number: int
    status: ProviderCallStatus
    error_message: str | None = None
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    prompt_cost: float = 0.0
    completion_cost: float = 0.0
    total_cost: float = 0.0
    latency_ms: int = 0
    created_at: int = 0


@dataclass
class GatewayInvocation(_MlflowObject):
    """
    Represents a gateway API invocation.

    A gateway invocation is a single request to a gateway endpoint.
    It may result in multiple provider calls in fallback scenarios.

    Args:
        invocation_id: Unique identifier for this invocation.
        endpoint_id: ID of the gateway endpoint that was called.
        endpoint_type: Type of the endpoint (e.g., "llm/v1/chat", "llm/v1/completions").
        status: Overall status of the invocation (SUCCESS, ERROR, PARTIAL).
        total_prompt_tokens: Total prompt tokens across all provider calls.
        total_completion_tokens: Total completion tokens across all provider calls.
        total_tokens: Total tokens across all provider calls.
        total_cost: Total cost across all provider calls in USD.
        total_latency_ms: Total time taken for the invocation in milliseconds.
        provider_calls: List of provider calls made during this invocation.
        created_at: Timestamp (milliseconds) when the invocation started.
        username: Username/identity of the caller (if available).
        error_message: Error message if the invocation failed.
    """

    invocation_id: str
    endpoint_id: str
    endpoint_type: str
    status: InvocationStatus
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_tokens: int = 0
    total_cost: float = 0.0
    total_latency_ms: int = 0
    provider_calls: list[GatewayProviderCall] = field(default_factory=list)
    created_at: int = 0
    username: str | None = None
    error_message: str | None = None


@dataclass
class GatewayUsageMetrics(_MlflowObject):
    """
    Aggregated metrics for gateway usage over a time period.

    Args:
        endpoint_id: ID of the gateway endpoint.
        time_bucket: Start of the time bucket (timestamp in milliseconds).
        bucket_size: Size of the time bucket in seconds (e.g., 3600 for hourly, 86400 for daily).
        total_invocations: Total number of invocations in this bucket.
        successful_invocations: Number of successful invocations.
        failed_invocations: Number of failed invocations.
        total_prompt_tokens: Total prompt tokens in this bucket.
        total_completion_tokens: Total completion tokens in this bucket.
        total_tokens: Total tokens in this bucket.
        total_cost: Total cost in this bucket in USD.
        avg_latency_ms: Average latency in milliseconds.
        p50_latency_ms: 50th percentile latency in milliseconds.
        p95_latency_ms: 95th percentile latency in milliseconds.
        p99_latency_ms: 99th percentile latency in milliseconds.
    """

    endpoint_id: str
    time_bucket: int
    bucket_size: int  # in seconds
    total_invocations: int = 0
    successful_invocations: int = 0
    failed_invocations: int = 0
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_tokens: int = 0
    total_cost: float = 0.0
    avg_latency_ms: float = 0.0
    p50_latency_ms: float = 0.0
    p95_latency_ms: float = 0.0
    p99_latency_ms: float = 0.0
