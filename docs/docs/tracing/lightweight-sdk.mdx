---
description: MLflow Tracing SDK (mlflow-tracing) is a lightweight package that includes only the essential functionality for tracing and monitoring your GenAI applications.
sidebar_position: 5
sidebar_label: Lightweight SDK
---

import { Card, CardGroup, TitleCard } from "@site/src/components/Card";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Lightweight Tracing SDK Optimized for Production Usage

MLflow offers a lightweight tracing SDK package called `mlflow-tracing` that includes only the essential functionality for tracing and monitoring of your GenAI applications. This package is designed for production environments where minimizing dependencies and deployment size is ideal.

## Why Use the Lightweight SDK?


<CardGroup cols={2}>
    <TitleCard
        title="🏎️ Faster Deployment"
        description="The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications."
    />
    <TitleCard
        title="🔧 Simplified Dependency Management"
        description="A smaller set of dependencies means less work keeping up with dependency updates, security patches, and potential breaking changes from upstream libraries. It also reduces the chances of dependency collisions and incompatibilities."
    />
    <TitleCard
        title="📦 Portability"
        description="With a fewer number of dependencies, MLflow Tracing can be seamlessly deployed across differing environments and platforms, without worrying about compatibility issues."
    />
    <TitleCard
        title="🔒 Fewer Security Risks"
        description="Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches."
    />
</CardGroup>

## Installation

Install the lightweight SDK using pip:

```bash
pip install mlflow-tracing
```

:::warning

Do not install the full `mlflow` package together with the lightweight `mlflow-tracing` SDK, as this may cause version conflicts and namespace resolution issues.

:::

## Quickstart

Here's a simple example using the lightweight SDK with OpenAI for logging traces to an experiment
on a remote MLflow server. If you haven't set up a remote MLflow server yet, please refer to the [Choose Your Backend](#choose-your-backend) section below.


```python
import mlflow
import openai

# Set the tracking URI (e.g., Databricks, SageMaker, or self-hosted server)
mlflow.set_tracking_uri("<your-tracking-uri>")
mlflow.set_experiment("<your-experiment-name>")

# Enable auto-tracing for OpenAI
mlflow.openai.autolog()

# Use OpenAI as usual
client = openai.OpenAI()
response = client.chat.completions.create(
    model="gpt-4.1-mini", messages=[{"role": "user", "content": "What is MLflow?"}]
)
```


## Choose Your Backend

The lightweight SDK works with various observability platforms. Choose your favorite one and follow the instructions to set up your tracing backend.

<Tabs>
  <TabItem value="databricks-monitoring" label="Databricks">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

          [Databricks Lakehouse Monitoring for GenAI](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring) is a go-to solution for monitoring your
          GenAI application with MLflow Tracing. It provides access to a robust, fully functional monitoring dashboard for operational excellence and quality analysis.

          Lakehouse Monitoring for GenAI can be used regardless of whether your application is hosted on Databricks or not.

          [Sign up for free](https://docs.databricks.com/aws/en/getting-started/free-trial) and [get started in a minute](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring) to run your GenAI application with complete observability.
        </div>

        <div class="flex-item padding-md">
          ![Monitoring](https://assets.docs.databricks.com/_static/images/generative-ai/monitoring/monitoring-hero.gif)
        </div>
      </div>
    </div>

  </TabItem>
  <TabItem value="self-hosting" label="Self-hosting">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

          MLflow is a **fully open-source project**, allowing you to self-host your own MLflow server in your own infrastructure. This is a great option if you want to have full control over your data and are restricted in using cloud-based services.

          In self-hosting mode, you will be responsible for running the tracking server instance
          and scaling it to your needs. We **strongly recommend** using a SQL-based tracking server
          on top of the performant database of your choice to minimize the operational overhead
          and high availability. Please refer to the [tracking server setup guide](/tracking#tracking-setup) for more guidance.

        </div>
        <div class="flex-item padding-md">
          ![OSS Tracing](/images/llms/tracing/tracing-top.gif)
        </div>
      </div>
    </div>
  </TabItem>
  <TabItem value="other" label="OpenTelemetry">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

          MLflow Tracing is **built on the OpenTelemetry tracing spec**, an industry-standard framework for observability, making it vendor-neutral solution for monitoring your GenAI applications.

          If you are using OpenTelemetry as a part of observability tech stack in your
          team or organization, you can use MLflow Tracing without any additional
          service onboarding. Please refer to the [OpenTelemetry Integration](/tracing/production#opentelemetry-integration) to connect MLflow Tracing to your
          OpenTelemetry backend.
        </div>
        <div class="flex-item padding-md">
          ![OpenTelemetry Backend Examples](/images/llms/tracing/otel-backend-examples.png)
        </div>
      </div>
    </div>
  </TabItem>
  <TabItem value="sagemaker" label="Amazon SageMaker">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

          [MLflow on Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.

          MLflow Tracing offers a one-line solution for [tracing Amazon Bedrock](http://localhost:3000/tracing/integrations/bedrock),
          making it the best suitable solution for monitoring GenAI application powered by
          AWS and Amazon Bedrock.

        </div>

        <div class="flex-item padding-md">
          ![Managed MLflow on SageMaker](https://d1.awsstatic.com/deploy-mlflow-models.3eb857c5790a44b461845a630e3a231229838443.png)
        </div>
      </div>
    </div>
  </TabItem>
  <TabItem value="nebius" label="Nebius">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

          [Nebuis](https://nebius.com/services/managed-mlflow), a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server. Combining the powerful GPU infrastructure of Nebuis for training and hosting LLMs / foundation models with the observability capabilities of MLflow Tracing, Nebius serves as a comprehensive platform for AI/ML developers.

          Refer to the [Nebius documentation](https://nebius.com/services/managed-mlflow) for more details about the managed MLflow service.

        </div>
        <div class="flex-item padding-md">
          <img src="/images/logos/nebius-logo.png" alt="Nebuis Logo" width="80%" />
        </div>
      </div>
    </div>
  </TabItem>
</Tabs>


## Supported Features

The lightweight SDK includes all essential tracing functionalities for monitoring your GenAI applications. Click the cards below to learn more about each supported feature.

<CardGroup cols={1}>
    <TitleCard
        title="⚡️ Automatic Tracing for 15+ AI libraries"
        description="MLflow Tracing SDK supports one-line integration with all of the most popular LLM/GenAI libraries including OpenAI, Anthropic, LangChain, LlamaIndex, Amazon Bedrock, DSPy, and any LLM provides that conforms to OpenAI API format. This
        automatic tracing capability allows you to monitor your GenAI application with MLflow Tracing with minimal effort and switch between different libraries easily."
        link="/tracing/integrations/"
    />
    <TitleCard
        title="⚙️ Manual Instrumentation"
        description="MLflow Tracing SDK provides a simple and intuitive API for manually
        instrumenting your GenAI application. Manual instrumentation and automatic tracing can
        be used together, allowing you to trace advanced applications containing custom code and have
        fine-grained control over the tracing behavior."
        link="/tracing/api/manual-instrumentation"
    />
    <TitleCard
        title="✍ Tagging and Filtering Traces"
        description="By annotating traces with custom tags, you can add more context to your traces to group them and simplify the process of searching for them later. This is useful when you want to trace
        an application that runs across multiple request sessions."
        link="/tracing/session"
    />
</CardGroup>


## Features Not Included

The following MLflow features are not available in the lightweight package:

- MLflow tracking server and UI
- Run management APIs (e.g. `mlflow.start_run`)
- Model logging and evaluation
- Model / prompt registry
- MLflow AI Gateway
- Other MLflow features unrelated to tracing

For these features, use the full MLflow package by installing `pip install mlflow`.