---
sidebar_position: 8
sidebar_label: LiteLLM
---

import { APILink } from "@site/src/components/APILink";
import { Card, CardGroup, SmallLogoCard } from "@site/src/components/Card";
import TOCInline from "@theme/TOCInline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Tracing LiteLLMðŸš„

![LiteLLM Tracing via autolog](/images/llms/litellm/litellm-tracing.png)

[LiteLLM](https://www.litellm.ai/) is an open-source LLM Gateway that allow accessing 100+ LLMs in the unified interface.

[MLflow Tracing](../) provides automatic tracing capability for LiteLLM. By enabling auto tracing
for LiteLLM by calling the <APILink fn="mlflow.litellm.autolog" /> function,  MLflow will capture traces for LLM invocation and logged them to the active MLflow Experiment.

```python
import mlflow

mlflow.litellm.autolog()
```

MLflow trace automatically captures the following information about LiteLLM calls:

* Prompts and completion responses
* Latencies
* Metadata about the LLM provider, such as model name and endpoint URL
* Token usages and cost
* Cache hit
* Any exception if raised

### Basic Example


```python
import mlflow
import litellm

# Enable auto-tracing for LiteLLM
mlflow.litellm.autolog()

# Optional: Set a tracking URI and an experiment
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("LiteLLM")

# Call Anthropic API via LiteLLM
response = litellm.completion(
    model="claude-3-5-sonnet-20240620",
    messages=[{"role": "user", "content": "Hey! how's it going?"}],
)
```

### Async API

MLflow supports tracing LiteLLM's async APIs:

```python
mlflow.litellm.autolog()

response = await litellm.acompletion(
    model="claude-3-5-sonnet-20240620",
    messages=[{"role": "user", "content": "Hey! how's it going?"}],
)
```

### Streaming

MLflow supports tracing LiteLLM's sync and async streaming APIs:

```python
mlflow.litellm.autolog()

response = litellm.completion(
    model="claude-3-5-sonnet-20240620",
    messages=[{"role": "user", "content": "Hey! how's it going?"}],
    stream=True,
)
for chunk in response:
    print(chunk.choices[0].delta.content, end="|")
```

MLflow will record concatenated outputs from the stream chunks as a span output.


### Disable auto-tracing

Auto tracing for LiteLLM can be disabled globally by calling `mlflow.litellm.autolog(disable=True)` or `mlflow.autolog(disable=True)`.