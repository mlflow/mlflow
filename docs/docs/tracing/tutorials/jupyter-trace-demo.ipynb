{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bac0fa2",
   "metadata": {},
   "source": [
    "# MLflow Trace UI in Jupyter Notebook Demo\n",
    "\n",
    "This notebook is a quick showcase of the MLflow Trace UI within Jupyter Notebooks.\n",
    "\n",
    "We begin with some toy examples to explain the display functionality, and end\n",
    "by building a simple RAG demo to showcase more of the UI features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f6c4a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Please make sure you have the following packages installed for this demo.\n",
    "\n",
    "- mlflow >= 2.20\n",
    "- openai\n",
    "\n",
    "Optionally, for the RAG demo at the end, you'll need:\n",
    "\n",
    "- langchain\n",
    "- langchain-community\n",
    "- beautifulsoup4\n",
    "\n",
    "You can run the cell below to install all these packages (make sure to restart the kernel afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow>=2.20 openai langchain langchain-community beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1dc398",
   "metadata": {},
   "source": [
    "## When is the MLflow Trace UI displayed?\n",
    "\n",
    "The UI is only displayed when the MLflow Tracking URI is set to an HTTP tracking server, as this is where the UI assets are served from. If you don't use a remote tracking server, you can always start one locally by running the `mlflow server` CLI command. By default, the tracking server will be running at `http://localhost:5000`.\n",
    "\n",
    "For this tutorial, please make sure your tracking URI is set correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# replace with your own URI\n",
    "tracking_uri = \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "# set a new experiment to avoid\n",
    "# cluttering the default experiment\n",
    "experiment = mlflow.set_experiment(\"mlflow-trace-ui-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f21a2",
   "metadata": {},
   "source": [
    "Once that's set up, the trace UI should automatically show up for the following events. Examples of each will be provided below:\n",
    "\n",
    "1. When the cell code generates a trace\n",
    "2. When a `mlflow.entities.Trace` object is displayed (e.g. via IPython's `display` function, or when it is the last value returned in a cell)\n",
    "3. When `mlflow.search_traces()` is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0726fc8",
   "metadata": {},
   "source": [
    "### Example 1: Generating a trace within a cell\n",
    "\n",
    "Traces can be generated by automatic tracing integrations (e.g. with `mlflow.openai.autolog()`), or when you run a manually traced function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c37b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple manual tracing example\n",
    "import mlflow\n",
    "\n",
    "\n",
    "@mlflow.trace\n",
    "def foo(input):\n",
    "    return input + 1\n",
    "\n",
    "\n",
    "# running foo() generates a trace\n",
    "foo(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic tracing with OpenAI\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b397f-b816-4bb0-bb9c-103124f65385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# creating a chat completion will generate a trace\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello!\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ddd6c",
   "metadata": {},
   "source": [
    "### Example 2: Displaying a Trace object\n",
    "\n",
    "The trace UI will also show up when an MLflow Trace entity is displayed. This can happen in two ways:\n",
    "\n",
    "1. Explicitly displaying a trace object with IPython's `display()`\n",
    "2. When a trace object happens to be the last evaluated expression in a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24faa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly calling `display()`\n",
    "trace = mlflow.get_last_active_trace()\n",
    "display(trace)\n",
    "\n",
    "# Even if the last expression does not result in a trace,\n",
    "# display(trace) will still trigger the UI display\n",
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying as a result of the trace being the last expression\n",
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b3f4b",
   "metadata": {},
   "source": [
    "### Example 3: Calling `mlflow.search_traces()`\n",
    "\n",
    "MLflow provides the `mlflow.search_traces()` API to conveniently search through all traces in an experiment. When this API is called in a Jupyter notebook, the trace UI will render all the traces in a paginated view. There is a limit to how many traces can be rendered in a single cell output. By default the maximum is 10, but this can be configured by setting the `MLFLOW_MAX_TRACES_TO_DISPLAY_IN_NOTEBOOK` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab87aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_traces(experiment_ids=[experiment.experiment_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79d9d9",
   "metadata": {},
   "source": [
    "## Disabling the UI\n",
    "\n",
    "The display is enabled by default, but if you'd prefer for it not to be shown, you can run `mlflow.tracing.disable_notebook_display()` disable it. You will have to rerun the cells (or simply clear the cell outputs) in order to remove the displays that have already rendered.\n",
    "\n",
    "If you'd like to re-enable the auto-display functionality, simply call `mlflow.tracing.enable_notebook_display()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.tracing.disable_notebook_display()\n",
    "\n",
    "# no UI will be rendered\n",
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.tracing.enable_notebook_display()\n",
    "\n",
    "# re-enable display\n",
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b34b5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's the basics! We hope you'll find the Jupyter integration useful. As always, please file an issue at https://github.com/mlflow/mlflow/issues if you find any problems, or if you want to leave any feedback.\n",
    "\n",
    "In the next few cells, we have a short RAG demo that will create a trace with more realistic data, so you can get a better feel of what working with this UI will be like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0af01-3327-4559-9e13-46efe6dcad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# define necessary RAG entities\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b5236-360d-46c5-96fb-0bae1daba29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# generate sample doc chunks from the MLflow documentation\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://mlflow.org/docs/latest/llms/tracing/index.html\",),\n",
    "    bs_kwargs={\"parse_only\": bs4.SoupStrainer(class_=(\"document\"))},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# add documents to the vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1880d-846c-4dbd-9bde-6da0b0efd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our chain\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ffe16-aee0-42f7-8723-df5850fdf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# call the langchain autolog function so that traces will be generated\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "response = chain.invoke(\"What is MLflow Tracing?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
