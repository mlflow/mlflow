---
title: Querying Endpoints
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Querying Endpoints

Once you've created an endpoint, you can call it through several different API styles depending on your needs.

## Viewing Usage Examples

To see code examples for your endpoint, navigate to the Endpoints list and click either the Use button or the endpoint name itself. This opens a modal with comprehensive usage examples tailored to your specific endpoint.

![Usage Modal](/images/genai/governance/ai-gateway/usage-modal.png)

The usage modal organizes examples into two categories: unified APIs that work across any provider, and passthrough APIs that expose provider-specific features.

## Unified APIs

Unified APIs provide a consistent interface regardless of the underlying model provider. These APIs make it easy to switch between different models or providers without changing your application code.

### MLflow Invocations API

The MLflow Invocations API is the native interface for calling gateway endpoints. This API seamlessly handles model switching and advanced routing features like traffic splitting and fallbacks:

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/my-endpoint/mlflow/invocations \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
import requests

response = requests.post(
    "http://localhost:5000/gateway/my-endpoint/mlflow/invocations",
    json={"messages": [{"role": "user", "content": "Hello!"}], "temperature": 0.7},
)
print(response.json())
```

</TabItem>
</Tabs>

#### API Specification

The MLflow Invocations API supports both OpenAI-style chat completions and embeddings endpoints.

**Endpoint URL Pattern:**

```
POST /gateway/{endpoint_name}/mlflow/invocations
```

**Chat Completions Request Body:**

The request body follows the OpenAI chat completions format with these supported parameters. See [OpenAI Chat Completions API Reference](https://platform.openai.com/docs/api-reference/chat) for complete documentation.

| Parameter           | Type             | Required | Description                                                                                                           |
| ------------------- | ---------------- | -------- | --------------------------------------------------------------------------------------------------------------------- |
| `messages`          | array            | Yes      | Array of message objects with `role` and `content` fields                                                             |
| `temperature`       | number           | No       | Sampling temperature between 0 and 2. Higher values make output more random. Default varies by model.                 |
| `max_tokens`        | integer          | No       | Maximum number of tokens to generate. Default varies by model.                                                        |
| `top_p`             | number           | No       | Nucleus sampling parameter. Alternative to temperature. Default is 1.                                                 |
| `n`                 | integer          | No       | Number of completions to generate. Default is 1.                                                                      |
| `stream`            | boolean          | No       | Whether to stream responses. Default is false.                                                                        |
| `stop`              | string or array  | No       | Up to 4 sequences where the API will stop generating tokens.                                                          |
| `presence_penalty`  | number           | No       | Penalizes new tokens based on presence in text so far. Range: -2.0 to 2.0.                                            |
| `frequency_penalty` | number           | No       | Penalizes new tokens based on frequency in text so far. Range: -2.0 to 2.0.                                           |
| `tools`             | array            | No       | List of tools the model can call. Each tool includes `type`, `function` with `name`, `description`, and `parameters`. |
| `tool_choice`       | string or object | No       | Controls which tool is called. Can be "none", "auto", or specify a particular tool.                                   |

**Response Format:**

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-5",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Hello! How can I assist you today?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

**Streaming Responses:**

When `stream: true` is set, the response is sent as Server-Sent Events (SSE):

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-5","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-5","choices":[{"index":0,"delta":{"content":"!"},"finish_reason":null}]}

data: [DONE]
```

**Embeddings Request Body:**

For embeddings endpoints, the request body follows the OpenAI embeddings format:

| Parameter         | Type            | Required | Description                                                          |
| ----------------- | --------------- | -------- | -------------------------------------------------------------------- |
| `input`           | string or array | Yes      | Input text(s) to embed. Can be a single string or array of strings.  |
| `encoding_format` | string          | No       | Format to return embeddings. Options: "float" (default) or "base64". |

**Embeddings Response Format:**

```json
{
  "object": "list",
  "data": [{
    "object": "embedding",
    "embedding": [0.0023064255, -0.009327292, ...],
    "index": 0
  }],
  "model": "text-embedding-ada-002",
  "usage": {
    "prompt_tokens": 8,
    "total_tokens": 8
  }
}
```

### OpenAI-Compatible Chat Completions API

For teams already using the OpenAI chat completion style APIs, the gateway provides an OpenAI-compatible interface. Simply point your OpenAI client to the gateway's base URL and use your endpoint name as the model parameter. This lets you leverage existing OpenAI-based code while gaining the gateway's routing capabilities:

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/mlflow/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-endpoint",
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:5000/gateway/mlflow/v1",
    api_key="",  # API key not needed, configured server-side
)

response = client.chat.completions.create(
    model="my-endpoint",
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7,
)
print(response.choices[0].message.content)
```

</TabItem>
</Tabs>

## Passthrough APIs

The Passthrough API relays requests to the provider's LLM endpoint using its native formats, allowing you to use their native client SDKs with the MLflow Gateway. While unified APIs work for most use cases, passthrough APIs give you full access to provider-specific features that may not be available through the unified interface. Gateway provides a passthrough endpoint for each supported provider.

### OpenAI Passthrough

The OpenAI passthrough API exposes the full OpenAI API including Chat Completions, Embeddings, and Responses endpoints. See [OpenAI API Reference](https://platform.openai.com/docs/api-reference) for complete documentation.

#### Chat Completions

See [OpenAI Chat Completions API Reference](https://platform.openai.com/docs/api-reference/chat) for complete documentation.

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-endpoint",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:5000/gateway/openai/v1",
    api_key="dummy",  # API key not needed, configured server-side
)

response = client.chat.completions.create(
    model="my-endpoint", messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

</TabItem>
</Tabs>

#### Embeddings

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/openai/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-endpoint",
    "input": "The food was delicious and the waiter..."
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:5000/gateway/openai/v1",
    api_key="dummy",  # API key not needed, configured server-side
)

response = client.embeddings.create(
    model="my-endpoint",
    input="The food was delicious and the waiter..."
)
print(response.data[0].embedding)
```

</TabItem>
</Tabs>

#### Responses API

The Responses API provides a streamlined interface for creating multi-turn conversational experiences with vision and audio capabilities. For detailed Responses API usage, see [OpenAI Responses API Reference](https://platform.openai.com/docs/api-reference/responses).

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/openai/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-endpoint",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:5000/gateway/openai/v1",
    api_key="dummy",  # API key not needed, configured server-side
)

response = client.responses.create(
    model="my-endpoint",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.output)
```

</TabItem>
</Tabs>

### Anthropic Passthrough

Access Anthropic's Messages API directly through the gateway, including Claude-specific features. See [Anthropic API Reference](https://docs.anthropic.com/en/api) for complete documentation.

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/anthropic/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-endpoint",
    "max_tokens": 1024,
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
import anthropic

client = anthropic.Anthropic(
    base_url="http://localhost:5000/gateway/anthropic",
    api_key="dummy",  # API key not needed, configured server-side
)

response = client.messages.create(
    model="my-endpoint",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}],
)
print(response.content[0].text)
```

</TabItem>
</Tabs>

### Google Gemini Passthrough

The Gemini passthrough API follows Google's API structure, with the endpoint name embedded in the URL path. See [Google Gemini API Reference](https://ai.google.dev/gemini-api/docs) for complete documentation.

<Tabs>
<TabItem value="curl" label="cURL" default>

```bash
curl -X POST http://localhost:5000/gateway/gemini/v1beta/models/my-endpoint:generateContent \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [{"text": "Hello!"}]
    }]
  }'
```

</TabItem>
<TabItem value="python" label="Python">

```python
from google import genai

# Configure with custom endpoint
client = genai.Client(
    api_key="dummy",
    http_options={
        "base_url": "http://localhost:5000/gateway/gemini",
    },
)

response = client.models.generate_content(
    model="my-endpoint",
    contents={"text": "Hello!"},
)
client.close()
print(response.candidates[0].content.parts[0].text)
```

</TabItem>
</Tabs>
