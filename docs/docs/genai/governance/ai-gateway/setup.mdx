---
title: Gateway Setup
---

# Gateway Setup

The MLflow AI Gateway provides a web-based interface for managing AI endpoints, API keys, and routing configurations. This guide walks you through setting up and configuring your gateway.

## Prerequisites

To use the AI Gateway, you'll need MLflow installed with the GenAI extras, which include the necessary dependencies for gateway functionality:

```bash
pip install "mlflow[genai]"
```

The gateway requires a SQL-based tracking store such as SQLite, PostgreSQL, MySQL, or MSSQL. Start your MLflow server with a SQL backend:

```bash
mlflow server --backend-store-uri sqlite:///mlflow.db --port 5000
```

For production deployments, you'll want to configure encryption for secrets. Generate a secure passphrase and set it as an environment variable before starting the server:

```bash
# Generate a secure passphrase
python -c "import secrets; print(secrets.token_urlsafe(32))"

# Set the passphrase as an environment variable
export MLFLOW_CRYPTO_KEK_PASSPHRASE="your-secure-passphrase-here"
```

:::note
For local development, MLflow uses a default passphrase if none is configured. This is acceptable for development or single-user deployments, but production environments should always use a custom passphrase.
:::

## Accessing the Gateway

Once your MLflow server is running, navigate to the AI Gateway section at `http://localhost:5000/#/gateway`.

![Gateway Overview](/images/genai/governance/ai-gateway/gateway-overview.png)

The gateway interface is organized into two main areas. The Endpoints section lets you configure and manage your AI model endpoints, while the API Keys section provides centralized management of credentials for different providers.

## Managing API Keys

API keys serve as reusable credentials that can be shared across multiple endpoints. When you have several endpoints using the same provider, this approach simplifies both initial setup and ongoing credential management.

![API Keys Page](/images/genai/governance/ai-gateway/api-keys-page.png)

### Creating an API Key

Navigate to the API Keys tab and click Create API Key. You'll need to provide a unique name for the key (such as `my-openai-key`) and select your provider from the dropdown. Common providers like OpenAI, Anthropic, and Google Gemini are readily available.

Some providers offer multiple authentication methods. For example, OpenAI supports both standard API key authentication and Azure-specific authentication. Select the appropriate method for your setup, then enter your credentials. The interface displays credential fields as masked inputs to protect sensitive information.

Provider-specific configuration fields appear based on your selection:

- **Azure**: Endpoint URL
- **GCP**: Project ID

Fill in these additional fields as needed, then click Create to save your API key.

### Working with Existing Keys

The API Keys page shows all your configured credentials along with important metadata. You can see which endpoints are currently using each key, when it was last updated, and when it was originally created. The credential values themselves remain masked for security.

To update credentials for a provider, use the Edit function to modify the key without needing to reconfigure all endpoints that use it. When deleting a key, the system warns you if any endpoints currently depend on it, preventing accidental disruptions.

:::tip
Creating reusable API keys simplifies credential rotation. When you need to update a credential, edit it once rather than updating every endpoint individually.
:::

## Creating an Endpoint

Endpoints define how requests are routed to AI models. Beyond basic routing, endpoints support advanced features like traffic splitting for A/B testing and automatic fallbacks for high availability.

### Basic Endpoint Setup

To create an endpoint:

1. Navigate to the Endpoints tab and click **Create Endpoint**
2. Enter a unique endpoint name (e.g., `my-chat-endpoint`) - this becomes part of your API path
3. Select your provider from 100+ supported options
   - Common providers (OpenAI, Anthropic, Google Gemini) appear first
   - Click "View all providers" for the full LiteLLM catalog
4. Choose your model - the selector displays:
   - Capability badges (Tools, Reasoning, Caching)
   - Context window size and token costs
   - Search function for quick filtering
5. Configure API key:
   - **Create new API key**: Configure credentials inline (convenient for first-time setup)
   - **Use existing API key**: Select from previously created keys (recommended for consistency)
6. Review your configuration in the summary panel
7. Click **Create Endpoint**

![Create Endpoint](/images/genai/governance/ai-gateway/create-endpoint.png)

### Traffic Splitting (Load Balancing)

Traffic splitting lets you distribute requests across multiple models based on percentage weights. This capability supports several important use cases:

- **A/B testing**: Compare performance between different models
- **Gradual migration**: Shift traffic incrementally without disrupting service
- **Load distribution**: Balance requests across multiple providers for reliability

![Traffic Split Configuration](/images/genai/governance/ai-gateway/traffic-split-configuration.png)

To configure traffic splitting, open your endpoint's details page and locate the Priority 1 (Traffic Split) section. Click Add Model to include additional models in your traffic split configuration. For each model, select the provider and specific model you want to use, then configure the API key by either creating a new one or selecting from existing keys.

The critical part of traffic splitting is setting the weight percentage for each model. Weights range from 1% to 100%, and the system requires that all weights sum to exactly 100%. The interface displays the current total in green when valid, turning red if the weights don't sum correctly. This validation ensures your configuration will work as expected before you save it.

For example, you might configure 50% of traffic to route to OpenAI GPT-5 and 50% to Anthropic Claude 4.5 Sonnet. This split lets you gradually evaluate Claude's performance on a subset of real traffic before committing to a larger migration.

### Fallback Configuration

Fallback models provide automatic recovery when primary models encounter errors or rate limits. The gateway tries fallback models sequentially until one succeeds, ensuring your application remains operational even when individual providers experience issues.

Fallback configuration supports several scenarios:

- **High availability**: Failover to alternative providers during outages
- **Cost optimization**: Use cheaper models as fallbacks when expensive models hit rate limits
- **Regional failover**: Route to providers in different geographic regions

![Fallback Configuration](/images/genai/governance/ai-gateway/fallback-configuration.png)

To configure fallbacks, navigate to the Priority 2 (Fallback) section in your endpoint details page. Click Add Fallback Model to begin adding fallback options. For each fallback, select your provider and model, then configure the appropriate API key.

The order of fallback models matters, as the gateway tries them sequentially from top to bottom. Use the drag handle icon to reorder your fallbacks, placing your preferred alternatives first and less desirable options toward the end of the list.

Consider a production configuration where you have GPT-5 mini as the first fallback and Anthropic Claude 4.5 Haiku as the second fallback. If the primary model fails, the gateway falls back first to OpenAI GPT-5 mini, which is cheaper and faster but less capable. If that also fails, a second fallback to Anthropic Claude 4.5 Haiku provides another alternative before the request fails entirely.
