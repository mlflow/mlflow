# Online Evaluations

*Automatically evaluate traces as they're logged*

Online evaluations run your judges automatically on traces as they're logged to MLflow, without requiring manual invocation of `mlflow.genai.evaluate()`. This enables two key use cases:

- **Development**: Seamlessly measure quality as you iterate on your application, getting immediate feedback and quality insights without extra evaluation steps
- **Production**: Continuously monitor for issues like hallucinations, PII leakage, or user frustration on live traffic

:::info Coming Soon
Documentation for Online Evaluations is in progress.
:::
