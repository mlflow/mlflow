import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TabsWrapper from "@site/src/components/TabsWrapper";
import { CardGroup, TitleCard } from "@site/src/components/Card";
import ImageBox from "@site/src/components/ImageBox";

# Online Evaluations

*Automatically evaluate traces and multi-turn conversations as they're logged - no code required*

Online evaluations run your LLM judges automatically on traces and multi-turn conversations as they're logged to MLflow, without requiring manual execution of code. This enables two key use cases:

- **Streamlined Quality Improvement**: Seamlessly measure quality as you iterate on your agent or LLM application, getting immediate feedback and quality insights without extra evaluation steps
- **Production Monitoring**: Continuously monitor for issues like hallucinations, PII leakage, or user frustration on live traffic

## Online vs Offline Evaluation

| | Online Evaluation | Offline Evaluation |
|---|---|---|
| **When it runs** | Automatically, as traces and conversations are logged | Manually, when you call [`mlflow.genai.evaluate()`](/genai/eval-monitor/running-evaluation/traces) |
| **Use case** | Continuous monitoring, internal QA, interactive testing during development | Regression testing, bug fix verification |
| **Data source** | Live traces and conversations from your application | Curated datasets |

## Prerequisites

Before setting up online evaluations, ensure that:

1. **The MLflow Server is running** (see [Set Up MLflow Server](/genai/getting-started/connect-environment))
2. **MLflow Tracing is enabled** in your agent or LLM application (see [Tracing Quickstart](/genai/tracing/quickstart))
   - **For multi-turn conversation evaluation**: Traces must include session IDs (see [Track Users & Sessions](/genai/tracing/track-users-sessions))
3. **An AI Gateway endpoint is configured** for LLM judge execution (see [Create and Manage Endpoints](/genai/governance/ai-gateway/endpoints/create-and-manage))

## Setting Up Online Evaluation

These examples show how to set up LLM judges that automatically evaluate traces and multi-turn conversations as they're logged, and how to update or disable existing judges. For more details on creating LLM judges, see [LLM-as-a-Judge](/genai/eval-monitor/scorers/llm-judge/).

<TabsWrapper>
<Tabs>
<TabItem value="ui" label="UI" default>

1. Navigate to your experiment and select the **Judges** tab
   ![Judges tab](/images/genai/eval-monitor/judges-tab.png)
2. Click **+ New LLM judge**
   ![New LLM judge button](/images/genai/eval-monitor/new-llm-judge-button.png)
3. **Select scope**:
   - **Traces**: Evaluate individual traces for quality and correctness
   - **Sessions**: Evaluate entire sessions for conversation quality and outcomes
4. **Configure the judge**:
   - **LLM judge**: Select a built-in judge or create a custom one
   - **Name**: Unique identifier (cannot be changed after creation)
   - **Instructions**: Define evaluation criteria for the judge
   - **Output type**: Select the type of value the judge will return
   - **Model**: Select an AI Gateway endpoint to run the judge
5. **Evaluation settings**:
   - Check **"Automatically evaluate future traces using this judge"**
   - Set the **Sample rate** (percentage of traces to evaluate)
   - Optionally add a **Filter string** to target specific traces
   ![Evaluation settings](/images/genai/eval-monitor/evaluation-settings.png)
6. Click **Save**

To edit or disable an existing judge, click on it in the **Judges** tab and modify the settings.

</TabItem>
<TabItem value="sdk" label="SDK">

For more details about the APIs used in this example, see [`Scorer.start()`](/api_reference/python_api/mlflow.genai.html), [`Scorer.update()`](/api_reference/python_api/mlflow.genai.html), and [`Scorer.stop()`](/api_reference/python_api/mlflow.genai.html).

```python
from mlflow.genai.scorers import (
    ToolCallCorrectness,
    ConversationalGuidelines,
    ScorerSamplingConfig,
)

# Create and start a trace-level judge
tool_judge = ToolCallCorrectness(model="gateway:/your-endpoint")
registered_tool_judge = tool_judge.register(name="tool_call_correctness")
registered_tool_judge.start(
    sampling_config=ScorerSamplingConfig(sample_rate=0.5),  # Evaluate 50% of traces
)

# Create and start a session-level judge for multi-turn conversations
frustration_judge = ConversationalGuidelines(
    name="user_frustration",
    guidelines="The user should not express frustration, confusion, or dissatisfaction during the conversation.",
    model="gateway:/your-endpoint",
)
registered_frustration_judge = frustration_judge.register(name="user_frustration")
registered_frustration_judge.start(
    sampling_config=ScorerSamplingConfig(sample_rate=1.0),  # Evaluate all conversations
)

# Update or disable an existing judge
from mlflow.genai.scorers import get_scorer

judge = get_scorer(name="tool_call_correctness")
judge.update(
    sampling_config=ScorerSamplingConfig(sample_rate=0.3)
)  # Change sample rate
judge.stop()  # Or, disable the judge
```

</TabItem>
</Tabs>
</TabsWrapper>

## Configuration Options

### Sampling Rate

Control what percentage of traces are evaluated (0-100%). Balance cost and coverage based on your needs:
- **Development**: Use higher rates (50-100%) for comprehensive feedback
- **Production**: Use lower rates (1-20%) to manage costs at scale

### Filtering Traces

Use [trace search syntax](/genai/tracing/search-traces) to target specific traces. Examples:

```python
# Only evaluate successful traces
filter_string = "trace.status = 'OK'"

# Only evaluate traces from production environment
filter_string = "metadata.environment = 'production'"
```

:::note
For session-level evaluation, filters apply to the **first trace** in the session.
:::

### Session-Level Evaluation

For multi-turn conversations, online evaluation can assess entire sessions, in addition to individual traces.

- **Session completion**: A session is considered complete (ready for online evaluation) after no new traces arrive for 5 minutes ([configurable](/api_reference/python_api/mlflow.environment_variables.html#mlflow.environment_variables.MLFLOW_ONLINE_SCORING_DEFAULT_SESSION_COMPLETION_BUFFER_SECONDS))
- **Re-evaluation**: If new traces arrive after evaluation, the session is re-evaluated and previous online evaluation results are overwritten

For more information about session evaluation, see [Evaluate Conversations](/genai/eval-monitor/running-evaluation/multi-turn).

## Viewing Results

Assessments from online evaluation appear directly in the MLflow UI. Navigate to your experiment's **Traces** or **Sessions** tab to see results.

<ImageBox
  src="/images/genai/eval-monitor/online-evaluation-results.png"
  alt="Online evaluation results showing assessment scores for traces"
  caption="Assessment scores from online evaluation appear as columns in the traces tab"
  width="80%"
/>

## Best Practices

- **Combine judges**: Use multiple judges for comprehensive quality coverage
- **Start high, scale down as needed**: Use 100% sampling during development, then reduce for production if necessary to control costs
- **Monitor costs**: LLM-based evaluation has associated costs—adjust sampling accordingly
- **Use filters strategically in production**: Focus evaluation on high-value or high-risk traces

## Troubleshooting

| Issue | Solution |
|---|---|
| **Missing assessments** | Verify the judge is active, traces match the filter, and sampling rate is greater than zero |
| **Unexpected or unsatisfactory judge results** | Review the judge's prompt/guidelines and test with known inputs |

For further debugging, enable debug logging on the MLflow server by setting the [`MLFLOW_LOGGING_LEVEL=DEBUG`](/api_reference/python_api/mlflow.environment_variables.html#mlflow.environment_variables.MLFLOW_LOGGING_LEVEL) environment variable and checking the MLflow server logs.

## Next Steps

<CardGroup cols={2}>
  <TitleCard title="LLM-as-a-Judge →" link="/genai/eval-monitor/scorers/llm-judge/">
    Learn how to create and customize LLM judges for your specific evaluation needs.
  </TitleCard>
  <TitleCard title="Evaluate Conversations →" link="/genai/eval-monitor/running-evaluation/multi-turn">
    Set up evaluation for multi-turn conversations and chat sessions.
  </TitleCard>
</CardGroup>
