import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TabsWrapper from "@site/src/components/TabsWrapper";
import { CardGroup, TitleCard } from "@site/src/components/Card";
import ImageBox from "@site/src/components/ImageBox";
import { APILink } from "@site/src/components/APILink";

# Online Evaluations

_Automatically evaluate traces and multi-turn conversations as they're logged - no code required_

Online evaluations run your LLM judges automatically on traces and multi-turn conversations as they're logged to MLflow, without requiring manual execution of code. This enables two key use cases:

- **Streamlined Quality Improvement**: Seamlessly measure quality as you iterate on your agent or LLM application, getting immediate feedback and quality insights without extra evaluation steps
- **Production Monitoring**: Continuously monitor for issues like hallucinations, PII leakage, or user frustration on live traffic

## Online vs Offline Evaluation

|                  | Online Evaluation                                                          | Offline Evaluation                                                                                 |
| ---------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| **When it runs** | Automatically, as traces and conversations are logged                      | Manually, when you call [`mlflow.genai.evaluate()`](/genai/eval-monitor/running-evaluation/traces) |
| **Use case**     | Continuous monitoring, internal QA, interactive testing during development | Regression testing, bug fix verification                                                           |
| **Data source**  | Live traces and conversations from your application                        | Curated datasets                                                                                   |

## Prerequisites

Before setting up online evaluations, ensure that:

1. **The MLflow Server is running** (see [Set Up MLflow Server](/genai/getting-started/connect-environment))
2. **MLflow Tracing is enabled** in your agent or LLM application (see [Tracing Quickstart](/genai/tracing/quickstart))
   - **For multi-turn conversation evaluation**, traces must include session IDs (see [Track Users & Sessions](/genai/tracing/track-users-sessions))
3. **An AI Gateway endpoint is configured** for LLM judge execution (see [Create and Manage Endpoints](/genai/governance/ai-gateway/endpoints/create-and-manage))

## Setting Up Online Evaluation

These examples show how to set up LLM judges that automatically evaluate traces and multi-turn conversations as they're logged to an [MLflow Experiment](/genai/tracing/quickstart#create-a-mlflow-experiment), and how to update or disable existing judges. For more details on creating LLM judges, see [LLM-as-a-Judge](/genai/eval-monitor/scorers/llm-judge/).

:::note
When a judge is created or enabled, it evaluates traces and sessions that are **at most one hour old**. Updating a judge's configuration does not trigger re-evaluation of previously assessed traces.
:::

<TabsWrapper>
<Tabs>
<TabItem value="ui" label="UI" default>

1. Navigate to your experiment and select the **Judges** tab

   <img src={require('@site/static/images/genai/eval-monitor/judges-tab.png').default} alt="Judges tab" width="108" style={{ borderRadius: '8px', border: '1px solid #ddd', boxShadow: '0 1px 4px rgba(0,0,0,0.05)' }} />

2. Click **+ New LLM judge**

   <img src={require('@site/static/images/genai/eval-monitor/new-llm-judge-button.png').default} alt="New LLM judge button" width="180" style={{ borderRadius: '8px', border: '1px solid #ddd', boxShadow: '0 1px 4px rgba(0,0,0,0.05)' }} />

3. **Select scope**:
   - **Traces**: Evaluate individual traces
   - **Sessions**: Evaluate entire multi-turn conversations
4. **Configure the judge**:
   - **LLM judge**: Select a built-in judge or create a custom one
   - **Name**: A unique name for the judge
   - **Instructions**: Define evaluation criteria for the judge
   - **Output type**: Select the type of value the judge will return
   - **Model**: Select an [AI Gateway](/genai/governance/ai-gateway/endpoints/create-and-manage) endpoint (LLM) to run the judge
5. **Evaluation settings**:
   - Check **"Automatically evaluate future traces using this judge"**
   - Set the **Sample rate** (percentage of traces or sessions to evaluate)
   - Optionally add a **Filter string** to target specific traces or sessions

   <img src={require('@site/static/images/genai/eval-monitor/evaluation-settings.png').default} alt="Evaluation settings" width="400" style={{ borderRadius: '8px', border: '1px solid #ddd', boxShadow: '0 1px 4px rgba(0,0,0,0.05)' }} />

6. Click **Save**
7. To edit or disable an existing judge, click on it in the **Judges** tab.

   <img src={require('@site/static/images/genai/eval-monitor/edit-llm-judge-button.png').default} alt="Edit LLM judge button" width="270" style={{ borderRadius: '8px', border: '1px solid #ddd', boxShadow: '0 1px 4px rgba(0,0,0,0.05)' }} />

</TabItem>
<TabItem value="sdk" label="SDK">

For more details about the APIs used in this example, see <APILink fn="mlflow.genai.scorers.Scorer.start" />, <APILink fn="mlflow.genai.scorers.Scorer.update" />, and <APILink fn="mlflow.genai.scorers.Scorer.stop" />.

```python
import mlflow
from mlflow.genai.scorers import (
    ToolCallCorrectness,
    ConversationalGuidelines,
    ScorerSamplingConfig,
)

# Set the experiment for the judges
mlflow.set_experiment("my-experiment")

# Create and start a trace-level judge
tool_judge = ToolCallCorrectness(model="gateway:/your-endpoint")
registered_tool_judge = tool_judge.register(name="tool_call_correctness")
registered_tool_judge.start(
    sampling_config=ScorerSamplingConfig(sample_rate=0.5),  # Evaluate 50% of traces
)

# Create and start a session-level judge for multi-turn conversations
frustration_judge = ConversationalGuidelines(
    name="user_frustration",
    guidelines="The user should not express frustration, confusion, or dissatisfaction during the conversation.",
    model="gateway:/your-endpoint",
)
registered_frustration_judge = frustration_judge.register(name="user_frustration")
registered_frustration_judge.start(
    sampling_config=ScorerSamplingConfig(sample_rate=1.0),  # Evaluate all conversations
)

# Update or disable an existing judge
from mlflow.genai.scorers import get_scorer

judge = get_scorer(name="tool_call_correctness")
judge.update(
    sampling_config=ScorerSamplingConfig(sample_rate=0.3)
)  # Change sample rate
judge.stop()  # Or, disable the judge
```

</TabItem>
</Tabs>
</TabsWrapper>

## Viewing Results

Assessments from online evaluation appear directly in the MLflow UI. For traces, assessments typically appear within a minute or two of logging. For multi-turn sessions, evaluation begins after 5 minutes of inactivity (no new traces) by default—this is <APILink fn="mlflow.environment_variables.MLFLOW_ONLINE_SCORING_DEFAULT_SESSION_COMPLETION_BUFFER_SECONDS">configurable</APILink>. Navigate to your experiment's **Traces** or **Sessions** tab to see results.

<ImageBox
  src="/images/genai/eval-monitor/online-evaluation-results.png"
  alt="Online evaluation results showing assessment scores for traces"
  caption="Assessment scores from online evaluation appear as columns in the traces tab"
  width="90%"
/>

## Configuration Options

### Sampling Rate

Control what percentage of traces are evaluated (0-100%). Balance cost and coverage based on your needs:

- **Development**: Use a high sampling rate to detect as many issues as possible before production deployment
- **Production**: Consider using lower rates if necessary to control costs

### Filtering Traces

Use [trace search syntax](/genai/tracing/search-traces) to target specific traces. Examples:

```python
# Only evaluate successful traces
filter_string = "trace.status = 'OK'"

# Only evaluate traces from production environment
filter_string = "metadata.environment = 'production'"
```

:::note
For session-level evaluation, filters apply to the **first trace** in the session.
:::

### Session-Level Evaluation

Online evaluation can assess entire multi-turn conversations (sessions), in addition to individual traces.

- **Session completion**: A session is considered complete (ready for online evaluation) after no new traces arrive for 5 minutes (<APILink fn="mlflow.environment_variables.MLFLOW_ONLINE_SCORING_DEFAULT_SESSION_COMPLETION_BUFFER_SECONDS">configurable</APILink>)
- **Re-evaluation**: If new traces are added to the session after evaluation, the session is re-evaluated and previous online evaluation results are overwritten

For more information about session evaluation, see [Evaluate Conversations](/genai/eval-monitor/running-evaluation/multi-turn).

## Best Practices

- **Combine judges**: Use multiple judges for comprehensive quality coverage
- **Start with a high sampling rate, then scale down as needed**: Use a high sampling rate during development to detect as many issues as possible before production deployment, then reduce for production if necessary to control costs
- **Monitor costs**: LLM-based evaluation has associated costs—adjust sampling accordingly
- **Use filters strategically in production**: Focus evaluation on high-value or high-risk traces

## Troubleshooting

| Issue                                          | Solution                                                                                                                                           |
| ---------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Missing assessments**                        | Verify that the judge is active, the filter matches your traces, the sampling rate is greater than zero, and the traces are less than one hour old |
| **Unexpected or unsatisfactory judge results** | Review the judge's prompt/guidelines and test with known inputs                                                                                    |
| **Evaluation errors**                          | Check trace/session assessments in the UI or SDK, or server logs, for details. Failed evaluations are not retried automatically                    |

For further debugging, enable debug logging on the MLflow server by setting the <APILink fn="mlflow.environment_variables.MLFLOW_LOGGING_LEVEL"><code>MLFLOW_LOGGING_LEVEL=DEBUG</code></APILink> environment variable and checking the MLflow server logs.

## Next Steps

<CardGroup cols={2}>
  <TitleCard title="LLM-as-a-Judge →" link="/genai/eval-monitor/scorers/llm-judge/">
    Learn how to create and customize LLM judges for your specific evaluation needs.
  </TitleCard>
  <TitleCard title="Evaluate Conversations →" link="/genai/eval-monitor/running-evaluation/multi-turn">
    Set up evaluation for multi-turn conversations and chat sessions.
  </TitleCard>
</CardGroup>
