import { APILink } from "@site/src/components/APILink";
import WorkflowSteps from '@site/src/components/WorkflowSteps';
import TilesGrid from '@site/src/components/TilesGrid';
import TileCard from '@site/src/components/TileCard';
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TabsWrapper from "@site/src/components/TabsWrapper";
import { Target, Play, Code, Database, Users, BookOpen } from 'lucide-react';
import ServerSetup from "@site/src/content/setup_server.mdx";

# Conversation Simulation

Conversation simulation enables you to generate synthetic multi-turn conversations for testing your conversational AI agents. Instead of manually creating test conversations or waiting for production data, you can define test scenarios and let MLflow automatically simulate realistic user interactions.

:::info[Experimental Feature]
Conversation simulation is experimental in MLflow 3.10.0. The API and behavior may change in future releases.
:::

## Why Simulate Conversations?

Static test data has significant limitations for conversational AI:

- **Version testing**: You can't test a new agent version against conversations that were generated by a different version
- **Coverage gaps**: Manual test cases often miss edge cases and unexpected user behaviors
- **Scale limitations**: Creating diverse, realistic conversations manually is time-consuming

Conversation simulation addresses these challenges by programmatically generating conversations based on defined goals and personas, enabling:

- **Systematic evaluation**: Test different agent versions with consistent goals and personas
- **Red-teaming**: Stress-test agents with diverse user behaviors at scale
- **Rapid iteration**: Generate new test conversations instantly when requirements change

## Workflow

<WorkflowSteps
  steps={[
    {
      icon: Target,
      title: "Define test cases or extract from existing conversations",
      description: "Specify goals, personas, and context for each simulated conversation, or generate them from production sessions."
    },
    {
      icon: Users,
      title: "Create simulator",
      description: "Initialize ConversationSimulator with your test cases and configuration."
    },
    {
      icon: Code,
      title: "Define your agent",
      description: "Implement your agent in a function that accepts conversation history."
    },
    {
      icon: Play,
      title: "Run evaluation",
      description: "Pass the simulator to mlflow.genai.evaluate() with your scorers."
    }
  ]}
/>

## Prerequisites

Install the required packages:

```bash
pip install --upgrade 'mlflow[genai]>=3.10'
```

MLflow stores evaluation results in a tracking server. Connect your local environment to the tracking server by one of the following methods.

<ServerSetup />

Finally, configure your LLM provider credentials. The simulator defaults to using OpenAI for user simulation:

```python
import os

os.environ["OPENAI_API_KEY"] = "your-api-key-here"  # Replace with your API key
```

## Quick Start

Here's a complete example that simulates conversations and evaluates them:

```python
import os
import mlflow
from mlflow.genai.simulators import ConversationSimulator
from mlflow.genai.scorers import ConversationCompleteness, Safety
from openai import OpenAI

os.environ["OPENAI_API_KEY"] = "your-api-key-here"  # Replace with your API key

client = OpenAI()

# 1. Define test cases with goals (required) and optional persona/context
test_cases = [
    {
        "goal": "Learn how to track experiments in MLflow",
    },
    {
        "goal": "Debug a model deployment issue",
        "persona": "You are a frustrated data scientist who has been stuck on this issue for hours",
    },
    {
        "goal": "Set up model versioning for a production ML pipeline",
        "persona": "You are a beginner who needs step-by-step guidance",
        "context": {
            "user_id": "beginner_123"
        },  # user_id is passed to predict_fn via kwargs
    },
]

# 2. Create the simulator
simulator = ConversationSimulator(
    test_cases=test_cases,
    max_turns=5,
)


# 3. Define your predict function (OpenAI Responses API format)
def predict_fn(input: list[dict], **kwargs) -> dict:
    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=input,
    )
    return {"output": response.choices[0].message.content}


# 4. Run evaluation with conversation and single-turn scorers
results = mlflow.genai.evaluate(
    data=simulator,
    predict_fn=predict_fn,
    scorers=[
        ConversationCompleteness(),  # Multi-turn scorer
        Safety(),  # Single-turn scorer (applied to each turn)
    ],
)
```

## Defining Test Cases

Each test case represents a conversation scenario. Test cases support three fields:

| Field     | Required | Description                                                   |
| --------- | -------- | ------------------------------------------------------------- |
| `goal`    | Yes      | What the simulated user is trying to accomplish               |
| `persona` | No       | Description of the user's personality and communication style |
| `context` | No       | Additional kwargs passed to your predict function             |

### Goal

The goal describes what the simulated user wants to achieve. It should be specific enough to guide the conversation but open-ended enough to allow natural dialogue. Good goals also describe the expected outcome so the simulator knows when the user's intent has been accomplished:

```python
# Good goals - specific, actionable, and describe expected outcomes
{
    "goal": "Get help configuring MLflow tracking for a distributed training job. The response should include code examples and links to relevant documentation."
}
{
    "goal": "Understand the difference between experiments and runs in MLflow. The explanation should include concrete examples of when to use each."
}
{
    "goal": "Troubleshoot why model artifacts aren't being logged. The assistant should help identify the root cause and provide a working solution."
}

# Less effective goals - too vague, no expected outcome
{"goal": "Learn about MLflow"}
{"goal": "Get help"}
```

### Persona

The persona shapes how the simulated user communicates. If not specified, a default helpful user persona is used:

```python
# Technical expert who asks detailed questions
{
    "goal": "Optimize model serving latency",
    "persona": "You are a senior ML engineer who asks precise technical questions and expects detailed answers",
}

# Beginner who needs more guidance
{
    "goal": "Set up experiment tracking",
    "persona": "You are new to MLflow and need step-by-step explanations with examples",
}

# Frustrated user testing agent resilience
{
    "goal": "Fix a broken deployment",
    "persona": "You are impatient and frustrated because this is blocking a production release",
}
```

### Context

The context field passes additional parameters to your predict function. This is useful for:

- Passing user identifiers for personalization
- Providing session state or configuration
- Including metadata your agent needs

```python
{
    "goal": "Get personalized model recommendations",
    "context": {
        "user_id": "enterprise_user_42",  # user_id is passed to predict_fn via kwargs
        "subscription_tier": "premium",
        "preferred_framework": "pytorch",
    },
}
```

## Test Case Sources

You can define test cases in multiple ways:

<TabsWrapper>
<Tabs>
<TabItem value="dicts" label="From Dictionaries" default>

```python
test_cases = [
    {"goal": "Learn about experiment tracking"},
    {"goal": "Debug deployment issue", "persona": "Senior engineer"},
    {"goal": "Set up CI/CD pipeline", "context": {"team": "platform"}},
]

simulator = ConversationSimulator(test_cases=test_cases)
```

</TabItem>
<TabItem value="dataframe" label="From DataFrame">

```python
import pandas as pd

df = pd.DataFrame(
    [
        {"goal": "Learn about experiment tracking"},
        {"goal": "Debug deployment issue", "persona": "Senior engineer"},
        {"goal": "Set up CI/CD pipeline"},
    ]
)

simulator = ConversationSimulator(test_cases=df)
```

</TabItem>
<TabItem value="dataset" label="From Evaluation Dataset">

```python
from mlflow.genai.datasets import create_dataset, get_dataset

# Create and populate a dataset
dataset = create_dataset(name="conversation_test_cases")
dataset.merge_records(
    [
        {"inputs": {"goal": "Learn about experiment tracking"}},
        {"inputs": {"goal": "Debug deployment issue", "persona": "Senior engineer"}},
    ]
)

# Use the dataset with the simulator
dataset = get_dataset(name="conversation_test_cases")
simulator = ConversationSimulator(test_cases=dataset)
```

</TabItem>
</Tabs>
</TabsWrapper>

### From Existing Sessions

:::info[Experimental Feature]
The `generate_test_cases` API is experimental in MLflow 3.10.0. The API and behavior may change in future releases.
:::

Generate test cases from existing conversation sessions using <APILink fn="mlflow.genai.simulators.generate_test_cases">`generate_test_cases`</APILink>. This is useful for creating test cases that reflect real user behavior from production conversations:

```python
import mlflow
from mlflow.genai.simulators import generate_test_cases, ConversationSimulator

# Get existing sessions from your experiment
sessions = mlflow.search_sessions(
    locations=["<experiment-id>"],
    max_results=50,
)

# Generate test cases by extracting goals and personas from sessions
test_cases = generate_test_cases(sessions)  # clint: disable=unknown-mlflow-function

# Use generated test cases with the simulator
simulator = ConversationSimulator(test_cases=test_cases)
```

## Predict Function Interface

Your predict function must follow the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) format:

```python
def predict_fn(input: list[dict], **kwargs) -> dict:
    """
    Args:
        input: Conversation history as a list of message dicts.
               Each message has "role" ("user" or "assistant") and "content".
        **kwargs: Additional arguments including:
            - mlflow_session_id: Unique ID for this conversation session
            - Any fields from your test case's "context"

    Returns:
        A dict containing the response. The assistant's message content
        will be automatically extracted.
    """
```

### Basic Example

```python
from openai import OpenAI

client = OpenAI()


def predict_fn(input: list[dict], **kwargs) -> dict:
    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=input,
    )
    return {"output": response.choices[0].message.content}
```

### With Context

```python
def predict_fn(input: list[dict], **kwargs) -> dict:
    # user_id is passed from test case's "context" field
    user_id = kwargs.get("user_id")

    # Customize system prompt based on context
    system_message = f"You are helping user {user_id}. Be helpful and concise."

    messages = [{"role": "system", "content": system_message}] + input

    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=messages,
    )
    return {"output": response.choices[0].message.content}
```

### With Stateful Agents

For agents that maintain state across turns, use the `mlflow_session_id` to manage conversation state:

```python
# Simple in-memory state management for stateful agents
conversation_state = {}  # Maps session_id -> conversation context


def predict_fn(input: list[dict], **kwargs) -> dict:
    session_id = kwargs.get("mlflow_session_id")

    # Initialize or retrieve state for this session
    if session_id not in conversation_state:
        conversation_state[session_id] = {
            "turn_count": 0,
            "topics_discussed": [],
        }

    state = conversation_state[session_id]
    state["turn_count"] += 1

    # Your agent logic here - can use state for context
    system_message = f"You are a helpful assistant. This is turn {state['turn_count']} of the conversation."
    messages = [{"role": "system", "content": system_message}] + input

    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=messages,
    )

    return {"output": response.choices[0].message.content}
```

## Configuration Options

### ConversationSimulator Parameters

| Parameter           | Type                                              | Default              | Description                                       |
| ------------------- | ------------------------------------------------- | -------------------- | ------------------------------------------------- |
| `test_cases`        | `list[dict]`, `DataFrame`, or `EvaluationDataset` | Required             | Test case definitions                             |
| `max_turns`         | `int`                                             | 10                   | Maximum conversation turns before stopping        |
| `user_model`        | `str`                                             | `openai:/gpt-5-mini` | Model for simulating user messages                |
| `**user_llm_params` | `dict`                                            | `{}`                 | Additional parameters for the user simulation LLM |

### Model Selection

The simulator uses an LLM to generate realistic user messages. The default model is `openai:/gpt-5-mini`.

You can specify a different model:

```python
simulator = ConversationSimulator(
    test_cases=test_cases,
    user_model="anthropic:/claude-sonnet-4-20250514",
    temperature=0.7,  # Passed to the user simulation LLM
)
```

Supported model formats follow the pattern `"<provider>:/<model>"`. See [Supported Models](/genai/eval-monitor/scorers/llm-judge#supported-models) for the full list of supported providers.

### Conversation Stopping

Conversations stop when any of these conditions are met:

1. **Max turns reached**: The `max_turns` limit is hit
2. **Goal achieved**: The simulator detects the user's goal has been accomplished

## Viewing Results

Simulated conversations appear in the MLflow UI with special metadata:

- **Session ID**: Each conversation gets a unique session ID (prefixed with `sim-`)
- **Simulation metadata**: Goal, persona, and turn number are stored on each trace

Navigate to the **Sessions** tab in your experiment to view conversations grouped by session. Click into a session to see individual turns and their assessments.

## Next Steps

<TilesGrid>
  <TileCard
    icon={BookOpen}
    iconSize={48}
    title="Evaluate Conversations"
    description="Learn about static conversation evaluation and multi-turn scorers."
    href="/genai/eval-monitor/running-evaluation/multi-turn"
    linkText="View conversation evaluation →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Multi-Turn Scorers"
    description="Explore predefined scorers for conversation completeness, user frustration, and more."
    href="/genai/eval-monitor/scorers/llm-judge/predefined#multi-turn"
    linkText="View scorers →"
    containerHeight={64}
  />
  <TileCard
    icon={Database}
    iconSize={48}
    title="Evaluation Datasets"
    description="Persist your test cases in evaluation datasets for reproducible testing."
    href="/genai/datasets"
    linkText="Learn about datasets →"
    containerHeight={64}
  />
</TilesGrid>
