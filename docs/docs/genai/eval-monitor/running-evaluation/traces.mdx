import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { APILink } from "@site/src/components/APILink";
import ImageBox from '@site/src/components/ImageBox';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Evaluating Traces

Evaluate the quality of your production GenAI applications by analyzing traces captured from real-world usage. This approach provides insights into actual performance without requiring model re-execution.

<table style={{ width: "100%" }}>
  <thead>
    <tr>
      <th style={{ width: "30%" }}>Component</th>
      <th style={{ width: "70%" }}>What we use in this guide</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Dataset</strong><br/><small>Inputs &amp; expectations</small></td>
      <td>
        Traces logged in MLflow from production traffic
      </td>
    </tr>
    <tr>
      <td><strong>Scorer</strong><br/><small>Evaluation criteria</small></td>
      <td>
        Built-in scorers (Correctness, Relevance) and custom scorers
      </td>
    </tr>
    <tr>
      <td><strong>Predict Function</strong><br/><small></small></td>
      <td>
        Not needed - traces already contain outputs
      </td>
    </tr>
  </tbody>
</table>

## Key Components

- Online evaluation
- Annotation
- Flexible Search

## Example: Evaluating Production Traces

### Prerequisites

```bash
pip install --upgrade mlflow>=3.3 openai
```

Simulate production traces

First, ensure your production application is logging traces. Enable tracing for your LLM calls:

```python
import mlflow
from openai import OpenAI

# Enable automatic tracing
mlflow.openai.autolog()

# Your production application
client = OpenAI()

@mlflow.trace(span_type="AGENT")
def answer_question(question: str) -> str:
    """Production Q&A application with tracing enabled"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": question}
        ]
    )
    return response.choices[0].message.content

# In production, this generates traces automatically
answer = answer_question("What is MLflow?")
```

:::tip
For production tracing best practices, see the [Production Tracing Guide](/genai/tracing/prod-tracing).
:::

### Step 1: Search and retrieve traces

Retrieve traces from your MLflow tracking server for evaluation:

```python
import mlflow

# Search for traces from a specific experiment
traces = mlflow.search_traces(
    experiment_ids=["123"],  # Your experiment ID
    filter_string="attributes.status = 'OK'",  # Optional: filter successful traces
    max_results=100
)

print(f"Found {len(traces)} traces to evaluate")
```

You can also search traces by time range:

```python
from datetime import datetime, timedelta

# Get traces from the last 24 hours
yesterday = datetime.now() - timedelta(days=1)
traces = mlflow.search_traces(
    experiment_ids=["123"],
    filter_string=f"timestamp > {int(yesterday.timestamp() * 1000)}"
)
```

### Step 2: Add human feedback (optional)

Enhance evaluation quality by incorporating human feedback on traces:

```python
# Add feedback to a specific trace
trace_id = traces[0].trace_id

mlflow.set_trace_feedback(
    trace_id=trace_id,
    feedback_name="quality",
    feedback_value=4,  # 1-5 scale
    metadata={"reviewer": "domain_expert", "notes": "Accurate but verbose"}
)

# Retrieve feedback for evaluation
feedback = mlflow.get_trace_feedback(trace_id)
```

### Step 4: Evaluate trace quality

Run evaluation on the collected traces using built-in and custom scorers:

```python
from mlflow.genai.scorers import Correctness, Relevance, Guidelines

# Evaluate traces with multiple scorers
results = mlflow.genai.evaluate(
    data=traces,
    scorers=[
        # Built-in scorers
        Correctness(),
        Relevance(),
        
        # Custom criteria
        Guidelines(
            name="conciseness",
            guidelines="Responses should be under 100 words"
        ),
        Guidelines(
            name="tone",
            guidelines="Maintain professional and helpful tone"
        )
    ],
    experiment_name="Trace Evaluation"
)

# View aggregated metrics
print(f"Average Correctness: {results.metrics['correctness/v1/mean']:.2f}")
print(f"Average Relevance: {results.metrics['relevance/v1/mean']:.2f}")
```

## Create Custom Trace Scorers

Build evaluators that leverage trace-specific information:

```python
from mlflow.genai.scorers import make_scorer

@make_scorer
def latency_scorer(trace) -> dict:
    """Score based on response latency"""
    latency_ms = trace.latency_ms
    
    if latency_ms < 500:
        score = 5
    elif latency_ms < 1000:
        score = 4
    elif latency_ms < 2000:
        score = 3
    elif latency_ms < 5000:
        score = 2
    else:
        score = 1
    
    return {
        "score": score,
        "justification": f"Response time: {latency_ms}ms"
    }

@make_scorer
def token_efficiency_scorer(trace) -> dict:
    """Evaluate token usage efficiency"""
    total_tokens = trace.attributes.get("llm.token_count.total", 0)
    
    if total_tokens < 100:
        efficiency = "excellent"
        score = 5
    elif total_tokens < 500:
        efficiency = "good"
        score = 4
    elif total_tokens < 1000:
        efficiency = "acceptable"
        score = 3
    else:
        efficiency = "poor"
        score = 2
    
    return {
        "score": score,
        "justification": f"Used {total_tokens} tokens - {efficiency} efficiency"
    }

# Use custom scorers in evaluation
results = mlflow.genai.evaluate(
    data=traces,
    scorers=[latency_scorer, token_efficiency_scorer]
)
```

## Annotating Traces

## A/B Testing


## Recording End-user Feedbacks in Production



## Next steps

- [Custom Scorers](/genai/eval-monitor/scorers/custom): Build advanced evaluation criteria
- [Production Monitoring](/genai/tracing/prod-tracing): Optimize trace collection in production
- [Dataset Creation](/genai/tracing/search-traces#creating-evaluation-datasets): Build evaluation datasets from traces