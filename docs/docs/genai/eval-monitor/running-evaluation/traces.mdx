import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { APILink } from "@site/src/components/APILink";
import ImageBox from '@site/src/components/ImageBox';
import TabsWrapper from '@site/src/components/TabsWrapper';
import WorkflowSteps from '@site/src/components/WorkflowSteps';
import TilesGrid from '@site/src/components/TilesGrid';
import TileCard from '@site/src/components/TileCard';
import { Search, BookMarked, Target, Play, MessageSquare, Code, Activity, Zap } from 'lucide-react';
import ServerSetup from "@site/src/content/setup_server.mdx";

# Evaluating (Production) Traces

[Traces](/genai/tracing) are the core data of MLflow. They capture the complete execution flow of your LLM applications. Evaluating traces is a powerful way to understand the performance of your LLM applications and get insights for quality improvement.

Evaluating traces is also a useful trick for offline evaluation. Instead of running prediction on every evaluation run, you can generate traces at once and re-use them for multiple evaluation runs, to reduce the computation and LLM costs.

<ImageBox src="/images/mlflow-3/eval-monitor/trace-evaluation-hero.png" alt="Evaluate traces overview" width="95%"/>

## Workflow

<WorkflowSteps
  steps={[
    {
      icon: BookMarked,
      title: "Annotate traces with ground truth (Optional)",
      description: "Add expected outputs and ground truth labels to traces to establish evaluation baselines and correct answers."
    },
    {
      icon: Search,
      title: "Search and retrieve traces",
      description: "Find and collect traces from your MLflow tracking server using filters for time range, experiment, or trace status."
    },
    {
      icon: Target,
      title: "Define scorers",
      description: "Create built-in and custom scorers to measure quality, accuracy, latency, and trace-specific metrics."
    },
    {
      icon: Play,
      title: "Run evaluation",
      description: "Execute the evaluation on your trace collection and analyze results in MLflow UI for insights."
    },
  ]}
/>

## Example: Evaluating Production Traces

### Prerequisites

First, install the required packages by running the following command:

```bash
pip install --upgrade mlflow>=3.3 openai
```

MLflow stores evaluation results in a tracking server. Connect your local environment to the tracking server by one of the following methods.

<ServerSetup />

### Step 0: Simulate Production Traces

First, let's simulate some production traces to use for evaluation. Here we define a simple email automation app that uses a CRM database to generate emails. If you already have traces, you can skip this step.

```python
import mlflow
from mlflow.entities import Document
import openai

client = openai.OpenAI()
mlflow.openai.autolog()  # Enable automatic tracing for OpenAI calls

# Simulated CRM database
CRM_DATA = {
    "Acme Corp": {
        "contact_name": "Alice Chen",
        "recent_meeting": "Product demo on Monday, very interested in enterprise features. They asked about: advanced analytics, real-time dashboards, API integrations, custom reporting, multi-user support, SSO authentication, data export capabilities, and pricing for 500+ users",
        "support_tickets": [
            "Ticket #123: API latency issue (resolved last week)",
            "Ticket #124: Feature request for bulk import",
            "Ticket #125: Question about GDPR compliance",
        ],
    },
    "TechStart": {
        "contact_name": "Bob Martinez",
        "recent_meeting": "Initial sales call last Thursday, requested pricing",
        "support_tickets": [
            "Ticket #456: Login issues (open - critical)",
            "Ticket #457: Performance degradation reported",
            "Ticket #458: Integration failing with their CRM",
        ],
    },
    "Global Retail": {
        "contact_name": "Carol Wang",
        "recent_meeting": "Quarterly review yesterday, happy with platform performance",
        "support_tickets": [],
    },
}


@mlflow.trace(span_type="RETRIEVER")
def retrieve_customer_info(customer_name: str) -> list[Document]:
    """Retrieve customer information from CRM database"""
    if data := CRM_DATA.get(customer_name):
        return [
            Document(
                id=f"{customer_name}_meeting",
                page_content=f"Recent meeting: {data['recent_meeting']}",
            ),
            Document(
                id=f"{customer_name}_tickets",
                page_content=f"Support tickets: {', '.join(data['support_tickets']) if data['support_tickets'] else 'No open tickets'}",
            ),
            Document(
                id=f"{customer_name}_contact",
                page_content=f"Contact: {data['contact_name']}",
            ),
        ]
    return []


@mlflow.trace(span_type="AGENT")
def generate_sales_email(customer_name: str, objective: str) -> dict[str, str]:
    """Generate personalized sales email based on customer data & given objective."""
    # Retrieve customer information
    customer_docs = retrieve_customer_info(customer_name)
    context = "\n".join([doc.page_content for doc in customer_docs])

    # Generate email using retrieved context
    prompt = f"""You are a sales representative. Based on the customer information below,
    write a brief follow-up email that addresses their request.

    Customer Information: {context}

    User instructions: {objective}"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2000,
    )
    return {"email": response.choices[0].message.content}
```

Let's run the app and generate some traces.

```python
test_requests = [
    {"customer_name": "Acme Corp", "user_instructions": "Follow up after product demo"},
    {
        "customer_name": "TechStart",
        "user_instructions": "Check on support ticket status",
    },
    {
        "customer_name": "Global Retail",
        "user_instructions": "Send quarterly review summary",
    },
    {
        "customer_name": "Acme Corp",
        "user_instructions": "Write a very detailed email explaining all our product features, pricing tiers, implementation timeline, and support options",
    },
    {
        "customer_name": "TechStart",
        "user_instructions": "Send an enthusiastic thank you for their business!",
    },
    {"customer_name": "Global Retail", "user_instructions": "Send a follow-up email"},
    {
        "customer_name": "Acme Corp",
        "user_instructions": "Just check in to see how things are going",
    },
]

# Run requests and capture traces
print("Simulating production traffic...")
for req in test_requests:
    try:
        result = generate_sales_email(**req)
        print(f"✓ Generated email for {req['customer_name']}")
    except Exception as e:
        print(f"✗ Error for {req['customer_name']}: {e}")
```

This generates a list of traces as follows:

<ImageBox src="/images/mlflow-3/eval-monitor/trace-evaluation-list.png" alt="Simulated traces" width="95%"/>

### Step 1: Search and retrieve traces

Traces stored in the MLflow backend can be retrieved using the <APILink fn="mlflow.search_traces" /> API.
The following code retrieves all traces from the last 24 hours. See [Query Traces via SDK](/genai/tracing/observe-with-traces/query-via-sdk/) for the full supported syntax.

```python
import mlflow
from datetime import datetime, timedelta

# Get traces from the last 24 hours
yesterday = datetime.now() - timedelta(days=1)
traces = mlflow.search_traces(
    filter_string=f"timestamp > {int(yesterday.timestamp() * 1000)}"
)
```

The API returns a set of traces as a pandas DataFrame, where various data in the trace is expanded into columns. The dataframe can be directly passed into the <APILink fn="mlflow.genai.evaluate" /> function for evaluation.

### Step 2: Define application-specific scorers

[Scorer](/genai/eval-monitor/scorers) is the core component of evaluation, which defines the criteria for evaluating the quality of the traces. MLflow provides a set of built-in scorers for common evaluation criteria, and you can also define your own custom scorers for application-specific criteria.

In this example, we use three different types of scorers:

- <APILink fn="mlflow.genai.scorers.RetrievalGroundedness">RetrievalGroundedness</APILink>: Built-in scorer checks if the output is grounded in the retrieved data.
- <APILink fn="mlflow.genai.scorers.RelevanceToQuery">RelevanceToQuery</APILink>: Built-in scorer checks if the output is relevant to the user's request.
- <APILink fn="mlflow.genai.scorers.Guidelines">Guidelines</APILink>: Built-in scorer that allows you to judge the output against custom guidelines using LLMs.

These scorers uses LLM for judging the criteria. The default model is `openai:/gpt-4.1-mini`. You can also specify a different model by passing the `model` parameter to the scorer constructor.

```python
email_scorers = [
    RetrievalGroundedness(),
    RelevanceToQuery(),  # Checks if email addresses the user's request
    Guidelines(
        name="follows_objective",
        guidelines="The generated email must follow the objective in the request.",
    ),
    Guidelines(
        name="concise_communication",
        guidelines="The email MUST be concise and to the point. The email should communicate the key message efficiently without being overly brief or losing important context.",
    ),
    Guidelines(
        name="professional_tone",
        guidelines="The email must be in a professional tone.",
    ),
]
```

:::tip Scoring Intermediate Information in Traces

Scorers have access to the complete MLflow traces, including spans, attributes, and outputs. This allows you to evaluate the agent's behavior precisely, not only the final output, such as the **tool call trajectory**, the **sub-agents routing**, the **retrieved document recall**, etc. See <ins>[Parsing Traces for Scoring](/genai/eval-monitor/scorers/custom#parsing-traces-for-scoring)</ins> for more details.

:::

### Step 3: Evaluate trace quality

Now we are ready to run the evaluation. One notable difference from other examples is that we don't need to specify a `predict_fn` function. The <APILink fn="mlflow.genai.evaluate" /> function will automatically extract the inputs, outputs, and other intermediate information from the trace object and use them for scoring.

```python
results = mlflow.genai.evaluate(
    data=traces,
    scorers=email_scorers,
)
```

Once the evaluation is done, open the MLflow UI in your browser and navigate to the experiment page. You should see MLflow creates a new Run and logs the evaluation results.

<ImageBox src="/images/mlflow-3/eval-monitor/trace-evaluation-result.png" alt="Evaluate traces result" width="95%"/>

By clicking on the each row in the result, you can open the trace and see the detailed score and rationale.

## Annotate Traces with Ground Truth and Manual Feedbacks

Some evaluation criteria require ground truths to be defined. MLflow allows you to directly annotate traces with ground truths and any other human feedbacks.

To annotate a trace with ground truth or manual feedback, open the trace in the MLflow UI and click the **Assessments** button to add expectations or feedback directly through the web interface.

<ImageBox src="/images/mlflow-3/eval-monitor/trace-evaluation-assessments.png" alt="Annotate traces with ground truth" width="80%"/>

Alternatively, you can also annotate traces with ground truth or manual feedbacks using the <APILink fn="mlflow.log_expectation" /> and the <APILink fn="mlflow.log_feedback" /> APIs respectively.

## Recording End-user Feedbacks from Production

Using the <APILink fn="mlflow.log_feedback" /> API, you can record end-user feedbacks from your production application directly and monitor them in MLflow.

<ImageBox src="/images/mlflow-3/eval-monitor/trace-evaluation-user-feedback.png" alt="Annotate traces with feedback" width="95%"/>

```python
# Decorate the endpoint with MLflow tracing
@mlflow.trace(span_type="LLM")
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint that answers user questions and returns response with MLflow trace ID.
    """
    try:
        response = await openai.AsyncOpenAI().chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": request.prompt}],
        )

        # Get the active trace ID for the request
        trace_id = mlflow.get_current_active_span().trace_id

        return ChatResponse(
            response=response.choices[0].message.content,
            trace_id=trace_id,
            timestamp=time.time(),
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error processing chat request: {str(e)}"
        )


@app.post("/feedback", response_model=FeedbackResponse)
async def feedback(request: FeedbackRequest):
    """
    Feedback endpoint that annotates MLflow traces with user feedback.
    """
    try:
        # Record the given user feedback to the Trace
        mlflow.log_feedback(
            trace_id=request.trace_id,
            name="user_satisfaction",
            value=request.thumbs_up,
            source=AssessmentSource(
                source_type=AssessmentSourceType.HUMAN, source_id=request.user_id
            ),
            rationale=request.rationale,
        )
        return FeedbackResponse(
            message="Feedback recorded successfully", trace_id=request.trace_id
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error processing feedback: {str(e)}"
        )
```

## Next steps

<TilesGrid>
  <TileCard
    icon={Code}
    iconSize={48}
    title="Custom Scorers"
    description="Build advanced evaluation criteria and metrics tailored to your specific trace analysis needs."
    href="/genai/eval-monitor/scorers"
    linkText="Create custom scorers →"
    containerHeight={64}
  />
  <TileCard
    icon={Activity}
    iconSize={48}
    title="Production Monitoring"
    description="Optimize trace collection in production environments for efficient monitoring and analysis."
    href="/genai/tracing/prod-tracing"
    linkText="Set up monitoring →"
    containerHeight={64}
  />
  <TileCard
    icon={Zap}
    iconSize={48}
    title="Tracing Integrations"
    description="Use MLflow Tracing with other LLM providers and frameworks, such as LangGraph, Pydantic AI."
    href="/genai/tracing/integrations"
    linkText="Explore integrations →"
    containerHeight={64}
  />
</TilesGrid>
