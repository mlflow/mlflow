import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { APILink } from "@site/src/components/APILink";
import ImageBox from '@site/src/components/ImageBox';
import TabsWrapper from '@site/src/components/TabsWrapper';
import TilesGrid from '@site/src/components/TilesGrid';
import TileCard from '@site/src/components/TileCard';
import { Target, Bot, Zap } from 'lucide-react';
import ServerSetup from "@site/src/content/setup_server.mdx";

# Evaluating Prompts

Prompts are the core components of GenAI applications. However, iterating over prompts can be challenging because it is hard to know if the new prompt is better than the old one. MLflow provides a framework to systematically evaluate prompt templates and track performance over time.

<ImageBox src="/images/mlflow-3/eval-monitor/prompt-evaluation-hero.png" alt="Prompt Evaluation" width="95%"/>

## Key Components

- **[Scorers](/genai/eval-monitor/scorers)** allows you to define any qualitative and heuristic evaluation criteria for your prompts.
- **[Experiment and Runs]** let you compare the performance of prompt versions visually and keep track of multiple trials in your projects.
- **[Prompt Registry](/genai/prompt-registry)** allows you to version control your prompt templates and maintain the changes over time like Git source control.
- **[Prompt Optimizers](/genai/prompt-registry/optimize-prompts)** empowers you with the state-of-the-art optimization algorithms and automates the prompt engineering process.

## Example: Evaluating a Prompt Template

### Prerequisites

First, install the required packages by running the following command:

```bash
pip install --upgrade mlflow>=3.3 openai
```

MLflow stores evaluation results in a tracking server. Connect your local environment to the tracking server by one of the following methods.

<ServerSetup />

### Step 1: Create prompt templates

Let's define a simple prompt template to evaluate. We use [MLflow Prompt Registry](/genai/mlflow-3/prompt-registry) to save the prompt and version control it, but it is optional for evaluation.

```python
import mlflow

# Define prompt templates. MLflow support both text and chat format prompt templates.
PROMPT_V1 = [
    {
        "role": "system",
        "content": "You are a helpful assistant. Answer the following question."
    },
    {
        "role": "user",
        # Use double curly braces to indicate variables.
        "content": "Question: {{question}}"
    },
]

# Register the prompt template to the MLflow Prompt Registry for version control
# and convenience of loading the prompt template. This is optional.
mlflow.genai.register_prompt(
    name="qa_prompt",
    template=PROMPT_V1,
    commit_message="Initial prompt",
)
```

### Step 2: Create evaluation dataset

The evaluation dataset is defined as a list of dictionaries, each with an `inputs` and `expectations` field.

```python
eval_dataset = [
    {
        "inputs": {"question": "What causes rain?"},
        "expectations": {"key_concepts": ["evaporation", "condensation", "precipitation"]}
    },
    {
        "inputs": {"question": "Explain the difference between AI and ML"},
        "expectations": {"key_concepts": ["artificial intelligence", "machine learning", "subset"]}
    },
    {
        "inputs": {"question": "How do vaccines work?"},
        "expectations": {"key_concepts": ["immune", "antibodies", "protection"]}
    },
]
```
### Step 3: Create prediction function

Now wrap the prompt template in a simple function that takes a question to generate responses using the prompt template. **IMPORTANT: The function must take the keyword arguments used in the `inputs` field of the dataset.** Therefore, we use `question` as the argument of the function here.

```python
from openai import OpenAI

client = OpenAI()

@mlflow.trace
def predict_fn(question: str) -> str:
    prompt = mlflow.genai.load_prompt("prompts:/qa_prompt/latest")
    rendered_prompt = prompt.format(question=question)

    response = client.chat.completions.create(model="gpt-4.1-mini", messages=rendered_prompt)
    return response.choices[0].message.content
```

### Step 4: Define task-specific scorers

Finally, let's define a few [scorers](/genai/eval-monitor/scorers) that decides the evaluation criteria. Here we use two types of scorers:
- Built-in LLM scorers for evaluating the qualitative aspects of the response.
- Custom heuristic scorer for evaluating the coverage of the key concepts.

```python
from mlflow.entities import Feedback
from mlflow.genai import scorer
from mlflow.genai.scorers import Guidelines

# Define LLM scorers
is_concise = Guidelines(name="is_concise", guidelines="The response should be concise and to the point.")
is_professional = Guidelines(name="is_professional", guidelines="The response should be in professional tone.")

# Evaluate the coverage of the key concepts using custom scorer
@scorer
def concept_coverage(outputs: str, expectations: dict) -> Feedback:
    concepts = set(expectations.get("key_concepts", []))
    included = {c for c in concepts if c.lower() in outputs.lower()}
    return Feedback(
        value=len(included) / len(concepts),
        rationale=(f"Included {len(included)} out of {len(concepts)} concepts. Missing: {concepts - included}"
        )
    )
```

:::tip

LLM scorers uses OpenAI's GPT 4.1-mini by default. You can use different models by passing the `model` parameter to the scorer constructor.

:::

### Step 5: Run evaluation

Now we are ready to run the evaluation!

```python
mlflow.genai.evaluate(
    data=eval_dataset,
    predict_fn=predict_fn,
    scorers=[is_concise, is_professional, concept_coverage]
)
```

Once the evaluation is done, open the MLflow UI in your browser and navigate to the experiment page. You should see MLflow creates a new Run and logs the evaluation results.

<ImageBox src="/images/mlflow-3/eval-monitor/prompt-evaluation-results.png" alt="Prompt Evaluation" width="95%"/>

By clicking on the each row in the result, you can open the trace and see the detailed score and rationale.

<ImageBox src="/images/mlflow-3/eval-monitor/prompt-evaluation-trace.png" alt="Prompt Evaluation" width="95%"/>




## Iterating on Prompts

The prompt evaluation is a iterative process. You can register a new prompt version, run the same eval again, and compare the evaluation results. The prompt registry keep track of the version changes and lineage between the prompt versions and evaluation results.

```python
# Define V2 prompt template
PROMPT_V2 = [
    {
        "role": "system",
        "content": "You are a helpful assistant. Answer the following question in three sentences."
    },
    {
        "role": "user",
        "content": "Question: {{question}}"
    },
]

mlflow.genai.register_prompt(name="qa_prompt", template=PROMPT_V2)

# Run the same evaluation again.
# MLflow automatically loads the latest prompt template via the `@latest` alias.
mlflow.genai.evaluate(
    data=eval_dataset,
    predict_fn=predict_fn,
    scorers=[is_concise, is_professional, concept_coverage]
)
```

<ImageBox src="/images/mlflow-3/eval-monitor/prompt-evaluation-compare.png" alt="Prompt Evaluation" width="95%"/>

## Next steps

<TilesGrid>
  <TileCard
    icon={Target}
    iconSize={48}
    title="Customize Scorers"
    description="Build specialized evaluation metrics for your specific use cases and requirements."
    href="/genai/eval-monitor/scorers/custom"
    linkText="Learn about custom scorers →"
    containerHeight={64}
  />
  <TileCard
    icon={Bot}
    iconSize={48}
    title="Evaluate Agents"
    description="Evaluate complex AI agents with tool calling and multi-step workflows."
    href="/genai/eval-monitor/running-evaluation/agents"
    linkText="Evaluate agents →"
    containerHeight={64}
  />
  <TileCard
    icon={Zap}
    iconSize={48}
    title="Optimize Prompts"
    description="Use automated optimization techniques to systematically improve your prompts."
    href="/genai/prompt-registry/optimize-prompts"
    linkText="Optimize prompts →"
    containerHeight={64}
  />
</TilesGrid>

