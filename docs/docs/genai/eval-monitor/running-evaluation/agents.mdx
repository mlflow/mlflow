import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { APILink } from "@site/src/components/APILink";
import ImageBox from '@site/src/components/ImageBox';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Evaluating Agents

Evaluate the performance and reliability of AI agents that use tools, make decisions, and execute multi-step workflows. This guide demonstrates comprehensive agent evaluation using MLflow.

## Key Concepts

- Observability: Agent execution is a complex multi-step workflow, so we need to evaluate the agent's behavior precisely, not only the final output.
- Scorers: 
- Parallelization: Concurrency is a key for evaluating complex agent workflow effectively. MLflow 

## Example: Evaluating a Tool-Calling Agent

```bash
pip install 'mlflow>=3.3' openai-agents -qU
```

### Step 1: Build an agent

Create a math agent that can use tools to answer questions:

```python
from agents import Agent, Runner, function_tool
from agents.agent import StopAtTools


@function_tool
def add(a: float, b: float) -> float:
    """Adds two numbers."""
    return a + b

@function_tool
def multiply(a: float, b: float) -> float:
    """Multiply two numbers."""
    return a * b

@function_tool
def modular(a: int, b: int) -> int:
    """Modular arithmetic"""
    return a % b

agent = Agent(
    name="Math Agent",
    instructions=(
        "You will be given a math question. Calculate the answer using the given calculator tools. "
        "Return the final number only as an integer."
    ),
    tools=[add, multiply, modular],
)
```

Make sure you can run the agent locally.

```python
from agents import Runner

result = await Runner.run(agent, "What is 15% of 240?")
print(result.final_output)
# 36
```

### Step 2: Create evaluation dataset

Design test cases as a list of dictionaries, each with an `inputs` and `expectations` field. We would like to evaluate the correctness of the output, but also the tool calls used by the agent.

```python
eval_dataset = [
    {
        "inputs": {"task": "What is 15% of 240?"},
        "expectations": {
            "answer": 36,
            "tool_calls": ["multiply"]
        }
    },
    {
        "inputs": {"task": "I have 8 cookies and 3 friends. How many more cookies should I buy to share equally?"},
        "expectations": {
            "answer": 1,
            "tool_calls": ["modular", "add"]
        }
    },
    {
        "inputs": {"task": "I bought 2 shares of stock at $100 each. It's now worth $150. How much profit did I make?"},
        "expectations": {
            "answer": 100,
            "tool_calls": ["add", "multiply"]
        }
    },
]
```

### Step 3: Define agent-specific scorers

Create scorers that evaluate agent-specific behaviors.

:::tip

MLflow's scorer can take the **Trace** from the agent execution. Trace is a powerful way to evaluate the agent's behavior precisely, not only the final output. For example, here we use the <APILink fn="mlflow.entities.Trace.search_spans">`Trace.search_spans`</APILink> method to extract the order of tool calls and compare it with the expected tool calls.

For more details, see the <ins>[Evaluate Traces](/genai/eval-monitor/running-evaluation/traces)</ins> guide.

:::

```python
from mlflow.entities import Feedback, SpanType, Trace
from mlflow.genai import scorer


@scorer
def exact_match(outputs, expectations) -> bool:
    return int(outputs) == expectations["answer"]

@scorer
def uses_correct_tools(trace: Trace, expectations: dict) -> Feedback:
    """Evaluate if agent used tools appropriately"""
    expected_tools = expectations["tool_calls"]

    # Parse the trace to get the actual tool calls
    tool_spans = trace.search_spans(span_type=SpanType.TOOL)
    tool_names = [span.name for span in tool_spans]

    score = "yes" if tool_names == expected_tools else "no"
    rationale = (
        "The agent used the correct tools."
        if tool_names == expected_tools
        else f"The agent used the incorrect tools: {tool_names}"
    )
    # Return a Feedback object with the score and rationale
    return Feedback(value=score, rationale=rationale)
```

### Step 4: Run the evaluation

Evaluate the agent with comprehensive metrics:

```python
mlflow.genai.evaluate(
    data=eval_dataset,
    predict_fn=predict_fn,
    scorers=[exact_match, uses_correct_tools]
)
```

## Configure parallelization



## Evaluating MLflow Models



## Evaluating Deployed Endpoints


## Next steps

- [Custom Scorers](/genai/eval-monitor/scorers/custom): Build advanced evaluation criteria for agents
- [Trace Evaluation](/genai/eval-monitor/running-evaluation/traces): Analyze detailed agent execution traces
- [LLM Judge](/genai/eval-monitor/scorers/llm-judge): Use LLMs to evaluate complex agent behaviors