import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { APILink } from "@site/src/components/APILink";
import ImageBox from '@site/src/components/ImageBox';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Evaluating Agents

Evaluate the performance and reliability of AI agents that use tools, make decisions, and execute multi-step workflows. This guide demonstrates comprehensive agent evaluation using MLflow.

<table style={{ width: "100%" }}>
  <thead>
    <tr>
      <th style={{ width: "30%" }}>Component</th>
      <th style={{ width: "70%" }}>What we use in this guide</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Dataset</strong><br/><small>Inputs &amp; expectations</small></td>
      <td>
        Task descriptions with expected outcomes
      </td>
    </tr>
    <tr>
      <td><strong>Scorer</strong><br/><small>Evaluation criteria</small></td>
      <td>
        Built-in scorers (Correctness, Guidelines) and agent-specific custom scorers
      </td>
    </tr>
    <tr>
      <td><strong>Predict Function</strong><br/><small></small></td>
      <td>
        Agent implementation with tool calling capabilities
      </td>
    </tr>
  </tbody>
</table>

## Prerequisites

```bash
pip install 'mlflow>=3.3' openai-agents -qU
```

## Step 1: Build an agent

Create a math agent that can use tools to answer questions:

```python
from agents import Agent, Runner, function_tool
from agents.agent import StopAtTools


@function_tool
def add(a: float, b: float) -> float:
    """Adds two numbers."""
    return a + b

@function_tool
def multiply(a: float, b: float) -> float:
    """Multiply two numbers."""
    return a * b

@function_tool
def modular(a: int, b: int) -> int:
    """Modular arithmetic"""
    return a % b

agent = Agent(
    name="Math Agent",
    instructions=(
        "You will be given a math question. Calculate the answer using the given calculator tools. "
        "Return the final number only as an integer."
    ),
    tools=[add, multiply, modular],
)
```

Make sure you can run the agent locally.

```python
from agents import Runner

result = await Runner.run(agent, "What is 15% of 240?")
print(result.final_output)
# 36
```

## Step 2: Create evaluation dataset

Design test cases as a list of dictionaries, each with an `inputs` and `expectations` field. We would like to evaluate the correctness of the output, but also the tool calls used by the agent.

```python
eval_dataset = [
    {
        "inputs": {"task": "What is 15% of 240?"},
        "expectations": {
            "answer": 36,
            "tool_calls": ["multiply"]
        }
    },
    {
        "inputs": {"task": "I have 8 cookies and 3 friends. How many more cookies should I buy to share equally?"},
        "expectations": {
            "answer": 1,
            "tool_calls": ["modular", "add"]
        }
    },
    {
        "inputs": {"task": "I bought 2 shares of stock at $100 each. It's now worth $150. How much profit did I make?"},
        "expectations": {
            "answer": 100,
            "tool_calls": ["add", "multiply"]
        }
    },
]
```

## Step 4: Define agent-specific scorers

Create scorers that evaluate agent-specific behaviors.

:::tip

MLflow's scorer can evaluate the **trace** from the agent execution. Trace is a powerful way to evaluate the agent's behavior precisely, not only the final output. For example, here we use the <APILink>Trace.search_spans</APILink> method to extract the order of tool calls and compare it with the expected tool calls.

:::

```python
from mlflow.entities import Feedback, SpanType, Trace
from mlflow.genai import scorer


@scorer
def exact_match(outputs, expectations) -> bool:
    return int(outputs) == expectations["answer"]

@scorer
def uses_correct_tools(trace: Trace, expectations: dict) -> bool:
    """Evaluate if agent used tools appropriately"""
    expected_tools = expectations["tool_calls"]

    # Parse the trace to get the actual tool calls
    tool_spans = trace.search_spans(span_type=SpanType.TOOL)
    tool_names = [span.name for span in tool_spans]

    score = "yes" if tool_names == expected_tools else "no"
    rationale = (
        "The agent used the correct tools."
        if tool_names == expected_tools
        else f"The agent used the incorrect tools: {tool_names}"
    )
    # Return a Feedback object with the score and rationale
    return Feedback(value=score, rationale=rationale)
```

## Step 5: Run the evaluation

Evaluate the agent with comprehensive metrics:

```python
mlflow.genai.evaluate(
    data=eval_dataset,
    predict_fn=predict_fn,
    scorers=[exact_match, uses_correct_tools]
)
```


## Next steps

- [Custom Scorers](/genai/eval-monitor/scorers/custom): Build advanced evaluation criteria for agents
- [Trace Evaluation](/genai/eval-monitor/running-evaluation/traces): Analyze detailed agent execution traces
- [LLM Judge](/genai/eval-monitor/scorers/llm-judge): Use LLMs to evaluate complex agent behaviors