import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Evaluating a Version of Your App

Systematically evaluate new versions of your GenAI application to verify improvements, prevent regressions, and identify further optimization opportunities. Whether you're testing a new prompt, switching models, or adjusting parameters, proper evaluation ensures your changes actually improve performance rather than introducing unexpected issues.

To run an evaluation, you need three key components:

1. **Evaluation dataset** - Test inputs (user queries) and optionally expectations for corresponding outputs
2. **Target function** - Your application to evaluate  
3. **Scorers** - Quality metrics that compute scores on your target function's outputs

Given these components, you can evaluate your application using MLflow's evaluation harness: `mlflow.evaluate()`.

## Prerequisites

Before starting, ensure your environment is properly configured for evaluation:

```bash
# Install required packages
pip install --upgrade "mlflow>=3.1.0" openai

# Set up MLflow tracking
export MLFLOW_TRACKING_URI="your-tracking-server-uri"
```

## Evaluation Process Overview

The evaluation process follows a clear data flow that transforms your test cases into actionable quality insights:

1. **Input Data** → Your evaluation dataset with test cases
2. **Target Function** → Your instrumented GenAI application  
3. **Execution** → MLflow calls your function for each test case
4. **Scoring** → Scorers evaluate the outputs for quality
5. **Results** → Comprehensive metrics and traces for analysis

This systematic approach ensures consistent, reproducible evaluation results that you can trust for making development decisions.

## Step 1: Create Your Target Function

MLflow evaluations require a Python function that represents your application. This function must be instrumented with MLflow Tracing to capture execution details during evaluation, providing visibility into how your application behaves for each test case.

We'll create an app that generates personalized sales emails based on customer data:

```python
import os
import mlflow
from openai import OpenAI

# Enable automatic tracing for all OpenAI API calls
mlflow.openai.autolog()

# Initialize OpenAI client
client = OpenAI(api_key="your-openai-api-key")

# @mlflow.trace decorator creates a span for this function during evaluation
@mlflow.trace
def my_app(customer_name: str, last_conversation_details: str, known_issues: str) -> dict:
    system_prompt = """Please generate an email for a sales rep to send based on the provided data"""

    user_message = f"""Customer name: {customer_name}
Details of the last conversation: {last_conversation_details}
Known support issues: {known_issues}"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
    )
    return {"email": response.choices[0].message.content}

# Test the predict_fn
output = my_app(
    customer_name="Acme Corporation",
    last_conversation_details="Met with CTO Sarah Johnson who expressed interest in our enterprise security features. She mentioned budget approval in Q3.",
    known_issues="Their IT team reported slow response times with our API during peak hours."
)
print(output)
```

**Key Points**:
- The `@mlflow.trace` decorator is essential for capturing execution details
- Your function signature must match the input keys in your evaluation dataset
- Return structured outputs (like dictionaries) for easier scorer analysis

## Step 2: Prepare Your Evaluation Dataset

Define test cases that represent realistic user scenarios your application will encounter in production. Each test case comprises an `inputs` dictionary with the arguments that will be passed to your function. Quality evaluation datasets capture the diversity and complexity of real-world usage patterns.

```python
eval_data = [
    {
        "inputs": {
            "customer_name": "Acme Corporation",
            "last_conversation_details": "Met with CTO Sarah Johnson who expressed interest in our enterprise security features. She mentioned budget approval in Q3.",
            "known_issues": "Their IT team reported slow response times with our API during peak hours.",
        }
    },
    {
        "inputs": {
            "customer_name": "TechNova Solutions", 
            "last_conversation_details": "Product demo with their engineering team last Thursday. They were particularly impressed with the scalability options.",
            "known_issues": "Integration with their legacy systems has been challenging. Support ticket #4532 is still open.",
        }
    },
    {
        "inputs": {
            "customer_name": "Global Retail Partners",
            "last_conversation_details": "Conference call with procurement director Marcus Lee who requested pricing for 500+ user licenses.",
            "known_issues": "Mobile app crashes reported by several users in their organization after our latest update.",
        }
    },
    {
        "inputs": {
            "customer_name": "Healthcare Innovations",
            "last_conversation_details": "On-site meeting with their compliance team regarding HIPAA requirements and our data protection measures.",
            "known_issues": "Concerns about data migration timeline and potential service interruptions.",
        }
    },
    {
        "inputs": {
            "customer_name": "EduTech Systems",
            "last_conversation_details": "Virtual presentation to their board about our new learning management features. Decision maker Dr. Williams asked for case studies.",
            "known_issues": "SSO implementation has been problematic for their IT department.",
        }
    },
]
```

**Dataset Design Tips**:
- Include edge cases and challenging scenarios, not just happy path examples
- Cover different input types and lengths to test robustness
- Consider creating versioned evaluation datasets to track test data over time

## Step 3: Configure Your Scorers

Scorers evaluate different aspects of your application's quality. Combining multiple scorer types provides comprehensive quality assessment across various dimensions. Each scorer type serves a different purpose in understanding your application's performance.

<Tabs>
<TabItem value="basic_scorers" label="Basic & Predefined Scorers">

```python
from mlflow.metrics import exact_match

# Predefined scorer for basic evaluation
basic_scorer = exact_match()

# This scorer is useful when you have expected outputs to compare against
# It provides a baseline measurement of output consistency
```

Predefined scorers offer standardized metrics that are commonly used across ML applications. They're particularly valuable for establishing baselines and comparing against industry standards.

</TabItem>
<TabItem value="custom_deterministic" label="Custom Deterministic Scorers">

```python
from mlflow.genai.scorers import scorer

@scorer
def company_name_in_email(inputs, outputs):
    """Check if the company name is included in the email."""
    return inputs["customer_name"] in outputs["email"]

@scorer
def acknowledges_issues(inputs, outputs):
    """Check if known issues are acknowledged in the email."""
    if not inputs.get("known_issues"):
        return True  # No issues to acknowledge
    
    email_content = outputs["email"].lower()
    issue_keywords = ["issue", "problem", "concern", "challenge", "support"]
    
    return any(keyword in email_content for keyword in issue_keywords)
```

Deterministic scorers check for specific business logic requirements. They provide binary or precise measurements of whether your application meets defined criteria.

</TabItem>
<TabItem value="custom_heuristic" label="Custom Heuristic Scorers">

```python
@scorer
def professional_tone_check(inputs, outputs):
    """Check if the email maintains a professional tone."""
    email_content = outputs["email"].lower()
    
    # Check for professional indicators
    professional_words = ["thank you", "please", "sincerely", "best regards", "appreciate"]
    unprofessional_words = ["hey", "gonna", "wanna", "sup"]
    
    professional_count = sum(1 for word in professional_words if word in email_content)
    unprofessional_count = sum(1 for word in unprofessional_words if word in email_content)
    
    # Return boolean indicating professional tone
    return professional_count > 0 and unprofessional_count == 0

@scorer
def response_completeness(inputs, outputs):
    """Check if the response addresses all key elements."""
    email = outputs["email"]
    
    # Check for key components of a good sales email
    has_greeting = any(word in email.lower() for word in ["dear", "hello", "hi"])
    has_closing = any(word in email.lower() for word in ["sincerely", "regards", "best"])
    has_contact_info = any(word in email.lower() for word in ["contact", "reach", "call"])
    mentions_company = inputs["customer_name"].lower() in email.lower()
    
    score = sum([has_greeting, has_closing, has_contact_info, mentions_company]) / 4
    return score
```

Heuristic scorers use rule-based logic to assess quality dimensions that are difficult to measure directly. They're especially useful for domain-specific requirements and style guidelines.

</TabItem>
</Tabs>

## Step 4: Execute the Evaluation

Run the evaluation to generate quality scores and detailed traces for each test case. MLflow orchestrates the entire process, calling your function for each dataset row and applying all scorers to the results. This creates a comprehensive quality assessment with detailed execution traces.

```python
# MLflow will compute and record metrics on quality, token usage, and latency
evaluation_result = mlflow.evaluate(
    data=eval_data,
    model=my_app,
    extra_metrics=[
        company_name_in_email, 
        professional_tone_check, 
        acknowledges_issues,
        response_completeness,
        basic_scorer
    ],
)

print("Evaluation completed!")
print(f"Run ID: {evaluation_result.run_id}")
```

**What Happens During Evaluation**:
- MLflow calls your function for each test case in the dataset
- All specified scorers are applied to each output
- Execution traces are captured for debugging and analysis
- Results are aggregated into comprehensive metrics and detailed tables

## Step 5: Analyze Evaluation Results

Review the evaluation results to understand your application's quality and identify improvement opportunities. The analysis phase is crucial for translating raw scores into actionable insights that guide your development decisions.

### Accessing Results Programmatically

```python
# Access evaluation metrics
print("Evaluation Metrics:")
for metric_name, metric_value in evaluation_result.metrics.items():
    print(f"  {metric_name}: {metric_value}")

# Access detailed results DataFrame
results_df = evaluation_result.tables["eval_results_table"]
print("\nDetailed Results:")
print(results_df.head())

# Access individual traces for debugging
print(f"\nView detailed results in MLflow UI:")
print(f"Run: {evaluation_result.run_id}")
```

### Using the MLflow UI

The MLflow UI provides rich visualization and analysis tools for your evaluation results. Navigate to your MLflow tracking server to explore:

- **Overall metric summaries** showing aggregate performance across all test cases
- **Per-example scores and outputs** for detailed analysis of individual responses
- **Execution traces with timing information** to understand performance characteristics
- **Scorer rationales and feedback** to understand why specific scores were assigned

This visual analysis complements the programmatic approach and is especially valuable for qualitative assessment of response quality.

## Advanced Evaluation Patterns

### Working with Expected Outputs

When you have reference outputs or expected responses, you can include them in your evaluation dataset for more sophisticated comparison:

<Tabs>
<TabItem value="with_targets" label="Using Expected Outputs">

```python
# Dataset with expected outputs
eval_data_with_expectations = [
    {
        "inputs": {
            "customer_name": "Acme Corporation",
            "last_conversation_details": "Interested in enterprise features",
            "known_issues": "API performance issues"
        },
        "targets": """Dear Acme Corporation team,

Thank you for your interest in our enterprise security features. Based on our conversation with CTO Sarah Johnson, I understand you're planning budget approval in Q3.

I wanted to follow up on the API performance concerns your IT team has experienced. Our engineering team has identified optimizations that should significantly improve response times during peak hours.

I'd be happy to schedule a technical deep-dive with your team to discuss these improvements and demonstrate how they align with your Q3 implementation timeline.

Best regards,
[Sales Rep Name]"""
    }
]

# Evaluate with target comparison
evaluation_result = mlflow.evaluate(
    data=eval_data_with_expectations,
    model=my_app,
    targets="targets",  # Column name containing expected outputs
    extra_metrics=[exact_match(), company_name_in_email],
)
```

</TabItem>
<TabItem value="semantic_similarity" label="Semantic Similarity Scoring">

```python
from mlflow.metrics.genai import answer_similarity

# Use LLM-based semantic similarity when exact match is too strict
evaluation_result = mlflow.evaluate(
    data=eval_data_with_expectations,
    model=my_app,
    targets="targets",
    extra_metrics=[
        answer_similarity(model="openai:/gpt-4o-mini"),
        company_name_in_email,
        professional_tone_check
    ],
)
```

</TabItem>
</Tabs>

### Batch Evaluation for Version Comparison

When comparing multiple versions of your application, use a structured approach to ensure consistent evaluation conditions:

```python
def evaluate_app_version(app_function, version_name):
    """Evaluate a specific version of the app with consistent methodology."""
    with mlflow.start_run(run_name=f"evaluation_{version_name}"):
        result = mlflow.evaluate(
            data=eval_data,
            model=app_function,
            extra_metrics=[
                company_name_in_email,
                professional_tone_check,
                acknowledges_issues,
                response_completeness
            ],
        )
        
        # Log version metadata for comparison
        mlflow.set_tag("app_version", version_name)
        mlflow.set_tag("evaluation_type", "version_comparison")
        mlflow.log_param("model_used", "gpt-4o-mini")  # Track model consistency
        
        return result

# Evaluate multiple versions
# v1_results = evaluate_app_version(my_app_v1, "v1.0")
# v2_results = evaluate_app_version(my_app_v2, "v2.0")
```

## Best Practices

### Evaluation Design

**Representative Test Cases**: Ensure your evaluation dataset covers the full spectrum of user scenarios, including edge cases and challenging inputs. A diverse dataset provides more reliable quality assessment.

**Balanced Scoring**: Use multiple scorer types (deterministic, heuristic, semantic) for comprehensive assessment. No single metric captures all aspects of quality, so combining different approaches gives you a complete picture.

**Consistent Testing**: Use the same evaluation dataset across app versions for reliable comparisons. Changing the dataset makes it impossible to determine if performance differences are due to app improvements or test variations.

### Performance Optimization

**Scorer Efficiency**: Design custom scorers to be computationally efficient, especially for large evaluation datasets. Simple heuristic checks should run quickly, while more complex analysis can be reserved for critical quality dimensions.

**Resource Management**: Monitor token usage and API costs during evaluation, particularly when using LLM-based scorers. Consider sampling strategies for expensive evaluations on large datasets.

### Results Analysis

**Failure Analysis**: When scores are low, examine the specific test cases and outputs to understand root causes. This qualitative analysis often reveals patterns that aggregate metrics miss.

**Trend Tracking**: Monitor metric trends across app versions over time to understand your development trajectory and identify when changes help or hurt performance.

## Troubleshooting Common Issues

<Tabs>
<TabItem value="function_issues" label="Function Signature Problems">

```python
# ❌ Function signature doesn't match dataset
@mlflow.trace
def my_app(wrong_param_name):  # Dataset has 'customer_name', not 'wrong_param_name'
    pass

# ✅ Correct signature matching dataset inputs
@mlflow.trace
def my_app(customer_name, last_conversation_details, known_issues):
    # Your app logic here
    pass
```

**Solution**: Ensure your function parameters exactly match the keys in your dataset's `inputs` dictionaries.

</TabItem>
<TabItem value="scorer_issues" label="Scorer Errors">

```python
@scorer
def robust_scorer(inputs, outputs):
    """Example of robust scorer with error handling."""
    try:
        # Your scoring logic
        if "email" not in outputs:
            return False  # Handle missing output gracefully
        
        email_content = outputs["email"]
        # Perform your checks
        return email_content is not None and len(email_content) > 0
        
    except Exception as e:
        # Log the error for debugging
        print(f"Scorer error: {e}")
        return None  # Return None to indicate scorer failure
```

**Solution**: Always include error handling in custom scorers to prevent evaluation failures from individual scorer issues.

</TabItem>
<TabItem value="tracing_issues" label="Tracing Configuration">

```python
# Ensure MLflow tracing is properly configured
import mlflow

# Enable autologging for your LLM provider
mlflow.openai.autolog()  # For OpenAI
# mlflow.langchain.autolog()  # For LangChain if using

# Use @mlflow.trace decorator on your main function
@mlflow.trace
def my_app(inputs):
    # Your app logic - all LLM calls will be automatically traced
    pass
```

**Solution**: Verify that autologging is enabled for your LLM provider and that your main function uses the `@mlflow.trace` decorator.

</TabItem>
</Tabs>

## Complete Reference Example

Here's the complete working example that ties together all the concepts:

```python
import os
import mlflow
from openai import OpenAI
from mlflow.metrics import exact_match
from mlflow.genai.scorers import scorer

# Enable automatic tracing for all OpenAI API calls
mlflow.openai.autolog()

# Initialize OpenAI client
client = OpenAI(api_key="your-openai-api-key")

@mlflow.trace
def my_app(customer_name: str, last_conversation_details: str, known_issues: str):
    system_prompt = """Please generate an email for a sales rep to send based on the provided data"""

    user_message = f"""Customer name: {customer_name}
Details of the last conversation: {last_conversation_details}
Known support issues: {known_issues}"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
    )
    return {"email": response.choices[0].message.content}

# Evaluation dataset
eval_data = [
    {
        "inputs": {
            "customer_name": "Acme Corporation",
            "last_conversation_details": "Met with CTO Sarah Johnson who expressed interest in our enterprise security features. She mentioned budget approval in Q3.",
            "known_issues": "Their IT team reported slow response times with our API during peak hours.",
        }
    },
    {
        "inputs": {
            "customer_name": "TechNova Solutions",
            "last_conversation_details": "Product demo with their engineering team last Thursday. They were particularly impressed with the scalability options.",
            "known_issues": "Integration with their legacy systems has been challenging. Support ticket #4532 is still open.",
        }
    },
    # Add more test cases as needed...
]

# Define scorers
@scorer
def company_name_in_email(inputs, outputs):
    """Check if the company name is included in the email."""
    return inputs["customer_name"] in outputs["email"]

@scorer
def professional_tone_check(inputs, outputs):
    """Check if the email maintains a professional tone."""
    email_content = outputs["email"].lower()
    professional_words = ["thank you", "please", "sincerely", "best regards"]
    return any(word in email_content for word in professional_words)

@scorer
def acknowledges_issues(inputs, outputs):
    """Check if known issues are acknowledged in the email."""
    if not inputs.get("known_issues"):
        return True
    
    email_content = outputs["email"].lower()
    issue_keywords = ["issue", "problem", "concern", "challenge"]
    return any(keyword in email_content for keyword in issue_keywords)

# Run evaluation
evaluation_result = mlflow.evaluate(
    data=eval_data,
    model=my_app,
    extra_metrics=[
        company_name_in_email,
        professional_tone_check, 
        acknowledges_issues,
        exact_match()
    ],
)

print("Evaluation completed!")
print(f"Run ID: {evaluation_result.run_id}")
print("View results in MLflow UI for detailed analysis")
```

## Next Steps

- Create [custom scorers](/genai/eval-monitor/custom-scorers) for your specific business logic and quality requirements
- Build comprehensive [evaluation datasets](/genai/eval-monitor/build-eval-dataset) from production traces and user scenarios  
- Learn to [compare evaluation results](/genai/eval-monitor/version-comparison) between different app versions
- Set up [production monitoring](/genai/eval-monitor/production-monitoring) to continuously track quality in live environments

You've successfully evaluated your GenAI application! Use these results to identify improvement opportunities, prevent quality regressions, and make data-driven development decisions.