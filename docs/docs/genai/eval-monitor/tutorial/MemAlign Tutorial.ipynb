{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MemAlign: Aligning LLM Judges with Human Feedback\n",
    "This notebook demonstrates how to use MemAlign to align an LLM judge with human preferences.\n",
    "\n",
    "MemAlign uses a dual-memory system:\n",
    "\n",
    "Semantic Memory: Distills general guidelines from human feedback patterns\n",
    "Episodic Memory: Retrieves similar past examples using embeddings for few-shot learning\n",
    "## What you'll learn:\n",
    "1. Create an LLM judge for evaluating responses\n",
    "2. Prepare alignment and test datasets with edge cases\n",
    "3. Evaluate the judge before alignment (baseline)\n",
    "4. Align the judge using human feedback\n",
    "5. Evaluate the improved judge (post-alignment)\n",
    "6. Inspect the learned guidelines\n",
    "7. Unalign (remove) specific feedback from the judge\n",
    "8. Register the judge as a scorer for future experiments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "First, let's import the required modules and set up the environment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install --upgrade mlflow>=3.9.0 openai dspy",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "from mlflow.genai.judges.optimizers import MemAlignOptimizer\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import time\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up your provider and model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:27:29.891152Z",
     "start_time": "2026-01-29T07:27:29.829778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For example, to use OpenAI API, uncomment the following lines and comment out Option 1 above:\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # TODO: set your OpenAI API key\n",
    "mlflow.set_tracking_uri(\"your mlflow tracking uri\")\n",
    "mlflow.set_registry_uri(\"your mlflow registry uri\")\n",
    "experiment_name = \"memalign-demo\"\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment.experiment_id"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/28 23:27:29 INFO mlflow.tracking.fluent: Experiment with name 'memalign-demo' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Create an LLM Judge\n",
    "We'll create a judge that evaluates whether customer service responses are helpful."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:28:39.514231Z",
     "start_time": "2026-01-29T07:28:39.511627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "JUDGE_NAME = \"helpfulness\"\n",
    "\n",
    "initial_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"Evaluate whether the customer support bot’s response is helpful \"\n",
    "        \"given the user query.\\n\\n\"\n",
    "        \"User query: {{ inputs }}\\n\"\n",
    "        \"Assistant response: {{ outputs }}\\n\"\n",
    "    ),\n",
    "    feedback_value_type=bool,\n",
    "    model=\"openai:/gpt-5.2\",\n",
    ")\n",
    "\n",
    "print(f\"Created judge: {initial_judge.name}\")\n",
    "print(f\"Model: {initial_judge.model}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created judge: helpfulness\n",
      "Model: openai:/gpt-5.2\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Create Toy Datasets\n",
    "We'll create two datasets:\n",
    "\n",
    "1. **Alignment set** (5 examples): Used to teach the judge our preferences\n",
    "2. **Test set** (5 examples): Used to evaluate the judge's performance\n",
    "## The tricky case: Factually correct but emotionally cold\n",
    "LLM judges often rate **factually correct responses as helpful**, even when they lack empathy. But in customer service, a cold transactional response to a frustrated user is unhelpful - it should acknowledge emotions first."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:27:36.275334Z",
     "start_time": "2026-01-29T07:27:36.270587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alignment dataset - 4 easy examples + 1 tricky case\n",
    "alignment_examples = [\n",
    "    {\n",
    "        \"inputs\": \"What are your store hours?\",\n",
    "        \"outputs\": \"We're open Monday to Friday, 9am to 6pm.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Direct, accurate answer to a simple question.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"Thanks for your help!\",\n",
    "        \"outputs\": \"You're welcome! Let me know if you need anything else.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Warm, friendly acknowledgment.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"Can you help me track my order?\",\n",
    "        \"outputs\": \"Figure it out yourself.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Rude and dismissive.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"I have a question about returns.\",\n",
    "        \"outputs\": \"Whatever.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Dismissive and unprofessional.\",\n",
    "    },\n",
    "    # Tricky: Factually correct with solution, but lacks empathy\n",
    "    {\n",
    "        \"inputs\": \"The sweater I ordered looks completely different from what was shown on the website.\",\n",
    "        \"outputs\": \"Product colors may vary slightly due to lighting and display settings. You can initiate a return through your order history if needed.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Response provides factual explanation and offers a solution (return option), \"\n",
    "                     \"but fails to acknowledge the customer's disappointment. Should start with \"\n",
    "                     \"'I'm sorry the product didn't meet your expectations' before explaining.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(alignment_examples)} alignment examples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 alignment examples\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:27:37.886569Z",
     "start_time": "2026-01-29T07:27:37.882852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test dataset - 4 easy examples + 1 tricky case\n",
    "test_examples = [\n",
    "    {\n",
    "        \"inputs\": \"Do you offer gift wrapping?\",\n",
    "        \"outputs\": \"Yes! You can select gift wrapping at checkout for $3.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Helpful and informative.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"I love your product!\",\n",
    "        \"outputs\": \"Thank you so much! We're glad you're enjoying it.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Warm acknowledgment of positive feedback.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"How do I cancel my subscription?\",\n",
    "        \"outputs\": \"Why would you want to do that? That's stupid.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Insulting and unprofessional.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"Is this item in stock?\",\n",
    "        \"outputs\": \"I don't care.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Rude and unhelpful.\",\n",
    "    },\n",
    "    # Tricky: Factually correct but lacks empathy for frustrated user\n",
    "    {\n",
    "        \"inputs\": \"I've been charged twice for my subscription this month. This is really frustrating!\",\n",
    "        \"outputs\": \"We see two charges on your account because you updated your payment method. \"\n",
    "                   \"One charge will be reversed automatically within 5-7 business days.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Factually correct but too cold and transactional. \"\n",
    "                     \"Should start with empathy (e.g., 'Sorry for the confusion') and end with \"\n",
    "                     \"support-oriented language when responding to a frustrated customer.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_examples)} test examples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 test examples\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Create Traces and Log Human Feedback\n",
    "MemAlign learns from traces that have human feedback attached. We'll:\n",
    "\n",
    "1. Create traces for each example\n",
    "2. Log human feedback (ground truth) for alignment examples You can either log feedback programmatically (like below) or with the MLflow UI (see [here](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/alignment/#collecting-feedback-for-alignment))."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:27:42.016603Z",
     "start_time": "2026-01-29T07:27:39.543903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Create all traces first (separate from feedback logging)\n",
    "def create_traces(examples, prefix):\n",
    "    \"\"\"Create traces from examples.\"\"\"\n",
    "    trace_ids = []\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        with mlflow.start_span(f\"{prefix}_{i}\") as span:\n",
    "            span.set_inputs({\"inputs\": example[\"inputs\"]})\n",
    "            span.set_outputs({\"outputs\": example[\"outputs\"]})\n",
    "            trace_ids.append(span.trace_id)\n",
    "\n",
    "    return trace_ids\n",
    "\n",
    "# Create traces for alignment and test sets\n",
    "alignment_trace_ids = create_traces(alignment_examples, \"alignment\")\n",
    "print(f\"Created {len(alignment_trace_ids)} alignment traces\")\n",
    "\n",
    "test_trace_ids = create_traces(test_examples, \"test\")\n",
    "print(f\"Created {len(test_trace_ids)} test traces\")\n",
    "time.sleep(2)  # Ensure traces are committed before adding assessments"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 alignment traces\n",
      "Created 5 test traces\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Trace(trace_id=tr-5eb8755cf81e1efd8b877ea0f4ea9eef), Trace(trace_id=tr-102dc4a9f3938d1c88918825170dc323), Trace(trace_id=tr-6916f34c685b83443ade43846ed65c29), Trace(trace_id=tr-a17c7a58f276ce30b1ee1ccd2198312c), Trace(trace_id=tr-567a2812d28ec9cb80eec5a51f01abfa), Trace(trace_id=tr-e7339ea3448bc38e45d57055ccff7744), Trace(trace_id=tr-a8ef3ef80adf71c18aef6685ae036d06), Trace(trace_id=tr-886890c48d5991b2aed8f84f217119a4), Trace(trace_id=tr-cf7efd50d62ffab0fd0f5c7e82365258), Trace(trace_id=tr-215054648216a41ab12e08bbf536aa56)]"
      ],
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:60968/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-5eb8755cf81e1efd8b877ea0f4ea9eef&amp;experiment_id=1&amp;trace_id=tr-102dc4a9f3938d1c88918825170dc323&amp;experiment_id=1&amp;trace_id=tr-6916f34c685b83443ade43846ed65c29&amp;experiment_id=1&amp;trace_id=tr-a17c7a58f276ce30b1ee1ccd2198312c&amp;experiment_id=1&amp;trace_id=tr-567a2812d28ec9cb80eec5a51f01abfa&amp;experiment_id=1&amp;trace_id=tr-e7339ea3448bc38e45d57055ccff7744&amp;experiment_id=1&amp;trace_id=tr-a8ef3ef80adf71c18aef6685ae036d06&amp;experiment_id=1&amp;trace_id=tr-886890c48d5991b2aed8f84f217119a4&amp;experiment_id=1&amp;trace_id=tr-cf7efd50d62ffab0fd0f5c7e82365258&amp;experiment_id=1&amp;trace_id=tr-215054648216a41ab12e08bbf536aa56&amp;experiment_id=1&amp;version=3.9.1.dev0\"\n",
       "  />\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:27:56.950826Z",
     "start_time": "2026-01-29T07:27:56.918201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Log human feedback for alignment examples\n",
    "\n",
    "for i, (trace_id, example) in enumerate(zip(alignment_trace_ids, alignment_examples)):\n",
    "    mlflow.log_feedback(\n",
    "        trace_id=trace_id,\n",
    "        name=JUDGE_NAME,\n",
    "        value=example[\"is_helpful\"],\n",
    "        rationale=example[\"rationale\"],\n",
    "        source=AssessmentSource(\n",
    "            source_type=AssessmentSourceType.HUMAN,\n",
    "            source_id=\"human_expert\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(f\"Logged human feedback for {len(alignment_trace_ids)} alignment traces\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged human feedback for 5 alignment traces\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Evaluate Baseline Judge Performance\n",
    "Before alignment, let's see how the initial judge performs on both datasets. We expect the judge to make mistakes on edge cases like the tricky empathy examples.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:27:58.877166Z",
     "start_time": "2026-01-29T07:27:58.869965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_judge(judge, examples, dataset_name):\n",
    "    \"\"\"Evaluate judge on examples and compute accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating on {dataset_name} ({len(examples)} examples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        # Run judge\n",
    "        feedback = judge(\n",
    "            inputs=example[\"inputs\"],\n",
    "            outputs=example[\"outputs\"]\n",
    "        )\n",
    "\n",
    "        predicted = feedback.value\n",
    "        expected = example[\"is_helpful\"]\n",
    "        is_correct = predicted == expected\n",
    "\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        results.append({\n",
    "            \"example\": i + 1,\n",
    "            \"predicted\": predicted,\n",
    "            \"expected\": expected,\n",
    "            \"correct\": is_correct,\n",
    "            \"rationale\": feedback.rationale[:100] + \"...\" if len(feedback.rationale) > 100 else feedback.rationale,\n",
    "        })\n",
    "\n",
    "        # Print result\n",
    "        status = \"CORRECT\" if is_correct else \"WRONG\"\n",
    "        print(f\"\\nExample {i+1}: [{status}]\")\n",
    "        print(f\"  Input: {example['inputs'][:50]}...\")\n",
    "        print(f\"  Predicted: {predicted}, Expected: {expected}\")\n",
    "        if not is_correct:\n",
    "          print(f\"  Judge rationale: {feedback.rationale[:150]}...\")\n",
    "\n",
    "    accuracy = correct / len(examples) * 100\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Accuracy: {correct}/{len(examples)} ({accuracy:.1f}%)\")\n",
    "    print(f\"{'-'*60}\")\n",
    "\n",
    "    return accuracy, results"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:29:05.862654Z",
     "start_time": "2026-01-29T07:28:47.857381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate baseline on alignment set\n",
    "baseline_align_accuracy, baseline_align_results = evaluate_judge(\n",
    "    initial_judge, alignment_examples, \"Alignment Set (Baseline)\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating on Alignment Set (Baseline) (5 examples)\n",
      "============================================================\n",
      "\n",
      "Example 1: [CORRECT]\n",
      "  Input: What are your store hours?...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 2: [CORRECT]\n",
      "  Input: Thanks for your help!...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 3: [CORRECT]\n",
      "  Input: Can you help me track my order?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 4: [CORRECT]\n",
      "  Input: I have a question about returns....\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 5: [CORRECT]\n",
      "  Input: The sweater I ordered looks completely different f...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy: 5/5 (100.0%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:29:18.814435Z",
     "start_time": "2026-01-29T07:29:05.871581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate baseline on test set\n",
    "baseline_test_accuracy, baseline_test_results = evaluate_judge(\n",
    "    initial_judge, test_examples, \"Test Set (Baseline)\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating on Test Set (Baseline) (5 examples)\n",
      "============================================================\n",
      "\n",
      "Example 1: [CORRECT]\n",
      "  Input: Do you offer gift wrapping?...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 2: [CORRECT]\n",
      "  Input: I love your product!...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 3: [CORRECT]\n",
      "  Input: How do I cancel my subscription?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 4: [CORRECT]\n",
      "  Input: Is this item in stock?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 5: [WRONG]\n",
      "  Input: I've been charged twice for my subscription this m...\n",
      "  Predicted: True, Expected: False\n",
      "  Judge rationale: The response directly addresses the user’s issue (duplicate charges) with a plausible explanation (payment method update can trigger an authorization/...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy: 4/5 (80.0%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 5: Align the Judge with MemAlign\n",
    "Now we'll use MemAlign to align the judge with our human feedback.\n",
    "\n",
    "MemAlign will:\n",
    "\n",
    "1. **Distill guidelines** from the feedback rationales (semantic memory)\n",
    "2. **Store examples** for few-shot retrieval (episodic memory)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:29:55.566327Z",
     "start_time": "2026-01-29T07:29:55.563492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the MemAlign optimizer\n",
    "optimizer = MemAlignOptimizer(\n",
    "    reflection_lm=\"openai:/gpt-5.2\",  # Model for distilling guidelines\n",
    "    embedding_model=\"openai:/text-embedding-3-large\",  # Model for embeddings\n",
    "    retrieval_k=3,  # Number of similar examples to retrieve during evaluation\n",
    ")\n",
    "\n",
    "print(\"Created MemAlign optimizer\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MemAlign optimizer\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:31:32.249698Z",
     "start_time": "2026-01-29T07:31:32.206032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve traces with human feedback for alignment\n",
    "all_traces = mlflow.search_traces(\n",
    "    locations=[experiment_id],\n",
    "    return_type=\"list\",\n",
    ")\n",
    "\n",
    "alignment_traces = [\n",
    "    trace for trace in all_traces\n",
    "    if trace.info.trace_id in alignment_trace_ids\n",
    "]\n",
    "\n",
    "print(f\"Retrieved {len(alignment_traces)} traces for alignment\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 traces for alignment\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:30:03.113638Z",
     "start_time": "2026-01-29T07:29:55.993380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Align the judge\n",
    "aligned_judge = initial_judge.align(\n",
    "    traces=alignment_traces,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "print(f\"\\nAlignment complete!\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distilling guidelines: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 6: Inspect Learned Guidelines (Semantic Memory)\n",
    "Let's see what guidelines MemAlign distilled from our feedback."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:31:37.881049Z",
     "start_time": "2026-01-29T07:31:37.877550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# View the full instructions (original + distilled guidelines)\n",
    "print(\"\\nFull Judge Instructions (with guidelines)\")\n",
    "print(\"=\"*60)\n",
    "print(aligned_judge.instructions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Judge Instructions (with guidelines)\n",
      "============================================================\n",
      "Evaluate whether the customer support bot’s response is helpful given the user query.\n",
      "\n",
      "User query: {{ inputs }}\n",
      "Assistant response: {{ outputs }}\n",
      "\n",
      "\n",
      "Distilled Guidelines (4):\n",
      "  - When a customer is unhappy or disappointed, the bot should explicitly acknowledge and empathize (e.g., apologize) before moving into explanations or procedural solutions like returns.\n",
      "  - Responses must remain professional and respectful; dismissive one-word replies or telling the customer to solve it themselves are unacceptable.\n",
      "  - For simple informational queries, a concise, direct, accurate answer is considered helpful without extra elaboration.\n",
      "  - When the user expresses gratitude, a friendly, warm acknowledgment and an offer of further help is preferred.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 7: Evaluate Aligned Judge Performance\n",
    "Let's see how the aligned judge performs compared to the baseline."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:31:54.555707Z",
     "start_time": "2026-01-29T07:31:40.434082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate aligned judge on alignment set\n",
    "aligned_align_accuracy, aligned_align_results = evaluate_judge(\n",
    "    aligned_judge, alignment_examples, \"Alignment Set (Aligned)\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating on Alignment Set (Aligned) (5 examples)\n",
      "============================================================\n",
      "\n",
      "Example 1: [CORRECT]\n",
      "  Input: What are your store hours?...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 2: [CORRECT]\n",
      "  Input: Thanks for your help!...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 3: [CORRECT]\n",
      "  Input: Can you help me track my order?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 4: [CORRECT]\n",
      "  Input: I have a question about returns....\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 5: [CORRECT]\n",
      "  Input: The sweater I ordered looks completely different f...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy: 5/5 (100.0%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:08.904945Z",
     "start_time": "2026-01-29T07:31:54.561309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate aligned judge on test set\n",
    "aligned_test_accuracy, aligned_test_results = evaluate_judge(\n",
    "    aligned_judge, test_examples, \"Test Set (Aligned)\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating on Test Set (Aligned) (5 examples)\n",
      "============================================================\n",
      "\n",
      "Example 1: [CORRECT]\n",
      "  Input: Do you offer gift wrapping?...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 2: [CORRECT]\n",
      "  Input: I love your product!...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 3: [CORRECT]\n",
      "  Input: How do I cancel my subscription?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 4: [CORRECT]\n",
      "  Input: Is this item in stock?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 5: [CORRECT]\n",
      "  Input: I've been charged twice for my subscription this m...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy: 5/5 (100.0%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:08.943431Z",
     "start_time": "2026-01-29T07:32:08.940547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Dataset':<25} {'Baseline':<15} {'Aligned':<15} {'Improvement':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Alignment Set':<25} {baseline_align_accuracy:<15.1f} {aligned_align_accuracy:<15.1f} {aligned_align_accuracy - baseline_align_accuracy:+.1f}%\")\n",
    "print(f\"{'Test Set':<25} {baseline_test_accuracy:<15.1f} {aligned_test_accuracy:<15.1f} {aligned_test_accuracy - baseline_test_accuracy:+.1f}%\")\n",
    "print(\"-\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Dataset                   Baseline        Aligned         Improvement    \n",
      "------------------------------------------------------------\n",
      "Alignment Set             100.0           100.0           +0.0%\n",
      "Test Set                  80.0            100.0           +20.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Step 8: Unalign - Remove Specific Feedback\n",
    "Sometimes you may want to remove specific examples from the judge's memory. For instance, if some feedback was incorrect or is no longer relevant.\n",
    "\n",
    "Let's remove one of the alignment traces (say, the last one where the judge fails initially) and see how it affects the performance."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:38.282792Z",
     "start_time": "2026-01-29T07:32:38.279123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check current memory state\n",
    "print(f\"Before unalignment:\")\n",
    "print(f\"  Semantic memory: {len(aligned_judge._semantic_memory)} guidelines\")\n",
    "print(f\"  Episodic memory: {len(aligned_judge._episodic_memory)} examples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before unalignment:\n",
      "  Semantic memory: 4 guidelines\n",
      "  Episodic memory: 5 examples\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:39.681595Z",
     "start_time": "2026-01-29T07:32:38.535181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove the last alignment example\n",
    "traces_to_remove = [t for t in alignment_traces if t.info.trace_id == alignment_trace_ids[-1]]\n",
    "\n",
    "print(f\"Removing {len(traces_to_remove)} trace(s) from the judge's memory...\")\n",
    "for trace in traces_to_remove:\n",
    "    print(f\"  - Trace ID: {trace.info.trace_id}\")\n",
    "\n",
    "# Unalign\n",
    "updated_judge = aligned_judge.unalign(traces=traces_to_remove)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 trace(s) from the judge's memory...\n",
      "  - Trace ID: tr-567a2812d28ec9cb80eec5a51f01abfa\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:40.299546Z",
     "start_time": "2026-01-29T07:32:40.296621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# View updated instructions\n",
    "print(\"\\nUpdated instructions (after unalignment)\")\n",
    "print(\"=\"*60)\n",
    "print(updated_judge.instructions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated instructions (after unalignment)\n",
      "============================================================\n",
      "Evaluate whether the customer support bot’s response is helpful given the user query.\n",
      "\n",
      "User query: {{ inputs }}\n",
      "Assistant response: {{ outputs }}\n",
      "\n",
      "\n",
      "Distilled Guidelines (3):\n",
      "  - Responses must remain professional and respectful; dismissive one-word replies or telling the customer to solve it themselves are unacceptable.\n",
      "  - For simple informational queries, a concise, direct, accurate answer is considered helpful without extra elaboration.\n",
      "  - When the user expresses gratitude, a friendly, warm acknowledgment and an offer of further help is preferred.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:51.202165Z",
     "start_time": "2026-01-29T07:32:41.446229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate updated judge on test set\n",
    "updated_test_accuracy, updated_test_results = evaluate_judge(\n",
    "    updated_judge, test_examples, \"Test Set (After Unalignment)\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating on Test Set (After Unalignment) (5 examples)\n",
      "============================================================\n",
      "\n",
      "Example 1: [CORRECT]\n",
      "  Input: Do you offer gift wrapping?...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 2: [CORRECT]\n",
      "  Input: I love your product!...\n",
      "  Predicted: True, Expected: True\n",
      "\n",
      "Example 3: [CORRECT]\n",
      "  Input: How do I cancel my subscription?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 4: [CORRECT]\n",
      "  Input: Is this item in stock?...\n",
      "  Predicted: False, Expected: False\n",
      "\n",
      "Example 5: [WRONG]\n",
      "  Input: I've been charged twice for my subscription this m...\n",
      "  Predicted: True, Expected: False\n",
      "  Judge rationale: The response is professional and directly addresses the customer’s concern about being charged twice by providing a plausible reason (payment method u...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy: 4/5 (80.0%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "After unalignment, we see the guideline on response empathy is removed from the instructions, and the judge's prediction on the relevant test example also degrades back to incorrect."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 9: Register the Judge as a Scorer\n",
    "Finally, let's register the aligned judge so it can be used in future MLflow experiments. This allows you to:\n",
    "\n",
    "- Use the judge consistently across experiments\n",
    "- Share the judge with team members\n",
    "- Track judge versions over time"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:51.237474Z",
     "start_time": "2026-01-29T07:32:51.209062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Register the aligned judge\n",
    "registered_judge = aligned_judge.register()\n",
    "\n",
    "print(f\"Judge registered successfully!\")\n",
    "print(f\"  Name: {registered_judge.name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge registered successfully!\n",
      "  Name: helpfulness\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:51.318491Z",
     "start_time": "2026-01-29T07:32:51.291791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List all registered scorers\n",
    "from mlflow.genai import list_scorers\n",
    "\n",
    "scorers = list_scorers(experiment_id=experiment_id)\n",
    "print(f\"\\nRegistered scorers in experiment:\")\n",
    "for scorer in scorers:\n",
    "    print(f\"  - {scorer.name} (model: {scorer.model})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registered scorers in experiment:\n",
      "  - helpfulness (model: openai:/gpt-5.2)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:52.025709Z",
     "start_time": "2026-01-29T07:32:52.004565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve the registered scorer\n",
    "from mlflow.genai import get_scorer\n",
    "\n",
    "retrieved_judge = get_scorer(name=\"helpfulness\", experiment_id=experiment_id)\n",
    "\n",
    "print(f\"Retrieved registered judge: {retrieved_judge.name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved registered judge: helpfulness\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:32:57.292729Z",
     "start_time": "2026-01-29T07:32:54.819884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the retrieved judge\n",
    "test_result = retrieved_judge(\n",
    "    inputs=\"I'm having trouble with my order and feeling frustrated.\",\n",
    "    outputs=\"I understand this is frustrating. Let me look into your order right away and help resolve this.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTest evaluation:\")\n",
    "print(f\"  helpful: {test_result.value}\")\n",
    "print(f\"  Rationale: {test_result.rationale}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test evaluation:\n",
      "  helpful: True\n",
      "  Rationale: The user is frustrated about trouble with an order. The response is professional, acknowledges the frustration (“I understand this is frustrating”), and indicates immediate intent to help resolve the issue. This aligns with the expectation to empathize before moving into assistance. While it could be improved by asking for an order number or specific details, it is still a helpful and appropriate first step.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Summary\n",
    "In this notebook, we demonstrated the complete MemAlign workflow:\n",
    "\n",
    "1. Created an LLM judge for evaluating response helpfulness\n",
    "2. Prepared datasets with a tricky case: factually correct but emotionally cold responses\n",
    "3. Evaluated baseline performance - the judge incorrectly rated cold responses as helpful\n",
    "4. Aligned the judge using human feedback with MemAlign\n",
    "5. Inspected learned guidelines - MemAlign learned that empathy matters\n",
    "6. Evaluated improved performance - the aligned judge now considers emotional tone\n",
    "7. Unaligned specific traces - removed feedback from the judge's memory\n",
    "8. Registered the judge for use in future experiments\n",
    "# Key takeaways:\n",
    "- MemAlign captures nuance: It learned that factual correctness alone isn't enough\n",
    "- Dual memory system: Guidelines (semantic) + examples (episodic) provide robust alignment\n",
    "- Incremental updates: Use .align() to add feedback and .unalign() to remove it\n",
    "- Persistence: Register judges to share and reuse across experiments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleanup (optional) - delete the registered scorer"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:33:03.968771Z",
     "start_time": "2026-01-29T07:33:03.931602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlflow.genai.scorers import delete_scorer\n",
    "\n",
    "delete_scorer(name=\"helpfulness\", experiment_id=experiment_id, version=\"all\")"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MemAlign Tutorial (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
