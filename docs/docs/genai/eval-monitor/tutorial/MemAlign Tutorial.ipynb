{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MemAlign: Aligning LLM Judges with Human Feedback\n",
    "This notebook demonstrates how to use MemAlign to align an LLM judge with human preferences.\n",
    "\n",
    "MemAlign uses a dual-memory system:\n",
    "\n",
    "Semantic Memory: Distills general guidelines from human feedback patterns\n",
    "Episodic Memory: Retrieves similar past examples using embeddings for few-shot learning\n",
    "## What you'll learn:\n",
    "1. Create an LLM judge for evaluating responses\n",
    "2. Prepare alignment and test datasets with edge cases\n",
    "3. Evaluate the judge before alignment (baseline)\n",
    "4. Align the judge using human feedback\n",
    "5. Evaluate the improved judge (post-alignment)\n",
    "6. Inspect the learned guidelines\n",
    "7. Unalign (remove) specific feedback from the judge\n",
    "8. Register the judge as a scorer for future experiments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "First, let's import the required modules and set up the environment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install --upgrade mlflow>=3.9.0 openai litellm dspy"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "from mlflow.genai.judges.optimizers import MemAlignOptimizer\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up your provider and model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For example, to use OpenAI API, uncomment the following lines and comment out Option 1 above:\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"  # TODO: set your OpenAI API key\n",
    "mlflow.set_tracking_uri(\"your MLflow server\")\n",
    "mlflow.set_registry_uri(\"your MLflow server\")\n",
    "experiment_name = \"memalign-demo\"\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment.experiment_id"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Create an LLM Judge\n",
    "We'll create a judge that evaluates whether customer service responses are helpful."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "JUDGE_NAME = \"helpfulness\"\n",
    "\n",
    "initial_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"Evaluate whether the customer support botâ€™s response is helpful \"\n",
    "        \"given the user query.\\n\\n\"\n",
    "        \"User query: {{ inputs }}\\n\"\n",
    "        \"Assistant response: {{ outputs }}\\n\"\n",
    "    ),\n",
    "    feedback_value_type=bool,\n",
    "    model=\"openai:/gpt-5_2\", # Alternative: Example using OpenAI model\n",
    ")\n",
    "\n",
    "print(f\"Created judge: {initial_judge.name}\")\n",
    "print(f\"Model: {initial_judge.model}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Create Toy Datasets\n",
    "We'll create two datasets:\n",
    "\n",
    "1. **Alignment set** (5 examples): Used to teach the judge our preferences\n",
    "2. **Test set** (5 examples): Used to evaluate the judge's performance\n",
    "## The tricky case: Factually correct but emotionally cold\n",
    "LLM judges often rate **factually correct responses as helpful**, even when they lack empathy. But in customer service, a cold transactional response to a frustrated user is unhelpful - it should acknowledge emotions first."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Alignment dataset - 4 easy examples + 1 tricky case\n",
    "alignment_examples = [\n",
    "    {\n",
    "        \"inputs\": \"What are your store hours?\",\n",
    "        \"outputs\": \"We're open Monday to Friday, 9am to 6pm.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Direct, accurate answer to a simple question.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"Thanks for your help!\",\n",
    "        \"outputs\": \"You're welcome! Let me know if you need anything else.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Warm, friendly acknowledgment.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"Can you help me track my order?\",\n",
    "        \"outputs\": \"Figure it out yourself.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Rude and dismissive.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"I have a question about returns.\",\n",
    "        \"outputs\": \"Whatever.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Dismissive and unprofessional.\",\n",
    "    },\n",
    "    # Tricky: Factually correct with solution, but lacks empathy\n",
    "    {\n",
    "        \"inputs\": \"The sweater I ordered looks completely different from what was shown on the website.\",\n",
    "        \"outputs\": \"Product colors may vary slightly due to lighting and display settings. You can initiate a return through your order history if needed.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Response provides factual explanation and offers a solution (return option), \"\n",
    "                     \"but fails to acknowledge the customer's disappointment. Should start with \"\n",
    "                     \"'I'm sorry the product didn't meet your expectations' before explaining.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(alignment_examples)} alignment examples\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test dataset - 4 easy examples + 1 tricky case\n",
    "test_examples = [\n",
    "    {\n",
    "        \"inputs\": \"Do you offer gift wrapping?\",\n",
    "        \"outputs\": \"Yes! You can select gift wrapping at checkout for $3.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Helpful and informative.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"I love your product!\",\n",
    "        \"outputs\": \"Thank you so much! We're glad you're enjoying it.\",\n",
    "        \"is_helpful\": True,\n",
    "        \"rationale\": \"Warm acknowledgment of positive feedback.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"How do I cancel my subscription?\",\n",
    "        \"outputs\": \"Why would you want to do that? That's stupid.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Insulting and unprofessional.\",\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": \"Is this item in stock?\",\n",
    "        \"outputs\": \"I don't care.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Rude and unhelpful.\",\n",
    "    },\n",
    "    # Tricky: Factually correct but lacks empathy for frustrated user\n",
    "    {\n",
    "        \"inputs\": \"I've been charged twice for my subscription this month. This is really frustrating!\",\n",
    "        \"outputs\": \"We see two charges on your account because you updated your payment method. \"\n",
    "                   \"One charge will be reversed automatically within 5-7 business days.\",\n",
    "        \"is_helpful\": False,\n",
    "        \"rationale\": \"Factually correct but too cold and transactional. \"\n",
    "                     \"Should start with empathy (e.g., 'Sorry for the confusion') and end with \"\n",
    "                     \"support-oriented language when responding to a frustrated customer.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_examples)} test examples\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Create Traces and Log Human Feedback\n",
    "MemAlign learns from traces that have human feedback attached. We'll:\n",
    "\n",
    "1. Create traces for each example\n",
    "2. Log human feedback (ground truth) for alignment examples You can either log feedback programmatically (like below) or with the MLflow UI (see [here](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/alignment/#collecting-feedback-for-alignment))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Create all traces first (separate from feedback logging)\n",
    "def create_traces(examples, prefix):\n",
    "    \"\"\"Create traces from examples.\"\"\"\n",
    "    trace_ids = []\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        with mlflow.start_span(f\"{prefix}_{i}\") as span:\n",
    "            span.set_inputs({\"inputs\": example[\"inputs\"]})\n",
    "            span.set_outputs({\"outputs\": example[\"outputs\"]})\n",
    "            trace_ids.append(span.trace_id)\n",
    "\n",
    "    return trace_ids\n",
    "\n",
    "# Create traces for alignment and test sets\n",
    "alignment_trace_ids = create_traces(alignment_examples, \"alignment\")\n",
    "print(f\"Created {len(alignment_trace_ids)} alignment traces\")\n",
    "\n",
    "test_trace_ids = create_traces(test_examples, \"test\")\n",
    "print(f\"Created {len(test_trace_ids)} test traces\")\n",
    "time.sleep(2)  # Ensure traces are committed before adding assessments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Log human feedback for alignment examples\n",
    "\n",
    "for i, (trace_id, example) in enumerate(zip(alignment_trace_ids, alignment_examples)):\n",
    "    mlflow.log_feedback(\n",
    "        trace_id=trace_id,\n",
    "        name=JUDGE_NAME,\n",
    "        value=example[\"is_helpful\"],\n",
    "        rationale=example[\"rationale\"],\n",
    "        source=AssessmentSource(\n",
    "            source_type=AssessmentSourceType.HUMAN,\n",
    "            source_id=\"human_expert\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(f\"Logged human feedback for {len(alignment_trace_ids)} alignment traces\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Evaluate Baseline Judge Performance\n",
    "Before alignment, let's see how the initial judge performs on both datasets. We expect the judge to make mistakes on edge cases like the tricky empathy examples.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_judge(judge, examples, dataset_name):\n",
    "    \"\"\"Evaluate judge on examples and compute accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating on {dataset_name} ({len(examples)} examples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        # Run judge\n",
    "        feedback = judge(\n",
    "            inputs=example[\"inputs\"],\n",
    "            outputs=example[\"outputs\"]\n",
    "        )\n",
    "\n",
    "        predicted = feedback.value\n",
    "        expected = example[\"is_helpful\"]\n",
    "        is_correct = predicted == expected\n",
    "\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        results.append({\n",
    "            \"example\": i + 1,\n",
    "            \"predicted\": predicted,\n",
    "            \"expected\": expected,\n",
    "            \"correct\": is_correct,\n",
    "            \"rationale\": feedback.rationale[:100] + \"...\" if len(feedback.rationale) > 100 else feedback.rationale,\n",
    "        })\n",
    "\n",
    "        # Print result\n",
    "        status = \"CORRECT\" if is_correct else \"WRONG\"\n",
    "        print(f\"\\nExample {i+1}: [{status}]\")\n",
    "        print(f\"  Input: {example['inputs'][:50]}...\")\n",
    "        print(f\"  Predicted: {predicted}, Expected: {expected}\")\n",
    "        if not is_correct:\n",
    "          print(f\"  Judge rationale: {feedback.rationale[:150]}...\")\n",
    "\n",
    "    accuracy = correct / len(examples) * 100\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Accuracy: {correct}/{len(examples)} ({accuracy:.1f}%)\")\n",
    "    print(f\"{'-'*60}\")\n",
    "\n",
    "    return accuracy, results"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate baseline on alignment set\n",
    "baseline_align_accuracy, baseline_align_results = evaluate_judge(\n",
    "    initial_judge, alignment_examples, \"Alignment Set (Baseline)\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate baseline on test set\n",
    "baseline_test_accuracy, baseline_test_results = evaluate_judge(\n",
    "    initial_judge, test_examples, \"Test Set (Baseline)\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 5: Align the Judge with MemAlign\n",
    "Now we'll use MemAlign to align the judge with our human feedback.\n",
    "\n",
    "MemAlign will:\n",
    "\n",
    "1. **Distill guidelines** from the feedback rationales (semantic memory)\n",
    "2. **Store examples** for few-shot retrieval (episodic memory)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the MemAlign optimizer\n",
    "optimizer = MemAlignOptimizer(\n",
    "    reflection_lm=\"openai:/gpt-5_2\",  # Model for distilling guidelines\n",
    "    embedding_model=\"openai/text-embedding-3-large\",  # Model for embeddings\n",
    "    retrieval_k=3,  # Number of similar examples to retrieve during evaluation\n",
    ")\n",
    "\n",
    "print(\"Created MemAlign optimizer\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Retrieve traces with human feedback for alignment\n",
    "all_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    return_type=\"list\",\n",
    ")\n",
    "\n",
    "alignment_traces = [\n",
    "    trace for trace in all_traces\n",
    "    if trace.info.trace_id in alignment_trace_ids\n",
    "]\n",
    "\n",
    "print(f\"Retrieved {len(alignment_traces)} traces for alignment\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Align the judge\n",
    "aligned_judge = initial_judge.align(\n",
    "    traces=alignment_traces,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "print(f\"\\nAlignment complete!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 6: Inspect Learned Guidelines (Semantic Memory)\n",
    "Let's see what guidelines MemAlign distilled from our feedback."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# View the full instructions (original + distilled guidelines)\n",
    "print(\"\\nFull Judge Instructions (with guidelines)\")\n",
    "print(\"=\"*60)\n",
    "print(aligned_judge.instructions)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 7: Evaluate Aligned Judge Performance\n",
    "Let's see how the aligned judge performs compared to the baseline."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate aligned judge on alignment set\n",
    "aligned_align_accuracy, aligned_align_results = evaluate_judge(\n",
    "    aligned_judge, alignment_examples, \"Alignment Set (Aligned)\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate aligned judge on test set\n",
    "aligned_test_accuracy, aligned_test_results = evaluate_judge(\n",
    "    aligned_judge, test_examples, \"Test Set (Aligned)\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Dataset':<25} {'Baseline':<15} {'Aligned':<15} {'Improvement':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Alignment Set':<25} {baseline_align_accuracy:<15.1f} {aligned_align_accuracy:<15.1f} {aligned_align_accuracy - baseline_align_accuracy:+.1f}%\")\n",
    "print(f\"{'Test Set':<25} {baseline_test_accuracy:<15.1f} {aligned_test_accuracy:<15.1f} {aligned_test_accuracy - baseline_test_accuracy:+.1f}%\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Step 8: Unalign - Remove Specific Feedback\n",
    "Sometimes you may want to remove specific examples from the judge's memory. For instance, if some feedback was incorrect or is no longer relevant.\n",
    "\n",
    "Let's remove one of the alignment traces (say, the last one where the judge fails initially) and see how it affects the performance."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check current memory state\n",
    "print(f\"Before unalignment:\")\n",
    "print(f\"  Semantic memory: {len(aligned_judge._semantic_memory)} guidelines\")\n",
    "print(f\"  Episodic memory: {len(aligned_judge._episodic_memory)} examples\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove the last alignment example\n",
    "traces_to_remove = [t for t in alignment_traces if t.info.trace_id == alignment_trace_ids[-1]]\n",
    "\n",
    "print(f\"Removing {len(traces_to_remove)} trace(s) from the judge's memory...\")\n",
    "for trace in traces_to_remove:\n",
    "    print(f\"  - Trace ID: {trace.info.trace_id}\")\n",
    "\n",
    "# Unalign\n",
    "updated_judge = aligned_judge.unalign(traces=traces_to_remove)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# View updated instructions\n",
    "print(\"\\nUpdated instructions (after unalignment)\")\n",
    "print(\"=\"*60)\n",
    "print(updated_judge.instructions)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate updated judge on test set\n",
    "updated_test_accuracy, updated_test_results = evaluate_judge(\n",
    "    updated_judge, test_examples, \"Test Set (After Unalignment)\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "After unalignment, we see the guideline on response empathy is removed from the instructions, and the judge's prediction on the relevant test example also degrades back to incorrect."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 9: Register the Judge as a Scorer\n",
    "Finally, let's register the aligned judge so it can be used in future MLflow experiments. This allows you to:\n",
    "\n",
    "- Use the judge consistently across experiments\n",
    "- Share the judge with team members\n",
    "- Track judge versions over time"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Register the aligned judge\n",
    "registered_judge = aligned_judge.register()\n",
    "\n",
    "print(f\"Judge registered successfully!\")\n",
    "print(f\"  Name: {registered_judge.name}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List all registered scorers\n",
    "from mlflow.genai import list_scorers\n",
    "\n",
    "scorers = list_scorers(experiment_id=experiment_id)\n",
    "print(f\"\\nRegistered scorers in experiment:\")\n",
    "for scorer in scorers:\n",
    "    print(f\"  - {scorer.name} (model: {scorer.model})\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Retrieve the registered scorer\n",
    "from mlflow.genai import get_scorer\n",
    "\n",
    "retrieved_judge = get_scorer(name=\"helpfulness\", experiment_id=experiment_id)\n",
    "\n",
    "print(f\"Retrieved registered judge: {retrieved_judge.name}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the retrieved judge\n",
    "test_result = retrieved_judge(\n",
    "    inputs=\"I'm having trouble with my order and feeling frustrated.\",\n",
    "    outputs=\"I understand this is frustrating. Let me look into your order right away and help resolve this.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTest evaluation:\")\n",
    "print(f\"  helpful: {test_result.value}\")\n",
    "print(f\"  Rationale: {test_result.rationale}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Summary\n",
    "In this notebook, we demonstrated the complete MemAlign workflow:\n",
    "\n",
    "1. Created an LLM judge for evaluating response helpfulness\n",
    "2. Prepared datasets with a tricky case: factually correct but emotionally cold responses\n",
    "3. Evaluated baseline performance - the judge incorrectly rated cold responses as helpful\n",
    "4. Aligned the judge using human feedback with MemAlign\n",
    "5. Inspected learned guidelines - MemAlign learned that empathy matters\n",
    "6. Evaluated improved performance - the aligned judge now considers emotional tone\n",
    "7. Unaligned specific traces - removed feedback from the judge's memory\n",
    "8. Registered the judge for use in future experiments\n",
    "# Key takeaways:\n",
    "- MemAlign captures nuance: It learned that factual correctness alone isn't enough\n",
    "- Dual memory system: Guidelines (semantic) + examples (episodic) provide robust alignment\n",
    "- Incremental updates: Use .align() to add feedback and .unalign() to remove it\n",
    "- Persistence: Register judges to share and reuse across experiments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleanup (optional) - delete the registered scorer"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from mlflow.genai.scorers import delete_scorer\n",
    "\n",
    "delete_scorer(name=\"helpfulness\", experiment_id=experiment_id, version=\"all\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MemAlign Tutorial (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
