{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# GenAI Evaluation Quickstart\n",
    "\n",
    "This notebook will walk you through evaluating your GenAI applications with MLflow's comprehensive evaluation framework. In less than 5 minutes, you'll learn how to evaluate LLM outputs, use built-in and custom evaluation criteria, and analyze results in the MLflow UI.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install the required packages by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298aa346-256d-4edb-9aa3-a37ccdf5ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlflow openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Step 1: Set up your environment\n",
    "\n",
    "### Connect to MLflow\n",
    "\n",
    "Before running evaluation, start the MLflow tracking server:\n",
    "\n",
    "```bash\n",
    "mlflow server\n",
    "```\n",
    "\n",
    "This starts MLflow at http://localhost:5000 with a SQLite backend (default).\n",
    "\n",
    "Then configure your environment in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Configure environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # Replace with your API key\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"GenAI Evaluation Quickstart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent",
   "metadata": {},
   "source": [
    "## Step 2: Define your mock agent's prediction function\n",
    "\n",
    "Create a prediction function that takes a question and returns an answer using OpenAI's gpt-4o-mini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def my_agent(question: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Answer questions concisely.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Wrapper function for evaluation\n",
    "def qa_predict_fn(question: str) -> str:\n",
    "    return my_agent(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": [
    "## Step 3: Prepare an evaluation dataset\n",
    "\n",
    "The evaluation dataset is a list of samples, each with an `inputs` and `expectations` field.\n",
    "\n",
    "- `inputs`: The input to the `predict_fn` function. **The key(s) must match the parameter name of the `predict_fn` function**.\n",
    "- `expectations`: The expected output from the `predict_fn` function, namely, ground truth for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Q&A dataset with questions and expected answers\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the capital of France?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Paris\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who was the first person to build an airplane?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Wright Brothers\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who wrote Romeo and Juliet?\"},\n",
    "        \"expectations\": {\"expected_response\": \"William Shakespeare\"},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scorers",
   "metadata": {},
   "source": [
    "## Step 4: Define evaluation criteria using Scorers\n",
    "\n",
    "**Scorer** is a function that computes a score for a given input-output pair against various evaluation criteria.\n",
    "You can use built-in scorers provided by MLflow for common evaluation criteria, as well as create your own custom scorers.\n",
    "\n",
    "Here we use three scorers:\n",
    "\n",
    "- **Correctness**: Evaluates if the answer is factually correct, using the \"expected_response\" field in the dataset.\n",
    "- **Guidelines**: Evaluates if the answer meets the given guidelines.\n",
    "- **is_concise**: A custom scorer to judge if the answer is concise (less than 5 words).\n",
    "\n",
    "The first two scorers use LLMs to evaluate the response, so-called **LLM-as-a-Judge**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scorers-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai import scorer\n",
    "from mlflow.genai.scorers import Correctness, Guidelines\n",
    "\n",
    "\n",
    "@scorer\n",
    "def is_concise(outputs: str) -> bool:\n",
    "    \"\"\"Evaluate if the answer is concise (less than 5 words)\"\"\"\n",
    "    return len(outputs.split()) <= 5\n",
    "\n",
    "\n",
    "scorers = [\n",
    "    Correctness(),\n",
    "    Guidelines(name=\"is_english\", guidelines=\"The answer must be in English\"),\n",
    "    is_concise,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate",
   "metadata": {},
   "source": [
    "## Step 5: Run the evaluation\n",
    "\n",
    "Now we have all three components of the evaluation: dataset, prediction function, and scorers. Let's run the evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=qa_predict_fn,\n",
    "    scorers=scorers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "After running the evaluation, go to the MLflow UI and navigate to your experiment. You'll see the evaluation results with detailed metrics for each scorer.\n",
    "\n",
    "By clicking on each row in the table, you can see the detailed rationale behind the score and the trace of the prediction.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "- ✅ Set up MLflow GenAI Evaluation for your applications\n",
    "- ✅ Evaluated a Q&A application with built-in scorers\n",
    "- ✅ Created custom evaluation guidelines\n",
    "- ✅ Learned to analyze results in the MLflow UI\n",
    "\n",
    "MLflow's evaluation framework provides comprehensive tools for assessing GenAI application quality, helping you build more reliable and effective AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
