import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Evaluate within a CI/CD Pipeline

Integrating automated quality evaluation into your Continuous Integration/Continuous Deployment (CI/CD) pipeline is a critical step towards building robust and reliable GenAI applications. By running evaluations automatically whenever new code is committed or a release candidate is prepared, you can:

- **Prevent Regressions**: Catch quality degradations before they reach production
- **Ensure Consistent Quality**: Enforce quality standards and ensure that all changes meet predefined criteria
- **Automate Quality Gates**: Make deployment decisions based on objective evaluation results
- **Increase Confidence in Deployments**: Reduce the risk associated with deploying new versions

This guide outlines how to incorporate MLflow evaluations into your CI/CD workflow, enabling automated quality assurance for your GenAI applications.

## Core Concepts for CI/CD Evaluation

Integrating evaluation into your CI/CD pipeline transforms quality assurance from a manual, error-prone process into an automated, reliable system. Understanding these core concepts will help you design an effective evaluation strategy.

### Automated Quality Gates

Quality gates are automated checkpoints that evaluate your application against predefined criteria. They act as gatekeepers, preventing low-quality code from reaching production. Think of them as automated code reviewers that focus specifically on application quality rather than code structure.

**Key Components:**
- **Evaluation Datasets**: Version-controlled, standardized datasets representing critical scenarios
- **Quality Metrics**: Well-defined scorers and judges reflecting important quality dimensions
- **Thresholds**: Pass/fail criteria based on metric scores
- **Reporting**: Clear communication of results and any failures

Quality gates work by running your application against a curated set of test cases, measuring the results using automated metrics, and comparing those results to predetermined thresholds. If all thresholds are met, the pipeline continues; if any fail, the deployment is blocked and the team is notified.

### CI/CD Integration Points

The timing and frequency of evaluations significantly impact both development velocity and quality assurance effectiveness. Different integration points serve different purposes in your quality strategy.

<Tabs>
  <TabItem value="triggers" label="Evaluation Triggers" default>
    **When to Run Evaluations:**
    - **Pull Request Reviews**: Validate changes before merging
    - **Main Branch Commits**: Ensure main branch quality
    - **Release Candidates**: Final quality check before deployment
    - **Nightly Builds**: Regular quality monitoring
    - **Pre-deployment**: Last check before production release
  </TabItem>
  <TabItem value="environments" label="Environment Setup">
    **CI/CD Environment Requirements:**
    - Python environment with MLflow and dependencies
    - Access to MLflow Tracking server
    - Evaluation datasets and application code
    - LLM API credentials and configuration
    - Appropriate compute resources for evaluation
  </TabItem>
  <TabItem value="artifacts" label="Artifacts & Reporting">
    **Generated Artifacts:**
    - MLflow runs with detailed evaluation results
    - CI/CD job logs with pass/fail status
    - Evaluation summary reports
    - Quality trend analysis
    - Alert notifications for failures
  </TabItem>
</Tabs>

## Implementation Steps

Successfully integrating MLflow evaluation into your CI/CD pipeline requires careful planning and systematic implementation. This section walks you through each step, from creating your evaluation script to configuring your pipeline.

### Step 1: Create Your Evaluation Script

The foundation of CI/CD evaluation is a robust Python script that can run your application against test datasets and determine whether quality standards are met. This script serves as the bridge between your application code and your deployment pipeline.

Your evaluation script needs to be parameterized and robust, handling various inputs like application versions, dataset locations, and configuration options. It should also be designed to run in automated environments where human intervention isn't possible.

The script must clearly communicate its results through exit codes - this is how your CI/CD system knows whether to proceed with deployment or halt the process. A zero exit code indicates success, while non-zero indicates failure.

Build a Python script that performs automated evaluation using `mlflow.evaluate()`:

```python
#!/usr/bin/env python3
"""
CI/CD Evaluation Script for GenAI Application
Evaluates application quality and enforces quality gates
"""

import os
import sys
import argparse
import mlflow
import pandas as pd
from typing import Dict, Any, List


def load_evaluation_dataset(dataset_path: str) -> pd.DataFrame:
    """Load evaluation dataset from specified path"""
    if dataset_path.endswith(".csv"):
        return pd.read_csv(dataset_path)
    elif dataset_path.endswith(".json"):
        return pd.read_json(dataset_path)
    else:
        # Load from MLflow dataset
        dataset = mlflow.data.load_delta(dataset_path)
        return dataset.df


def load_application_model(model_uri: str):
    """Load the application model for evaluation"""
    # This will depend on your specific application
    # Examples:
    # return mlflow.pyfunc.load_model(model_uri)
    # return load_custom_model(model_uri)
    pass


def define_quality_gates() -> Dict[str, Dict[str, Any]]:
    """Define quality gates and thresholds"""
    return {
        "exact_match": {
            "threshold": 0.7,
            "comparison": "greater_than_or_equal",
            "critical": True,
        },
        "relevance": {
            "threshold": 0.8,
            "comparison": "greater_than_or_equal",
            "critical": True,
        },
        "groundedness": {
            "threshold": 0.9,
            "comparison": "greater_than_or_equal",
            "critical": True,
        },
        "toxicity": {
            "threshold": 0.1,
            "comparison": "less_than_or_equal",
            "critical": True,
        },
        "response_time": {
            "threshold": 2000,  # milliseconds
            "comparison": "less_than_or_equal",
            "critical": False,
        },
    }


def evaluate_quality_gates(eval_results, quality_gates: Dict) -> tuple[bool, List[str]]:
    """Evaluate results against quality gates"""
    all_gates_passed = True
    failure_reasons = []
    warnings = []

    for metric_name, gate_config in quality_gates.items():
        if metric_name not in eval_results.metrics:
            message = f"Metric '{metric_name}' not found in evaluation results"
            if gate_config["critical"]:
                all_gates_passed = False
                failure_reasons.append(message)
            else:
                warnings.append(message)
            continue

        # Extract metric value (handle different MLflow metric formats)
        metric_data = eval_results.metrics[metric_name]
        if isinstance(metric_data, dict) and "value" in metric_data:
            current_score = metric_data["value"]
        else:
            current_score = float(metric_data)

        threshold = gate_config["threshold"]
        comparison = gate_config["comparison"]

        # Evaluate threshold
        passed = False
        if comparison == "greater_than_or_equal":
            passed = current_score >= threshold
        elif comparison == "less_than_or_equal":
            passed = current_score <= threshold
        elif comparison == "greater_than":
            passed = current_score > threshold
        elif comparison == "less_than":
            passed = current_score < threshold

        if not passed:
            message = f"Metric '{metric_name}' failed: {current_score} {comparison} {threshold}"
            if gate_config["critical"]:
                all_gates_passed = False
                failure_reasons.append(message)
            else:
                warnings.append(message)
        else:
            print(
                f"✓ Metric '{metric_name}' passed: {current_score} {comparison} {threshold}"
            )

    # Print warnings
    for warning in warnings:
        print(f"⚠️  Warning: {warning}")

    return all_gates_passed, failure_reasons


def run_evaluation(args):
    """Main evaluation function"""

    # Set MLflow experiment
    mlflow.set_experiment(args.experiment_name)

    with mlflow.start_run(run_name=f"ci_evaluation_{args.app_version}") as run:
        # Log CI/CD context
        mlflow.log_params(
            {
                "app_version": args.app_version,
                "git_commit": args.git_commit,
                "dataset_path": args.dataset_path,
                "ci_pipeline": args.pipeline_name,
                "evaluation_type": "ci_cd_quality_gate",
            }
        )

        # Load dataset and model
        print("Loading evaluation dataset...")
        eval_dataset = load_evaluation_dataset(args.dataset_path)

        print("Loading application model...")
        model = load_application_model(args.model_uri)

        # Define evaluation metrics
        evaluation_metrics = [
            mlflow.metrics.exact_match(),
            mlflow.metrics.genai.relevance(),
            mlflow.metrics.genai.groundedness(),
            mlflow.metrics.genai.toxicity(),
            # Add custom metrics as needed
        ]

        print("Running MLflow evaluation...")
        eval_results = mlflow.evaluate(
            model=model,
            data=eval_dataset,
            targets="expected_response",
            extra_metrics=evaluation_metrics,
            evaluator_config={
                "col_mapping": {"inputs": "inputs", "targets": "expected_response"}
            },
        )

        # Evaluate quality gates
        print("Evaluating quality gates...")
        quality_gates = define_quality_gates()
        gates_passed, failure_reasons = evaluate_quality_gates(
            eval_results, quality_gates
        )

        # Log quality gate results
        mlflow.log_params(
            {
                "quality_gates_passed": gates_passed,
                "total_metrics_evaluated": len(quality_gates),
            }
        )

        if gates_passed:
            print("✅ All quality gates passed!")
            mlflow.set_tag("ci_quality_check", "passed")
            mlflow.set_tag("deployment_ready", "true")
            return 0
        else:
            print("❌ Quality gate failures detected:")
            for reason in failure_reasons:
                print(f"  - {reason}")
            mlflow.set_tag("ci_quality_check", "failed")
            mlflow.set_tag("deployment_ready", "false")

            # Log failure reasons
            failure_summary = "; ".join(failure_reasons)
            mlflow.log_param("failure_reasons", failure_summary)

            return 1


def main():
    parser = argparse.ArgumentParser(description="CI/CD Evaluation Script")
    parser.add_argument(
        "--app-version", required=True, help="Application version being tested"
    )
    parser.add_argument("--git-commit", required=True, help="Git commit hash")
    parser.add_argument(
        "--model-uri", required=True, help="URI to the model being evaluated"
    )
    parser.add_argument(
        "--dataset-path", required=True, help="Path to evaluation dataset"
    )
    parser.add_argument(
        "--experiment-name", default="ci_cd_evaluation", help="MLflow experiment name"
    )
    parser.add_argument(
        "--pipeline-name", default="unknown", help="CI/CD pipeline name"
    )

    args = parser.parse_args()

    # Validate required environment variables
    required_env_vars = ["MLFLOW_TRACKING_URI"]
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        print(f"❌ Missing required environment variables: {missing_vars}")
        return 1

    try:
        exit_code = run_evaluation(args)
        sys.exit(exit_code)
    except Exception as e:
        print(f"❌ Evaluation failed with error: {str(e)}")
        # Log error to MLflow if possible
        try:
            mlflow.set_tag("ci_quality_check", "error")
            mlflow.log_param("error_message", str(e))
        except:
            pass
        sys.exit(1)


if __name__ == "__main__":
    main()
```

### Step 2: Configure CI/CD Pipeline

Once your evaluation script is ready, the next step is integrating it into your CI/CD system. The configuration varies by platform, but the core principles remain the same: trigger evaluation at appropriate times, provide necessary resources and credentials, and handle results appropriately.

Your pipeline configuration should include proper error handling, artifact management, and notification systems. It's important to balance thoroughness with speed - evaluations should be comprehensive enough to catch issues but fast enough not to slow down development velocity significantly.

Consider different evaluation strategies for different branches or deployment targets. For example, you might run lightweight evaluations on feature branches but comprehensive evaluations before production deployment.

Set up your CI/CD pipeline to run the evaluation script:

<Tabs>
  <TabItem value="github-actions" label="GitHub Actions" default>
    ```yaml
    name: GenAI App Quality Evaluation

    on:
      push:
        branches: [main]
      pull_request:
        branches: [main]

    jobs:
      evaluate-quality:
        runs-on: ubuntu-latest

        steps:
        - name: Checkout code
          uses: actions/checkout@v4
          with:
            fetch-depth: 0  # Fetch full history for versioning

        - name: Set up Python
          uses: actions/setup-python@v4
          with:
            python-version: '3.9'

        - name: Cache dependencies
          uses: actions/cache@v3
          with:
            path: ~/.cache/pip
            key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
            restore-keys: |
              ${{ runner.os }}-pip-

        - name: Install dependencies
          run: |
            python -m pip install --upgrade pip
            pip install mlflow pandas scikit-learn openai
            pip install -r requirements.txt

        - name: Download evaluation dataset
          run: |
            # Example: Download from artifact storage
            mkdir -p ./data
            # aws s3 cp s3://my-bucket/eval-datasets/golden_set.csv ./data/
            # Or use dataset versioned in Git LFS
            git lfs pull

        - name: Package application
          run: |
            # Package your application for evaluation
            python setup.py bdist_wheel
            pip install dist/*.whl

        - name: Run quality evaluation
          env:
            MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
            DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          run: |
            python scripts/ci_evaluation.py \
              --app-version "${{ github.ref_name }}-${{ github.sha }}" \
              --git-commit "${{ github.sha }}" \
              --model-uri "runs:/${{ github.sha }}/model" \
              --dataset-path "./data/evaluation_dataset.csv" \
              --experiment-name "ci_cd_evaluation" \
              --pipeline-name "github-actions"

        - name: Upload evaluation artifacts
          if: always()
          uses: actions/upload-artifact@v3
          with:
            name: evaluation-results
            path: |
              evaluation_summary.txt
              evaluation_report.json
            retention-days: 30

        - name: Notify on failure
          if: failure()
          uses: 8398a7/action-slack@v3
          with:
            status: failure
            text: "Quality evaluation failed for commit ${{ github.sha }}"
          env:
            SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
    ```
  </TabItem>
  <TabItem value="jenkins" label="Jenkins Pipeline">
    ```groovy
    pipeline {
        agent any

        environment {
            MLFLOW_TRACKING_URI = credentials('mlflow-tracking-uri')
            OPENAI_API_KEY = credentials('openai-api-key')
            PYTHON_VERSION = '3.9'
        }

        stages {
            stage('Setup') {
                steps {
                    script {
                        // Set up Python environment
                        sh '''
                            python${PYTHON_VERSION} -m venv venv
                            source venv/bin/activate
                            pip install --upgrade pip
                            pip install mlflow pandas scikit-learn openai
                            pip install -r requirements.txt
                        '''
                    }
                }
            }

            stage('Prepare Evaluation') {
                steps {
                    script {
                        // Download evaluation dataset
                        sh '''
                            mkdir -p ./data
                            # Download dataset from your artifact storage
                            # aws s3 cp s3://my-bucket/eval-datasets/golden_set.csv ./data/
                        '''

                        // Package application
                        sh '''
                            source venv/bin/activate
                            python setup.py bdist_wheel
                            pip install dist/*.whl
                        '''
                    }
                }
            }

            stage('Quality Evaluation') {
                steps {
                    script {
                        def exitCode = sh(
                            script: '''
                                source venv/bin/activate
                                python scripts/ci_evaluation.py \
                                  --app-version "${BRANCH_NAME}-${GIT_COMMIT}" \
                                  --git-commit "${GIT_COMMIT}" \
                                  --model-uri "runs:/${GIT_COMMIT}/model" \
                                  --dataset-path "./data/evaluation_dataset.csv" \
                                  --experiment-name "ci_cd_evaluation" \
                                  --pipeline-name "jenkins"
                            ''',
                            returnStatus: true
                        )

                        if (exitCode != 0) {
                            error("Quality evaluation failed - quality gates not met")
                        }
                    }
                }
            }
        }

        post {
            always {
                archiveArtifacts artifacts: '**/*evaluation*.txt,**/*evaluation*.json',
                                allowEmptyArchive: true
            }
            failure {
                emailext (
                    subject: "Quality Evaluation Failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}",
                    body: "The quality evaluation failed for commit ${env.GIT_COMMIT}. Check the build logs for details.",
                    to: "${env.CHANGE_AUTHOR_EMAIL}"
                )
            }
        }
    }
    ```
  </TabItem>
  <TabItem value="gitlab-ci" label="GitLab CI">
    ```yaml
    stages:
      - setup
      - evaluate
      - report

    variables:
      PYTHON_VERSION: "3.9"
      PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

    cache:
      paths:
        - .cache/pip
        - venv/

    before_script:
      - python -V
      - pip install virtualenv
      - virtualenv venv
      - source venv/bin/activate

    setup:
      stage: setup
      script:
        - pip install --upgrade pip
        - pip install mlflow pandas scikit-learn openai
        - pip install -r requirements.txt
        - python setup.py bdist_wheel
        - pip install dist/*.whl

    evaluate_quality:
      stage: evaluate
      script:
        # Download evaluation dataset
        - mkdir -p ./data
        # Add your dataset download logic here

        # Run evaluation
        - |
          python scripts/ci_evaluation.py \
            --app-version "$CI_COMMIT_REF_NAME-$CI_COMMIT_SHA" \
            --git-commit "$CI_COMMIT_SHA" \
            --model-uri "runs:/$CI_COMMIT_SHA/model" \
            --dataset-path "./data/evaluation_dataset.csv" \
            --experiment-name "ci_cd_evaluation" \
            --pipeline-name "gitlab-ci"

      artifacts:
        reports:
          junit: evaluation_report.xml
        paths:
          - evaluation_summary.txt
          - evaluation_report.json
        expire_in: 1 week

      only:
        - main
        - merge_requests

    notify_failure:
      stage: report
      script:
        - echo "Quality evaluation failed"
        # Add notification logic (Slack, email, etc.)
      when: on_failure
      only:
        - main
        - merge_requests
    ```
  </TabItem>
</Tabs>

### Step 3: Advanced Quality Gates

Basic threshold checking is just the beginning. Advanced quality gates include baseline comparisons, regression detection, and sophisticated failure analysis. These features help you catch subtle quality degradations that might not trigger absolute thresholds but represent concerning trends.

Baseline comparison is particularly valuable - it allows you to detect when your application's performance degrades relative to the current production version, even if it still meets minimum quality standards. This helps prevent gradual quality erosion over time.

Regression detection should account for the natural variance in AI model outputs. Small fluctuations are normal, but consistent patterns of decline indicate real problems that need attention.

Implement sophisticated quality gates that compare against baselines and handle different scenarios:

```python
def compare_with_baseline(
    current_metrics: Dict, baseline_run_id: str
) -> Dict[str, Any]:
    """Compare current evaluation with baseline (e.g., production version)"""

    try:
        baseline_run = mlflow.get_run(baseline_run_id)
        baseline_metrics = baseline_run.data.metrics

        comparison_results = {}

        for metric_name, current_value in current_metrics.items():
            if metric_name in baseline_metrics:
                baseline_value = baseline_metrics[metric_name]

                # Calculate percentage change
                if baseline_value != 0:
                    pct_change = (
                        (current_value - baseline_value) / baseline_value
                    ) * 100
                else:
                    pct_change = float("inf") if current_value > 0 else 0

                comparison_results[metric_name] = {
                    "current": current_value,
                    "baseline": baseline_value,
                    "change": current_value - baseline_value,
                    "pct_change": pct_change,
                }

        return comparison_results

    except Exception as e:
        print(f"Warning: Could not compare with baseline: {e}")
        return {}


def evaluate_regression_gates(
    comparison_results: Dict, thresholds: Dict
) -> tuple[bool, List[str]]:
    """Evaluate for regressions compared to baseline"""

    regression_gates_passed = True
    regression_failures = []

    for metric_name, comparison in comparison_results.items():
        if metric_name in thresholds:
            max_regression = thresholds[metric_name]["max_regression_pct"]
            pct_change = comparison["pct_change"]

            # For metrics where higher is better (e.g., accuracy)
            if thresholds[metric_name].get("higher_is_better", True):
                if pct_change < -max_regression:
                    regression_gates_passed = False
                    regression_failures.append(
                        f"Regression in {metric_name}: {pct_change:.2f}% decline exceeds threshold of {max_regression}%"
                    )
            else:
                # For metrics where lower is better (e.g., toxicity)
                if pct_change > max_regression:
                    regression_gates_passed = False
                    regression_failures.append(
                        f"Regression in {metric_name}: {pct_change:.2f}% increase exceeds threshold of {max_regression}%"
                    )

    return regression_gates_passed, regression_failures


# Enhanced quality gates with regression checking
def enhanced_quality_evaluation(eval_results, baseline_run_id: str = None):
    """Enhanced quality evaluation with baseline comparison"""

    # Standard quality gates
    quality_gates = define_quality_gates()
    gates_passed, failure_reasons = evaluate_quality_gates(eval_results, quality_gates)

    # Regression gates (if baseline provided)
    regression_passed = True
    regression_failures = []

    if baseline_run_id:
        comparison_results = compare_with_baseline(
            eval_results.metrics, baseline_run_id
        )

        regression_thresholds = {
            "exact_match": {"max_regression_pct": 5, "higher_is_better": True},
            "relevance": {"max_regression_pct": 3, "higher_is_better": True},
            "toxicity": {"max_regression_pct": 10, "higher_is_better": False},
        }

        regression_passed, regression_failures = evaluate_regression_gates(
            comparison_results, regression_thresholds
        )

        # Log comparison results
        for metric_name, comparison in comparison_results.items():
            mlflow.log_metric(
                f"{metric_name}_baseline_comparison", comparison["pct_change"]
            )

    # Combined results
    overall_passed = gates_passed and regression_passed
    all_failures = failure_reasons + regression_failures

    return overall_passed, all_failures
```

### Step 4: Reporting and Monitoring

Effective reporting transforms raw evaluation data into actionable insights. Your reports should serve multiple audiences - developers need detailed debugging information, while stakeholders need high-level summaries of quality trends.

Good reporting includes both immediate feedback (did this specific evaluation pass or fail?) and trend analysis (how is our application quality changing over time?). This historical perspective helps teams understand whether they're improving or declining in quality.

Monitoring integration allows you to track evaluation metrics alongside other system metrics, providing a comprehensive view of your application's health. This is particularly important for detecting patterns that might not be obvious from individual evaluation runs.

Create comprehensive reporting for evaluation results:

```python
import json


def generate_evaluation_report(eval_results, gates_passed: bool, failures: List[str]):
    """Generate detailed evaluation report"""

    report = {
        "timestamp": pd.Timestamp.now().isoformat(),
        "overall_status": "PASSED" if gates_passed else "FAILED",
        "metrics_summary": {},
        "quality_gates": {"passed": gates_passed, "failures": failures},
        "detailed_results": {},
    }

    # Extract key metrics
    for metric_name, metric_data in eval_results.metrics.items():
        if isinstance(metric_data, dict) and "value" in metric_data:
            report["metrics_summary"][metric_name] = metric_data["value"]
        else:
            report["metrics_summary"][metric_name] = float(metric_data)

    with open("evaluation_report.json", "w") as f:
        json.dump(report, f, indent=2)

    # Generate human-readable summary
    summary = f"""
    CI/CD EVALUATION REPORT
    =======================
    Status: {report['overall_status']}
    Timestamp: {report['timestamp']}

    METRICS SUMMARY:
    """

    for metric, value in report["metrics_summary"].items():
        summary += f"  {metric}: {value:.4f}\n"

    if failures:
        summary += "\nFAILURES:\n"
        for failure in failures:
            summary += f"  - {failure}\n"

    with open("evaluation_summary.txt", "w") as f:
        f.write(summary)

    return report


def setup_monitoring_dashboard():
    """Set up monitoring for CI/CD evaluation trends"""

    # This would integrate with your monitoring system
    # Example: Send metrics to Prometheus, DataDog, etc.

    metrics_to_track = [
        "ci_evaluation_success_rate",
        "average_evaluation_time",
        "quality_gate_failure_rate",
        "regression_detection_rate",
    ]

    # Implementation depends on your monitoring stack
    pass
```

## Best Practices

Successful CI/CD evaluation requires more than just technical implementation - it requires thoughtful strategy around thresholds, datasets, and performance optimization. These best practices are derived from real-world experience implementing evaluation systems at scale.

### Quality Gate Design

The design of your quality gates significantly impacts both the effectiveness of your quality assurance and the productivity of your development team. Well-designed gates catch real issues without creating false positives that slow down development.

Threshold setting is both an art and a science. Start with achievable thresholds based on your current performance, then gradually raise them as your application improves. This progressive approach builds confidence in the system while continuously pushing for better quality.

Different metrics require different approaches - some metrics like safety scores should have very strict thresholds, while others like exact match might have more lenient thresholds that account for the subjective nature of language generation.

<Tabs>
  <TabItem value="threshold-setting" label="Setting Thresholds" default>
    **Guidelines for Setting Quality Thresholds:**

    - **Start Conservative**: Begin with achievable thresholds based on current performance
    - **Gradual Improvement**: Incrementally raise thresholds as quality improves
    - **Business Alignment**: Align thresholds with business requirements and user expectations
    - **Metric-Specific**: Different metrics may need different approaches

    ```python
    # Example: Progressive threshold strategy
    QUALITY_THRESHOLDS = {
        "development": {"exact_match": 0.6, "relevance": 0.7, "groundedness": 0.8},
        "staging": {"exact_match": 0.7, "relevance": 0.8, "groundedness": 0.9},
        "production": {"exact_match": 0.8, "relevance": 0.85, "groundedness": 0.95},
    }
    ```
  </TabItem>
  <TabItem value="dataset-management" label="Dataset Management">
    **Managing Evaluation Datasets:**

    - **Version Control**: Store datasets in version control (Git LFS for large files)
    - **Regular Updates**: Refresh datasets with new production examples
    - **Stratified Sampling**: Ensure datasets represent different use cases
    - **Size Optimization**: Balance comprehensiveness with evaluation speed

    ```python
    # Example: Dataset versioning strategy
    DATASET_VERSIONS = {
        "regression_tests": "v1.2.0",  # Critical functionality tests
        "edge_cases": "v1.1.0",  # Edge case and adversarial examples
        "performance": "v1.0.0",  # Performance and latency tests
        "safety": "v1.3.0",  # Safety and toxicity tests
    }
    ```
  </TabItem>
  <TabItem value="performance" label="Performance Optimization">
    **Optimizing CI/CD Performance:**

    - **Parallel Execution**: Run different evaluation categories in parallel
    - **Smart Caching**: Cache model loading and dataset preparation
    - **Incremental Evaluation**: Only re-run affected tests when possible
    - **Resource Management**: Right-size compute resources for evaluation needs

    ```python
    from concurrent.futures import ThreadPoolExecutor
    import asyncio


    # Example: Parallel evaluation strategy
    def run_parallel_evaluations(model, datasets):
        """Run multiple evaluations in parallel"""

        evaluation_tasks = [
            ("regression", datasets["regression"]),
            ("performance", datasets["performance"]),
            ("safety", datasets["safety"]),
        ]

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(run_single_evaluation, model, name, dataset)
                for name, dataset in evaluation_tasks
            ]

            results = {}
            for future, (name, _) in zip(futures, evaluation_tasks):
                results[name] = future.result()

        return results
    ```
  </TabItem>
</Tabs>

## Troubleshooting Common Issues

Even well-designed CI/CD evaluation systems encounter issues. Understanding common problems and their solutions helps you build more robust systems and respond quickly when issues arise.

### Environment and Setup Issues

CI/CD environments are often different from development environments, leading to configuration issues, missing dependencies, or networking problems. The key is building comprehensive environment validation into your evaluation script.

Network connectivity issues are particularly common when evaluating GenAI applications that depend on external APIs. Your system should be resilient to temporary network issues while still detecting persistent problems that indicate real configuration errors.

Resource constraints in CI/CD environments can also cause problems. Evaluation datasets that work fine on developer machines might be too large for CI/CD runners, requiring optimization or different dataset strategies.

```python
try:
    import mlflow

    mlflow.get_tracking_uri()

    mlflow_available = True
except ImportError:
    mlflow_available = False


def validate_ci_environment():
    """Validate CI/CD environment setup"""

    checks = {
        "python_version": sys.version_info >= (3, 8),
        "mlflow_installed": True,
        "tracking_uri_set": bool(os.getenv("MLFLOW_TRACKING_URI")),
        "api_keys_available": bool(os.getenv("OPENAI_API_KEY")),
        "dataset_accessible": os.path.exists("./data/evaluation_dataset.csv"),
        "mlflow_connection": mlflow_available,
    }

    failed_checks = [check for check, passed in checks.items() if not passed]

    if failed_checks:
        print(f"❌ Environment validation failed: {failed_checks}")
        return False

    print("✅ Environment validation passed")
    return True
```

### Handling Evaluation Failures

Not all evaluation failures indicate problems with your application - sometimes the evaluation system itself encounters issues. Building robust failure handling ensures that temporary problems don't block legitimate deployments while still catching real quality issues.

Classification of failure types is crucial for appropriate response. Network errors might warrant retries, while data parsing errors might indicate problems with the evaluation dataset itself. Your system should handle these different scenarios appropriately.

Graceful degradation is also important - if non-critical metrics fail to evaluate, the system should continue with available metrics rather than failing entirely. This ensures that temporary service outages don't completely block your deployment pipeline.

```python
def handle_evaluation_failure(error: Exception, context: Dict):
    """Handle evaluation failures gracefully"""

    failure_strategies = {
        "network_error": "retry_with_backoff",
        "rate_limit": "exponential_backoff",
        "data_error": "skip_problematic_examples",
        "memory_error": "reduce_batch_size",
    }

    error_type = classify_error(error)
    strategy = failure_strategies.get(error_type, "fail_fast")

    if strategy == "retry_with_backoff":
        return retry_evaluation_with_backoff(context)
    elif strategy == "skip_problematic_examples":
        return run_partial_evaluation(context)
    else:
        raise error


def classify_error(error: Exception) -> str:
    """Classify error type for appropriate handling"""

    error_msg = str(error).lower()

    if "network" in error_msg or "connection" in error_msg:
        return "network_error"
    elif "rate limit" in error_msg or "429" in error_msg:
        return "rate_limit"
    elif "memory" in error_msg or "oom" in error_msg:
        return "memory_error"
    elif "data" in error_msg or "parse" in error_msg:
        return "data_error"
    else:
        return "unknown_error"
```

## Integration with Deployment

The ultimate goal of CI/CD evaluation is to make intelligent deployment decisions. This requires sophisticated logic that considers not just whether quality gates passed, but also the confidence level in those results and the specific requirements of your deployment environment.

### Conditional Deployment

Simple pass/fail gates are often insufficient for complex GenAI applications. Instead, you need nuanced deployment logic that considers factors like confidence scores, the severity of any failures, and the target environment.

For example, you might automatically deploy to staging if basic quality gates pass, but require manual approval for production deployment unless confidence scores are very high. This allows for rapid iteration while maintaining high standards for production releases.

Risk-based deployment decisions also consider the impact of potential failures. A customer-facing chatbot might have stricter requirements than an internal tool, requiring different thresholds and approval processes.

```python
def create_deployment_decision(evaluation_results: Dict) -> Dict[str, Any]:
    """Create deployment decision based on evaluation results"""

    decision = {
        "deploy": False,
        "confidence": 0.0,
        "reasons": [],
        "recommendations": [],
    }

    # Check critical metrics
    critical_passed = all(
        evaluation_results["metrics"][metric] >= threshold
        for metric, threshold in CRITICAL_THRESHOLDS.items()
        if metric in evaluation_results["metrics"]
    )

    if critical_passed:
        decision["deploy"] = True
        decision["confidence"] = calculate_deployment_confidence(evaluation_results)
        decision["reasons"].append("All critical quality gates passed")
    else:
        decision["reasons"].append("Critical quality gates failed")
        decision["recommendations"].append("Review failed metrics before deployment")

    return decision


def calculate_deployment_confidence(results: Dict) -> float:
    """Calculate confidence score for deployment decision"""

    # Implement your confidence calculation logic
    # Example: weighted average of metric scores

    weights = {
        "exact_match": 0.3,
        "relevance": 0.3,
        "groundedness": 0.25,
        "safety": 0.15,
    }

    weighted_score = 0.0
    total_weight = 0.0

    for metric, weight in weights.items():
        if metric in results["metrics"]:
            weighted_score += results["metrics"][metric] * weight
            total_weight += weight

    return weighted_score / total_weight if total_weight > 0 else 0.0
```

### Automated Deployment Pipeline

```yaml
# Extended GitHub Actions workflow with conditional deployment
- name: Make deployment decision
  id: deployment_decision
  run: |
    python scripts/deployment_decision.py \
      --evaluation-run-id "${{ steps.evaluation.outputs.run_id }}" \
      --output-file "deployment_decision.json"

    # Read decision and set output
    DEPLOY=$(jq -r '.deploy' deployment_decision.json)
    CONFIDENCE=$(jq -r '.confidence' deployment_decision.json)
    echo "deploy=$DEPLOY" >> $GITHUB_OUTPUT
    echo "confidence=$CONFIDENCE" >> $GITHUB_OUTPUT

- name: Deploy to staging
  if: steps.deployment_decision.outputs.deploy == 'true'
  run: |
    echo "Deploying to staging with confidence: ${{ steps.deployment_decision.outputs.confidence }}"
    # Your staging deployment commands here

- name: Run staging validation
  if: steps.deployment_decision.outputs.deploy == 'true'
  run: |
    # Additional validation in staging environment
    python scripts/staging_validation.py

- name: Deploy to production
  if: success() && steps.deployment_decision.outputs.confidence > '0.9'
  run: |
    echo "Deploying to production"
    # Your production deployment commands here
```

## Monitoring and Alerting

Effective monitoring transforms your CI/CD evaluation system from a simple pass/fail check into a comprehensive quality intelligence system. This monitoring serves multiple purposes: ensuring the evaluation system itself is working correctly, tracking quality trends over time, and providing early warning of systemic issues.

### Evaluation Metrics Dashboard

A well-designed dashboard provides visibility into both individual evaluation results and long-term trends. This visibility is crucial for understanding whether your quality is improving over time and for identifying patterns in failures.

Real-time monitoring of evaluation success rates helps you quickly identify when something in your evaluation system breaks. If success rates suddenly drop without corresponding code changes, it often indicates problems with the evaluation infrastructure rather than application quality.

Trend analysis helps you understand the long-term trajectory of your application quality. Are you improving over time? Are certain types of changes more likely to cause quality regressions? This historical perspective informs both immediate decisions and long-term strategy.

```python
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway


def send_metrics_to_monitoring(evaluation_results: Dict, pipeline_context: Dict):
    """Send evaluation metrics to monitoring system"""

    registry = CollectorRegistry()

    # Create metrics
    evaluation_score = Gauge(
        "ci_evaluation_score",
        "CI/CD evaluation score for quality metrics",
        ["metric_name", "app_version", "environment"],
        registry=registry,
    )

    evaluation_duration = Gauge(
        "ci_evaluation_duration_seconds",
        "Duration of CI/CD evaluation",
        ["app_version", "environment"],
        registry=registry,
    )

    # Record metrics
    for metric_name, score in evaluation_results["metrics"].items():
        evaluation_score.labels(
            metric_name=metric_name,
            app_version=pipeline_context["app_version"],
            environment=pipeline_context.get("environment", "ci"),
        ).set(score)

    evaluation_duration.labels(
        app_version=pipeline_context["app_version"],
        environment=pipeline_context.get("environment", "ci"),
    ).set(evaluation_results.get("duration_seconds", 0))

    # Push to monitoring system
    push_to_gateway(
        gateway=os.getenv("PROMETHEUS_PUSHGATEWAY"),
        job="ci_evaluation",
        registry=registry,
    )


def setup_evaluation_alerts():
    """Configure alerts for evaluation failures"""

    alert_config = {
        "channels": {
            "slack": os.getenv("SLACK_WEBHOOK_URL"),
            "email": os.getenv("ALERT_EMAIL_LIST"),
            "pagerduty": os.getenv("PAGERDUTY_INTEGRATION_KEY"),
        },
        "thresholds": {
            "critical_failure": 0.7,  # Alert if less than 70% of evaluations pass
            "performance_degradation": 0.1,  # Alert if 10% regression
            "consecutive_failures": 3,  # Alert after 3 consecutive failures
        },
    }

    return alert_config
```

### Advanced Monitoring

Beyond basic pass/fail tracking, advanced monitoring provides deeper insights into your application's quality evolution. This includes analyzing patterns in failures, tracking the relationship between code changes and quality metrics, and identifying early warning signs of quality degradation.

Failure pattern analysis helps you understand whether quality issues are random occurrences or systematic problems. If certain types of changes consistently cause specific quality metrics to fail, that indicates a need for targeted improvements in your development process.

Performance trend analysis tracks not just whether evaluations pass, but how they perform over time. Gradual degradation in quality scores, even within acceptable thresholds, might indicate emerging issues that need attention.

```python
def track_evaluation_trends():
    """Track evaluation trends over time"""

    # Query recent evaluation runs
    recent_runs = mlflow.search_runs(
        experiment_ids=["ci_cd_evaluation"],
        filter_string="tags.ci_quality_check != ''",
        max_results=100,
        order_by=["start_time DESC"],
    )

    # Calculate trends
    trends = {
        "success_rate": calculate_success_rate(recent_runs),
        "average_scores": calculate_average_scores(recent_runs),
        "failure_patterns": analyze_failure_patterns(recent_runs),
        "performance_trends": calculate_performance_trends(recent_runs),
    }

    # Generate trend report
    generate_trend_report(trends)

    return trends


def calculate_success_rate(runs_df: pd.DataFrame) -> Dict:
    """Calculate evaluation success rates"""

    total_runs = len(runs_df)
    successful_runs = len(runs_df[runs_df["tags.ci_quality_check"] == "passed"])

    return {
        "overall": successful_runs / total_runs if total_runs > 0 else 0,
        "last_7_days": calculate_recent_success_rate(runs_df, days=7),
        "last_30_days": calculate_recent_success_rate(runs_df, days=30),
    }


def analyze_failure_patterns(runs_df: pd.DataFrame) -> Dict:
    """Analyze patterns in evaluation failures"""

    failed_runs = runs_df[runs_df["tags.ci_quality_check"] == "failed"]

    if failed_runs.empty:
        return {"message": "No recent failures to analyze"}

    patterns = {
        "most_common_failures": extract_common_failure_reasons(failed_runs),
        "failure_by_time": analyze_temporal_patterns(failed_runs),
        "failure_by_version": analyze_version_patterns(failed_runs),
    }

    return patterns
```

## Security and Compliance

CI/CD evaluation systems handle sensitive data and have significant access to your infrastructure, making security a critical consideration. Your evaluation system needs to balance accessibility with security, ensuring that automated systems can function while maintaining appropriate controls.

### Secure CI/CD Practices

Security in CI/CD evaluation involves multiple layers: secure storage and transmission of evaluation datasets, proper management of API credentials, network security for communication with external services, and audit logging for compliance purposes.

Credential management is particularly important since evaluation systems often need access to multiple external services (LLM APIs, MLflow tracking servers, monitoring systems). Use your platform's secrets management features and rotate credentials regularly.

Data security is crucial since evaluation datasets often contain sensitive or proprietary information. Ensure datasets are encrypted both at rest and in transit, and consider data masking techniques for evaluation datasets that reduce sensitivity while maintaining utility.

```python
def setup_secure_evaluation_environment():
    """Configure secure environment for CI/CD evaluation"""

    security_config = {
        "secrets_management": {
            "mlflow_tracking_uri": "use_encrypted_secrets",
            "api_keys": "rotate_regularly",
            "access_tokens": "short_lived_tokens",
        },
        "network_security": {
            "restrict_outbound": ["approved_apis_only"],
            "use_vpc": True,
            "enable_audit_logging": True,
        },
        "data_protection": {
            "encrypt_datasets": True,
            "mask_sensitive_data": True,
            "secure_artifact_storage": True,
        },
    }

    return security_config


def audit_evaluation_access():
    """Audit access to evaluation resources"""

    audit_log = {
        "timestamp": pd.Timestamp.now().isoformat(),
        "user": os.getenv("CI_USER", "ci_system"),
        "resources_accessed": [
            "evaluation_dataset",
            "mlflow_tracking_server",
            "model_artifacts",
            "api_endpoints",
        ],
        "actions_performed": [
            "dataset_read",
            "model_load",
            "evaluation_run",
            "metrics_log",
        ],
    }

    # Log to audit system
    log_audit_event(audit_log)

    return audit_log
```

### Compliance Reporting

Many organizations require detailed audit trails for AI system deployments. Your CI/CD evaluation system should generate comprehensive compliance reports that document what was tested, when, by whom, and with what results.

Compliance reporting should include not just the final pass/fail decision, but also the complete evaluation process: which datasets were used, which metrics were applied, what thresholds were enforced, and who was responsible for the evaluation configuration.

Traceability is crucial for compliance - you should be able to trace from any production deployment back through the evaluation process to the specific code changes and evaluation results that led to that deployment.

```python
def generate_compliance_report(evaluation_results: Dict) -> Dict:
    """Generate compliance report for audit purposes"""

    report = {
        "evaluation_metadata": {
            "timestamp": pd.Timestamp.now().isoformat(),
            "evaluation_id": evaluation_results.get("run_id"),
            "dataset_version": evaluation_results.get("dataset_version"),
            "model_version": evaluation_results.get("model_version"),
        },
        "quality_assurance": {
            "metrics_evaluated": list(evaluation_results["metrics"].keys()),
            "thresholds_applied": evaluation_results.get("thresholds"),
            "gates_passed": evaluation_results.get("gates_passed"),
            "approval_status": "approved"
            if evaluation_results.get("gates_passed")
            else "rejected",
        },
        "traceability": {
            "code_commit": evaluation_results.get("git_commit"),
            "pipeline_run": evaluation_results.get("pipeline_id"),
            "responsible_team": evaluation_results.get("team"),
            "reviewer": evaluation_results.get("reviewer"),
        },
    }

    return report
```

## Scaling Considerations

As your organization grows and your GenAI applications become more sophisticated, your CI/CD evaluation system needs to scale accordingly. This involves both technical scaling (handling larger datasets and more frequent evaluations) and organizational scaling (supporting multiple teams and applications).

### Multi-Environment Strategy

Different environments have different evaluation needs. Development environments prioritize fast feedback over comprehensive testing, while production environments require thorough validation. Your evaluation strategy should reflect these different priorities.

Environment-specific configurations allow you to optimize the trade-off between speed and thoroughness for each context. Development evaluations might use smaller datasets and faster metrics, while production evaluations use comprehensive datasets and the full suite of quality metrics.

Resource allocation should also scale with environment importance. It's reasonable to use more computational resources for production evaluations than for development evaluations, reflecting the higher stakes involved.

<Tabs>
  <TabItem value="environment-config" label="Environment Configuration" default>
    ```python
    # Environment-specific configurations
    ENVIRONMENT_CONFIGS = {
        "development": {
            "evaluation_frequency": "on_commit",
            "dataset_size": "small",  # Faster feedback
            "metrics": ["basic_quality", "functionality"],
            "thresholds": "relaxed",
        },
        "staging": {
            "evaluation_frequency": "on_pr_merge",
            "dataset_size": "medium",
            "metrics": ["quality", "performance", "safety"],
            "thresholds": "moderate",
        },
        "production": {
            "evaluation_frequency": "on_release",
            "dataset_size": "full",
            "metrics": ["comprehensive"],
            "thresholds": "strict",
        },
    }
    ```
  </TabItem>
  <TabItem value="resource-scaling" label="Resource Scaling">
    ```python
    def configure_evaluation_resources(environment: str, dataset_size: str):
        """Configure compute resources based on evaluation needs"""

        resource_configs = {
            "small": {"cpu": 2, "memory": "4Gi", "timeout": "10m"},
            "medium": {"cpu": 4, "memory": "8Gi", "timeout": "20m"},
            "large": {"cpu": 8, "memory": "16Gi", "timeout": "45m"},
            "full": {"cpu": 16, "memory": "32Gi", "timeout": "90m"},
        }

        return resource_configs.get(dataset_size, resource_configs["medium"])
    ```
  </TabItem>
  <TabItem value="parallel-execution" label="Parallel Execution">
    ```python
    from concurrent.futures import ProcessPoolExecutor
    import multiprocessing as mp


    def run_distributed_evaluation(evaluation_config: Dict):
        """Run evaluation across multiple workers"""

        # Split evaluation dataset into chunks
        dataset_chunks = split_dataset_for_parallel_processing(
            evaluation_config["dataset"], num_workers=mp.cpu_count()
        )

        # Run evaluation in parallel
        with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
            futures = [
                executor.submit(evaluate_chunk, chunk, evaluation_config)
                for chunk in dataset_chunks
            ]

            # Collect results
            chunk_results = [future.result() for future in futures]

        # Aggregate results
        final_results = aggregate_evaluation_results(chunk_results)

        return final_results
    ```
  </TabItem>
</Tabs>

## Key Takeaways

### Critical Success Factors

- **Automation is Essential**: Automated evaluations ensure consistent quality checks for every change
- **Quality Gates Prevent Regressions**: Well-defined thresholds act as objective gatekeepers
- **Early Feedback Saves Time**: Catching issues in CI/CD is much cheaper than fixing them in production
- **Baseline Comparisons Add Value**: Comparing against production baselines helps detect subtle regressions
- **Comprehensive Reporting Enables Debugging**: Detailed evaluation reports help teams understand and fix issues

### Implementation Recommendations

1. **Start Simple**: Begin with basic quality gates and gradually add sophistication
2. **Focus on Critical Metrics**: Prioritize metrics that directly impact user experience
3. **Invest in Monitoring**: Set up comprehensive monitoring and alerting for evaluation trends
4. **Document Everything**: Maintain clear documentation of thresholds, datasets, and processes
5. **Regular Reviews**: Periodically review and update quality gates as your application evolves

### Prerequisites for Success

- **Mature Evaluation Process**: Well-defined evaluation datasets, metrics, and processes from development
- **Robust CI/CD Infrastructure**: Reliable CI/CD system with appropriate compute resources
- **Version-Controlled Assets**: Evaluation datasets and configurations under version control
- **Monitoring Infrastructure**: Systems to track evaluation trends and alert on issues
- **Team Training**: Development team understands the evaluation process and quality gates

By integrating MLflow evaluations into your CI/CD pipeline, you establish a robust automated quality assurance process that's critical for delivering high-quality GenAI applications reliably and efficiently. This systematic approach provides the confidence needed to deploy AI systems at scale while maintaining quality standards.
