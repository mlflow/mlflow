import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Using Evaluation Results to Compare Versions of Your App

As you iterate on your GenAI application‚Äîrefining prompts, switching models, or adjusting parameters‚Äîyou need a systematic way to determine if your changes actually improve performance. MLflow's evaluation and tracking capabilities provide a structured approach to version comparison, enabling data-driven decisions about which changes to keep.

## Why Version Comparison Matters

Effective version comparison transforms GenAI development from guesswork into a scientific process:

- **Objective Validation**: Measure concrete improvements across quality dimensions rather than relying on subjective impressions
- **Regression Detection**: Catch when new features break existing functionality before they reach production
- **Trade-off Understanding**: Recognize when improvements in one area (like accuracy) come at the cost of another (like response time)
- **Progress Tracking**: Build a history of quality evolution to guide future development decisions

## Core Methodology: Consistent Evaluation Framework

The foundation of reliable version comparison is consistency. You must evaluate each application version under identical conditions so that performance differences can be attributed to your changes rather than evaluation variations.

This requires three key components:

1. **Standardized Evaluation Setup**: Same datasets, metrics, and evaluation environment
2. **Systematic Evaluation Execution**: Running `mlflow.evaluate()` for each version with proper tracking
3. **Structured Comparison Analysis**: Using MLflow's UI and APIs to analyze results

## Setting Up Consistent Evaluation

### Preparing Your Evaluation Dataset

Your evaluation dataset serves as the common benchmark for all versions. Using different datasets makes it impossible to determine whether performance changes result from your improvements or dataset variations.

<Tabs>
<TabItem value="dataset_prep" label="Dataset Preparation">

```python
import mlflow
import pandas as pd

# Create a standardized evaluation dataset
evaluation_data = pd.DataFrame(
    {
        "inputs": [
            "What is MLflow and how does it help with ML lifecycle management?",
            "Explain the difference between Apache Spark and traditional MapReduce.",
            "How do you handle model versioning in production ML systems?",
            "What are the key considerations for deploying ML models at scale?",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for managing the end-to-end machine learning lifecycle...",
            "Apache Spark improves upon MapReduce by providing in-memory processing capabilities...",
            "Model versioning in production requires systematic tracking of model artifacts...",
            "Key deployment considerations include scalability, monitoring, and model governance...",
        ],
    }
)

# Save dataset for consistent reuse
eval_dataset = mlflow.data.from_pandas(
    evaluation_data, name="app_eval_dataset", version="v1.0"
)
print(f"Created evaluation dataset with {len(evaluation_data)} samples")
```

</TabItem>
<TabItem value="metrics_suite" label="Metrics Suite">

```python
from mlflow.metrics import exact_match, flesch_kincaid_grade_level
from mlflow.metrics.genai import answer_relevance, answer_correctness


# Define standardized metrics suite
def get_standard_metrics():
    """Return the standard metrics suite for version comparison."""
    return [
        exact_match(),
        flesch_kincaid_grade_level(),
        answer_relevance(model="openai:/gpt-4o-mini"),
        answer_correctness(model="openai:/gpt-4o-mini"),
    ]


standard_metrics = get_standard_metrics()
print(f"Standard metrics suite includes {len(standard_metrics)} metrics")
```

</TabItem>
</Tabs>

## Defining and Evaluating Application Versions

### Application Version Examples

Before running evaluations, clearly define what constitutes each "version" of your application. Each version should represent a specific change you want to test‚Äîwhether that's a different prompting strategy, model choice, or parameter setting. Clear version definitions help you understand which changes drive performance improvements.

<Tabs>
<TabItem value="prompt_versions" label="Prompt Variations">

```python
import openai
from typing import List


def app_v1_basic(inputs_df: pd.DataFrame) -> List[str]:
    """Version 1: Basic prompting approach."""
    predictions = []
    for _, row in inputs_df.iterrows():
        completion = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": row["inputs"]}],
            temperature=0.1,
        )
        predictions.append(completion.choices[0].message.content)
    return predictions


def app_v2_enhanced(inputs_df: pd.DataFrame) -> List[str]:
    """Version 2: Enhanced with system instructions."""
    predictions = []
    system_prompt = """You are a knowledgeable technical assistant. Provide accurate,
    well-structured answers with specific examples when relevant."""

    for _, row in inputs_df.iterrows():
        completion = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": row["inputs"]},
            ],
            temperature=0.1,
        )
        predictions.append(completion.choices[0].message.content)
    return predictions
```

</TabItem>
<TabItem value="model_versions" label="Model Variations">

```python
def app_gpt4_mini(inputs_df: pd.DataFrame) -> List[str]:
    """Version using GPT-4o-mini."""
    return _generate_responses(inputs_df, "gpt-4o-mini")


def app_gpt4(inputs_df: pd.DataFrame) -> List[str]:
    """Version using GPT-4."""
    return _generate_responses(inputs_df, "gpt-4")


def _generate_responses(inputs_df: pd.DataFrame, model: str) -> List[str]:
    """Helper function to generate responses with different models."""
    predictions = []
    system_prompt = "Provide accurate, helpful responses to technical questions."

    for _, row in inputs_df.iterrows():
        completion = openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": row["inputs"]},
            ],
            temperature=0.1,
        )
        predictions.append(completion.choices[0].message.content)
    return predictions
```

</TabItem>
</Tabs>

### Running Systematic Evaluations

Execute evaluations for each version while maintaining consistent tracking and documentation. This process creates separate MLflow runs for each version, making it easy to compare results later. The key is to use identical evaluation data and metrics for all versions so that performance differences reflect your application changes, not evaluation inconsistencies.

```python
# Set up experiment for version comparison
mlflow.set_experiment("GenAI_App_Version_Comparison")

# Define versions to evaluate
apps_to_evaluate = {
    "v1_basic": app_v1_basic,
    "v2_enhanced": app_v2_enhanced,
    "v3_gpt4": app_gpt4,
}

# Execute evaluations
evaluation_run_ids = {}

for version_name, app_function in apps_to_evaluate.items():
    print(f"Evaluating {version_name}...")

    with mlflow.start_run(run_name=f"eval_{version_name}") as run:
        try:
            eval_results = mlflow.evaluate(
                model=app_function,
                data=evaluation_data,
                targets="ground_truth",
                extra_metrics=standard_metrics,
                evaluators="default",
            )

            evaluation_run_ids[version_name] = run.info.run_id

            # Log version-specific context
            mlflow.set_tag("version_type", "comparison_candidate")
            if "enhanced" in version_name:
                mlflow.log_param("prompt_strategy", "enhanced_system_prompt")
            elif "gpt4" in version_name:
                mlflow.log_param("model", "gpt-4")

            print(f"‚úÖ Completed {version_name}")

        except Exception as e:
            print(f"‚ùå Error evaluating {version_name}: {e}")
            evaluation_run_ids[version_name] = None

print(
    f"Successfully evaluated: {sum(1 for rid in evaluation_run_ids.values() if rid is not None)}/{len(apps_to_evaluate)} versions"
)
```

## Analyzing Comparison Results

### Visual Comparison in MLflow UI

The MLflow UI provides comprehensive comparison tools that make it easy to spot performance differences across versions. The visual interface is particularly helpful for quickly identifying which versions excel in specific metrics and understanding overall performance patterns.

1. Navigate to your "GenAI_App_Version_Comparison" experiment
2. Switch to the "Evaluation" tab
3. Select the runs corresponding to versions you want to compare
4. Click "Compare" to see side-by-side metrics and detailed analysis

### Programmatic Analysis

For deeper analysis, use MLflow's tracking API to compare results programmatically. This approach is especially valuable when you need to automate decision-making, perform statistical analysis, or generate custom reports. The programmatic approach also allows you to implement your own ranking algorithms based on your specific priorities.

<Tabs>
<TabItem value="basic_comparison" label="Basic Comparison">

```python
def analyze_version_performance(run_ids: dict):
    """Analyze and compare performance across different versions."""

    # Filter out failed runs
    valid_runs = {name: rid for name, rid in run_ids.items() if rid is not None}

    if not valid_runs:
        print("No valid runs available for comparison")
        return None

    # Fetch run data
    runs_df = mlflow.search_runs(run_ids=list(valid_runs.values()))
    runs_df["version_name"] = runs_df["run_id"].map(
        {v: k for k, v in valid_runs.items()}
    )

    # Extract metrics for comparison
    metric_columns = [col for col in runs_df.columns if col.startswith("metrics.")]
    comparison_cols = ["version_name"] + metric_columns
    comparison_df = runs_df[comparison_cols].set_index("version_name")

    # Clean column names
    comparison_df.columns = [
        col.replace("metrics.", "") for col in comparison_df.columns
    ]

    return comparison_df


# Perform the analysis
comparison_results = analyze_version_performance(evaluation_run_ids)

if comparison_results is not None:
    print("üìä Version Performance Comparison:")
    print(comparison_results.round(3))

    # Identify best performing versions
    key_metrics = ["answer_correctness/v1/mean", "answer_relevance/v1/mean"]
    available_metrics = [m for m in key_metrics if m in comparison_results.columns]

    if available_metrics:
        print("\nüèÜ Best Performing Versions:")
        for metric in available_metrics:
            best_version = comparison_results[metric].idxmax()
            best_score = comparison_results.loc[best_version, metric]
            print(f"   {metric}: {best_version} ({best_score:.3f})")
```

</TabItem>
<TabItem value="recommendations" label="Generate Recommendations">

```python
def generate_recommendations(comparison_df):
    """Generate version selection recommendations."""

    if comparison_df is None or len(comparison_df) == 0:
        return

    print("\nüí° Recommendations:")

    # Simple weighted scoring
    key_metrics = ["answer_correctness/v1/mean", "answer_relevance/v1/mean"]
    available_metrics = [m for m in key_metrics if m in comparison_df.columns]

    if len(available_metrics) >= 2:
        # Calculate overall score
        weights = {"answer_correctness/v1/mean": 0.6, "answer_relevance/v1/mean": 0.4}

        overall_scores = pd.Series(0.0, index=comparison_df.index)
        for metric in available_metrics:
            if metric in weights:
                # Normalize to 0-1 scale
                metric_values = comparison_df[metric].dropna()
                if len(metric_values) > 1:
                    normalized = (metric_values - metric_values.min()) / (
                        metric_values.max() - metric_values.min()
                    )
                    overall_scores += normalized * weights[metric]

        # Show top recommendations
        ranked = overall_scores.sort_values(ascending=False)
        print("   Overall Rankings:")
        for i, (version, score) in enumerate(ranked.items(), 1):
            print(f"   {i}. {version} (score: {score:.3f})")

    print("\n   Consider:")
    print("   ‚Ä¢ Review top performer for production deployment")
    print("   ‚Ä¢ Analyze individual responses for quality assessment")
    print("   ‚Ä¢ Factor in computational cost and latency requirements")


# Generate recommendations
if comparison_results is not None:
    generate_recommendations(comparison_results)
```

</TabItem>
</Tabs>

## Key Considerations

### Statistical Significance

For meaningful comparisons, especially with smaller datasets, consider whether observed differences are statistically significant. Small performance differences might simply be due to random variation rather than actual improvements. Understanding the reliability of your comparisons helps you avoid making decisions based on noise.

```python
def assess_comparison_reliability(comparison_df, dataset_size):
    """Assess the reliability of version comparisons."""

    print(f"\n‚öñÔ∏è Comparison Reliability (Dataset size: {dataset_size} samples):")

    if dataset_size < 10:
        print("‚ö†Ô∏è  Small dataset - differences may not be meaningful")
        print("   Consider expanding evaluation dataset for reliable comparisons")
    else:
        print("‚úÖ Dataset size adequate for comparison")

    # Check metric variance
    if comparison_df is not None:
        numeric_cols = comparison_df.select_dtypes(include=[np.number]).columns
        for metric in numeric_cols:
            values = comparison_df[metric].dropna()
            if len(values) > 1:
                cv = (
                    values.std() / values.mean() if values.mean() != 0 else float("inf")
                )
                if cv < 0.1:
                    print(
                        f"üìä {metric}: Low variance - differences may not be meaningful"
                    )


# Run reliability assessment
if comparison_results is not None:
    assess_comparison_reliability(comparison_results, len(evaluation_data))
```

### Version Documentation

Document what changed between versions for better decision-making. Good documentation helps you understand not just which version performed best, but why certain changes improved or hurt performance. This knowledge is invaluable for guiding future development efforts.

```python
# Example version documentation
version_docs = {
    "v1_basic": "Baseline with minimal prompting",
    "v2_enhanced": "Added system prompt with role definition",
    "v3_gpt4": "Upgraded to GPT-4 for improved quality",
}

# Log during evaluation runs
for version_name, description in version_docs.items():
    # This would be logged during the evaluation run
    print(f"{version_name}: {description}")
```

## Related Concepts

- **[Evaluation Runs](/genai/eval-monitor/concepts/evaluation-runs)**: Understanding how evaluation results are stored and organized
- **[Scorers](/genai/eval-monitor/concepts/scorers)**: Deep dive into the metrics used for comparison
- **[Evaluation Datasets](/genai/eval-monitor/concepts/eval-datasets)**: Learn about versioned datasets for consistent testing
- **[LLM Evaluation](/genai/eval-monitor)**: Comprehensive guide to LLM evaluation metrics

## Summary

Systematic version comparison enables data-driven GenAI development. By maintaining consistent evaluation setups, running structured comparisons, and analyzing results both visually and programmatically, you can confidently iterate while maintaining quality standards.

**Key Success Factors**:
- Use identical datasets and metrics across all version evaluations
- Document what changed between versions and why
- Consider multiple quality dimensions, not just single metrics
- Validate that differences are meaningful given your dataset size

**Quick Start Checklist**:
1. Create standardized evaluation dataset and metrics suite
2. Define clear version naming and documentation practices
3. Run systematic evaluations with MLflow tracking
4. Use UI and programmatic analysis to compare results
5. Make data-driven decisions about which versions to adopt