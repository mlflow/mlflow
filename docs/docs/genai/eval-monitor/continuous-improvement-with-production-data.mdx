# Continuous Improvement of GenAI Apps with Production Data

Continuously improving your GenAI application involves monitoring its performance in production, identifying areas for enhancement, and using real-world data to guide development and testing. Production traffic and traces provide invaluable insights into how your application performs, where it excels, and where it falls short.

MLflow's comprehensive tracing and evaluation capabilities support this entire continuous improvement lifecycle, creating a powerful feedback loop to debug issues, refine prompts, enhance retrieval strategies, and deliver better user experiences.

## Why Use Production Data?

Production data offers unique advantages for improving GenAI applications:

- **Real-World Relevance**: Reflects actual user behavior and usage patterns
- **Identify Blind Spots**: Uncover edge cases and unexpected query patterns not anticipated during development
- **Data for Iteration**: Create datasets for offline evaluation, regression testing, and fine-tuning
- **Debug Complex Issues**: Individual traces provide detailed context for diagnosing specific failures
- **Understand User Intent**: Analyze popular queries and interaction patterns to better meet user needs
- **Maintain Quality Over Time**: Detect and address quality degradation from model drift or changing data distributions
- **Ensure Operational Health**: Track performance metrics like latency, error rates, and costs

## The Continuous Improvement Workflow

### Step 1: Monitor Your Application in Production

Continuous production monitoring extends evaluation practices from development into the live environment.

#### Operational Metrics

Track essential operational health indicators:

```python
import mlflow
import time


def track_operational_metrics(start_time, tokens_used, cost):
    """Track operational metrics during production requests"""

    # Calculate latency
    latency = time.time() - start_time

    # Log operational metrics
    mlflow.log_metric("request_latency_ms", latency * 1000)
    mlflow.log_metric("tokens_used", tokens_used)
    mlflow.log_metric("request_cost", cost)

    # Set operational tags
    mlflow.set_tag("request_status", "success")
    mlflow.set_tag("model_version", "v1.2.0")
```

**Key Metrics to Monitor:**
- Request latency (end-to-end, LLM, retrieval)
- Error rates (application, LLM, dependency)
- Throughput and token consumption
- API costs and resource usage

#### Quality Monitoring

Schedule automated quality scorers to run on production traces:

```python
import mlflow
from mlflow.metrics import exact_match


def setup_quality_monitoring():
    """Set up automated quality scoring for production traces"""

    # Define quality metrics
    quality_metrics = [
        exact_match(),
        # Add custom quality scorers
    ]

    # Sample production traces for evaluation
    recent_traces = mlflow.search_traces(
        filter_string="timestamp > '2024-01-01'", max_results=100
    )

    # Run quality assessment
    for trace in recent_traces.iterrows():
        try:
            # Extract relevant data
            trace_data = trace[1]

            # Run quality evaluation
            quality_scores = evaluate_trace_quality(trace_data, quality_metrics)

            # Log quality scores back to the trace
            mlflow.log_metric("relevance_score", quality_scores.get("relevance", 0))
            mlflow.log_metric(
                "groundedness_score", quality_scores.get("groundedness", 0)
            )

        except Exception as e:
            print(
                f"Quality evaluation failed for trace {trace_data.get('request_id')}: {e}"
            )


def evaluate_trace_quality(trace_data, metrics):
    """Evaluate quality of a single trace"""
    # Implementation depends on your specific metrics
    # Return dictionary of quality scores
    return {"relevance": 0.8, "groundedness": 0.9}
```

#### Custom Dashboards and Alerting

Build monitoring dashboards to visualize trends:

```python
def create_monitoring_dashboard():
    """Create monitoring dashboard for production metrics"""

    # Query recent metrics
    recent_runs = mlflow.search_runs(
        experiment_ids=["production_monitoring"],
        max_results=1000,
        order_by=["start_time DESC"],
    )

    # Calculate key statistics
    avg_latency = recent_runs["metrics.request_latency_ms"].mean()
    error_rate = (recent_runs["tags.request_status"] == "error").mean()
    avg_relevance = recent_runs["metrics.relevance_score"].mean()

    # Set up alerts based on thresholds
    alerts = []
    if avg_latency > 2000:  # 2 seconds
        alerts.append(f"High latency detected: {avg_latency:.0f}ms")

    if error_rate > 0.05:  # 5% error rate
        alerts.append(f"High error rate detected: {error_rate:.2%}")

    if avg_relevance < 0.7:  # 70% relevance threshold
        alerts.append(f"Low relevance score detected: {avg_relevance:.2f}")

    return {
        "avg_latency": avg_latency,
        "error_rate": error_rate,
        "avg_relevance": avg_relevance,
        "alerts": alerts,
    }
```

### Step 2: Identify and Select Relevant Traces

When monitoring reveals issues, investigate by selecting relevant traces for analysis.

#### Selection Criteria

Identify traces that are particularly interesting for deeper analysis:

- Traces with negative user feedback
- Traces where automated quality judges indicated poor performance
- Traces that resulted in errors or unexpected behavior
- Frequently occurring query patterns with suboptimal responses
- Examples of successful interactions for regression testing
- Traces representing critical user journeys

#### Tools for Selection

```python
def find_problematic_traces():
    """Find traces that need investigation"""

    # Find traces with negative feedback
    negative_feedback_traces = mlflow.search_traces(
        filter_string="assessments.user_feedback.value = 'negative'"
    )

    # Find traces with low quality scores
    low_quality_traces = mlflow.search_traces(
        filter_string="assessments.relevance_score.value < 0.5"
    )

    # Find error traces
    error_traces = mlflow.search_traces(filter_string="status = 'ERROR'")

    # Find high-latency traces
    slow_traces = mlflow.search_traces(
        filter_string="metrics.request_latency_ms > 5000"
    )

    return {
        "negative_feedback": negative_feedback_traces,
        "low_quality": low_quality_traces,
        "errors": error_traces,
        "slow_responses": slow_traces,
    }


def analyze_trace_patterns(traces_df):
    """Analyze patterns in problematic traces"""

    # Common query patterns
    query_patterns = traces_df["inputs"].value_counts().head(10)

    # Error distribution
    error_types = traces_df["tags.error_type"].value_counts()

    # Time-based patterns
    traces_df["hour"] = pd.to_datetime(traces_df["start_time"]).dt.hour
    hourly_distribution = traces_df["hour"].value_counts().sort_index()

    return {
        "common_queries": query_patterns,
        "error_types": error_types,
        "hourly_pattern": hourly_distribution,
    }
```

### Step 3: Bring Production Insights into Development

Convert selected production traces into evaluation datasets for systematic improvement.

#### Create Evaluation Datasets from Traces

```python
import pandas as pd
from mlflow.data.evaluation_dataset import EvaluationDataset


def create_dataset_from_traces(traces_df, dataset_name):
    """Create an evaluation dataset from production traces"""

    # Extract relevant fields
    evaluation_data = []
    for _, trace in traces_df.iterrows():
        evaluation_data.append(
            {
                "inputs": trace.get("inputs", {}),
                "outputs": trace.get("outputs", {}),
                "expected_response": trace.get("outputs", {}).get("response", ""),
                "trace_id": trace.get("request_id", ""),
                "original_score": trace.get("assessments", {})
                .get("relevance_score", {})
                .get("value", 0),
            }
        )

    # Create DataFrame
    eval_df = pd.DataFrame(evaluation_data)

    # Create MLflow EvaluationDataset
    dataset = mlflow.data.from_pandas(
        eval_df, source=f"production_traces_{dataset_name}", name=dataset_name
    )

    # Log the dataset
    with mlflow.start_run(run_name=f"dataset_creation_{dataset_name}"):
        mlflow.log_input(dataset, context="evaluation")
        mlflow.set_tag("dataset_type", "production_derived")
        mlflow.set_tag("dataset_purpose", "continuous_improvement")

    return dataset


def augment_existing_dataset(existing_dataset_name, new_traces_df):
    """Add new traces to an existing evaluation dataset"""

    # Load existing dataset
    existing_dataset = mlflow.data.load_dataset(existing_dataset_name)
    existing_df = existing_dataset.df

    # Create new data from traces
    new_data = create_dataset_from_traces(new_traces_df, "temp")
    new_df = new_data.df

    # Combine datasets
    combined_df = pd.concat([existing_df, new_df], ignore_index=True)

    # Remove duplicates based on inputs
    combined_df = combined_df.drop_duplicates(subset=["inputs"], keep="last")

    # Create updated dataset
    updated_dataset = mlflow.data.from_pandas(
        combined_df,
        source=f"augmented_{existing_dataset_name}",
        name=f"{existing_dataset_name}_v2",
    )

    return updated_dataset
```

#### Direct Ad-hoc Evaluation

```python
def quick_evaluation_on_production_samples():
    """Quick evaluation using production samples"""

    # Get recent problematic queries
    recent_issues = mlflow.search_traces(
        filter_string="assessments.relevance_score.value < 0.6", max_results=20
    )

    # Extract just the inputs for quick testing
    test_inputs = []
    for _, trace in recent_issues.iterrows():
        test_inputs.append(trace.get("inputs", {}))

    # Test with current development model
    results = []
    for input_data in test_inputs:
        try:
            # Run your development model
            response = your_development_model.predict(input_data)
            results.append(
                {
                    "input": input_data,
                    "response": response,
                    "timestamp": pd.Timestamp.now(),
                }
            )
        except Exception as e:
            print(f"Evaluation failed for input {input_data}: {e}")

    return results
```

### Step 4: Evaluate, Iterate, and Refine

Use production-derived datasets to systematically improve your application.

#### Systematic Evaluation

```python
def evaluate_with_production_data(model, dataset_name):
    """Evaluate model using production-derived dataset"""

    # Load the dataset
    dataset = mlflow.data.load_dataset(dataset_name)

    # Run evaluation
    with mlflow.start_run(run_name=f"production_eval_{dataset_name}"):
        results = mlflow.evaluate(
            model=model,
            data=dataset.df,
            targets="expected_response",
            extra_metrics=[
                mlflow.metrics.exact_match(),
                # Add other relevant metrics
            ],
        )

        # Log additional context
        mlflow.set_tag("evaluation_type", "production_derived")
        mlflow.set_tag("dataset_source", "production_traces")
        mlflow.log_param("dataset_size", len(dataset.df))

        return results


def compare_against_production_baseline(current_results, production_traces_df):
    """Compare current model performance against production baseline"""

    comparison_results = {}

    # Calculate baseline metrics from production
    baseline_scores = production_traces_df["original_score"].mean()
    current_scores = current_results.metrics["exact_match"]

    improvement = current_scores - baseline_scores

    comparison_results = {
        "baseline_score": baseline_scores,
        "current_score": current_scores,
        "improvement": improvement,
        "improvement_pct": (improvement / baseline_scores) * 100
        if baseline_scores > 0
        else 0,
    }

    # Log comparison results
    mlflow.log_metrics(
        {
            "baseline_vs_current_improvement": improvement,
            "baseline_vs_current_improvement_pct": comparison_results[
                "improvement_pct"
            ],
        }
    )

    return comparison_results
```

#### Iterative Improvement Process

```python
def iterative_improvement_workflow(problem_area="feature_x"):
    """Complete workflow for iterative improvement"""

    print(f"Starting improvement workflow for: {problem_area}")

    # Step 1: Identify problematic traces
    problematic_traces = mlflow.search_traces(
        filter_string=f"inputs.query CONTAINS '{problem_area}' AND assessments.relevance_score.value < 0.5"
    )

    print(f"Found {len(problematic_traces)} problematic traces")

    # Step 2: Create evaluation dataset
    dataset = create_dataset_from_traces(
        problematic_traces, f"{problem_area}_improvement_dataset"
    )

    # Step 3: Baseline evaluation
    print("Running baseline evaluation...")
    baseline_results = evaluate_with_production_data(
        your_current_model, f"{problem_area}_improvement_dataset"
    )

    # Step 4: Iterate on improvements
    improvement_iterations = []

    for iteration in range(3):  # Try 3 different improvements
        print(f"Iteration {iteration + 1}: Implementing improvements...")

        # Implement your improvements here
        improved_model = implement_improvements(your_current_model, iteration)

        # Evaluate improved model
        improved_results = evaluate_with_production_data(
            improved_model, f"{problem_area}_improvement_dataset"
        )

        improvement_iterations.append(
            {
                "iteration": iteration + 1,
                "results": improved_results,
                "improvement_description": f"Iteration {iteration + 1} changes",
            }
        )

        # Check if improvement is significant
        improvement = (
            improved_results.metrics["exact_match"]
            - baseline_results.metrics["exact_match"]
        )
        if improvement > 0.1:  # 10% improvement threshold
            print(
                f"Significant improvement found in iteration {iteration + 1}: +{improvement:.2%}"
            )
            break

    return improvement_iterations


def implement_improvements(model, iteration):
    """Implement specific improvements based on iteration"""
    # This would contain your actual improvement logic
    # Examples:
    # - Iteration 0: Adjust prompts
    # - Iteration 1: Update retrieval strategy
    # - Iteration 2: Fine-tune model parameters

    # Return the improved model
    return model
```

## Preparing Data for Fine-Tuning

Production traces can provide excellent training data for fine-tuning models.

### Selection Criteria for Fine-Tuning

```python
def select_fine_tuning_data():
    """Select high-quality traces for fine-tuning"""

    # High-quality positive examples
    positive_examples = mlflow.search_traces(
        filter_string="assessments.relevance_score.value > 0.8 AND assessments.user_feedback.value = 'positive'"
    )

    # Poor examples that need correction (require human review)
    negative_examples = mlflow.search_traces(
        filter_string="assessments.relevance_score.value < 0.4 AND assessments.user_feedback.value = 'negative'"
    )

    return {
        "positive_examples": positive_examples,
        "negative_examples": negative_examples,
    }


def prepare_fine_tuning_dataset(positive_traces, negative_traces_with_corrections):
    """Prepare dataset for fine-tuning"""

    fine_tuning_data = []

    # Add positive examples
    for _, trace in positive_traces.iterrows():
        fine_tuning_data.append(
            {
                "prompt": trace.get("inputs", {}).get("query", ""),
                "completion": trace.get("outputs", {}).get("response", ""),
                "quality": "high",
            }
        )

    # Add corrected negative examples
    for _, trace in negative_traces_with_corrections.iterrows():
        fine_tuning_data.append(
            {
                "prompt": trace.get("inputs", {}).get("query", ""),
                "completion": trace.get(
                    "corrected_response", ""
                ),  # Human-corrected response
                "quality": "corrected",
            }
        )

    # Save for fine-tuning
    fine_tuning_df = pd.DataFrame(fine_tuning_data)

    # Log as MLflow dataset
    dataset = mlflow.data.from_pandas(
        fine_tuning_df,
        source="production_fine_tuning_data",
        name="fine_tuning_dataset_v1",
    )

    with mlflow.start_run(run_name="fine_tuning_data_prep"):
        mlflow.log_input(dataset, context="training")
        mlflow.set_tag("dataset_purpose", "fine_tuning")
        mlflow.log_param("positive_examples", len(positive_traces))
        mlflow.log_param("negative_examples", len(negative_traces_with_corrections))

    return dataset
```

## Monitoring and Alerting Setup

Set up comprehensive monitoring for continuous improvement.

```python
def setup_continuous_monitoring():
    """Set up monitoring for continuous improvement workflow"""

    monitoring_config = {
        "quality_thresholds": {
            "relevance_score": 0.7,
            "groundedness_score": 0.8,
            "user_satisfaction": 0.75,
        },
        "operational_thresholds": {
            "avg_latency_ms": 2000,
            "error_rate": 0.05,
            "cost_per_request": 0.10,
        },
        "alert_frequency": "hourly",
        "dashboard_refresh": "5min",
    }

    return monitoring_config


def generate_improvement_report():
    """Generate comprehensive improvement report"""

    # Query recent data
    recent_traces = mlflow.search_traces(
        filter_string="timestamp > '2024-01-01'", max_results=1000
    )

    # Calculate trends
    weekly_quality = calculate_weekly_quality_trends(recent_traces)
    improvement_opportunities = identify_improvement_opportunities(recent_traces)

    report = {
        "summary": {
            "total_traces_analyzed": len(recent_traces),
            "avg_quality_score": recent_traces[
                "assessments.relevance_score.value"
            ].mean(),
            "improvement_opportunities": len(improvement_opportunities),
        },
        "trends": weekly_quality,
        "recommendations": improvement_opportunities,
        "next_steps": generate_next_steps(improvement_opportunities),
    }

    # Log report
    with mlflow.start_run(run_name="improvement_report"):
        mlflow.log_dict(report, "improvement_report.json")
        mlflow.set_tag("report_type", "continuous_improvement")

    return report


def calculate_weekly_quality_trends(traces_df):
    """Calculate quality trends over time"""
    traces_df["week"] = pd.to_datetime(traces_df["start_time"]).dt.isocalendar().week
    weekly_avg = traces_df.groupby("week")["assessments.relevance_score.value"].mean()
    return weekly_avg.to_dict()


def identify_improvement_opportunities(traces_df):
    """Identify specific areas for improvement"""

    opportunities = []

    # Find query patterns with consistently low scores
    low_score_patterns = (
        traces_df[traces_df["assessments.relevance_score.value"] < 0.6]["inputs"]
        .value_counts()
        .head(5)
    )

    for pattern, count in low_score_patterns.items():
        opportunities.append(
            {
                "type": "query_pattern",
                "description": f"Query pattern '{pattern}' has low scores",
                "frequency": count,
                "priority": "high" if count > 10 else "medium",
            }
        )

    return opportunities


def generate_next_steps(opportunities):
    """Generate actionable next steps"""

    next_steps = []

    for opp in opportunities:
        if opp["priority"] == "high":
            next_steps.append(f"Create focused dataset for '{opp['description']}'")
            next_steps.append(f"Implement targeted improvements for this pattern")

    return next_steps
```

## Best Practices

### Data Quality Management

- **Human Review**: Always review production data before using for training
- **Data Cleaning**: Remove PII and sanitize sensitive information
- **Quality Validation**: Verify that selected traces represent real issues
- **Version Control**: Track dataset versions and improvements over time

### Continuous Monitoring Strategy

- **Balanced Sampling**: Don't only focus on negative examples - include positive ones for regression testing
- **Regular Updates**: Refresh evaluation datasets with new production patterns
- **Threshold Tuning**: Regularly review and adjust quality thresholds based on business requirements
- **Cross-Validation**: Validate improvements on both production-derived and synthetic datasets

### Implementation Guidelines

1. **Start Small**: Begin with a subset of traces to validate the workflow
2. **Automate Gradually**: Start with manual processes, then automate successful patterns
3. **Monitor Impact**: Track whether improvements in development translate to production gains
4. **Iterate Frequently**: Run improvement cycles regularly, not just when problems arise

## Key Takeaways

### Critical Success Factors

- **Iterative Cycle**: Treat GenAI development as continuous improvement, not one-time deployment
- **Real-World Focus**: Production traces provide the most realistic improvement data
- **Targeted Approach**: Focus improvements on specific, identified weaknesses
- **Balanced Insights**: Combine automated assessments with human feedback
- **Quality Assurance**: Maintain high data quality standards for training datasets

### Implementation Roadmap

1. **Set Up Production Monitoring**: Implement comprehensive tracing and quality monitoring
2. **Build Analysis Workflows**: Create processes to identify and analyze problematic traces
3. **Establish Dataset Management**: Develop systematic approaches to creating evaluation datasets
4. **Implement Improvement Cycles**: Create regular workflows for testing and deploying improvements
5. **Monitor Long-term Trends**: Track whether improvements sustain over time

## Prerequisites

**Technical Requirements:**
- GenAI application deployed with MLflow Tracing configured
- Access to MLflow Tracking server with trace data
- Evaluation infrastructure for testing improvements

**Process Requirements:**
- Defined quality metrics and acceptable thresholds
- Process for human review and data curation
- Development workflow that can incorporate production insights

**Tools and Skills:**
- Familiarity with `mlflow.search_traces()`, `mlflow.data.EvaluationDataset`, and `mlflow.evaluate()`
- Data analysis capabilities for identifying patterns and trends
- Understanding of your application's architecture and improvement points

By systematically using production data in a continuous monitoring and improvement loop, you can significantly enhance the relevance, effectiveness, and robustness of your GenAI application over time. This approach ensures your application evolves with real user needs and maintains high quality in production environments.