# Evaluate & Monitor

MLflow's evaluation and monitoring capabilities help you systematically measure, improve, and maintain the quality of your GenAI applications throughout their lifecycle. From development through production, use the same quality metrics and methodologies to ensure your applications deliver accurate, reliable responses while managing cost and latency.

## Overview

### Evaluation During Development

Building reliable GenAI applications requires systematic testing throughout the development process. MLflow's evaluation harness provides a comprehensive framework for measuring quality using both pre-built and custom scorers. As you iterate on your application, you can compare different versions side-by-side to understand the impact of changes. When issues arise, detailed traces help you identify root causes and fix problems before they reach production. Automated testing capabilities ensure that new improvements don't inadvertently break existing functionality.

### Monitoring in Production

Once your application is deployed, continuous monitoring becomes essential for maintaining quality and performance. MLflow extends the same evaluation framework used during development into production environments, automatically assessing quality on live traffic. Beyond quality metrics, the system tracks operational indicators like response latency, API costs, and error rates. Real-time identification of underperforming queries enables quick responses to issues, while production data creates valuable feedback loops that inform future development priorities.

## Getting Started

Start with evaluating your first GenAI app to understand quality measurement fundamentals, then progress to production monitoring for continuous improvement.

```python
import mlflow
from mlflow.metrics import exact_match

# Simple evaluation example
result = mlflow.genai.evaluate(
    data=your_test_data,
    predict_fn=your_genai_app,
    scorers=[exact_match()],
)

print(f"Evaluation completed: {result.metrics}")
```

## Core Workflows

### Evaluate and Improve Your App

Building high-quality GenAI applications requires a methodical approach to understanding and improving performance. The evaluation process typically begins with comprehensive assessments that establish baseline metrics and reveal areas needing attention. Our **[Evaluate your app](/genai/eval-monitor/evaluate-app)** guide walks through this initial assessment process in detail.

Once you have baseline measurements, understanding how changes affect performance becomes crucial. **[Compare app versions](/genai/eval-monitor/version-comparison)** provides techniques for analyzing differences between iterations, helping you verify that modifications actually improve rather than degrade quality. For teams working with continuous integration, **[Validate in CI/CD](/genai/eval-monitor/ci-cd-testing)** shows how to automate quality checks as part of your deployment pipeline.

#### Quality Measurement Strategies

Effective evaluation combines multiple approaches depending on your specific needs and constraints. Pre-built scorers offer the fastest path to initial assessment, leveraging research-backed metrics that work well for common evaluation scenarios:

```python
from mlflow.metrics import exact_match, rouge_score

# Use built-in metrics for common evaluation needs
basic_metrics = [exact_match(), rouge_score()]
```

However, many applications require evaluation criteria specific to their business domain or use case. Custom business logic enables precise measurement of what matters most for your application:

```python
from mlflow.genai.scorers import scorer


@scorer
def contains_required_info(inputs, outputs):
    """Check if response contains required business information."""
    required_fields = ["price", "availability", "specifications"]
    response = outputs.get("response", "").lower()
    return all(field in response for field in required_fields)
```

When evaluation requires understanding nuance, tone, or semantic meaning that simple rules can't capture, LLM-based judges provide sophisticated assessment capabilities:

```python
# Guidelines-based evaluation for business-specific criteria
business_guidelines = [
    "Response must be professional and helpful",
    "Technical accuracy is required for product information",
    "Customer privacy must be respected",
]
```

### Build Evaluation Datasets

The foundation of effective evaluation lies in building comprehensive test sets that accurately represent real-world usage. The most valuable datasets often come from production traces, which capture the complexity and variety of actual user interactions. These real-world examples reveal edge cases and interaction patterns that might not be obvious during initial design:

```python
# Extract problematic cases from production
problem_traces = mlflow.search_traces(
    filter_string="assessments.quality_score.value < 0.7"
)

# Convert to evaluation dataset
eval_dataset = create_dataset_from_traces(problem_traces)
```

While production data provides authenticity, curated test cases offer precision and control. By designing specific scenarios, you can ensure coverage of critical functionality and edge conditions that might be rare in production but important for robustness:

```python
evaluation_scenarios = [
    {
        "inputs": {"query": "What's the price of Product X?"},
        "targets": "Expected response with accurate pricing",
    },
    {
        "inputs": {"query": "How do I return an item?"},
        "targets": "Clear return policy explanation",
    },
]
```

When building datasets manually becomes time-consuming, synthetic data generation can help scale your test coverage efficiently. This approach is particularly valuable for exploring edge cases and ensuring your application handles diverse scenarios gracefully:

```python
# Generate diverse test cases for edge cases
synthetic_queries = generate_test_queries(
    categories=["pricing", "support", "technical"],
    complexity_levels=["simple", "moderate", "complex"],
)
```

### Production Workflows

The transition from development to production creates opportunities for continuous improvement when handled thoughtfully. Rather than treating deployment as an endpoint, successful teams establish monitoring systems that extend evaluation capabilities into live environments. **[Run scorers in production](/genai/eval-monitor/run-scorer-in-prod)** demonstrates how to automatically assess quality on live traffic without impacting user experience.

The real value emerges when production insights feed back into development processes. **[Continuous improvement](/genai/eval-monitor/continuous-improvement-with-production-data)** techniques help teams identify patterns in user interactions that reveal opportunities for enhancement. This creates a virtuous cycle where each deployment provides data that makes the next iteration more effective.

#### Feedback Loop Implementation

Setting up effective production monitoring requires balancing comprehensive assessment with operational efficiency. Start by identifying the most critical quality dimensions for your application, then implement monitoring that captures meaningful signals without overwhelming your infrastructure:

```python
# Monitor production quality
def setup_production_monitoring():
    quality_scorers = [relevance_scorer, safety_scorer, business_logic_scorer]

    # Run on sample of production traffic
    monitor = mlflow.create_monitor(
        model_uri="your-deployed-model",
        scorers=quality_scorers,
        sampling_rate=0.1,  # 10% of traffic
    )

    return monitor
```

The monitoring data becomes most valuable when analyzed systematically to identify improvement opportunities. Rather than reacting to individual issues, look for patterns that suggest systemic improvements:

```python
# Use insights for targeted improvements
def analyze_production_issues():
    # Find problematic patterns
    low_quality_traces = mlflow.search_traces(
        filter_string="metrics.quality_score < 0.6"
    )

    # Create focused evaluation dataset
    improvement_dataset = build_dataset_from_issues(low_quality_traces)

    # Test improvements
    evaluate_improvements(improvement_dataset)
```

## Key Concepts

### Quality Measurement Components

Understanding how evaluation works requires familiarity with its core building blocks. **Scorers** form the foundation of any evaluation system - these are functions that assess specific aspects of your application's outputs. Simple deterministic scorers apply rule-based evaluation for clearly defined criteria, while semantic scorers leverage LLMs to assess nuanced qualities like tone or relevance. When comprehensive assessment requires multiple perspectives, composite scorers combine different evaluation dimensions into unified quality measures.

**Evaluation Datasets** serve as the testing ground where scorers are applied. These versioned collections of test inputs and expected outputs take several forms depending on their purpose. Development datasets consist of carefully curated test cases designed to systematically probe your application's capabilities. Production-derived datasets capture real-world complexity by including actual user interactions. Regression test suites focus on critical scenarios that must continue working correctly as your application evolves.

**Evaluation Runs** tie everything together by capturing the results when scorers are applied to datasets. Beyond simple pass/fail results, these runs provide rich context including quantitative metrics tracked over time, detailed trace analysis for debugging, and side-by-side version comparisons that reveal the impact of changes.

### Production Monitoring Architecture

Production monitoring extends evaluation capabilities into live environments while maintaining the operational requirements of real-world systems. **Continuous Assessment** provides ongoing measurement without disrupting user experience:

```python
# Real-time quality monitoring
production_metrics = {
    "response_quality": average_quality_score,
    "user_satisfaction": feedback_sentiment,
    "operational_health": {
        "latency_p95": response_time_95th_percentile,
        "error_rate": error_percentage,
        "throughput": requests_per_minute,
    },
}
```

**Alerting and Response** systems enable proactive issue management by detecting problems before they significantly impact users:

```python
# Set up quality alerts
alert_thresholds = {
    "quality_score_drop": 0.1,  # 10% decrease
    "error_rate_spike": 0.05,  # 5% error rate
    "latency_increase": 2.0,  # 2x latency increase
}
```

## Implementation Patterns

### Development Workflow

Successful development workflows treat evaluation as an integral part of the creation process rather than an afterthought. The journey typically begins with initial evaluation to establish baseline quality metrics and understand current capabilities. This foundation enables iterative improvement, where evaluation results guide development decisions and reveal which changes actually enhance performance.

As applications mature, version comparison becomes essential for validating that improvements don't introduce unexpected regressions. Teams often implement regression testing to ensure that new features or optimizations don't break existing functionality that users depend on.

### Production Workflow

Production workflows start immediately after deployment with monitoring systems that track quality and performance metrics. This deployment monitoring phase catches issues that might not surface during development testing. Continuous assessment then provides ongoing evaluation of production traffic, creating visibility into real-world application behavior.

Effective production workflows include robust issue detection systems that identify quality degradation or operational problems before they significantly impact users. The most valuable implementations establish feedback integration processes that channel production insights back into development cycles, creating continuous improvement loops.

### Quality Assurance Strategy

Comprehensive quality assurance operates across multiple levels to catch different types of issues. Unit-level testing validates individual components and functions in isolation. Integration-level evaluation assesses complete user workflows as they would be experienced in practice. System-level assessment examines overall application performance under realistic conditions.

The most effective approaches adopt a balanced scorecard methodology that considers quality metrics like accuracy, relevance, safety, and groundedness alongside operational metrics including latency, throughput, error rates, and cost. User experience metrics such as satisfaction, task completion, and engagement provide additional perspective on application effectiveness.

## Best Practices

### Evaluation Design

Building effective evaluation requires thoughtful consideration of what you're testing and how you test it. The most reliable evaluations use datasets that genuinely represent the scenarios your application will encounter in production. This means going beyond simple happy-path examples to include edge cases, ambiguous inputs, and the kind of complex queries that real users generate.

Effective evaluation also employs multiple measurement approaches rather than relying on a single metric. Different scorer types reveal different aspects of quality - what appears successful according to one measure might reveal problems when viewed through another lens. Maintaining consistency in your evaluation methodology across different application versions ensures that comparisons remain meaningful over time.

As your application evolves and you learn more about user needs, refreshing evaluation datasets with new examples keeps your testing relevant and comprehensive.

### Production Monitoring

Successful production monitoring balances thorough coverage with practical constraints. Since evaluating every interaction would be computationally expensive, effective sampling strategies focus monitoring resources where they provide the most value. This might mean higher sampling rates for new features or known problematic areas.

Alert systems require careful tuning to provide early warning of real issues without generating false alarms that erode trust. The most effective alerts trigger on sustained changes rather than momentary fluctuations, and they're coupled with clear response procedures that help teams act quickly when problems arise.

The best monitoring systems treat data collection as just the beginning - regular analysis of monitoring results reveals optimization opportunities and helps teams understand whether their monitoring strategy remains effective as applications evolve.

### Team Collaboration

Evaluation and monitoring work best when they support collaboration rather than creating silos. Using shared metrics with consistent definitions helps development and operations teams communicate effectively about quality and performance. Regular review processes ensure that monitoring strategies evolve along with applications and business needs.

Clear documentation of evaluation methodologies and monitoring decisions helps team members understand not just what is being measured, but why those measurements matter. Knowledge sharing ensures that insights from evaluation and monitoring influence decision-making across the entire team rather than staying isolated within individual roles.

## Next Steps

Starting your evaluation and monitoring journey doesn't require implementing everything at once. Begin with basic evaluation to understand your application's current quality levels and identify the most impactful areas for improvement. As you gain confidence with the evaluation process, expand your efforts by building comprehensive datasets that reflect your specific use cases and user needs.

Custom scorers become valuable as you discover evaluation needs that pre-built metrics don't address - these might be business-specific quality criteria or domain-specific accuracy requirements. Once you have solid evaluation practices in development, extending those same capabilities into production through monitoring creates continuous feedback loops that drive ongoing improvement.

The goal is creating a sustainable process where evaluation insights guide development decisions and production monitoring reveals opportunities for the next iteration. This cycle of measurement, improvement, and validation becomes a competitive advantage as your application evolves to better serve user needs.

Ready to begin? Start with the **[evaluation quickstart](/genai/eval-monitor/quickstart)** to measure your app's quality in minutes.

## Related Concepts

**MLflow Tracing** provides the detailed execution information that makes debugging and analysis possible when evaluation reveals issues. **Model Registry** helps manage versions of your GenAI applications as they evolve through the evaluation and improvement cycle. **Experiments** organize and compare different evaluation runs, making it easier to track progress over time. **Artifacts** enable you to store and share evaluation datasets and results, supporting collaboration across your team.