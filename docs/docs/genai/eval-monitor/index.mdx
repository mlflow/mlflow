import Tabs from "@theme/Tabs"
import TabItem from "@theme/TabItem"
import { CardGroup, TitleCard } from "@site/src/components/Card";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ImageBox from "@site/src/components/ImageBox";
import TabsWrapper from "@site/src/components/TabsWrapper";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { Rocket, Target, MessageSquare } from "lucide-react";

# Evaluating LLMs/Agents with MLflow

MLflow's evaluation and monitoring capabilities help you systematically measure, improve, and maintain the quality of your GenAI applications throughout their lifecycle from development through production.

<ImageBox src="/images/mlflow-3/eval-monitor/evaluation-driven-development.png" alt="Evaluation Driven Development" />

The tenets of MLflow's evaluation capabilities is **Evaluation-Driven Development**. This is an emerging practice to tackle the challenge of building high-quality LLM/Agentic applications. MLflow is an **end-to-end** platform that is designed to support this practice and help you deploy AI applications with confidence.


<TabsWrapper>
  <Tabs>
  <TabItem value="evaluation" label="Systematic Evaluation">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

        #### Evaluate and Enhance quality

        Systematically assessing and improving the quality of GenAI applications is a challenge. MLflow provides a comprehensive set of tools to help you evaluate and enhance the quality of your applications.

        Being the industry most-trusted experiment tracking platform, MLflow provides a strong foundation tracking your evaluation results and effectively collaborate with your team.

        [Learn more →](/genai/eval-monitor)

        </div>

        <div class="flex-item padding-md">
          ![Trace Evaluation](/images/llms/tracing/genai-trace-evaluation.png)
        </div>
      </div>
    </div>

  </TabItem>
  <TabItem value="feedback" label="Human Feedback">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

        #### Track Annotation and Human Feedbacks

        Human feedback is essential for building high-quality GenAI applications that meet user expectations. MLflow supports collecting, managing, and utilizing feedback from end-users and domain experts.

        Feedbacks are attached to traces and recorded with metadata, including user, timestamp, revisions, etc.

        [Learn more →](/genai/tracing/collect-user-feedback)

        </div>

        <div class="flex-item padding-md">
          ![Trace Feedback](/images/llms/tracing/genai-human-feedback.png)
        </div>
      </div>
    </div>

  </TabItem>
  <TabItem value="llm-judge" label="LLM-as-a-Judge">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

        #### Scale Quality Assessment with Automation

        Quality assessment is a critical part of building high-quality GenAI applications, however, it is often time-consuming and requires human expertise. LLMs are powerful tools to automate quality assessment.

        MLflow offers various built-in LLM-as-a-Judge scorers to help automating the process, as well as a flexible toolset to build you own LLM judges with ease.

        [Learn more →](/genai/eval-monitor)

        </div>

        <div class="flex-item padding-md">
          ![Trace Evaluation](/images/llms/tracing/genai-trace-evaluation.png)
        </div>
      </div>
    </div>

  </TabItem>

  <TabItem value="production-monitoring" label="Production Monitoring">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

        #### Monitor Applications in Production

        Understanding and optimizing GenAI application performance is crucial for efficient operations. MLflow Tracing captures key metrics like latency and token usage at each step, as well as various quality metrics, helping you identify bottlenecks, monitor efficiency, and find optimization opportunities.

        [Learn more →](/genai/tracing/prod-tracing)

        </div>

        <div class="flex-item padding-md">
          ![Monitoring](/images/llms/tracing/genai-monitoring.png)
        </div>
      </div>
    </div>

  </TabItem>
  <TabItem value="dataset" label="Dataset Collection">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">

        #### Create a High-Quality Dataset from Real World Traffic

        Evaluating the performance of your GenAI application is crucial, but creating a reliable evaluation dataset is challenging.

        Traces from production systems capture perfect data for building high-quality datasets with precise details for internal components like retrievers and tools.

        [Learn more →](/genai/tracing/search-traces/#creating-evaluation-datasets)

        </div>

        <div class="flex-item padding-md">
          ![Trace Dataset](/images/llms/tracing/genai-trace-dataset.png)
        </div>
      </div>
    </div>

  </TabItem>
  </Tabs>
</TabsWrapper>

## Running an Evaluation

Each evaluation is defined by three components:

1. **Dataset**: A collection of inputs and expectations (ground truth) to evaluate.
2. **Scorers**: A list of evaluation criteria to evaluate the dataset, defined as a scorer function.
3. **Predict function (Optional)**: A generic function that produces outputs to be evaluated by the scorers. Alternatively, you can pass the pre-generated outputs or traces in the form of a dataset, which is useful for evaluating existing apps.

The following example shows a simple evaluation of a dataset of questions and expected answers.

```python
import os
import openai
import mlflow
from mlflow.genai.scorers import Correctness, Guidelines

client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 1. Define a simple QA dataset
dataset = [
  {
      "inputs": {"question": "Can MLflow manage prompts?"},
      "expectations": {"expected_response": "Yes!"}
  },
  {
      "inputs": {"question": "Can MLflow create a taco for my lunch?"},
      "expectations": {"expected_response": "No, unfortunately, MLflow is not a taco maker."}
  },
]

# 2. Define a prediction function to generate responses
def predict_fn(question: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": question}]
    )
    return response.choices[0].message.content

# 3.Run the evaluation
results = mlflow.genai.evaluate(
  data=dataset,
  predict_fn=predict_fn,
  scorers=[
      # Built-in LLM judge 
      Correctness(),
      # Custom criteria using LLM judge
      Guidelines(name="is_english", guidelines="The answer must be in English")
  ]
)
```

## Review the results

Open the MLflow UI to review the evaluation results. If you are using OSS MLflow, you can use the following command to start the UI:

```bash
mlflow ui --port 5000
```

If you are using cloud-based MLflow, open the experiment page in the platform.

<ImageBox src="/images/mlflow-3/eval-monitor/landing-page-result.png" alt="Evaluation Results" />


## Next Steps

<TilesGrid>
  <TileCard
    icon={Rocket}
    iconSize={48}
    title="Quickstart"
    description="Learn the MLflow's evaluation workflow in action."
    href="/genai/eval-monitor/quickstart"
    linkText="Start evaluating →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Define Scorers"
    description="Learn how to implement scoring criteria for your applications."
    href="/genai/eval-monitor/scorers"
    linkText="Learn about scorers →"
    containerHeight={64}
  />
  <TileCard
    icon={MessageSquare}
    iconSize={48}
    title="Collect User Feedback"
    description="Understand how to collect user feedbacks for your applications."
    href="/genai/eval-monitor/collect-user-feedback"
    linkText="Learn about feedback tracking →"
    containerHeight={64}
  />
</TilesGrid>
