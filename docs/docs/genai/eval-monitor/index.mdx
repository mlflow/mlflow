import { CardGroup, TitleCard } from "@site/src/components/Card";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { MessageSquare, Target, BarChart3, ExternalLink, Bot, Users, CheckCircle, TrendingUp, Database, Book } from "lucide-react";

# Evaluate & Monitor

MLflow's evaluation and monitoring capabilities help you systematically measure, improve, and maintain the quality of your GenAI applications throughout their lifecycle from development through production.

## Why Evaluate GenAI Applications?

<FeatureHighlights features={[
  {
    icon: CheckCircle,
    title: "Quality Assurance",
    description: "Ensure your AI consistently produces accurate, helpful, and safe responses across different inputs and contexts."
  },
  {
    icon: TrendingUp,
    title: "Continuous Improvement",
    description: "Track performance over time and identify specific areas where your AI can be enhanced through systematic evaluation."
  },
  {
    icon: Users,
    title: "Human-AI Collaboration",
    description: "Combine automated evaluation with human expertise to create comprehensive quality assessment workflows."
  },
  {
    icon: Bot,
    title: "Production Monitoring",
    description: "Monitor AI performance in real-time production environments to maintain quality standards and catch issues early."
  }
]} />

## Evaluation Datasets

Before you can evaluate your GenAI application, you need test data. **Evaluation Datasets** provide a centralized repository for managing test cases, ground truth expectations, and evaluation data at scale.

Think of Evaluation Datasets as your "test database" - a single source of truth for all the data needed to evaluate your AI systems. They enable you to:

- **Centralize test data management** across teams and experiments
- **Build datasets from production traces** to ensure real-world relevance
- **Track ground truth expectations** alongside test inputs
- **Share datasets across teams** for consistent evaluation

By organizing your test data in Evaluation Datasets, you transform ad-hoc testing into systematic quality assurance, ensuring consistent and reliable evaluation across your GenAI development lifecycle.

<TilesGrid>
  <TileCard
    icon={Database}
    iconSize={48}
    title="Evaluation Datasets Overview"
    description="Learn how to create and manage centralized test data repositories for systematic evaluation"
    href="/genai/datasets"
    linkText="Get started →"
    containerHeight={64}
  />
  <TileCard
    icon={Book}
    iconSize={48}
    title="SDK Guide"
    description="Comprehensive guide to creating, updating, and using evaluation datasets programmatically"
    href="/genai/datasets/sdk-guide"
    linkText="View guide →"
    containerHeight={64}
  />
</TilesGrid>

## Feedback & Expectations

MLflow provides two complementary approaches to GenAI evaluation that work together to create comprehensive quality assessment:

**Feedback** captures quality evaluations of how well your AI actually performed. This can come from multiple sources:

- **Human reviewers** providing expert judgment on response quality
- **LLM judges** offering automated evaluation at scale
- **Programmatic checks** validating format, compliance, and business rules

**Expectations** define the ground truth - what your AI should produce for specific inputs. These establish reference points for objective accuracy measurement and enable systematic testing against known correct answers.

Together, feedback and expectations enable you to measure both subjective quality and objective accuracy, creating a complete evaluation framework for your GenAI applications.

<TilesGrid>
  <TileCard
    icon={MessageSquare}
    iconSize={48}
    title="Feedback Collection"
    description="Capture quality evaluations from LLM judges, programmatic checks, and human reviewers"
    href="/genai/assessments/feedback"
    linkText="Start collecting →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Ground Truth Expectations"
    description="Define expected outputs and correct answers to establish quality baselines"
    href="/genai/assessments/expectations"
    linkText="Start annotating →"
    containerHeight={64}
  />
</TilesGrid>

## Additional Evaluation Options

<TilesGrid>
  <TileCard
    icon={BarChart3}
    iconSize={48}
    title="LLM Evaluation (Legacy)"
    description="MLflow evaluation capabilities built on top of the classic mlflow.evaluate API for self-hosted or local MLflow"
    href="/genai/eval-monitor/llm-evaluation"
    linkText="Learn more →"
    containerHeight={64}
  />
  <TileCard
    icon={ExternalLink}
    iconSize={48}
    title="New Evaluation Suite (Managed-Only)"
    description="MLflow 3 introduces a new evaluation suite for LLMs/GenAI, available in Managed MLflow on Databricks"
    href="https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/"
    linkText="Try with Databricks →"
    containerHeight={64}
  />
</TilesGrid>
