import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Building MLflow Evaluation Datasets

To systematically test and improve the quality of your GenAI application, you need a consistent set of inputs and, optionally, corresponding expected outputs or labels. An **MLflow Evaluation Dataset** provides a structured and versioned way to manage this data, enabling repeatable evaluations as you iterate on your application.

This guide provides a comprehensive overview of why evaluation datasets are essential and details various methods to build them, ensuring you can effectively measure and enhance your GenAI app's performance.

## Why Build an Evaluation Dataset?

Curating inputs and their associated expected labels into an evaluation dataset is a key development task that becomes the cornerstone for systematically testing quality improvements and safeguarding against regressions.

Evaluation datasets are crucial for several reasons:

1. **Iterating on Known Issues**: Add problematic inputs and desired outputs to repeatedly test fixes until issues are resolved

2. **Preventing Regressions**: Create a "golden set" of examples where your application performs well to ensure new changes don't break existing functionality

3. **Comparing Versions**: Use consistent datasets to fairly compare different prompts, models, or application logic

4. **Targeted Testing**: Create specialized datasets for specific aspects like safety, knowledge domains, or query types

While you might eventually use multiple evaluation datasets for different purposes (e.g., a dataset for each major feature category or one specifically for safety testing), starting with a single, well-curated dataset is often a good approach.

## Evaluation Dataset Schema

An MLflow Evaluation Dataset is a structured table where each row represents a single evaluation instance. The schema is flexible but follows common patterns for GenAI applications.

### Core Schema

| Column | Type | Required | Description |
|--------|------|----------|-------------|
| `request_id` | string | No | Unique identifier for tracing back to original logs |
| `inputs` | dict | **Yes** | Input to your application (passed as `**kwargs` to your predict function) |
| `outputs` | dict | No | Actual output generated by the application |
| `trace` | JSON string | No | Complete MLflow Trace object capturing detailed execution |
| `expected` | dict | No | Ground truth information for evaluation |

### The `inputs` Column

The `inputs` column contains the data your application needs to generate a response. During evaluation, these are passed as keyword arguments to your prediction function.

<Tabs>
  <TabItem value="chat" label="Chat Applications" default>
    ```python
    # Chat application inputs
    chat_inputs = {
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is machine learning?"},
        ],
        "temperature": 0.7,
        "max_tokens": 500,
    }
    ```
  </TabItem>
  <TabItem value="rag" label="RAG Applications">
    ```python
    # RAG application inputs
    rag_inputs = {
        "query": "Explain MLflow tracing",
        "context": "MLflow provides comprehensive tracing capabilities for GenAI applications...",
        "retrieval_settings": {"top_k": 5, "similarity_threshold": 0.8},
    }
    ```
  </TabItem>
  <TabItem value="function" label="Function Calling">
    ```python
    # Function calling inputs
    function_inputs = {
        "user_query": "What's the weather like in San Francisco?",
        "available_tools": ["weather_api", "location_service"],
        "tool_choice": "auto",
    }
    ```
  </TabItem>
</Tabs>

### The `expected` Column

The optional `expected` column provides ground truth for evaluation metrics:

<Tabs>
  <TabItem value="basic" label="Basic Expected Data" default>
    ```python
    # Basic expected response
    basic_expected = {
        "expected_response": "Machine learning is a subset of artificial intelligence that uses algorithms to learn patterns from data and make predictions or decisions without being explicitly programmed for each task."
    }
    ```
  </TabItem>
  <TabItem value="facts" label="Expected Facts">
    ```python
    # Expected facts for factual accuracy checking
    facts_expected = {
        "expected_facts": [
            "Machine learning uses algorithms to learn from data",
            "It can make predictions without explicit programming",
            "It is a subset of artificial intelligence",
            "It identifies patterns in data",
        ]
    }
    ```
  </TabItem>
  <TabItem value="guidelines" label="Quality Guidelines">
    ```python
    # Quality guidelines for evaluation
    guidelines_expected = {
        "guidelines": {
            "accuracy": [
                "Must be factually correct",
                "Should cite sources when applicable",
                "Avoid speculation",
            ],
            "tone": [
                "Professional but accessible",
                "Avoid technical jargon unless necessary",
                "Be concise and clear",
            ],
            "completeness": [
                "Address all parts of the question",
                "Provide sufficient detail",
                "Include relevant examples",
            ],
        }
    }
    ```
  </TabItem>
  <TabItem value="rag-context" label="RAG Context">
    ```python
    # Expected retrieved context for RAG applications
    rag_expected = {
        "expected_retrieved_context": [
            {
                "page_content": "MLflow tracing provides end-to-end observability for GenAI applications...",
                "doc_uri": "https://docs.mlflow.org/en/latest/llms/tracing/index.html",
                "chunk_id": "tracing_overview_1",
            },
            {
                "page_content": "Traces capture the complete execution flow including inputs, outputs, and intermediate steps...",
                "doc_uri": "https://docs.mlflow.org/en/latest/llms/tracing/concepts.html",
                "chunk_id": "trace_structure_2",
            },
        ]
    }
    ```
  </TabItem>
</Tabs>

:::note
The `expected` dictionary can contain the following optional keys to provide ground truth for evaluation:

- **`expected_response`** (string): The ground-truth or ideal answer for the `inputs`.
- **`expected_facts`** (array of strings): A list of specific facts or pieces of information that are expected to be present in the `outputs`.
- **`guidelines`** (dict or list): A set of criteria or rules that the `outputs` are expected to follow. This can be a list of guideline strings or a dictionary mapping guideline names to lists of specific rules.
- **`expected_retrieved_context`** (array of objects): For RAG applications, an array of objects detailing the content and source URI (`doc_uri`) of context chunks that _should_ have been retrieved to answer the `inputs`.
:::

You can also include other custom columns relevant to your specific use case or for more granular analysis (e.g., `user_id`, `category`, `difficulty_level`). When using `mlflow.genai.evaluate()`, if your dataset uses different names for standard columns, you can map them using the `col_mapping` parameter.

## Building Your Dataset

MLflow offers several flexible approaches to construct evaluation datasets. Choose the method that best fits your data sources and evaluation goals.

<Tabs>
  <TabItem value="traces" label="From Existing Traces" default>
    Leverage the rich data already captured in your MLflow Traces - this is often the most effective approach for building relevant datasets.

    **Why Use Traces?**
    - **Real-world relevance**: Reflects actual user interactions
    - **Rich context**: Complete execution details and metadata
    - **Quality indicators**: Can include user feedback and automated assessments

    ### Using the MLflow SDK

    ```python
    import mlflow
    import pandas as pd

    # Set your experiment
    mlflow.set_experiment("your-genai-app")

    # Search for high-quality traces
    high_quality_traces = mlflow.search_traces(
        experiment_ids=["your-experiment-id"],
        filter_string="tags.user_feedback = 'positive' AND execution_time_ms < 2000",
        max_results=100,
    )

    # Transform traces to evaluation dataset format
    eval_data = []
    for _, trace in high_quality_traces.iterrows():
        eval_data.append(
            {
                "request_id": trace.request_id,
                "inputs": trace.inputs,  # Already in correct format
                "outputs": trace.outputs,
                "expected": {"expected_response": trace.outputs.get("response", "")},
            }
        )

    # Create evaluation dataset
    eval_df = pd.DataFrame(eval_data)

    # Log as MLflow dataset
    with mlflow.start_run():
        dataset = mlflow.data.from_pandas(
            eval_df,
            source="production_traces",
            name="high_quality_interactions",
            targets="expected",
        )
        mlflow.log_input(dataset, context="evaluation")
    ```

    ### Advanced Trace Curation

    ```python
    def curate_traces_by_category():
        """Curate traces for different evaluation scenarios"""

        # Get traces with errors for debugging dataset
        error_traces = mlflow.search_traces(
            filter_string="status = 'ERROR'", max_results=50
        )

        # Get slow but successful traces for performance testing
        slow_traces = mlflow.search_traces(
            filter_string="status = 'OK' AND execution_time_ms > 5000", max_results=30
        )

        # Get traces with specific user feedback
        feedback_traces = mlflow.search_traces(
            filter_string="tags.issue_type = 'hallucination'", max_results=25
        )

        return {
            "debugging": error_traces,
            "performance": slow_traces,
            "quality_issues": feedback_traces,
        }


    # Create specialized datasets
    trace_categories = curate_traces_by_category()

    for category, traces in trace_categories.items():
        if not traces.empty:
            # Transform and create dataset for each category
            eval_data = transform_traces_to_dataset(traces)

            with mlflow.start_run():
                dataset = mlflow.data.from_pandas(
                    eval_data, source=f"traces_{category}", name=f"eval_dataset_{category}"
                )
                mlflow.log_input(dataset, context="evaluation")
    ```

    ### Using the MLflow UI

    The MLflow UI offers an interactive way to inspect, filter, and select traces for your evaluation dataset.

    **Steps:**
    1. **Navigate to Logs**: In the MLflow UI, go to the **Monitoring Preview** for your application and select the **Logs** tab.
    2. **Select Traces**: Search or filter traces, then select the desired ones using checkboxes.
    3. **Export Traces**: Click the **Export traces** button.
    4. **Choose Dataset and Export**: In the dialog, select the target `EvaluationDataset` and click **Export**. The selected traces will be added to this dataset.
  </TabItem>
  <TabItem value="expert-feedback" label="From Domain Expert Feedback">
    Transform expert assessments from MLflow Review App into structured evaluation datasets.

    :::note Only on Databricks
    The Agent Review App is only available on Databricks Managed MLflow.
    :::

    ```python
    from databricks.agents import review_app

    # Retrieve completed labeling session
    my_app = review_app.ReviewApp("your-review-app")
    expert_session = my_app.get_labeling_session(name="quality_review_session")

    # Sync expert expectations to evaluation dataset
    expert_session.sync_expectations(to_dataset="catalog.schema.expert_labeled_dataset")

    # Load and inspect the enriched dataset
    enriched_dataset = mlflow.data.load_delta(
        table_name="catalog.schema.expert_labeled_dataset"
    )

    print("Dataset with expert labels:")
    print(enriched_dataset.df[["inputs", "expected"]].head())

    # Use the expert-labeled dataset for evaluation
    with mlflow.start_run():
        results = mlflow.genai.evaluate(
            predict_fn=your_genai_model,
            data=enriched_dataset.df,
            scorers=[
                mlflow.metrics.genai.relevance(),
                mlflow.metrics.genai.coherence(),
            ],
        )
    ```

    :::note
    The `sync_expectations` method specifically processes assessments of type `expectation`. Other feedback types (like general comments or ratings) are stored on the traces but not directly synced into the `expected` field of the evaluation dataset by this method.
    :::
  </TabItem>
  <TabItem value="from-scratch" label="From Scratch">
    Build datasets with precise control over inputs and expected outputs for targeted testing scenarios.

    ### Creating Structured Test Cases

    ```python
    import pandas as pd
    import mlflow

    # Define test cases for specific functionality
    test_cases = [
        {
            "inputs": {"query": "What is MLflow?", "context": "user_onboarding"},
            "expected": {
                "expected_response": "MLflow is an open source platform for managing machine learning lifecycles...",
                "expected_facts": [
                    "MLflow is open source",
                    "It manages ML lifecycles",
                    "It includes experiment tracking",
                ],
                "guidelines": {
                    "accuracy": ["Must mention core components"],
                    "clarity": ["Use simple language", "Provide examples"],
                },
            },
            "category": "product_knowledge",
        },
        {
            "inputs": {
                "query": "How do I log metrics in MLflow?",
                "context": "technical_support",
            },
            "expected": {
                "expected_response": "You can log metrics using mlflow.log_metric()...",
                "expected_facts": [
                    "Use mlflow.log_metric() function",
                    "Metrics are numeric values",
                    "They track model performance",
                ],
            },
            "category": "technical_guidance",
        },
        # Add edge cases
        {
            "inputs": {"query": "", "context": "error_handling"},  # Empty query test
            "expected": {
                "expected_response": "I'd be happy to help! Could you please provide more details about what you'd like to know?",
                "guidelines": {
                    "helpfulness": [
                        "Acknowledge empty input politely",
                        "Prompt for clarification",
                    ]
                },
            },
            "category": "edge_cases",
        },
    ]

    # Create DataFrame and dataset
    eval_df = pd.DataFrame(test_cases)

    with mlflow.start_run():
        dataset = mlflow.data.from_pandas(
            eval_df,
            source="manual_curation",
            name="comprehensive_test_suite",
            targets="expected",
        )
        mlflow.log_input(dataset, context="evaluation")

        # Add metadata about the dataset
        mlflow.log_params(
            {
                "dataset_size": len(eval_df),
                "categories": eval_df["category"].unique().tolist(),
                "creation_method": "manual_curation",
            }
        )
    ```

    ### Advanced Test Case Generation

    ```python
    def create_comprehensive_test_suite():
        """Create a comprehensive test suite covering various scenarios"""

        test_categories = {
            "basic_functionality": [
                "product_overview",
                "feature_explanation",
                "getting_started",
            ],
            "technical_support": ["api_usage", "troubleshooting", "best_practices"],
            "edge_cases": [
                "empty_input",
                "very_long_input",
                "special_characters",
                "multiple_questions",
            ],
            "safety_testing": [
                "inappropriate_content",
                "sensitive_information",
                "prompt_injection",
            ],
        }

        all_test_cases = []

        for category, subcategories in test_categories.items():
            for subcategory in subcategories:
                # Generate test cases for each subcategory
                test_case = generate_test_case(category, subcategory)
                all_test_cases.append(test_case)

        return pd.DataFrame(all_test_cases)


    def generate_test_case(category, subcategory):
        """Generate a test case for specific category/subcategory"""
        # This would contain your logic for creating test cases
        # based on your application's requirements
        pass
    ```
  </TabItem>
  <TabItem value="external-data" label="From External Data">
    Import and transform existing evaluation data from other formats into MLflow datasets.

    ### CSV Import Example

    ```python
    import pandas as pd
    import mlflow

    # Load external CSV data
    external_df = pd.read_csv("existing_evaluation_data.csv")
    print("External data columns:", external_df.columns.tolist())


    # Transform to MLflow schema
    def transform_external_data(df):
        """Transform external data to MLflow evaluation dataset format"""

        transformed_data = []

        for _, row in df.iterrows():
            # Create inputs dict from external columns
            inputs = {
                "query": row["user_question"],
                "context": row.get("additional_context", ""),
            }

            # Create expected dict if ground truth exists
            expected = {}
            if "correct_answer" in row and pd.notna(row["correct_answer"]):
                expected["expected_response"] = row["correct_answer"]

            if "key_facts" in row and pd.notna(row["key_facts"]):
                # Assuming key_facts is semicolon-separated
                expected["expected_facts"] = row["key_facts"].split(";")

            transformed_data.append(
                {
                    "request_id": str(row.get("id", f"external_{len(transformed_data)}")),
                    "inputs": inputs,
                    "expected": expected if expected else None,
                    "source_dataset": row.get("dataset_name", "external"),
                }
            )

        return pd.DataFrame(transformed_data)


    # Transform and create dataset
    eval_df = transform_external_data(external_df)

    with mlflow.start_run():
        dataset = mlflow.data.from_pandas(
            eval_df,
            source="external_benchmark",
            name="imported_evaluation_set",
            targets="expected",
        )
        mlflow.log_input(dataset, context="evaluation")
    ```

    ### JSON Import Example

    ```python
    import json
    import pandas as pd

    # Load JSON data
    with open("benchmark_dataset.json", "r") as f:
        json_data = json.load(f)


    # Transform JSON to evaluation format
    def transform_json_benchmark(json_data):
        """Transform JSON benchmark data to MLflow format"""

        eval_data = []

        for item in json_data:
            inputs = {
                "prompt": item["input_text"],
                "task_type": item.get("task", "general"),
            }

            expected = {"expected_response": item["target_output"]}

            # Add task-specific expectations
            if "evaluation_criteria" in item:
                expected["guidelines"] = item["evaluation_criteria"]

            eval_data.append(
                {
                    "inputs": inputs,
                    "expected": expected,
                    "difficulty": item.get("difficulty", "medium"),
                    "domain": item.get("domain", "general"),
                }
            )

        return pd.DataFrame(eval_data)


    benchmark_df = transform_json_benchmark(json_data)

    # Create and log dataset
    with mlflow.start_run():
        dataset = mlflow.data.from_pandas(
            benchmark_df, source="json_benchmark", name="standardized_benchmark"
        )
        mlflow.log_input(dataset, context="evaluation")
    ```
  </TabItem>
  <TabItem value="synthetic-data" label="With Synthetic Data">
    Generate diverse inputs programmatically to expand test coverage and explore edge cases.

    ### LLM-Based Generation

    ```python
    import json
    import openai
    import mlflow
    import pandas as pd


    def generate_synthetic_queries(topic, num_examples=20):
        """Generate synthetic user queries for a specific topic"""

        client = openai.OpenAI()

        prompt = f"""
        Generate {num_examples} diverse user questions about {topic}.
        Make them realistic and varied in:
        - Complexity (simple to advanced)
        - Phrasing (formal to casual)
        - Specific aspects of the topic

        Return as a JSON array of strings.
        """

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
        )

        queries = json.loads(response.choices[0].message.content)["queries"]
        return queries


    def create_synthetic_dataset():
        """Create a comprehensive synthetic evaluation dataset"""

        topics = [
            "machine learning fundamentals",
            "MLflow usage and features",
            "data science best practices",
            "Python programming for ML",
            "model deployment strategies",
        ]

        all_synthetic_data = []

        for topic in topics:
            # Generate queries for this topic
            queries = generate_synthetic_queries(topic, num_examples=15)

            for query in queries:
                synthetic_item = {
                    "inputs": {
                        "query": query,
                        "context": f"synthetic_{topic.replace(' ', '_')}",
                    },
                    "topic": topic,
                    "generation_method": "llm_synthetic",
                    "requires_human_review": True,  # Flag for manual review
                }
                all_synthetic_data.append(synthetic_item)

        return pd.DataFrame(all_synthetic_data)


    # Generate and create dataset
    synthetic_df = create_synthetic_dataset()

    with mlflow.start_run():
        dataset = mlflow.data.from_pandas(
            synthetic_df, source="synthetic_generation", name="synthetic_query_dataset"
        )
        mlflow.log_input(dataset, context="evaluation")

        # Log generation metadata
        mlflow.log_params(
            {
                "generation_method": "openai_gpt4",
                "total_queries": len(synthetic_df),
                "topics_covered": synthetic_df["topic"].nunique(),
                "requires_review": True,
            }
        )
    ```

    ### Template-Based Generation

    ```python
    def generate_template_based_queries():
        """Generate queries using templates for systematic coverage"""

        # Define query templates
        templates = {
            "how_to": [
                "How do I {action} in {context}?",
                "What's the best way to {action}?",
                "Can you explain how to {action}?",
            ],
            "definition": [
                "What is {concept}?",
                "Can you define {concept}?",
                "Explain {concept} to me",
            ],
            "comparison": [
                "What's the difference between {item1} and {item2}?",
                "Compare {item1} vs {item2}",
                "Which is better: {item1} or {item2}?",
            ],
            "troubleshooting": [
                "I'm getting an error with {feature}",
                "Why isn't {feature} working?",
                "{feature} is not behaving as expected",
            ],
        }

        # Define domain-specific content
        mlflow_concepts = ["experiments", "runs", "models", "artifacts", "tracking"]
        ml_concepts = ["supervised learning", "neural networks", "feature engineering"]
        actions = ["log metrics", "save models", "track experiments", "deploy models"]

        synthetic_queries = []

        for template_type, template_list in templates.items():
            for template in template_list:
                if template_type == "how_to":
                    for action in actions:
                        for context in ["MLflow", "production", "development"]:
                            query = template.format(action=action, context=context)
                            synthetic_queries.append(
                                {
                                    "inputs": {"query": query},
                                    "template_type": template_type,
                                    "synthetic": True,
                                }
                            )

                elif template_type == "definition":
                    for concept in mlflow_concepts + ml_concepts:
                        query = template.format(concept=concept)
                        synthetic_queries.append(
                            {
                                "inputs": {"query": query},
                                "template_type": template_type,
                                "synthetic": True,
                            }
                        )

        return pd.DataFrame(synthetic_queries)
    ```

    :::tip
    **Important**: Always review a subset of generated synthetic examples for quality, relevance, and diversity before extensive use. Synthetic data should complement, not replace, real-world examples.
    :::
  </TabItem>
</Tabs>

## Best Practices

### Dataset Quality Guidelines

<Tabs>
  <TabItem value="validation" label="Quality Validation" default>
    ```python
    def validate_dataset_quality(df):
        """Check dataset quality and completeness"""

        issues = []

        # Check for required columns
        if "inputs" not in df.columns:
            issues.append("Missing required 'inputs' column")

        # Check for empty inputs
        empty_inputs = df["inputs"].isna().sum()
        if empty_inputs > 0:
            issues.append(f"{empty_inputs} examples have empty inputs")

        # Check for duplicate inputs
        duplicate_inputs = df["inputs"].apply(str).duplicated().sum()
        if duplicate_inputs > 0:
            issues.append(f"{duplicate_inputs} duplicate inputs found")

        return issues


    def validate_dataset_coverage(df):
        """Validate that dataset covers important scenarios"""

        coverage_report = {
            "total_examples": len(df),
            "unique_input_patterns": df["inputs"].apply(str).nunique(),
            "has_expected_outputs": df["expected"].notna().sum()
            if "expected" in df.columns
            else 0,
            "categories_covered": df.get("category", pd.Series()).nunique(),
        }

        return coverage_report
    ```
  </TabItem>
  <TabItem value="versioning" label="Version Management">
    ```python
    def create_versioned_dataset(df, version_info):
        """Create a versioned dataset with metadata"""

        with mlflow.start_run():
            dataset = mlflow.data.from_pandas(
                df,
                source=version_info["source"],
                name=f"{version_info['name']}_v{version_info['version']}",
            )

            # Log version metadata
            mlflow.log_params(
                {
                    "dataset_version": version_info["version"],
                    "creation_date": version_info.get("date", ""),
                    "creator": version_info.get("creator", ""),
                    "description": version_info.get("description", ""),
                    "size": len(df),
                }
            )

            mlflow.log_input(dataset, context="evaluation")
            return dataset


    # Example usage
    version_info = {
        "name": "customer_support_eval",
        "version": "1.2",
        "source": "production_traces_v1.2",
        "description": "Updated with Q4 2024 production insights",
        "creator": "ml_team",
    }

    versioned_dataset = create_versioned_dataset(eval_df, version_info)
    ```
  </TabItem>
  <TabItem value="continuous-improvement" label="Continuous Improvement">
    ```python
    def update_dataset_with_production_insights():
        """Continuously improve dataset with production learnings"""

        # Get recent production traces with issues
        recent_issues = mlflow.search_traces(
            filter_string="timestamp_ms > {last_week} AND tags.needs_review = 'true'",
            max_results=50,
        )

        # Load existing dataset
        existing_dataset = mlflow.data.load_delta("catalog.schema.evaluation_dataset")

        # Add new examples from production
        new_examples = transform_traces_to_dataset(recent_issues)

        # Combine and deduplicate
        updated_df = pd.concat([existing_dataset.df, new_examples]).drop_duplicates(
            subset=["inputs"], keep="last"
        )

        # Create new version
        version_info = {
            "name": "production_enhanced_dataset",
            "version": "2.1",
            "source": "production_traces_enhancement",
            "description": "Enhanced with recent production insights",
        }

        return create_versioned_dataset(updated_df, version_info)


    def monitor_dataset_performance():
        """Monitor how well your dataset represents real usage"""

        # Get recent production queries
        recent_queries = mlflow.search_traces(
            filter_string="timestamp_ms > {last_month}", max_results=1000
        )

        # Load evaluation dataset
        eval_dataset = mlflow.data.load_delta("catalog.schema.evaluation_dataset")

        # Analyze coverage gaps
        production_patterns = analyze_query_patterns(recent_queries)
        eval_patterns = analyze_query_patterns(eval_dataset.df)

        # Identify underrepresented patterns
        gaps = find_coverage_gaps(production_patterns, eval_patterns)

        return gaps
    ```
  </TabItem>
</Tabs>

Building comprehensive evaluation datasets is foundational to maintaining and improving GenAI application quality. Start with one approach that fits your current data sources, then expand with additional methods as your evaluation needs grow.