import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Scorers

Scorers are functions that evaluate your GenAI app's quality by analyzing its outputs and producing structured feedback. They form the foundation of both offline evaluation during development and continuous monitoring in production.

## Write Once, Use Everywhere

A key design principle of MLflow scorers is **write once, use everywhere**. The same scorer function works seamlessly across your entire development lifecycle, from initial testing through production monitoring.

This unified approach means you can develop and test your quality metrics locally using [`mlflow.genai.evaluate()`](/genai/eval-monitor/concepts/eval-harness), then deploy the exact same logic to production monitoring without modification. When using Databricks MLflow, the [production monitoring service](/genai/eval-monitor/concepts/production-monitoring) applies your scorers to live traffic automatically.

:::note Only in Databricks
Production Monitoring is only supported in Databricks infrastructure at this time.
:::

```python
from mlflow.genai.scorers import scorer
from mlflow.entities import Feedback


# Define your scorer once
@scorer
def response_completeness(outputs):
    response = outputs.get("response", "")

    if len(response.strip()) < 10:
        return Feedback(value=False, rationale="Response too short to be meaningful")

    if response.lower().endswith(("...", "etc", "and so on")):
        return Feedback(value=False, rationale="Response appears incomplete")

    return Feedback(value=True, rationale="Response appears complete")


# Use in development evaluation
mlflow.genai.evaluate(
    data=test_dataset, predict_fn=my_app, scorers=[response_completeness]
)

# Same scorer in production monitoring (Databricks MLflow)
mlflow.genai.create_monitor(endpoint="my-endpoint", scorers=[response_completeness])
```

This consistency ensures that quality standards remain identical across environments, eliminating discrepancies between development assessment and production monitoring.

## How Scorers Work

Scorers analyze traces from your GenAI application and produce quality assessments through a systematic process. Your app runs and produces a [trace](/genai/tracing) capturing its execution, including inputs, outputs, and intermediate processing steps. MLflow passes this trace to your scorer function, which analyzes the execution using custom logic tailored to your quality requirements. The scorer produces feedback with scores and explanations that get attached to the trace for analysis and comparison.

### Scorer Input Parameters

Scorers receive the complete [MLflow trace](/genai/data-model/traces) containing all spans, attributes, and outputs. As a convenience, MLflow also extracts commonly needed data and passes it as named arguments:

```python
@scorer
def my_custom_scorer(
    *,  # All arguments are keyword-only
    inputs: Optional[Dict[str, Any]],  # App's raw input, parsed from trace
    outputs: Optional[Dict[str, Any]],  # App's raw output, parsed from trace
    expectations: Optional[Dict[str, Any]],  # Ground truth (only in offline eval)
    trace: Optional[mlflow.entities.Trace]  # Complete trace with all metadata
) -> Union[int, float, bool, str, Feedback, List[Feedback]]:
    # Your evaluation logic here
    return
```

All parameters are optional, so you declare only what your scorer needs. **Inputs** contain the request sent to your app, such as user queries or context information. **Outputs** contain the response from your app, including generated text, tool calls, or structured data. **Expectations** provide ground truth or labels, available only during offline evaluation when the dataset includes an `expectations` column. **Trace** contains the complete execution trace with all spans, allowing analysis of intermediate steps, latency, tool usage, and other execution details.

### Scorer Output Types

Scorers can return different types depending on your evaluation needs, giving you flexibility in how you represent quality assessments.

#### Simple Metrics

Return primitive values for straightforward pass/fail or numeric assessments. Pass/fail strings like `"yes"` or `"no"` render as "Pass" or "Fail" in the UI, making results immediately understandable. Boolean values provide binary evaluations for clear success/failure assessment. Numeric values work well for scores, counts, or measurements that need precise quantification.

```python
@scorer
def response_length(outputs):
    # Return a numeric metric
    return len(outputs.get("response", "").split())


@scorer
def contains_citation(outputs):
    # Return pass/fail string
    response = outputs.get("response", "")
    return "yes" if "[source]" in response else "no"
```

#### Rich Feedback

Return `Feedback` objects for detailed assessments with explanations that help understand the reasoning behind scores:

```python
from mlflow.entities import Feedback, AssessmentSource


@scorer
def content_quality(outputs):
    return Feedback(
        value=0.85,  # Can be numeric, boolean, or string
        rationale="Clear and accurate, minor grammar issues",
        source=AssessmentSource(source_type="HUMAN", source_id="grammar_checker_v1"),
    )
```

Multiple assessments can be returned as a list when you want to evaluate different quality dimensions in a single scorer:

```python
@scorer
def comprehensive_check(inputs, outputs):
    return [
        Feedback(name="relevance", value=True, rationale="Directly addresses query"),
        Feedback(
            name="tone", value="professional", rationale="Appropriate for audience"
        ),
        Feedback(name="length", value=150, rationale="Word count within limits"),
    ]
```

#### Feedback Naming Behavior

When using the `@scorer` decorator, feedback names follow specific rules that ensure consistent metric naming across your evaluation results. For single feedback or primitive returns, the function name becomes the feedback name automatically. This means you don't need to specify names for simple scorers, and any name parameter in Feedback objects gets ignored to maintain consistency.

```python
@scorer
def response_quality(outputs):
    # This name parameter is ignored - feedback will be named "response_quality"
    return Feedback(name="ignored_name", value=True, rationale="Good quality")


@scorer
def word_count(outputs):
    # Primitive returns also use function name - feedback will be named "word_count"
    return len(outputs.get("response", "").split())
```

For multiple feedbacks, names specified in Feedback objects are preserved, allowing you to create meaningful names for different aspects of quality:

```python
@scorer
def multi_aspect_check(outputs):
    # These names ARE used since multiple feedbacks are returned
    return [
        Feedback(name="grammar", value=True, rationale="No errors"),
        Feedback(name="clarity", value=0.9, rationale="Very clear"),
        Feedback(name="completeness", value="yes", rationale="All points addressed"),
    ]
```

This naming behavior ensures consistent metric names in your evaluation results and dashboards, making it easier to track quality trends over time.

### Error Handling

When a scorer encounters an error, MLflow provides two approaches that ensure evaluation continues even when individual scorers fail.

#### Let Exceptions Propagate (Recommended)

The simplest approach is to let exceptions throw naturally. MLflow automatically captures the exception and creates a Feedback object with the error details, allowing you to identify and debug issues without stopping the entire evaluation process:

```python
import json


@scorer
def strict_json_validator(outputs):
    # Let json.JSONDecodeError propagate if response isn't valid JSON
    data = json.loads(outputs.get("response", ""))

    # Let KeyError propagate if required fields are missing
    summary = data["summary"]
    confidence = data["confidence"]

    return Feedback(value=True, rationale=f"Valid JSON with confidence: {confidence}")
```

When an exception occurs, MLflow creates a Feedback with `value: None` and error details, ensuring you have visibility into what went wrong while maintaining evaluation continuity.

#### Handle Exceptions Explicitly

For custom error handling or to provide specific error messages, catch exceptions and return a Feedback with `None` value and error details:

```python
import json
from mlflow.entities import Feedback


@scorer
def safe_json_validator(outputs):
    try:
        data = json.loads(outputs.get("response", ""))
        required_fields = ["summary", "confidence", "sources"]
        missing = [f for f in required_fields if f not in data]

        if missing:
            return Feedback(
                value=None,
                error=f"Missing required fields: {missing}",
                rationale="Response structure incomplete",
            )

        return Feedback(value=True, rationale="Valid JSON with all required fields")

    except json.JSONDecodeError as e:
        return Feedback(
            value=None,
            error=e,  # Can pass exception object directly
            rationale=f"Invalid JSON at position {e.pos}",
        )
```

The `error` parameter accepts Python Exception objects passed directly or AssessmentError objects for structured error reporting with error codes.

## Working with Expectations

**Expectations** (ground truth or labels) are only available during offline evaluation when your evaluation dataset includes an `expectations` column. Production monitoring typically doesn't have expectations since you're evaluating live traffic without ground truth.

Design scorers to handle this gracefully by checking for expectations availability and providing meaningful assessment even when ground truth isn't available:

```python
@scorer
def accuracy_scorer(outputs, expectations=None):
    if expectations is None:
        # In production, can't compute accuracy without ground truth
        return None

    # In offline evaluation, compute accuracy
    expected = expectations.get("expected_response")
    is_correct = outputs.get("response") == expected

    return Feedback(
        value=is_correct,
        rationale=f"Response {'matches' if is_correct else 'differs from'} expected",
    )
```

This pattern enables scorers to work effectively in both development evaluation (with expectations) and production monitoring (without expectations), maintaining consistency across your quality assessment workflow.

## Using Trace Data

Scorers can access the full trace to evaluate complex application behavior beyond just inputs and outputs. This capability enables sophisticated assessment of execution patterns, tool usage, performance characteristics, and intermediate processing steps:

```python
@scorer
def tool_usage_scorer(trace):
    """Evaluate how effectively the app uses tools"""
    tool_calls = trace.search_spans(span_type="TOOL")

    if not tool_calls:
        return Feedback(value=None, rationale="No tool usage to evaluate")

    # Check for redundant calls
    tool_names = [span.name for span in tool_calls]
    if len(tool_names) != len(set(tool_names)):
        return Feedback(
            value=False, rationale=f"Redundant tool calls detected: {tool_names}"
        )

    # Check for errors
    failed_calls = [s for s in tool_calls if s.status.status_code != "OK"]
    if failed_calls:
        return Feedback(value=False, rationale=f"{len(failed_calls)} tool calls failed")

    return Feedback(
        value=True,
        rationale=f"Efficient tool usage: {len(tool_calls)} successful calls",
    )
```

Trace analysis enables assessment of retrieval quality in RAG applications, evaluation of reasoning chains in multi-step applications, performance analysis including latency and resource usage, and error detection across complex execution flows.

## Implementation Approaches

MLflow provides two ways to implement scorers, each suited for different complexity levels and reuse patterns.

### Decorator Approach (Recommended)

Use the `@scorer` decorator for simple, function-based scorers that focus on specific quality dimensions:

```python
from mlflow.genai.scorers import scorer


@scorer
def response_tone(outputs):
    """Check if response maintains professional tone"""
    response = outputs.get("response", "")

    informal_phrases = ["hey", "gonna", "wanna", "lol", "btw"]
    found = [p for p in informal_phrases if p in response.lower()]

    if found:
        return Feedback(
            value=False, rationale=f"Informal language detected: {', '.join(found)}"
        )

    return Feedback(value=True, rationale="Professional tone maintained")
```

The decorator approach works well for straightforward evaluation logic, quick prototyping of quality metrics, domain-specific rules that can be expressed simply, and scorers that don't require complex configuration or state management.

### Class-Based Approach

Use the `Scorer` base class for more complex scorers that need configuration, state management, or sophisticated evaluation logic:

```python
from mlflow.genai.scorers import Scorer
from mlflow.entities import Feedback


class ResponseQualityScorer(Scorer):
    name = "response_quality"

    def __init__(self, min_length=50, required_sections=None):
        super().__init__(name=self.name)
        self.min_length = min_length
        self.required_sections = required_sections or []

    def __call__(self, outputs, **kwargs):
        response = outputs.get("response", "")
        issues = []

        # Check length
        if len(response.split()) < self.min_length:
            issues.append(f"Too short (minimum {self.min_length} words)")

        # Check required sections
        missing = [s for s in self.required_sections if s not in response]
        if missing:
            issues.append(f"Missing sections: {', '.join(missing)}")

        if issues:
            return Feedback(value=False, rationale="; ".join(issues))

        return Feedback(value=True, rationale="Response meets all quality criteria")
```

Class-based scorers excel when you need configurable parameters for different evaluation contexts, stateful evaluation that tracks information across multiple calls, complex initialization logic or external resource management, and reusable scorer patterns that can be instantiated with different configurations.

## Platform Considerations

### MLflow OSS Capabilities

MLflow OSS provides the complete scorer framework with full support for custom scorer development using both decorator and class-based approaches. The platform excels at deterministic evaluation logic, business rule implementation, and integration with heuristic metrics.

```python
# OSS scorer pattern
from mlflow.genai.scorers import scorer
from mlflow.metrics import exact_match, latency


@scorer
def custom_business_rule(outputs, expectations):
    """Domain-specific evaluation logic"""
    response = outputs.get("response", "")
    required_terms = expectations.get("required_terms", [])

    found_terms = [term for term in required_terms if term in response.lower()]
    coverage = len(found_terms) / len(required_terms) if required_terms else 1.0

    return Feedback(
        value=coverage >= 0.8,
        rationale=f"Term coverage: {coverage:.1%} ({len(found_terms)}/{len(required_terms)})",
    )


# Combine with heuristic metrics
results = mlflow.genai.evaluate(
    data=eval_data,
    scorers=[custom_business_rule],
    extra_metrics=[exact_match(), latency()],
)
```

### Databricks MLflow Enhancements

Databricks MLflow extends the scorer framework with sophisticated LLM judge capabilities, production monitoring integration, and enterprise-grade evaluation infrastructure:

```python
# Databricks enhanced pattern
from mlflow.genai.scorers import RelevanceToQuery, Safety, scorer


@scorer
def custom_domain_scorer(outputs, trace):
    """Custom logic enhanced with predefined scorers"""
    # Custom evaluation logic
    response = outputs.get("response", "")
    domain_score = evaluate_domain_expertise(response)

    return Feedback(value=domain_score, rationale="Domain expertise assessment")


# Combine custom and predefined scorers
comprehensive_scorers = [
    RelevanceToQuery(),  # LLM-powered semantic evaluation
    Safety(),  # Advanced safety assessment
    custom_domain_scorer,  # Custom business logic
]

results = mlflow.genai.evaluate(data=eval_data, scorers=comprehensive_scorers)
```

The enhanced platform provides seamless integration between custom logic and sophisticated LLM judges, production monitoring that automatically applies scorers to live traffic, and enterprise features for collaboration and governance.

## Best Practices

### Scorer Design Principles

**Single Responsibility** means each scorer should evaluate one specific quality dimension rather than trying to assess multiple aspects simultaneously. This approach makes debugging easier, enables targeted improvements, and provides clearer insights into specific quality issues.

**Graceful Degradation** involves designing scorers to handle missing data, malformed inputs, and unexpected edge cases without failing the entire evaluation. Return meaningful feedback even when ideal conditions aren't met, and use appropriate error handling to maintain evaluation continuity.

**Clear Rationales** help users understand scorer decisions by providing specific, actionable explanations for scores. Good rationales explain what was evaluated, why the score was assigned, and what could be improved.

### Development Workflow Integration

**Start Simple** by beginning with basic quality checks using primitive return values, then evolving to rich Feedback objects as evaluation needs become more sophisticated. This progressive approach helps you understand scorer behavior before adding complexity.

**Test Thoroughly** by validating scorers against known good and bad examples, testing edge cases and error conditions, and ensuring consistent behavior across different input types.

**Version Control** your scorer implementations alongside your application code, maintaining clear documentation of evaluation criteria and changes over time.

### Production Readiness

**Performance Optimization** considers the computational cost of scorers, especially when they'll run on production traffic. Design efficient evaluation logic and consider sampling strategies for expensive assessments.

**Monitoring Integration** ensures scorers work effectively with production monitoring by handling the absence of expectations gracefully and providing meaningful assessment of live traffic quality.

**Error Resilience** builds robust error handling that doesn't disrupt production monitoring when individual scorers encounter issues with specific traces or edge cases.

## Summary

Scorers provide the foundation for systematic GenAI quality assessment, enabling consistent evaluation from development through production. Understanding the scorer framework, implementation patterns, and platform capabilities helps you build effective quality assessment that scales with your application development needs.

**Key Design Principles** include the write-once, use-everywhere approach that ensures consistency across environments, flexible input/output patterns that accommodate different evaluation needs, and robust error handling that maintains evaluation continuity.

**Implementation Strategy** involves choosing appropriate approaches based on complexity needs, integrating effectively with both development workflows and production monitoring, and designing scorers that provide clear, actionable insights into application quality.

The scorer framework transforms subjective quality assessment into systematic, data-driven evaluation that supports confident decision-making about GenAI application improvements and deployments.

## Next Steps

### How-to Guides

- [Use predefined scorers](/genai/eval-monitor/predefined-judge-scorers) for common quality metrics
- [Create custom scorers](/genai/eval-monitor/custom-scorers) for your specific evaluation needs
- [Run scorers in production](/genai/eval-monitor/run-scorer-in-prod) for continuous monitoring
- [Evaluate your app](/genai/eval-monitor/evaluate-app) using scorers in development

### Concepts

- Learn about [LLM judges](/genai/eval-monitor/concepts/judges) for semantic evaluation
- Understand the [evaluation harness](/genai/eval-monitor/concepts/eval-harness) that runs scorers
- Explore [production monitoring](/genai/eval-monitor/concepts/production-monitoring) capabilities