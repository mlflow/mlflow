import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Answer & Context Relevance Judge – Concepts

The **Answer & Context Relevance judge** (`judges.is_context_relevant()`) assesses whether the retrieved context or generated responses from your retrieval-augmented generation (RAG) system are directly relevant to the user's request. Ensuring context relevance is foundational for producing accurate and helpful responses.

## Overview

Relevance judges evaluate:

- **Direct Response Relevance**: Whether your app's response directly addresses the user's question.
- **Contextual Document Relevance**: Whether each document retrieved by your application’s retriever is relevant to the user's input.

## How Relevance Judges Work

Relevance judges follow a structured evaluation workflow:

1. **Context Analysis**: Extract the user request and retrieved/generated content.
2. **Semantic Assessment**: Evaluate semantic alignment between request and context.
3. **Judgment and Rationale**: Produce a clear "yes/no" relevance score with an explanatory rationale.

**Key Requirements:**

- User queries and contexts must be JSON-serializable.
- Traces must clearly identify inputs and outputs.

## Platform Availability

Relevance judges vary between MLflow Open Source and Databricks MLflow:

<Tabs>
<TabItem value="platform_overview" label="Platform Overview" default>

### MLflow Open Source vs Databricks MLflow

**MLflow Open Source:**

- Basic heuristic checks via custom functions
- Explicit rule definition required

**Databricks MLflow:** (includes all OSS features plus)

- Advanced semantic evaluation using LLM judges
- Built-in predefined scorers (`RelevanceToQuery`, `RetrievalRelevance`)
- Automatic interpretation of semantic context

### Feature Comparison Matrix

| Feature Category               | MLflow OSS              | Databricks MLflow             |
| ------------------------------ | ----------------------- | ----------------------------- |
| **Basic Heuristic Checks**     | ✅ Fully Supported       | ✅ Fully Supported             |
| **Advanced Semantic Judges**   | ❌ Not Available         | ✅ Fully Supported             |
| **Prebuilt Relevance Scorers** | ❌ Not Available         | ✅ Fully Supported             |
| **Automatic Context Parsing**  | ❌ Limited               | ✅ Comprehensive               |
| **Detailed Rationales**        | ✅ Manual Implementation | ✅ Automatic and Comprehensive |

</TabItem>

<TabItem value="migration_path" label="Migration Considerations">

### Moving from OSS to Databricks

- Replace manual heuristic evaluations with built-in semantic judges.
- Leverage predefined scorers for streamlined integration.
- Benefit from detailed automatic rationales and semantic evaluation.

### Hybrid Strategy

- Start with OSS for simple, deterministic checks.
- Move to Databricks for comprehensive semantic evaluation in production.

</TabItem>
</Tabs>

## API Signature

```python
from mlflow.genai.judges import is_context_relevant


def is_context_relevant(
    *,
    request: str,  # User's query
    context: Any,  # Context to evaluate (JSON-serializable)
    name: Optional[str] = None  # Optional identifier for MLflow UI
) -> mlflow.entities.Feedback:
    """Returns Feedback object indicating 'yes' or 'no' relevance and detailed rationale."""
```

## Direct SDK Usage

```python
from mlflow.genai.judges import is_context_relevant

# Relevant context example
feedback = is_context_relevant(
    request="What is the capital of France?", context="Paris is the capital of France."
)

# Outputs:
# feedback.value -> "yes"
# feedback.rationale -> detailed relevance explanation

# Irrelevant context example
feedback = is_context_relevant(
    request="What is the capital of France?",
    context="Paris is known for the Eiffel Tower.",
)

# Outputs:
# feedback.value -> "no"
# feedback.rationale -> detailed irrelevance explanation
```

## Using Prebuilt Scorers (Databricks MLflow Only)

Databricks MLflow provides two specialized scorers for relevance evaluation:

### 1. `RelevanceToQuery` scorer

Evaluates whether your application's responses directly address user queries:

```python
from mlflow.genai.scorers import RelevanceToQuery

# Evaluation dataset

eval_dataset = [
    {
        "inputs": {"query": "What is the capital of France?"},
        "outputs": {"response": "Paris is the capital of France."},
    },
    {
        "inputs": {"query": "What is the capital of France?"},
        "outputs": {"response": "France has great food."},
    },
]

# Evaluate
results = mlflow.genai.evaluate(data=eval_dataset, scorers=[RelevanceToQuery()])
```

### 2. `RetrievalRelevance` scorer

Evaluates relevance of each retrieved document:

```python
import mlflow
from mlflow.genai.scorers import RetrievalRelevance


@mlflow.trace(span_type="RETRIEVER")
def retrieve_docs(query):
    # Retrieve relevant documents
    return


@mlflow.trace
def rag_app(query):
    docs = retrieve_docs(query)
    return {"response": f"Retrieved {len(docs)} documents."}


# Evaluation dataset

eval_dataset = [{"inputs": {"query": "What is the capital of France?"}}]

# Evaluate
results = mlflow.genai.evaluate(
    data=eval_dataset, predict_fn=rag_app, scorers=[RetrievalRelevance()]
)
```

## Custom Scorer Implementation

When the prebuilt scorers do not fit your application structure, wrap the judge within a custom scorer:

```python
from mlflow.genai.judges import is_context_relevant
from mlflow.genai.scorers import scorer


@scorer
def custom_context_relevance(inputs, outputs):
    context = outputs["retrieved_context"][0]["content"]
    return is_context_relevant(request=inputs["query"], context=context)


# Evaluate
results = mlflow.genai.evaluate(
    data=custom_eval_dataset, scorers=[custom_context_relevance]
)
```

## Interpreting Results

Each judgment provides:

- **`value`**: "yes" (relevant) or "no" (irrelevant)
- **`rationale`**: Detailed explanation of the judgment, aiding debugging and iterative improvements.

## Summary

Relevance judges offer crucial insights into your RAG system's retrieval accuracy and response quality. MLflow OSS supports heuristic-based custom evaluations, while Databricks MLflow provides advanced semantic evaluation through integrated, prebuilt scorers. Understanding platform differences enables effective selection and implementation of evaluation tools for maintaining high-quality GenAI outputs.
