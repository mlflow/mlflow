import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Groundedness Judge & Scorer – Concepts

The **Groundedness judge** (`judges.is_grounded()`) evaluates whether the responses from your GenAI applications are factually supported by the context provided through retrieval-augmented generation (RAG) systems or tool-generated information, helping detect hallucinations or unsupported statements.

## Overview

Groundedness judges:

* Verify that responses are supported by provided context.
* Detect and prevent hallucinations or inaccuracies.

## How Groundedness Judges Work

Evaluation follows these steps:

1. **Contextual Comparison**: Checks the response against provided context.
2. **Groundedness Assessment**: Determines if the response claims are substantiated by context.
3. **Judgment and Explanation**: Delivers a clear groundedness verdict with detailed rationales.

**Key Requirements:**

* JSON-serializable contexts and responses.
* Clearly defined context and response structure.

## Platform Availability

Differences between MLflow Open Source and Databricks MLflow for groundedness evaluations:

<Tabs>
<TabItem value="platform_overview" label="Platform Overview" default>

### MLflow Open Source vs Databricks MLflow

**MLflow Open Source:**

* Manual heuristic-based groundedness checks.
* Explicit definition of evaluation rules required.

**Databricks MLflow:** (includes all OSS features plus)

* Advanced semantic groundedness evaluation via built-in LLM judges.
* Prebuilt `RetrievalGroundedness` scorer available.
* Automatic generation of detailed rationales.

### Feature Comparison Matrix

| Feature Category                  | MLflow OSS              | Databricks MLflow         |
| --------------------------------- | ----------------------- | ------------------------- |
| **Heuristic Groundedness Checks** | ✅ Fully Supported       | ✅ Fully Supported         |
| **Semantic LLM Judges**           | ❌ Not Available         | ✅ Fully Supported         |
| **Prebuilt Groundedness Scorer**  | ❌ Not Available         | ✅ RetrievalGroundedness   |
| **Automatic Context Parsing**     | ❌ Limited               | ✅ Advanced and Integrated |
| **Detailed Rationales**           | ✅ Manual Implementation | ✅ Automatic and Detailed  |

</TabItem>

<TabItem value="migration_path" label="Migration Considerations">

### Transitioning from OSS to Databricks

* Replace manual heuristic evaluations with semantic groundedness checks.
* Utilize prebuilt scorers for streamlined evaluation.
* Leverage detailed automatic rationales.

### Hybrid Strategy

* Use OSS for initial manual evaluations.
* Transition to Databricks MLflow for production-grade semantic assessments.

</TabItem>
</Tabs>

## API Signature

```python
from mlflow.genai.judges import is_grounded


def is_grounded(
    *, request: str, response: str, context: Any, name: Optional[str] = None
) -> mlflow.entities.Feedback:
    """Returns Feedback indicating groundedness with detailed rationale."""
```

## Direct SDK Usage

```python
from mlflow.genai.judges import is_grounded

# Grounded response example
feedback = is_grounded(
    request="Capital of France?",
    response="Paris",
    context=[{"content": "Paris is the capital of France."}],
)

# Outputs:
# feedback.value -> "yes"
# feedback.rationale -> groundedness details

# Hallucination example
feedback = is_grounded(
    request="Capital of France?",
    response="Paris, population 10 million",
    context=[{"content": "Paris is the capital of France."}],
)

# Outputs:
# feedback.value -> "no"
# feedback.rationale -> details of unsupported claims
```

## Prebuilt `RetrievalGroundedness` Scorer (Databricks MLflow Only)

Databricks MLflow includes the `RetrievalGroundedness` scorer for automatic groundedness evaluation:

**Trace Requirements:**

* Trace must include at least one `RETRIEVER` span.
* Clearly specified inputs and outputs on root span.

```python
from mlflow.genai.scorers import RetrievalGroundedness


@mlflow.trace(span_type="RETRIEVER")
def retrieve_docs(query):
    # Document retrieval logic
    return


@mlflow.trace
def rag_app(query):
    docs = retrieve_docs(query)
    return {"response": "Generated response based on context."}


# Evaluation dataset
eval_dataset = [{"inputs": {"query": "What is MLflow?"}}]

# Run evaluation
results = mlflow.genai.evaluate(
    data=eval_dataset, predict_fn=rag_app, scorers=[RetrievalGroundedness()]
)
```

## Custom Scorer Implementation

For custom data structures, encapsulate the judge within custom scorers:

```python
from mlflow.genai.judges import is_grounded
from mlflow.genai.scorers import scorer


@scorer
def custom_groundedness(inputs, outputs):
    return is_grounded(
        request=inputs["query"],
        response=outputs["response"],
        context=outputs["retrieved_context"],
    )


# Evaluation execution
results = mlflow.genai.evaluate(data=custom_eval_dataset, scorers=[custom_groundedness])
```

## Interpreting Results

Each groundedness assessment returns:

* **`value`**: "yes" (grounded) or "no" (contains hallucinations)
* **`rationale`**: Explanation identifying supported or unsupported claims, facilitating debugging and model refinement.

## Summary

Groundedness judges ensure responses generated by your GenAI applications are factually supported by the provided context, reducing hallucinations and inaccuracies. MLflow OSS provides foundational heuristic evaluations, while Databricks MLflow enhances this capability with prebuilt semantic evaluations, making groundedness assessments more robust and detailed.
