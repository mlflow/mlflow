# Guidelines-based Judges – Concepts

Guidelines-based judges evaluate GenAI outputs by applying clear, natural language pass/fail criteria. They facilitate straightforward, interpretable quality assessments without requiring extensive coding expertise.

## Overview

Guidelines-based judges utilize explicitly defined natural language criteria to determine whether GenAI outputs meet specified quality standards. Common evaluation areas include:

* **Compliance**: "Must not contain sensitive personal data."
* **Tone and Style**: "Maintain a courteous and supportive tone."
* **Specific Requirements**: "Include legal disclaimers as necessary."
* **Accuracy**: "Provide only facts supported by the given context."

### Benefits

* **Business-friendly**: Allows domain experts to directly define quality criteria without coding.
* **Flexible**: Quickly adjust evaluation criteria as needed without changing code.
* **Clear Interpretability**: Provides binary pass/fail outcomes with detailed rationales.
* **Rapid Iteration**: Enables swift testing and refinement of new quality standards.

## How Guidelines Work in OSS MLflow

In OSS MLflow, guidelines-based judges rely on heuristic functions or custom logic, enabling evaluations through clearly structured rule checks. Each evaluation involves:

1. **Context Data**: JSON-formatted inputs and outputs for clear evaluation contexts.
2. **Guidelines Application**: Natural language rules converted into explicit heuristic checks.
3. **Judgment**: Producing a binary pass/fail decision with supporting rationales for transparency.

## Implementing Guidelines in OSS MLflow

### Custom Heuristic Scorers

Explicitly define your evaluation rules using MLflow's custom scorer functionality:

```python
from mlflow.metrics.genai import scorer


@scorer
def compliance_check(inputs, outputs, trace=None):
    response = outputs.get("response", "")

    # Simple compliance heuristic
    if "price" in response.lower():
        return {"pass": False, "rationale": "Response includes pricing details."}
    else:
        return {"pass": True, "rationale": "No pricing details detected."}
```

### When to Use Custom Heuristic Scorers

* Your evaluation rules are straightforward and clearly definable through simple logic.
* You require precise control over the evaluation logic.
* Advanced semantic understanding via LLM-based judges is unnecessary or unavailable.

### Evaluating Outputs with MLflow

Use the defined custom scorers within MLflow evaluations:

```python
import mlflow

# Example dataset for evaluation
data = [
    {
        "inputs": {"question": "What's the price?"},
        "outputs": {"response": "Visit our pricing page for details."},
    },
    {
        "inputs": {"question": "Tell me the price."},
        "outputs": {"response": "$50 per month."},
    },
]

# Run evaluation
results = mlflow.evaluate(data=data, model=my_model, extra_metrics=[compliance_check])
```

## How Guidelines Work in Databricks MLflow

Databricks MLflow extends the functionality with advanced semantic evaluation through built-in guidelines-based judges powered by sophisticated language models:

1. **Context Data**: Automatically parses context data including user inputs, model outputs, retrieved documents, and additional metadata.
2. **Guidelines Application**: Interprets natural language guidelines directly using advanced semantic analysis without explicit heuristic logic.
3. **Judgment**: Provides detailed, explainable rationales for pass/fail decisions, improving transparency and actionable feedback.

### Advanced Features in Databricks MLflow

* **Semantic LLM-based Evaluation**: Natural language guidelines interpreted and evaluated using large language models.
* **Built-in Judging Capabilities**: No need for manual heuristic logic; guidelines can be directly written in natural language.
* **Retrieval-augmented Generation (RAG) Evaluation**: Ensures retrieved content relevance and grounding.
* **Safety and Correctness Checks**: Advanced semantic checks to identify and mitigate potential risks or inaccuracies.

## Writing Effective Guidelines

Guidelines must be clear, specific, and measurable:

* ✅ "Responses must clearly address user queries about product features."
* ❌ "Ensure answers are appropriate."

### Structuring Guidelines

Clearly structured guidelines enhance accuracy and ease of evaluation:

```markdown
The response must:
- Clearly acknowledge user concerns
- Provide accurate and relevant information
- Suggest follow-up actions if needed
```

## Platform Differences and Alternatives

### MLflow OSS Capabilities

* **Heuristic Evaluation**: Uses explicitly defined rule-based logic.
* **Performance Metrics**: Includes latency and token counts.
* **Custom Scorer Framework**: Enables creation of domain-specific heuristic evaluations.

### Databricks MLflow (Advanced Capabilities)

* Advanced semantic evaluations via LLM judges.
* Built-in natural language-based guideline interpretation.
* Retrieval-augmented generation (RAG) evaluation.
* Advanced safety, correctness, and compliance evaluations.

### OSS Alternative Approaches

* Combine heuristic checks with regex and keyword-based filtering.
* Integrate open-source NLP tools (e.g., spaCy, NLTK) for more sophisticated linguistic analysis.
* Aggregate multiple heuristic checks to approximate semantic evaluations provided by advanced LLM judges.

## Summary

Guidelines-based judges offer clear, interpretable evaluations of GenAI outputs using natural language criteria. OSS MLflow provides robust capabilities through heuristic-based custom scorers, while Databricks MLflow enhances evaluation with advanced semantic capabilities. Carefully structured guidelines and explicit rule definitions enable effective, transparent quality assurance tailored to your specific requirements.
