import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Correctness Judge & Scorer – Concepts

The **Correctness judge** (`judges.is_correct()`) evaluates whether responses generated by your GenAI application are factually accurate by comparing them against predefined ground truth (`expected_facts` or `expected_response`).

## Overview

Correctness judges:

* Determine factual accuracy of responses.
* Ensure responses adhere to provided ground-truth information.

## How Correctness Judges Work

The evaluation process follows:

1. **Ground Truth Comparison**: Responses are evaluated against known correct facts or full responses.
2. **Accuracy Assessment**: Checks if the provided response includes necessary correct information.
3. **Judgment and Explanation**: Provides a clear binary correctness result accompanied by detailed rationales.

**Key Requirements:**

* JSON-serializable responses and ground truth.
* Clear ground-truth specification (`expected_facts` or `expected_response`).

## Platform Availability

Differences between MLflow Open Source and Databricks MLflow for correctness evaluation:

<Tabs>
<TabItem value="platform_overview" label="Platform Overview" default>

### MLflow Open Source vs Databricks MLflow

**MLflow Open Source:**

* Basic heuristic correctness evaluations.
* Requires explicit custom-defined rules.

**Databricks MLflow:** (includes all OSS features plus)

* Advanced semantic correctness evaluations via built-in LLM judges.
* Prebuilt `Correctness` scorer.
* Automatic, detailed rationales for each correctness verdict.

### Feature Comparison Matrix

| Feature Category                | MLflow OSS              | Databricks MLflow         |
| ------------------------------- | ----------------------- | ------------------------- |
| **Basic Heuristic Checks**      | ✅ Fully Supported       | ✅ Fully Supported         |
| **Semantic Correctness Judges** | ❌ Not Available         | ✅ Fully Supported         |
| **Prebuilt Correctness Scorer** | ❌ Not Available         | ✅ Correctness             |
| **Detailed Rationales**         | ✅ Manual Implementation | ✅ Automatic and Detailed  |
| **Automatic Context Parsing**   | ❌ Limited               | ✅ Advanced and Integrated |

</TabItem>

<TabItem value="migration_path" label="Migration Considerations">

### Transitioning from OSS to Databricks

* Replace heuristic checks with semantic evaluations.
* Use prebuilt `Correctness` scorer for streamlined evaluations.
* Leverage detailed automatic rationales.

### Hybrid Approach

* Initial simple checks via OSS.
* Advanced semantic accuracy checks in Databricks MLflow for production.

</TabItem>
</Tabs>

## API Signature

```python
from mlflow.genai.judges import is_correct

def is_correct(
    *,
    request: str,
    response: str,
    expected_facts: Optional[list[str]] = None,
    expected_response: Optional[str] = None,
    name: Optional[str] = None
) -> mlflow.entities.Feedback:
    """Returns Feedback indicating correctness with detailed rationale."""
```

## Direct SDK Usage

```python
from mlflow.genai.judges import is_correct

# Correct response example
feedback = is_correct(
    request="What is MLflow?",
    response="MLflow is an open-source platform for managing ML lifecycles.",
    expected_facts=["MLflow is open-source", "MLflow manages ML lifecycles"]
)

# Outputs:
# feedback.value -> "yes"
# feedback.rationale -> correctness details

# Incorrect response example
feedback = is_correct(
    request="When was MLflow released?",
    response="MLflow was released in 2017.",
    expected_facts=["MLflow was released in June 2018"]
)

# Outputs:
# feedback.value -> "no"
# feedback.rationale -> details of incorrectness
```

## Prebuilt `Correctness` Scorer (Databricks MLflow Only)

Databricks MLflow provides a prebuilt scorer for correctness evaluation:

**Trace Requirements:**

* Inputs and outputs clearly specified on root trace span.
* Ground-truth provided as `expected_facts` or `expected_response`.

```python
from mlflow.genai.scorers import Correctness

# Evaluation dataset with ground truth

eval_dataset = [
    {"inputs": {"query": "Capital of France?"},
     "outputs": {"response": "Paris is France's capital."},
     "expectations": {"expected_facts": ["Paris is the capital of France."]}}
]

# Evaluation execution
results = mlflow.genai.evaluate(
    data=eval_dataset,
    scorers=[Correctness()]
)
```

## Alternative: Using `expected_response`

Use `expected_response` as an alternative to `expected_facts`:

```python
eval_dataset = [
    {"inputs": {"query": "What is MLflow?"},
     "outputs": {"response": "MLflow manages ML lifecycles."},
     "expectations": {"expected_response": "MLflow is a platform for managing ML lifecycles."}}
]

results = mlflow.genai.evaluate(
    data=eval_dataset,
    scorers=[Correctness()]
)
```

## Custom Scorer Implementation

Wrap the judge in custom scorers when needed:

```python
from mlflow.genai.judges import is_correct
from mlflow.genai.scorers import scorer

@scorer
def correctness_custom(inputs, outputs, expectations):
    return is_correct(
        request=inputs["question"],
        response=outputs["answer"],
        expected_facts=expectations["facts"]
    )

# Evaluate
results = mlflow.genai.evaluate(
    data=custom_eval_dataset,
    scorers=[correctness_custom]
)
```

## Interpreting Results

Each correctness judgment provides:

* **`value`**: "yes" (correct) or "no" (incorrect)
* **`rationale`**: Detailed explanation identifying accurate or missing information.

## Summary

Correctness judges ensure the factual accuracy of responses from your GenAI application. MLflow OSS supports basic correctness assessments through custom-defined heuristic checks, while Databricks MLflow offers robust, prebuilt semantic evaluations. Clearly distinguishing between these capabilities allows you to select the appropriate tools to ensure accurate, reliable GenAI outputs.
