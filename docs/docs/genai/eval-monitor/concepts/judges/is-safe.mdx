import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Safety Judge & Scorer – Concepts

The **Safety judge** (`judges.is_safe()`) assesses the safety of content generated by GenAI applications or provided by users. It identifies potentially harmful, unethical, or inappropriate material, ensuring that applications maintain safe interactions.

## Overview

Safety judges:

* Check for harmful, unethical, or inappropriate content.
* Protect users and platforms from unsafe interactions.

## How Safety Judges Work

Safety evaluation process:

1. **Content Evaluation**: Analyzes the provided text content.
2. **Risk Assessment**: Determines potential harm or appropriateness issues.
3. **Judgment and Explanation**: Returns clear safety verdicts with detailed rationales.

**Key Requirements:**

* JSON-serializable content.
* Clearly structured inputs and outputs.

## Platform Availability

Differences between MLflow Open Source and Databricks MLflow for safety evaluations:

<Tabs>
<TabItem value="platform_overview" label="Platform Overview" default>

### MLflow Open Source vs Databricks MLflow

**MLflow Open Source:**

* Basic keyword-based heuristic safety checks.
* Requires explicit custom-defined rules.

**Databricks MLflow:** (includes all OSS features plus)

* Advanced semantic safety evaluation via built-in LLM judges.
* Prebuilt `Safety` scorer available.
* Automatic detailed rationales explaining safety assessments.

### Feature Comparison Matrix

| Feature Category                | MLflow OSS              | Databricks MLflow         |
| ------------------------------- | ----------------------- | ------------------------- |
| **Basic Keyword Safety Checks** | ✅ Fully Supported       | ✅ Fully Supported         |
| **Semantic Safety Judges**      | ❌ Not Available         | ✅ Fully Supported         |
| **Prebuilt Safety Scorer**      | ❌ Not Available         | ✅ Safety                  |
| **Automatic Rationales**        | ✅ Manual Implementation | ✅ Automatic and Detailed  |
| **Automatic Content Parsing**   | ❌ Limited               | ✅ Advanced and Integrated |

</TabItem>

<TabItem value="migration_path" label="Migration Considerations">

### Transitioning from OSS to Databricks

* Upgrade from heuristic to semantic safety evaluations.
* Leverage built-in prebuilt scorers for streamlined checks.
* Benefit from detailed automatic rationales.

### Hybrid Strategy

* Initial heuristic checks via OSS.
* Production-grade semantic safety evaluations in Databricks MLflow.

</TabItem>
</Tabs>

## API Signature

```python
from mlflow.genai.judges import is_safe


def is_safe(*, content: str, name: Optional[str] = None) -> mlflow.entities.Feedback:
    """Returns Feedback indicating safety with detailed rationale."""
```

## Direct SDK Usage

```python
from mlflow.genai.judges import is_safe

# Safe content example
feedback = is_safe(content="I am happy.")

# Outputs:
# feedback.value -> "yes"
# feedback.rationale -> safety details

# Potentially unsafe content example
feedback = is_safe(content="Instructions on harmful actions.")

# Outputs:
# feedback.value -> "no"
# feedback.rationale -> identified risks
```

## Prebuilt `Safety` Scorer (Databricks MLflow Only)

Databricks MLflow provides a prebuilt safety scorer:

**Trace Requirements:**

* Clearly specified outputs on root trace span.

```python
from mlflow.genai.scorers import Safety

# Evaluation dataset

eval_dataset = [
    {
        "inputs": {"query": "Explain MLflow."},
        "outputs": {"response": "MLflow manages ML lifecycles."},
    }
]

# Execute evaluation
results = mlflow.genai.evaluate(data=eval_dataset, scorers=[Safety()])
```

## Custom Scorer Implementation

For custom evaluations, wrap the judge in scorers:

```python
from mlflow.genai.judges import is_safe
from mlflow.genai.scorers import scorer


@scorer
def safety_custom(inputs, outputs):
    return is_safe(content=outputs["response"])


@scorer
def input_safety(inputs, outputs):
    return is_safe(content=inputs["question"], name="input_safety")


# Evaluate
results = mlflow.genai.evaluate(
    data=custom_eval_dataset, scorers=[safety_custom, input_safety]
)
```

## Interpreting Results

Each safety evaluation provides:

* **`value`**: "yes" (safe) or "no" (unsafe)
* **`rationale`**: Explanation detailing safety concerns, including categories of detected harm.

## Summary

Safety judges provide critical assessments of content safety, preventing potentially harmful or inappropriate interactions. MLflow OSS allows basic safety checks via heuristic keyword evaluations, while Databricks MLflow enhances these checks with prebuilt semantic safety assessments, ensuring thorough and effective evaluations to safeguard your GenAI applications.
