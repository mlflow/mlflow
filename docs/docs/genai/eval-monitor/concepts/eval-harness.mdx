import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Evaluation Harness - Concepts

The Evaluation Harness (`mlflow.genai.evaluate()`) is the orchestration engine that systematically tests your GenAI application's quality by running it against test data and applying scorers to measure performance. It provides a standardized framework for consistent evaluation across development and production environments.

Understanding the evaluation harness is crucial for implementing reliable quality assessment workflows that scale from initial development through production monitoring. The harness ensures that your evaluation methodology remains consistent regardless of how your application evolves.

## How the Evaluation Harness Works

The evaluation process follows a systematic workflow that transforms test inputs into actionable quality insights:

**1. Test Execution**: The harness runs your application on each test input from your evaluation dataset, capturing complete execution traces that include inputs, outputs, and intermediate processing steps.

**2. Quality Assessment**: Scorers analyze the captured traces to assess various quality dimensions, generating detailed feedback with scores and rationales for each evaluation criterion.

**3. Result Organization**: All traces and their assessments are stored in an organized Evaluation Run, enabling analysis, comparison, and tracking over time.

This systematic approach ensures reproducible evaluation results and enables confident decision-making about application quality and improvements.

## Evaluation Modes: Flexibility for Different Scenarios

The evaluation harness supports two distinct modes, each optimized for different use cases and constraints:

<Tabs>
<TabItem value="direct_evaluation" label="Direct Evaluation (Recommended)" default>

### Mode 1: Direct Application Execution

In this mode, MLflow calls your GenAI application directly to generate fresh traces for each test case. This is the recommended approach for most evaluation scenarios.

**How it works**:
1. **Application Invocation**: MLflow calls your instrumented application function for each test input
2. **Trace Capture**: Complete execution traces are automatically captured, including all intermediate steps
3. **Scorer Application**: Quality scorers analyze the fresh traces to assess performance
4. **Result Storage**: Traces with attached assessments are stored in an Evaluation Run

**Key Benefits**:
- **Consistency**: Ensures identical trace structure between development evaluation and production monitoring
- **Performance**: Automatic parallelization of application calls for faster evaluation
- **Completeness**: Captures full execution context including intermediate operations and timing
- **Reliability**: Eliminates skew between offline evaluation and online production behavior

**Requirements**:
- Application must be instrumented with MLflow Tracing
- Function must accept inputs as keyword arguments matching your dataset schema
- Must return JSON-serializable outputs
- Should emit exactly one trace per call for predictable evaluation behavior

**Best for**:
- Development workflows where you can run application code directly
- Teams prioritizing consistency between offline and online evaluation
- Applications with proper MLflow tracing instrumentation
- Scenarios requiring complete execution context for accurate assessment

</TabItem>
<TabItem value="answer_sheet" label="Answer Sheet Evaluation">

### Mode 2: Pre-computed Results Evaluation

This mode evaluates pre-computed outputs or existing traces when direct application execution isn't feasible or practical.

**How it works**:
1. **Data Preparation**: You provide either inputs/outputs pairs or existing MLflow traces
2. **Trace Creation**: If using inputs/outputs, MLflow creates synthetic traces for evaluation
3. **Scorer Application**: Quality scorers analyze the provided or synthetic traces
4. **Result Storage**: Evaluation results are stored in an Evaluation Run for analysis

**Use Cases**:
- **External Systems**: Evaluating outputs from applications you can't run directly
- **Historical Analysis**: Assessing quality of past production outputs
- **Cross-Platform Comparison**: Comparing outputs from different systems or versions
- **Offline Analysis**: Working with pre-collected application outputs

**Data Format Options**:
- **Inputs + Outputs**: Provide the original inputs and corresponding outputs from your application
- **Existing Traces**: Use MLflow traces that were captured from previous application runs

**Important Considerations**:
- **Potential Skew**: Manually assembled inputs/outputs may not capture full execution context
- **Limited Context**: Synthetic traces may miss intermediate steps crucial for comprehensive evaluation
- **Scorer Compatibility**: Some scorers (especially RAG-specific ones) require complete trace information
- **Production Monitoring**: May require scorer modifications for production monitoring compatibility

**When to Use**:
- Application code cannot be run directly with tracing
- Evaluating historical outputs from production systems
- Integrating with existing evaluation pipelines that generate outputs separately
- Testing outputs from external APIs or third-party services

</TabItem>
</Tabs>

## Key Components and Parameters

Understanding the core parameters and components helps you configure evaluation effectively for your specific needs:

### Data Parameter: Flexible Input Formats

The `data` parameter accepts multiple formats to accommodate different workflow preferences and organizational needs:

<Tabs>
<TabItem value="evaluation_dataset" label="Evaluation Dataset (Recommended)" default>

**MLflow Evaluation Dataset**: Structured, versioned datasets with schema validation and lineage tracking

```python
# Using managed evaluation dataset (Databricks MLflow)
dataset = mlflow.data.get_evaluation_dataset("my_qa_dataset_v2")

# Using simple evaluation dataset format (Both platforms)
dataset = [
    {
        "inputs": {"question": "What is MLflow?"},
        "expectations": {"expected_facts": ["platform", "ML lifecycle"]}
    }
]

mlflow.genai.evaluate(
    data=dataset,
    model=my_app,
    extra_metrics=scorers
)
```

**Benefits**:
- **Schema Validation**: Ensures data consistency and catches errors early
- **Version Control**: Track dataset evolution alongside application development
- **Lineage Tracking**: Understand data provenance and trace origins
- **Collaboration**: Multiple team members can contribute to dataset development

</TabItem>
<TabItem value="dataframe_lists" label="DataFrames and Lists">

**Pandas DataFrame**: Familiar tabular format for data manipulation and analysis

```python
import pandas as pd

# DataFrame format
df = pd.DataFrame([
    {
        "inputs": {"question": "How do I install MLflow?"},
        "expectations": {"expected_response": "pip install mlflow"}
    },
    {
        "inputs": {"question": "What are MLflow experiments?"},
        "expectations": {"guidelines": "Explain clearly with examples"}
    }
])

mlflow.genai.evaluate(data=df, model=my_app, extra_metrics=scorers)
```

**List of Dictionaries**: Simple, direct format for quick evaluation setup

```python
# Simple list format
eval_data = [
    {
        "inputs": {"query": "user question"},
        "outputs": {"response": "pre-computed answer"},  # Answer sheet mode
        "expectations": {"guidelines": "be helpful"}
    }
]

mlflow.genai.evaluate(data=eval_data, extra_metrics=scorers)
```

**Use Cases**:
- **Quick Prototyping**: Fast setup for immediate evaluation needs
- **Data Analysis**: Leverage pandas for dataset manipulation and exploration
- **Simple Workflows**: Straightforward evaluation without advanced dataset management

</TabItem>
</Tabs>

### Predict Function: Application Integration

For direct evaluation mode, the `predict_fn` parameter defines how MLflow interacts with your application:

**Function Requirements**:
- **Keyword Arguments**: Must accept inputs dictionary keys as keyword arguments
- **JSON Serialization**: Must return JSON-serializable outputs (typically dictionaries)
- **Tracing Instrumentation**: Must use MLflow Tracing to capture execution details
- **Single Trace Emission**: Should emit exactly one trace per call for predictable behavior

**Integration Patterns**:

```python
# Pattern 1: Direct function wrapping
@mlflow.trace
def my_chatbot(question: str, context: str = None) -> dict:
    """Application function with proper tracing."""
    # Application logic here
    response = generate_response(question, context)
    return {"response": response}

# Pattern 2: Deployed endpoint integration
endpoint_predict_fn = mlflow.deployments.to_predict_fn("endpoints:/my-model")

# Pattern 3: Logged model evaluation
logged_model = mlflow.pyfunc.load_model("models:/my-model/latest")

@mlflow.trace  
def evaluate_logged_model(question: str) -> dict:
    """Wrapper for evaluating logged models."""
    result = logged_model.predict({"question": question})
    return {"response": result}
```

### Scorers: Quality Assessment Framework

Scorers define what quality dimensions are evaluated and how assessment is performed:

**Scorer Types**:
- **Predefined Scorers**: Ready-to-use implementations for common quality dimensions
- **Custom Scorers**: Application-specific evaluation logic tailored to your requirements
- **Hybrid Approaches**: Combinations of predefined and custom scorers for comprehensive assessment

**Scorer Configuration**:

```python
from mlflow.genai.scorers import RelevanceToQuery, Safety
from mlflow.genai.scorers import scorer

# Predefined scorers (Databricks MLflow)
predefined_scorers = [
    RelevanceToQuery(),  # Semantic relevance assessment
    Safety(),            # Content safety evaluation
]

# Custom scorer (Both platforms)
@scorer
def response_completeness(inputs, outputs):
    """Custom scorer for response completeness."""
    response = outputs.get("response", "")
    query = inputs.get("question", "")
    
    # Custom evaluation logic
    completeness_score = evaluate_completeness(query, response)
    return completeness_score

# Combined evaluation approach
all_scorers = predefined_scorers + [response_completeness]
```

## Data Schema and Requirements

Understanding the expected data schema ensures proper evaluation setup and helps avoid common configuration issues:

### Schema by Evaluation Mode

<Tabs>
<TabItem value="direct_schema" label="Direct Evaluation Schema" default>

#### Required Fields for Direct Evaluation

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `inputs` | `dict[str, Any]` | Arguments passed to your predict function | `{"question": "What is MLflow?", "context": "..."}` |
| `expectations` | `dict[str, Any]` *(Optional)* | Ground truth labels for scorer evaluation | `{"expected_facts": ["platform", "ML"], "guidelines": "be clear"}` |

**Important Notes**:
- **No outputs field**: MLflow generates outputs by calling your application
- **Keyword matching**: Input keys must match your predict function's parameter names
- **JSON serialization**: All data must be JSON-serializable for proper processing

**Example Direct Evaluation Dataset**:
```python
direct_eval_data = [
    {
        "inputs": {
            "question": "How do I log metrics in MLflow?",
            "user_context": "beginner"
        },
        "expectations": {
            "expected_facts": ["mlflow.log_metric", "tracking server"],
            "guidelines": "Provide code examples for beginners"
        }
    }
]
```

</TabItem>
<TabItem value="answer_sheet_schema" label="Answer Sheet Schema">

#### Required Fields for Answer Sheet Evaluation

**Option A - Inputs and Outputs**:

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `inputs` | `dict[str, Any]` | Original inputs to your application | `{"question": "What is MLflow?"}` |
| `outputs` | `dict[str, Any]` | Pre-computed outputs from your application | `{"response": "MLflow is a platform..."}` |
| `expectations` | `dict[str, Any]` *(Optional)* | Ground truth labels for evaluation | `{"expected_facts": ["platform"]}` |

**Option B - Existing Traces**:

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `trace` | `mlflow.entities.Trace` | Complete MLflow trace objects | Pre-captured trace with full execution context |
| `expectations` | `dict[str, Any]` *(Optional)* | Additional ground truth labels | Can also be included as trace assessments |

**Example Answer Sheet Dataset**:
```python
answer_sheet_data = [
    {
        "inputs": {"question": "What is MLflow?"},
        "outputs": {"response": "MLflow is an open-source platform for managing ML workflows."},
        "expectations": {"expected_facts": ["open-source", "platform", "ML workflows"]}
    }
]
```

</TabItem>
</Tabs>

### Reserved Expectation Keys

The `expectations` field uses specific reserved keys that predefined scorers recognize:

| Key | Used By | Description | Example |
|-----|---------|-------------|---------|
| `expected_facts` | Correctness scorers | List of factual claims that should appear in response | `["Paris is capital", "France in Europe"]` |
| `expected_response` | Response comparison scorers | Complete expected output for direct comparison | `"The capital of France is Paris."` |
| `guidelines` | Guidelines-based scorers | Natural language evaluation criteria | `"Response must be polite and professional"` |
| `expected_retrieved_context` | RAG evaluation scorers | Documents that should be retrieved | List of expected document references |

## Common Implementation Patterns

Understanding common patterns helps you implement evaluation effectively for different scenarios:

### Development Workflow Integration

**Iterative Development Pattern**:
```python
# Test specific improvements
def evaluate_current_version():
    results = mlflow.genai.evaluate(
        data=focused_test_set,
        model=my_app_v2,
        extra_metrics=standard_scorers
    )
    return results

# Compare with previous version
def compare_versions():
    baseline_results = mlflow.genai.evaluate(
        data=comprehensive_dataset,
        model=my_app_v1,
        extra_metrics=standard_scorers
    )
    
    current_results = mlflow.genai.evaluate(
        data=comprehensive_dataset, 
        model=my_app_v2,
        extra_metrics=standard_scorers
    )
    
    return analyze_improvement(baseline_results, current_results)
```

### Production Integration

**Endpoint Evaluation Pattern**:
```python
# Evaluate deployed models
deployed_predict_fn = mlflow.deployments.to_predict_fn("endpoints:/production-chatbot")

production_results = mlflow.genai.evaluate(
    data=production_test_dataset,
    model=deployed_predict_fn,
    extra_metrics=[RelevanceToQuery(), Safety()]
)
```

**Historical Analysis Pattern**:
```python
# Analyze past production traces
production_traces = mlflow.search_traces(
    experiment_names=["production"],
    filter_string="span.status = 'OK'"
)

historical_analysis = mlflow.genai.evaluate(
    data=production_traces,
    extra_metrics=quality_scorers
)
```

## Best Practices for Evaluation Harness Usage

### Mode Selection Guidelines

**Choose Direct Evaluation When**:
- You can run your application code with proper tracing instrumentation
- You want to ensure consistency between development and production evaluation
- You need complete execution context for accurate quality assessment
- You plan to use the same scorers for production monitoring

**Choose Answer Sheet Evaluation When**:
- Your application cannot be run directly (external APIs, legacy systems)
- You're analyzing historical outputs from production systems
- You're comparing outputs across different platforms or versions
- You have pre-computed results from batch processing workflows

### Performance Optimization

**Efficient Dataset Design**:
- Start with smaller, focused datasets for rapid iteration
- Expand to comprehensive datasets for thorough validation
- Use stratified sampling to ensure representative coverage
- Balance common cases with edge cases and challenging scenarios

**Scorer Optimization**:
- Use predefined scorers when they meet your evaluation needs
- Implement efficient custom scorers for business-specific requirements
- Consider scorer computational costs when designing evaluation suites
- Batch related evaluations to minimize overhead

### Error Handling and Validation

**Robust Evaluation Setup**:
- Validate data schema before running large evaluations
- Test scorers on small samples to ensure proper functionality
- Handle application errors gracefully during evaluation
- Monitor evaluation performance and resource usage

**Quality Assurance**:
- Review evaluation results for unexpected patterns or anomalies
- Validate scorer behavior against known good and bad examples
- Compare results across different evaluation runs to ensure consistency
- Document evaluation methodology and decision criteria

## Integration with MLflow Ecosystem

### Experiment Tracking Integration

**Run Organization**: Evaluation runs are standard MLflow runs, providing full integration with experiment tracking workflows and enabling comprehensive analysis and comparison capabilities.

**Model Versioning**: Link evaluation results to specific model versions using the `model_id` parameter to track quality evolution across development iterations.

**Metadata Capture**: Automatically capture evaluation configuration, dataset information, and environmental context for complete reproducibility.

### Production Monitoring Alignment

**Scorer Reusability**: Scorers developed for offline evaluation can be directly used in production monitoring, ensuring consistency between development and production quality assessment.

**Trace Compatibility**: Direct evaluation mode generates traces identical to production traces, enabling seamless transition from development evaluation to live monitoring.

**Quality Consistency**: Maintain consistent quality definitions and thresholds across development, testing, and production environments.

## Related Concepts

- **[Evaluation Datasets](/genai/eval-monitor/concepts/eval-datasets)**: Understanding the test data that powers evaluation
- **[Scorers](/genai/eval-monitor/concepts/scorers)**: Deep dive into quality assessment methodology  
- **[Evaluation Runs](/genai/eval-monitor/concepts/evaluation-runs)**: How results are stored and analyzed
- **[Production Monitoring](/genai/eval-monitor/production-monitoring)**: Extending evaluation to live applications

## Summary

The Evaluation Harness provides a robust, flexible framework for systematic GenAI quality assessment that adapts to different development workflows and organizational constraints. Understanding its modes, components, and best practices enables effective implementation of evaluation strategies that scale from development through production.

**Key Success Factors**:
- **Choose the right mode** based on your application architecture and constraints
- **Design appropriate data schemas** that support your evaluation objectives
- **Configure scorers effectively** to assess the quality dimensions that matter most
- **Integrate systematically** with your development and deployment workflows

**Implementation Recommendations**:
- **Start with direct evaluation** when possible for maximum consistency and completeness
- **Use structured datasets** to ensure reliable, reproducible evaluation results
- **Combine predefined and custom scorers** for comprehensive quality assessment
- **Plan for production integration** by designing evaluation workflows that extend to live monitoring

The evaluation harness transforms ad-hoc quality checking into systematic, data-driven assessment that supports confident decision-making about GenAI application improvements and deployments.