import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Evaluation Concepts Overview

MLflow's GenAI evaluation system provides a comprehensive framework for assessing and monitoring the quality of your AI applications. Understanding the core concepts and how they work together is essential for implementing effective evaluation strategies that scale from development through production.

This guide explains the key components of MLflow's evaluation ecosystem and how they interact to provide reliable, consistent quality assessment across your application's lifecycle.

## Core Evaluation Architecture

The MLflow evaluation system is built around several interconnected components that work together to provide comprehensive quality assessment:

<Tabs>
<TabItem value="data_model" label="Data Model Foundation" default>

### Traces and Assessments

At the foundation of MLflow evaluation is the **Trace data model**, which captures your application's execution and evaluation results:

**Traces** represent complete executions of your application, containing:
- Input data that triggered the execution
- Output data generated by your application  
- Execution spans showing internal operations (retrievals, LLM calls, tool usage)
- Timing and performance metadata

**Assessments** are attached to traces and come in two types:
- **Feedback**: Quality scores and evaluations generated by scorers (e.g., "relevance: 0.85")
- **Expectations**: Ground truth labels provided by domain experts (e.g., "expected_response: Paris is the capital of France")

This data model ensures that evaluation results are always linked to the specific execution context that generated them, providing full traceability and enabling detailed analysis of quality patterns.

</TabItem>
<TabItem value="evaluation_flow" label="Evaluation Flow">

### End-to-End Evaluation Process

The evaluation process follows a systematic flow from input to actionable insights:

1. **Input Preparation**: Evaluation datasets with representative user scenarios
2. **Execution**: Application runs (either fresh executions or pre-computed results)
3. **Assessment**: Scorers evaluate outputs against quality criteria
4. **Analysis**: Results aggregation and comparison across versions
5. **Action**: Quality improvements based on evaluation insights

**Key Benefits of This Architecture**:
- **Consistency**: Same evaluation logic works in development and production
- **Traceability**: Every quality score links back to specific execution context
- **Scalability**: Framework supports both offline evaluation and production monitoring
- **Flexibility**: Accommodates different evaluation approaches and custom criteria

</TabItem>
</Tabs>

## Evaluation Harness: Running Evaluations

The **Evaluation Harness** (`mlflow.evaluate()`) orchestrates the evaluation process, supporting two distinct modes depending on your needs and constraints:

<Tabs>
<TabItem value="app_execution" label="Application Execution Mode" default>

### Mode 1: Direct Application Execution *(Recommended)*

In this mode, the evaluation harness calls your application directly for each test case:

**How it works**:
1. Takes your instrumented application code, evaluation dataset, and scorers
2. Executes your application for each row in the evaluation dataset
3. Captures complete traces of each execution
4. Applies scorers to each trace, generating quality assessments
5. Stores results in an MLflow evaluation run for analysis

**Key advantages**:
- **Consistency**: Ensures scorers operate on traces generated by the same code used in production
- **Performance**: MLflow can parallelize application calls for faster evaluation
- **Completeness**: Captures full execution context including intermediate steps
- **Reliability**: Reduces skew between development and production evaluation results

**Best for**:
- Applications with proper MLflow tracing instrumentation
- Development workflows where you can run application code directly
- Teams prioritizing consistency between offline and online evaluation

</TabItem>
<TabItem value="answer_sheet" label="Answer Sheet Mode">

### Mode 2: Pre-computed Results ("Answer Sheet")

This mode evaluates pre-computed application outputs when direct execution isn't feasible:

**How it works**:
1. You provide either existing traces OR input/output dictionaries
2. If using input/output dicts, MLflow creates synthetic traces
3. Evaluation harness applies scorers to the provided/created traces
4. Results are stored in an MLflow evaluation run

**When to use**:
- Application code cannot be run directly with tracing
- Evaluating historical outputs from production systems
- Integrating with existing evaluation pipelines
- Testing with outputs from external systems

**Important considerations**:
- **Potential skew**: Manually assembled inputs/outputs may not capture full execution context
- **Limited context**: Synthetic traces may miss intermediate steps crucial for some scorers
- **Reduced accuracy**: Some scorers (especially RAG-specific ones) require complete trace information

**Recommendation**: Use this mode only when direct application execution isn't possible, and validate results against Mode 1 when feasible.

</TabItem>
</Tabs>

## Scorers: The Evaluation Engine

**Scorers** are the core evaluation components that assess your application's quality. They're designed with a crucial principle: **write once, use everywhere** - the same scorer works for both offline evaluation and production monitoring.

### How Scorers Work

Each scorer implements specific evaluation logic to assess quality dimensions:

**Input Processing**: Scorers receive traces from your application plus optional expectations (ground truth labels)

**Quality Assessment**: Scorers apply their evaluation logic, which varies by type:
- **Deterministic scorers**: Use Python code for objective checks (e.g., response length, keyword presence)
- **LLM judge scorers**: Leverage language models for semantic quality assessment (e.g., relevance, safety)
- **Hybrid scorers**: Combine multiple evaluation approaches for comprehensive assessment

**Output Generation**: Scorers produce one or more Feedback objects containing quality scores, rationales, and metadata

<Tabs>
<TabItem value="predefined_scorers" label="Predefined Scorers" default>

### Ready-to-Use Quality Assessment

MLflow provides predefined scorers for common evaluation scenarios:

**General Quality Scorers** (Databricks MLflow Only):
- **RelevanceToQuery**: Evaluates if responses address user questions directly
- **Safety**: Detects harmful, offensive, or toxic content
- **Correctness**: Assesses factual accuracy against known correct information

**RAG-Specific Scorers** (Databricks MLflow Only):
- **RetrievalRelevance**: Measures whether retrieved documents are relevant to the query
- **RetrievalGroundedness**: Verifies responses are supported by retrieved context
- **RetrievalSufficiency**: Determines if retrieved information is adequate for the expected response

**MLflow OSS Capabilities**:
- Heuristic metrics (exact match, ROUGE, BLEU, readability scores)
- Performance metrics (latency, token count)
- Custom scorer framework (@scorer decorator)
- Model type defaults for common evaluation scenarios

**Databricks MLflow Additional Capabilities**:
- Advanced LLM-based predefined scorers for semantic evaluation
- RAG-specific evaluation judges (retrieval relevance, groundedness, sufficiency)
- Built-in safety and correctness assessment using LLM judges
- Custom LLM judge creation with prompt templates and examples

**Getting Started Path**: Most teams begin with predefined scorers to establish baseline evaluation, then add custom scorers for application-specific requirements.

</TabItem>
<TabItem value="custom_scorers" label="Custom Scorers">

### Application-Specific Evaluation

As applications mature, custom scorers become essential for capturing unique quality requirements:

**Guidelines-Based Scorers** (Available on Both Platforms): Use natural language criteria for pass/fail evaluation
- **MLflow OSS**: Custom implementation using @scorer decorator
- **Databricks MLflow**: Built-in GuidelineAdherence scorer with LLM judge
- **Best for**: Business rule compliance, style guide adherence, content requirements
- **Example**: "The response must be polite and professional in tone"
- **Advantage**: Domain experts can write evaluation criteria directly

**Prompt-Based LLM Judges** (Databricks MLflow Only): Full control over LLM evaluation logic
- **Best for**: Complex, nuanced evaluation requiring sophisticated reasoning
- **Example**: Multi-category scoring with custom rubrics
- **Advantage**: Maximum flexibility in evaluation criteria and output formats

**Custom Python Scorers** (Available on Both Platforms): Arbitrary evaluation logic using any programming approach
- **Best for**: Complex business logic, integration with external systems, mathematical computations
- **Example**: API validation, format compliance, statistical analysis
- **Advantage**: Complete flexibility and integration capabilities

</TabItem>
</Tabs>

## Judges: LLM-Powered Evaluation Building Blocks *(Databricks MLflow Only)*

**Judges** are specialized building blocks available in Databricks MLflow that use language models to assess quality dimensions. They're designed as reusable components that can be incorporated into scorers or used directly within applications.

> **Note**: While the judge concept exists in MLflow OSS through custom scorers, the pre-built LLM judges described here are available only in Databricks MLflow.

### Judge Capabilities

**Research-Backed Quality**: MLflow's research team has invested significant effort in tuning judge quality across various evaluation dimensions.

**Flexible Usage Patterns**:
- **Within Scorers**: Wrap judges in custom scorers for evaluation workflows
- **In Applications**: Call judges directly within application logic for real-time quality decisions
- **Standalone Assessment**: Use judges for ad-hoc quality evaluation tasks

**Judge Types**:
- **Use-Case Specific**: Specialized judges for common scenarios (groundedness, correctness, relevance)
- **Guidelines-Based**: Flexible pass/fail evaluation against natural language criteria  
- **Prompt-Based**: Fully customizable evaluation logic with custom prompts and output formats

### Integration Patterns

**Evaluation Integration**: Judges wrapped in scorers provide consistent quality assessment across development and production environments.

**Application Integration**: Judges can be called directly within application logic for real-time quality decisions (e.g., checking context relevance before generating responses).

**Hybrid Approaches**: Combine multiple judge types to assess different quality dimensions comprehensively.

## Production Monitoring: Continuous Quality Assessment *(Databricks MLflow Only)*

The **Production Monitoring Service** extends evaluation capabilities to live applications, providing continuous quality oversight without manual intervention. This service is available exclusively in Databricks MLflow.

> **Note**: MLflow OSS users can implement custom monitoring solutions using the evaluation harness and custom scheduling, but the automated production monitoring service described here is a Databricks MLflow feature.

### How Production Monitoring Works

**Automated Evaluation**: The service automatically applies your scorers to production traces based on configured sampling rates.

**Quality Tracking**: Continuous assessment generates quality trends and identifies degradation patterns over time.

**Alert Integration**: Quality thresholds trigger alerts when application performance falls below acceptable levels.

### Configuration and Sampling

**Scorer Configuration**: Same scorers used in development work seamlessly in production monitoring.

**Sampling Strategies**: Configure different sampling rates for different scorers to balance evaluation coverage with computational costs:
- **Critical scorers**: Higher sampling rates (50-100%) for essential quality dimensions
- **Comprehensive scorers**: Moderate rates (10-20%) for detailed assessment
- **Experimental scorers**: Lower rates (1-5%) for testing new evaluation criteria

**Cost Optimization**: Intelligent sampling ensures comprehensive quality coverage while managing evaluation costs effectively.

## Evaluation Datasets: Systematic Test Data Management

**Evaluation Datasets** provide systematic management of test data used for quality assessment, supporting both simple and sophisticated evaluation scenarios.

<Tabs>
<TabItem value="dataset_types" label="Dataset Types and Formats" default>

### Flexible Data Input Options

**Manual Dataset Creation**: Simple arrays or dictionaries for quick evaluation setup
```python
# Simple format for immediate use
eval_data = [
    {
        "inputs": {"query": "What is the capital of France?"},
        "expectations": {"expected_response": "Paris is the capital of France."}
    }
]
```

**Trace-Based Datasets**: Use existing application traces as evaluation data
- **Advantage**: Real execution context preserved
- **Use case**: Converting production traces into test datasets
- **Benefit**: Ensures evaluation reflects actual application behavior

**Managed Evaluation Datasets** (Databricks MLflow Only): Delta table-backed datasets with full lifecycle management
- **Features**: Version control, audit trails, collaborative editing
- **Schema**: Standardized structure with metadata and lineage tracking
- **Governance**: User tracking, timestamps, and change history

> **Note**: MLflow OSS supports manual dataset creation and simple formats, while the managed dataset service with Delta table backing is available only in Databricks MLflow.

</TabItem>
<TabItem value="managed_datasets" label="Managed Dataset Features">

### Enterprise-Grade Dataset Management

**Schema Structure**:
- **dataset_record_id**: Unique identifier for each test case
- **inputs**: JSON-serialized application inputs
- **expectations**: Ground truth labels and expected outcomes
- **metadata**: Creation timestamps, user tracking, source lineage
- **tags**: Flexible key-value annotations for organization

**Collaboration Features**:
- **Multi-user editing**: Teams can collaboratively maintain evaluation datasets
- **Change tracking**: Complete audit trail of dataset modifications
- **Version management**: Track dataset evolution over time
- **Access control**: Manage who can view and modify evaluation data

**Source Lineage**:
- **Trace derivation**: Track when test cases come from production traces
- **Document synthesis**: Link test cases to source documents
- **Human curation**: Record when domain experts create or modify test cases

**Operational Benefits**:
- **Consistency**: Standardized evaluation datasets across teams
- **Reproducibility**: Version-controlled test data ensures consistent evaluation
- **Governance**: Full audit trail for compliance and quality assurance
- **Scalability**: Efficient storage and retrieval for large evaluation datasets

</TabItem>
</Tabs>

## Evaluation UI: Analysis and Insights

The **Evaluation UI** provides comprehensive visualization and analysis capabilities for understanding evaluation results and making data-driven quality improvements.

### Individual Evaluation Analysis

**Result Overview**: Comprehensive dashboard showing aggregate metrics across all test cases and scorers.

**Detailed Inspection**: Click-through capability to examine individual test cases, including:
- Complete trace visualization showing execution flow
- Scorer feedback with rationales and confidence scores
- Input/output examination for quality assessment validation

**Quality Patterns**: Identify systematic issues or successful patterns across evaluation results.

### Comparative Analysis

**Version Comparison**: Side-by-side analysis of different application versions to track quality improvements or identify regressions.

**Metric Trending**: Visualize quality trends over time to understand long-term application performance.

**Regression Detection**: Quickly identify when changes negatively impact quality dimensions.

## Evaluation Runs: Tracking and Reproducibility

**Evaluation Runs** leverage MLflow's tracking capabilities to provide comprehensive experiment management for evaluation activities.

### Run Organization

**MLflow Integration**: Evaluation runs are standard MLflow runs, providing full integration with existing experiment tracking workflows.

**Trace Storage**: All evaluation traces with their assessments are stored within the run for complete reproducibility.

**Version Linking**: Evaluation runs can be linked to specific application versions for tracking quality across development iterations.

### Historical Analysis

**Quality Evolution**: Track evaluation metrics over time to understand application improvement trajectories.

**Dataset Impact**: Analyze how changes in evaluation datasets affect quality assessment results.

**Scorer Performance**: Monitor scorer behavior across different application versions and datasets.

## Integration Patterns and Best Practices

### Development Workflow Integration

**Continuous Evaluation**: Integrate evaluation into CI/CD pipelines for automated quality assessment on every application change.

**Version Comparison**: Use consistent evaluation datasets and scorers across application versions for reliable progress tracking.

**Quality Gates**: Establish minimum quality thresholds that must be met before deploying application changes.

### Production Deployment Strategy

**Gradual Rollout**: Use evaluation results to guide gradual deployment strategies, starting with high-quality application versions.

**Monitoring Alignment**: Ensure production monitoring uses the same scorers validated during development evaluation.

**Feedback Loops**: Use production monitoring results to identify areas for evaluation dataset enhancement and scorer improvement.

## Getting Started Recommendations

### Evaluation Maturity Path

**Phase 1 - Foundation**: Start with predefined scorers and simple evaluation datasets to establish baseline quality measurement.

**Phase 2 - Customization**: Add custom scorers for application-specific quality requirements and expand evaluation datasets.

**Phase 3 - Production**: Deploy production monitoring with appropriate sampling strategies and integrate evaluation into development workflows.

**Phase 4 - Optimization**: Continuously refine evaluation criteria based on real-world performance and stakeholder feedback.

### Platform Selection

**MLflow OSS**: 
- **Ideal for**: Heuristic evaluation, custom scorer development, and cost-conscious quality assessment
- **Capabilities**: Basic evaluation harness, custom scorers, simple datasets, performance metrics
- **Best use cases**: Development workflows, basic quality assessment, custom business logic evaluation

**Databricks MLflow**: 
- **Best for**: Comprehensive LLM-based evaluation, RAG-specific assessment, and enterprise-scale monitoring
- **Additional capabilities**: Pre-built LLM judges, production monitoring service, managed datasets, advanced UI
- **Best use cases**: Production monitoring, semantic quality assessment, enterprise governance

**Hybrid Approach**: Many teams use both platforms, with OSS for development iteration and Databricks for production monitoring and comprehensive evaluation.

## Related Concepts

- **[Evaluation Harness](/genai/eval-monitor/concepts/eval-harness)**: Detailed guide to running evaluations with `mlflow.evaluate()`
- **[Scorers](/genai/eval-monitor/concepts/scorers)**: Deep dive into scorer types and implementation patterns
- **[Predefined LLM Scorers](/genai/eval-monitor/predefined-llm-scorers)**: Ready-to-use evaluation components
- **[Custom Scorers](/genai/eval-monitor/custom-scorers)**: Building application-specific evaluation logic
- **[Production Monitoring](/genai/eval-monitor/production-monitoring)**: Continuous quality assessment in live environments

## Summary

MLflow's evaluation ecosystem provides a comprehensive framework for GenAI quality assessment that scales from initial development through production monitoring. The key to success is understanding how the components work together:

- **Traces and Assessments** provide the foundation for linking quality scores to execution context
- **Evaluation Harness** orchestrates systematic quality assessment across application versions
- **Scorers** implement evaluation logic that works consistently across environments
- **Judges** provide LLM-powered building blocks for sophisticated quality assessment
- **Production Monitoring** extends evaluation capabilities to live applications
- **Evaluation Datasets** ensure systematic, reproducible quality assessment
- **Evaluation UI** enables analysis and comparison for data-driven improvements

By leveraging these components effectively, you can build robust quality assurance processes that improve your GenAI application's reliability and user experience.