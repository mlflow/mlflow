import { APILink } from "@site/src/components/APILink";
import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Bot, GitBranch, Shield } from "lucide-react";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Guardrails AI

[Guardrails AI](https://guardrailsai.com/) is a framework for validating LLM outputs using a community-driven hub of validators for safety, PII detection, content quality, and more. MLflow's Guardrails AI integration allows you to use Guardrails validators as MLflow scorers, providing rule-based evaluation without requiring LLM calls.

## Prerequisites

Guardrails AI scorers require the `guardrails-ai` package:

```bash
pip install guardrails-ai
```

## Quick Start

<TabsWrapper>
<Tabs groupId="invocation-method">
  <TabItem value="direct" label="Invoke directly" default>

```python
from mlflow.genai.scorers.guardrails import ToxicLanguage

scorer = ToxicLanguage(threshold=0.7)
feedback = scorer(
    outputs="This is a professional and helpful response.",
)

print(feedback.value)  # "yes" or "no"
```

  </TabItem>
  <TabItem value="evaluate" label="Invoke with evaluate()">

```python
import mlflow
from mlflow.genai.scorers.guardrails import ToxicLanguage, DetectPII

eval_dataset = [
    {
        "inputs": {"query": "What is MLflow?"},
        "outputs": "MLflow is an open-source platform for managing machine learning workflows.",
    },
    {
        "inputs": {"query": "How do I contact support?"},
        "outputs": "You can reach us at support@example.com or call 555-0123.",
    },
]

results = mlflow.genai.evaluate(
    data=eval_dataset,
    scorers=[
        ToxicLanguage(threshold=0.7),
        DetectPII(),
    ],
)
```

  </TabItem>
</Tabs>
</TabsWrapper>

## Available Guardrails AI Scorers

### Safety and Content Quality

Detect harmful content, PII, and other safety issues in LLM outputs:

| Scorer                                                                                  | What does it evaluate?                                          | Guardrails Hub                                                             |
| --------------------------------------------------------------------------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.guardrails.ToxicLanguage">ToxicLanguage</APILink>     | Does the output contain toxic or offensive language?            | [Link](https://guardrailsai.com/hub/validator/guardrails/toxic_language)   |
| <APILink fn="mlflow.genai.scorers.guardrails.NSFWText">NSFWText</APILink>               | Does the output contain NSFW or explicit content?               | [Link](https://guardrailsai.com/hub/validator/guardrails/nsfw_text)        |
| <APILink fn="mlflow.genai.scorers.guardrails.DetectJailbreak">DetectJailbreak</APILink> | Does the input contain a jailbreak or prompt injection attempt? | [Link](https://guardrailsai.com/hub/validator/guardrails/detect_jailbreak) |
| <APILink fn="mlflow.genai.scorers.guardrails.DetectPII">DetectPII</APILink>             | Does the output contain personally identifiable information?    | [Link](https://guardrailsai.com/hub/validator/guardrails/detect_pii)       |
| <APILink fn="mlflow.genai.scorers.guardrails.SecretsPresent">SecretsPresent</APILink>   | Does the output contain API keys, tokens, or other secrets?     | [Link](https://guardrailsai.com/hub/validator/guardrails/secrets_present)  |
| <APILink fn="mlflow.genai.scorers.guardrails.GibberishText">GibberishText</APILink>     | Does the output contain nonsensical or incoherent text?         | [Link](https://guardrailsai.com/hub/validator/guardrails/gibberish_text)   |

## Creating Scorers by Name

You can also create Guardrails AI scorers dynamically using <APILink fn="mlflow.genai.scorers.guardrails.get_scorer">get_scorer</APILink>:

```python
from mlflow.genai.scorers.guardrails import get_scorer

# Create scorer by name
scorer = get_scorer(
    validator_name="ToxicLanguage",
    threshold=0.7,
)

feedback = scorer(
    outputs="This is a professional response.",
)
```

## Configuration

Guardrails AI scorers accept validator-specific parameters. Any additional keyword arguments are passed directly to the underlying Guardrails validator:

```python
from mlflow.genai.scorers.guardrails import ToxicLanguage, DetectPII, DetectJailbreak

# Toxicity detection with custom threshold
scorer = ToxicLanguage(
    threshold=0.7,  # Confidence threshold for detection
    validation_method="sentence",  # "sentence" or "full" text validation
)

# PII detection with custom entity types
pii_scorer = DetectPII(
    pii_entities=["CREDIT_CARD", "SSN", "EMAIL_ADDRESS"],
)

# Jailbreak detection with custom sensitivity
jailbreak_scorer = DetectJailbreak(
    threshold=0.9,  # Lower values are more sensitive
)
```

Refer to the [Guardrails AI documentation](https://guardrailsai.com/) and the [Guardrails Hub](https://guardrailsai.com/hub) for validator-specific parameters and the full list of available validators.

## Next Steps

<TilesGrid>
  <TileCard
    icon={Bot}
    title="Evaluate Agents"
    description="Learn specialized techniques for evaluating AI agents with tool usage"
    href="/genai/eval-monitor/running-evaluation/agents"
  />
  <TileCard
    icon={GitBranch}
    title="Evaluate Traces"
    description="Evaluate production traces to understand application behavior"
    href="/genai/eval-monitor/running-evaluation/traces"
  />
  <TileCard
    icon={Shield}
    title="Built-in Judges"
    description="Explore MLflow's built-in evaluation judges"
    href="/genai/eval-monitor/scorers/llm-judge/predefined"
  />
</TilesGrid>
