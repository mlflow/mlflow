import { APILink } from "@site/src/components/APILink";
import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Bot, GitBranch, Shield } from "lucide-react";

# DeepEval

[DeepEval](https://docs.confident-ai.com/) is a comprehensive evaluation framework for LLM applications that provides metrics for RAG systems, agents, conversational AI, and safety evaluation. MLflow's DeepEval integration allows you to use most DeepEval metrics as MLflow scorers.

## Prerequisites

DeepEval scorers require the `deepeval` package:

```bash
pip install deepeval
```

## Quick Start

You can call DeepEval scorers directly:

```python
from mlflow.genai.scorers.deepeval import AnswerRelevancy

scorer = AnswerRelevancy(threshold=0.7, model="openai:/gpt-4")
feedback = scorer(
    inputs="What is MLflow?",
    outputs="MLflow is an open-source platform for managing machine learning workflows.",
)

print(feedback.value)  # "yes" or "no"
print(feedback.metadata["score"])  # 0.85
```

Or use them in <APILink fn="mlflow.genai.evaluate">mlflow.genai.evaluate</APILink>:

```python
import mlflow
from mlflow.genai.scorers.deepeval import AnswerRelevancy, Faithfulness

eval_dataset = [
    {
        "inputs": {"query": "What is MLflow?"},
        "outputs": "MLflow is an open-source platform for managing machine learning workflows.",
    },
    {
        "inputs": {"query": "How do I track experiments?"},
        "outputs": "You can use mlflow.start_run() to begin tracking experiments.",
    },
]

results = mlflow.genai.evaluate(
    data=eval_dataset,
    scorers=[
        AnswerRelevancy(threshold=0.7, model="openai:/gpt-4"),
        Faithfulness(threshold=0.8, model="openai:/gpt-4"),
    ],
)
```

## Available DeepEval Scorers

DeepEval scorers are organized into categories based on their evaluation focus:

### RAG (Retrieval-Augmented Generation) Metrics

Evaluate retrieval quality and answer generation in RAG systems:

| Scorer                                                                                        | What does it evaluate?                                     | DeepEval Docs                                                  |
| --------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.deepeval.AnswerRelevancy">AnswerRelevancy</APILink>         | Is the output relevant to the input query?                 | [Link](https://deepeval.com/docs/metrics-answer-relevancy)     |
| <APILink fn="mlflow.genai.scorers.deepeval.Faithfulness">Faithfulness</APILink>               | Is the output factually consistent with retrieval context? | [Link](https://deepeval.com/docs/metrics-faithfulness)         |
| <APILink fn="mlflow.genai.scorers.deepeval.ContextualRecall">ContextualRecall</APILink>       | Does retrieval context contain all necessary information?  | [Link](https://deepeval.com/docs/metrics-contextual-recall)    |
| <APILink fn="mlflow.genai.scorers.deepeval.ContextualPrecision">ContextualPrecision</APILink> | Are relevant nodes ranked higher than irrelevant ones?     | [Link](https://deepeval.com/docs/metrics-contextual-precision) |
| <APILink fn="mlflow.genai.scorers.deepeval.ContextualRelevancy">ContextualRelevancy</APILink> | Is the retrieval context relevant to the query?            | [Link](https://deepeval.com/docs/metrics-contextual-relevancy) |

### Agentic Metrics

Evaluate AI agent performance and behavior:

| Scorer                                                                                        | What does it evaluate?                                  | DeepEval Docs                                                  |
| --------------------------------------------------------------------------------------------- | ------------------------------------------------------- | -------------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.deepeval.TaskCompletion">TaskCompletion</APILink>           | Does the agent successfully complete its assigned task? | [Link](https://deepeval.com/docs/metrics-task-completion)      |
| <APILink fn="mlflow.genai.scorers.deepeval.ToolCorrectness">ToolCorrectness</APILink>         | Does the agent use the correct tools?                   | [Link](https://deepeval.com/docs/metrics-tool-correctness)     |
| <APILink fn="mlflow.genai.scorers.deepeval.ArgumentCorrectness">ArgumentCorrectness</APILink> | Are tool arguments correct?                             | [Link](https://deepeval.com/docs/metrics-argument-correctness) |
| <APILink fn="mlflow.genai.scorers.deepeval.StepEfficiency">StepEfficiency</APILink>           | Does the agent take an optimal path?                    | [Link](https://deepeval.com/docs/metrics-step-efficiency)      |
| <APILink fn="mlflow.genai.scorers.deepeval.PlanAdherence">PlanAdherence</APILink>             | Does the agent follow its plan?                         | [Link](https://deepeval.com/docs/metrics-plan-adherence)       |
| <APILink fn="mlflow.genai.scorers.deepeval.PlanQuality">PlanQuality</APILink>                 | Is the agent's plan well-structured?                    | [Link](https://deepeval.com/docs/metrics-plan-quality)         |

### Conversational Metrics

Evaluate multi-turn conversations and dialogue systems:

| Scorer                                                                                                  | What does it evaluate?                                  | DeepEval Docs                                                       |
| ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.deepeval.TurnRelevancy">TurnRelevancy</APILink>                       | Is each turn relevant to the conversation?              | [Link](https://deepeval.com/docs/metrics-turn-relevancy)            |
| <APILink fn="mlflow.genai.scorers.deepeval.RoleAdherence">RoleAdherence</APILink>                       | Does the assistant maintain its assigned role?          | [Link](https://deepeval.com/docs/metrics-role-adherence)            |
| <APILink fn="mlflow.genai.scorers.deepeval.KnowledgeRetention">KnowledgeRetention</APILink>             | Does the agent retain information across turns?         | [Link](https://deepeval.com/docs/metrics-knowledge-retention)       |
| <APILink fn="mlflow.genai.scorers.deepeval.ConversationCompleteness">ConversationCompleteness</APILink> | Are all user questions addressed?                       | [Link](https://deepeval.com/docs/metrics-conversation-completeness) |
| <APILink fn="mlflow.genai.scorers.deepeval.GoalAccuracy">GoalAccuracy</APILink>                         | Does the conversation achieve its goal?                 | [Link](https://deepeval.com/docs/metrics-goal-accuracy)             |
| <APILink fn="mlflow.genai.scorers.deepeval.ToolUse">ToolUse</APILink>                                   | Does the agent use tools appropriately in conversation? | [Link](https://deepeval.com/docs/metrics-tool-use)                  |
| <APILink fn="mlflow.genai.scorers.deepeval.TopicAdherence">TopicAdherence</APILink>                     | Does the conversation stay on topic?                    | [Link](https://deepeval.com/docs/metrics-topic-adherence)           |

### Safety Metrics

Detect harmful content, bias, and policy violations:

| Scorer                                                                            | What does it evaluate?                                               | DeepEval Docs                                            |
| --------------------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.deepeval.Bias">Bias</APILink>                   | Does the output contain biased content?                              | [Link](https://deepeval.com/docs/metrics-bias)           |
| <APILink fn="mlflow.genai.scorers.deepeval.Toxicity">Toxicity</APILink>           | Does the output contain toxic language?                              | [Link](https://deepeval.com/docs/metrics-toxicity)       |
| <APILink fn="mlflow.genai.scorers.deepeval.NonAdvice">NonAdvice</APILink>         | Does the model inappropriately provide advice in restricted domains? | [Link](https://deepeval.com/docs/metrics-non-advice)     |
| <APILink fn="mlflow.genai.scorers.deepeval.Misuse">Misuse</APILink>               | Could the output be used for harmful purposes?                       | [Link](https://deepeval.com/docs/metrics-misuse)         |
| <APILink fn="mlflow.genai.scorers.deepeval.PIILeakage">PIILeakage</APILink>       | Does the output leak personally identifiable information?            | [Link](https://deepeval.com/docs/metrics-pii-leakage)    |
| <APILink fn="mlflow.genai.scorers.deepeval.RoleViolation">RoleViolation</APILink> | Does the assistant break out of its assigned role?                   | [Link](https://deepeval.com/docs/metrics-role-violation) |

### Other

Additional evaluation metrics for common use cases:

| Scorer                                                                                | What does it evaluate?                                 | DeepEval Docs                                              |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------ | ---------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.deepeval.Hallucination">Hallucination</APILink>     | Does the LLM fabricate information not in the context? | [Link](https://deepeval.com/docs/metrics-hallucination)    |
| <APILink fn="mlflow.genai.scorers.deepeval.Summarization">Summarization</APILink>     | Is the summary accurate and complete?                  | [Link](https://deepeval.com/docs/metrics-summarization)    |
| <APILink fn="mlflow.genai.scorers.deepeval.JsonCorrectness">JsonCorrectness</APILink> | Does JSON output match the expected schema?            | [Link](https://deepeval.com/docs/metrics-json-correctness) |
| <APILink fn="mlflow.genai.scorers.deepeval.PromptAlignment">PromptAlignment</APILink> | Does the output align with prompt instructions?        | [Link](https://deepeval.com/docs/metrics-prompt-alignment) |

### Non-LLM

Fast, rule-based metrics that don't require LLM calls:

| Scorer                                                                          | What does it evaluate?                     | DeepEval Docs                                           |
| ------------------------------------------------------------------------------- | ------------------------------------------ | ------------------------------------------------------- |
| <APILink fn="mlflow.genai.scorers.deepeval.ExactMatch">ExactMatch</APILink>     | Does output exactly match expected output? | [Link](https://deepeval.com/docs/metrics-exact-match)   |
| <APILink fn="mlflow.genai.scorers.deepeval.PatternMatch">PatternMatch</APILink> | Does output match a regex pattern?         | [Link](https://deepeval.com/docs/metrics-pattern-match) |

## Creating Scorers by Name

You can also create DeepEval scorers dynamically using <APILink fn="mlflow.genai.scorers.deepeval.get_scorer">get_scorer</APILink>:

```python
from mlflow.genai.scorers.deepeval import get_scorer

# Create scorer by name
scorer = get_scorer(
    metric_name="AnswerRelevancy",
    threshold=0.7,
    model="openai:/gpt-4",
)

feedback = scorer(
    inputs="What is MLflow?",
    outputs="MLflow is a platform for ML workflows.",
)
```

## Configuration

DeepEval scorers accept all parameters supported by the underlying DeepEval metrics. Any additional keyword arguments are passed directly to the DeepEval metric constructor:

```python
from mlflow.genai.scorers.deepeval import AnswerRelevancy, TurnRelevancy

# Common parameters
scorer = AnswerRelevancy(
    model="openai:/gpt-4",  # Model URI (also supports "databricks", "databricks:/endpoint", etc.)
    threshold=0.7,  # Pass/fail threshold (0.0-1.0, scorer passes if score >= threshold)
    include_reason=True,  # Include detailed rationale in feedback
)

# Metric-specific parameters are passed through to DeepEval
conversational_scorer = TurnRelevancy(
    model="openai:/gpt-4o",
    threshold=0.8,
    window_size=3,  # DeepEval-specific: number of conversation turns to consider
    strict_mode=True,  # DeepEval-specific: enforce stricter evaluation criteria
)
```

Refer to the [DeepEval documentation](https://docs.confident-ai.com/) for metric-specific parameters.

## Next Steps

<TilesGrid>
  <TileCard
    icon={Bot}
    title="Evaluate Agents"
    description="Learn specialized techniques for evaluating AI agents with tool usage"
    href="/genai/eval-monitor/running-evaluation/agents"
  />
  <TileCard
    icon={GitBranch}
    title="Evaluate Traces"
    description="Evaluate production traces to understand application behavior"
    href="/genai/eval-monitor/running-evaluation/traces"
  />
  <TileCard
    icon={Shield}
    title="Predefined Scorers"
    description="Explore MLflow's built-in evaluation scorers"
    href="/genai/eval-monitor/scorers/llm-judge/predefined"
  />
</TilesGrid>
