import { APILink } from "@site/src/components/APILink";
import ImageBox from "@site/src/components/ImageBox";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Bot, GitBranch, Shield } from "lucide-react";

# Third-party Scorers

MLflow integrates with popular third-party evaluation frameworks, allowing you to leverage their specialized metrics within MLflow's evaluation workflow. This provides access to battle-tested evaluation metrics while maintaining a consistent MLflow interface.

## DeepEval Integration

[DeepEval](https://docs.confident-ai.com/) is a comprehensive evaluation framework for LLM applications that provides metrics for RAG systems, agents, conversational AI, and safety evaluation. MLflow's DeepEval integration allows you to use nearly any DeepEval metric as an MLflow scorer.

### Prerequisites

DeepEval scorers require the `deepeval` package:

```bash
pip install deepeval
```

### Quick Start

You can use DeepEval scorers ad-hoc or in batch evaluations.

**Ad-hoc Usage**

Call scorers directly on individual inputs:

```python
from mlflow.genai.scorers.deepeval import AnswerRelevancy

scorer = AnswerRelevancy(threshold=0.7, model="openai:/gpt-4")
feedback = scorer(
    inputs="What is MLflow?",
    outputs="MLflow is an open-source platform for managing machine learning workflows.",
)

print(feedback.value)  # "yes" or "no"
print(feedback.metadata["score"])  # 0.85
```

**Batch Evaluation**

Use DeepEval scorers with <APILink fn="mlflow.genai.evaluate">evaluate</APILink> for batch processing:

```python
import mlflow
from mlflow.genai.scorers.deepeval import AnswerRelevancy, Faithfulness

eval_dataset = [
    {
        "inputs": {"query": "What is MLflow?"},
        "outputs": "MLflow is an open-source platform for managing machine learning workflows.",
    },
    {
        "inputs": {"query": "How do I track experiments?"},
        "outputs": "You can use mlflow.start_run() to begin tracking experiments.",
    },
]

results = mlflow.genai.evaluate(
    data=eval_dataset,
    scorers=[
        AnswerRelevancy(threshold=0.7, model="openai:/gpt-4"),
        Faithfulness(threshold=0.8, model="openai:/gpt-4"),
    ],
)
```

### Available DeepEval Scorers

DeepEval scorers are organized into categories based on their evaluation focus:

#### RAG (Retrieval-Augmented Generation) Metrics

Evaluate retrieval quality and answer generation in RAG systems:

| Scorer | What does it evaluate? | Requires trace? |
|--------|------------------------|-----------------|
| <APILink fn="mlflow.genai.scorers.deepeval.AnswerRelevancy">AnswerRelevancy</APILink> | Is the output relevant to the input query? ([DeepEval docs](https://deepeval.com/docs/metrics-answer-relevancy)) | No |
| <APILink fn="mlflow.genai.scorers.deepeval.Faithfulness">Faithfulness</APILink> | Is the output factually consistent with retrieval context? ([DeepEval docs](https://deepeval.com/docs/metrics-faithfulness)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ContextualRecall">ContextualRecall</APILink> | Does retrieval context contain all necessary information? ([DeepEval docs](https://deepeval.com/docs/metrics-contextual-recall)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ContextualPrecision">ContextualPrecision</APILink> | Are relevant nodes ranked higher than irrelevant ones? ([DeepEval docs](https://deepeval.com/docs/metrics-contextual-precision)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ContextualRelevancy">ContextualRelevancy</APILink> | Is the retrieval context relevant to the query? ([DeepEval docs](https://deepeval.com/docs/metrics-contextual-relevancy)) | Yes |

#### Agentic Metrics

Evaluate AI agent performance and behavior:

| Scorer | What does it evaluate? | Requires trace? |
|--------|------------------------|-----------------|
| <APILink fn="mlflow.genai.scorers.deepeval.TaskCompletion">TaskCompletion</APILink> | Does the agent successfully complete its assigned task? ([DeepEval docs](https://deepeval.com/docs/metrics-task-completion)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ToolCorrectness">ToolCorrectness</APILink> | Does the agent use the correct tools? ([DeepEval docs](https://deepeval.com/docs/metrics-tool-correctness)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ArgumentCorrectness">ArgumentCorrectness</APILink> | Are tool arguments correct? ([DeepEval docs](https://deepeval.com/docs/metrics-argument-correctness)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.StepEfficiency">StepEfficiency</APILink> | Does the agent take an optimal path? ([DeepEval docs](https://deepeval.com/docs/metrics-step-efficiency)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.PlanAdherence">PlanAdherence</APILink> | Does the agent follow its plan? ([DeepEval docs](https://deepeval.com/docs/metrics-plan-adherence)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.PlanQuality">PlanQuality</APILink> | Is the agent's plan well-structured? ([DeepEval docs](https://deepeval.com/docs/metrics-plan-quality)) | No |

#### Conversational Metrics

Evaluate multi-turn conversations and dialogue systems:

| Scorer | What does it evaluate? | Requires session? |
|--------|------------------------|-------------------|
| <APILink fn="mlflow.genai.scorers.deepeval.TurnRelevancy">TurnRelevancy</APILink> | Is each turn relevant to the conversation? ([DeepEval docs](https://deepeval.com/docs/metrics-turn-relevancy)) | No |
| <APILink fn="mlflow.genai.scorers.deepeval.RoleAdherence">RoleAdherence</APILink> | Does the assistant maintain its assigned role? ([DeepEval docs](https://deepeval.com/docs/metrics-role-adherence)) | No |
| <APILink fn="mlflow.genai.scorers.deepeval.KnowledgeRetention">KnowledgeRetention</APILink> | Does the agent retain information across turns? ([DeepEval docs](https://deepeval.com/docs/metrics-knowledge-retention)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ConversationCompleteness">ConversationCompleteness</APILink> | Are all user questions addressed? ([DeepEval docs](https://deepeval.com/docs/metrics-conversation-completeness)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.GoalAccuracy">GoalAccuracy</APILink> | Does the conversation achieve its goal? ([DeepEval docs](https://deepeval.com/docs/metrics-goal-accuracy)) | Yes |
| <APILink fn="mlflow.genai.scorers.deepeval.ToolUse">ToolUse</APILink> | Does the agent use tools appropriately in conversation? ([DeepEval docs](https://deepeval.com/docs/metrics-tool-use)) | No |
| <APILink fn="mlflow.genai.scorers.deepeval.TopicAdherence">TopicAdherence</APILink> | Does the conversation stay on topic? ([DeepEval docs](https://deepeval.com/docs/metrics-topic-adherence)) | No |

:::info Multi-Turn Evaluation
Some conversational metrics (marked with "Yes" in Requires session?) evaluate entire conversation sessions rather than individual turns. These require traces with `mlflow.trace.session` metadata. See the [Evaluate Conversations](/genai/eval-monitor/running-evaluation/multi-turn) guide for details.
:::

#### Safety Metrics

Detect harmful content, bias, and policy violations:

| Scorer | What does it evaluate? |
|--------|------------------------|
| <APILink fn="mlflow.genai.scorers.deepeval.Bias">Bias</APILink> | Does the output contain biased content? ([DeepEval docs](https://deepeval.com/docs/metrics-bias)) |
| <APILink fn="mlflow.genai.scorers.deepeval.Toxicity">Toxicity</APILink> | Does the output contain toxic language? ([DeepEval docs](https://deepeval.com/docs/metrics-toxicity)) |
| <APILink fn="mlflow.genai.scorers.deepeval.NonAdvice">NonAdvice</APILink> | Does the model inappropriately provide advice in restricted domains? ([DeepEval docs](https://deepeval.com/docs/metrics-non-advice)) |
| <APILink fn="mlflow.genai.scorers.deepeval.Misuse">Misuse</APILink> | Could the output be used for harmful purposes? ([DeepEval docs](https://deepeval.com/docs/metrics-misuse)) |
| <APILink fn="mlflow.genai.scorers.deepeval.PIILeakage">PIILeakage</APILink> | Does the output leak personally identifiable information? ([DeepEval docs](https://deepeval.com/docs/metrics-pii-leakage)) |
| <APILink fn="mlflow.genai.scorers.deepeval.RoleViolation">RoleViolation</APILink> | Does the assistant break out of its assigned role? ([DeepEval docs](https://deepeval.com/docs/metrics-role-violation)) |

#### General Metrics

Additional evaluation metrics for common use cases:

| Scorer | What does it evaluate? |
|--------|------------------------|
| <APILink fn="mlflow.genai.scorers.deepeval.Hallucination">Hallucination</APILink> | Does the LLM fabricate information not in the context? ([DeepEval docs](https://deepeval.com/docs/metrics-hallucination)) |
| <APILink fn="mlflow.genai.scorers.deepeval.Summarization">Summarization</APILink> | Is the summary accurate and complete? ([DeepEval docs](https://deepeval.com/docs/metrics-summarization)) |
| <APILink fn="mlflow.genai.scorers.deepeval.JsonCorrectness">JsonCorrectness</APILink> | Does JSON output match the expected schema? ([DeepEval docs](https://deepeval.com/docs/metrics-json-correctness)) |
| <APILink fn="mlflow.genai.scorers.deepeval.PromptAlignment">PromptAlignment</APILink> | Does the output align with prompt instructions? ([DeepEval docs](https://deepeval.com/docs/metrics-prompt-alignment)) |

#### Deterministic Metrics

Fast, rule-based metrics that don't require LLM calls:

| Scorer | What does it evaluate? |
|--------|------------------------|
| <APILink fn="mlflow.genai.scorers.deepeval.ExactMatch">ExactMatch</APILink> | Does output exactly match expected output? ([DeepEval docs](https://deepeval.com/docs/metrics-exact-match)) |
| <APILink fn="mlflow.genai.scorers.deepeval.PatternMatch">PatternMatch</APILink> | Does output match a regex pattern? ([DeepEval docs](https://deepeval.com/docs/metrics-pattern-match)) |

### Configuration

DeepEval scorers support flexible configuration options:

```python
from mlflow.genai.scorers.deepeval import AnswerRelevancy

scorer = AnswerRelevancy(
    model="openai:/gpt-4",  # Model URI (also supports "databricks", "databricks:/endpoint", etc.)
    threshold=0.7,  # Pass/fail threshold (0.0-1.0, scorer passes if score >= threshold)
    include_reason=True,  # Include detailed rationale in feedback
)
```


### Dynamic Scorer Creation with get_judge()

The <APILink fn="mlflow.genai.scorers.deepeval.get_judge">get_judge</APILink> function allows you to create DeepEval scorers dynamically by name:

```python
from mlflow.genai.scorers.deepeval import get_judge

# Create scorer by name
scorer = get_judge(
    metric_name="AnswerRelevancy",
    threshold=0.7,
    model="openai:/gpt-4",
)

feedback = scorer(
    inputs="What is MLflow?",
    outputs="MLflow is a platform for ML workflows.",
)
```

## Next Steps

<TilesGrid>
  <TileCard
    icon={Bot}
    title="Evaluate Agents"
    description="Learn specialized techniques for evaluating AI agents with tool usage"
    href="/genai/eval-monitor/running-evaluation/agents"
  />
  <TileCard
    icon={GitBranch}
    title="Evaluate Traces"
    description="Evaluate production traces to understand application behavior"
    href="/genai/eval-monitor/running-evaluation/traces"
  />
  <TileCard
    icon={Shield}
    title="Predefined Scorers"
    description="Explore MLflow's built-in evaluation scorers"
    href="/genai/eval-monitor/scorers/llm-judge/predefined"
  />
</TilesGrid>
