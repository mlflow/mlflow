import { APILink } from "@site/src/components/APILink";
import ImageBox from "@site/src/components/ImageBox";
import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Bot, BookOpen, MessageSquare } from "lucide-react";

# Bring Your Own Prompts

:::info[Consider using make_judge API]

The new <APILink fn="mlflow.genai.judges.make_judge">make_judge</APILink> API (MLflow 3.4.0+) is the recommended approach for most use cases. It provides:

- **Cleaner data organization** with reserved variables (`inputs`, `outputs`, `expectations`, `trace`)
- **Version control** and team collaboration features
- **Judge alignment** capabilities with human feedback
- **Simpler API** for common evaluation patterns

Use `custom_prompt_judge` when you need:

- Complete control over prompt structure
- Custom variable names beyond the reserved set
- Complex multi-choice outputs

[Learn about make_judge â†’](/genai/eval-monitor/scorers/llm-judge/make-judge)

:::

The <APILink fn="mlflow.genai.judges.custom_prompt_judge">custom_prompt_judge</APILink> API is designed to help you quickly and easily create LLM scorers when you need full control over the judge's prompt or need to return multiple output values beyond "pass" / "fail", for example, "great", "ok", "bad".

You provide a prompt template that has placeholders for specific fields in your app's trace and define the output choices the judge can select. The LLM judge model uses these inputs to select the best output choice and provides a rationale for its selection.

:::tip

We recommend starting with <ins>[guidelines-based judges](/genai/eval-monitor/scorers/llm-judge/guidelines)</ins> and only using prompt-based judges if you need more control or can't write your evaluation criteria as pass/fail guidelines. Guidelines-based judges have the distinct advantage of being easy to explain to business stakeholders and can often be directly written by domain experts.

:::

## Example Usage

```python
from mlflow.genai.judges import custom_prompt_judge
from mlflow.genai.scorers import scorer


issue_resolution_prompt = """
Evaluate the entire conversation between a customer and an LLM-based agent. Determine if the issue was resolved in the conversation.

You must choose one of the following categories.

[[fully_resolved]]: The response directly and comprehensively addresses the user's question or problem, providing a clear solution or answer. No further immediate action seems required from the user on the same core issue.
[[partially_resolved]]: The response offers some help or relevant information but doesn't completely solve the problem or answer the question. It might provide initial steps, require more information from the user, or address only a part of a multi-faceted query.
[[needs_follow_up]]: The response does not adequately address the user's query, misunderstands the core issue, provides unhelpful or incorrect information, or inappropriately deflects the question. The user will likely need to re-engage or seek further assistance.

Conversation to evaluate: {{conversation}}
"""


# Define a custom scorer that wraps the custom prompt judge to check if the issue was resolved
@scorer
def is_issue_resolved(inputs, outputs):
    issue_judge = custom_prompt_judge(
        name="issue_resolution",
        prompt_template=issue_resolution_prompt,
        # Optionally map the categories to numeric values for ease
        # of aggregation and comparison. When not provided, the judge
        # directly returns the choice value as a string.
        numeric_values={
            "fully_resolved": 1,
            "partially_resolved": 0.5,
            "needs_follow_up": 0,
        },
    )

    # Pass values for the placeholders ({{conversation}}) as kwargs
    conversation = inputs["messages"] + outputs["messages"]
    return issue_judge(conversation=conversation)
```

## Prompt Requirements

The prompt template for `custom_prompt_judge` must have:

- **Template placeholders** with double curly braces, e.g., `{{conversation}}`. These placeholders are replaced with your data at evaluation time.
- **Output choices** enclosed in square brackets, e.g., `[[fully_resolved]]`. The choice name can contain alphanumeric characters and underscores.
- **Flexible variable names**: Unlike `make_judge`, you can use any variable names in your template, not just reserved ones.

:::tip Handling Parsing Errors

MLflow uses raw prompt-based instructions for handling structured outputs to make the API generic to all LLM providers. This may not be strict enough to enforce structured outputs in all cases. If you see output parsing errors frequently, consider using <ins>[code-based custom scorers](/genai/eval-monitor/scorers/custom)</ins> and invoke the specific structured output API for the LLM provider you are using to get more reliable results.

:::

## Maintaining Your Prompt

Writing good prompts for LLM judges requires iterative testing and refinement. [MLflow Prompt Registry](/genai/prompt-registry) is a great tool to help you manage and version control your prompts and share them with your team.

```python
from mlflow.genai import register_prompt

register_prompt(
    name="issue_resolution",
    template=issue_resolution_prompt,
)
```

<ImageBox src="/images/mlflow-3/eval-monitor/scorers/prompt-registry.png" alt="Prompt Registry" />

## Selecting Judge Models

MLflow supports all major LLM providers, such as OpenAI, Anthropic, Google, xAI, and more. See [Supported Models](/genai/eval-monitor/scorers/llm-judge#supported-models) for more details.

```python
from mlflow.genai.judges import custom_prompt_judge

custom_prompt_judge(
    name="is_issue_resolved",
    prompt_template=issue_resolution_prompt,
    model="anthropic:/claude-3-opus",
)
```

## Next Steps

<TilesGrid>
  <TileCard
    icon={Bot}
    title="Evaluate Agents"
    description="Learn how to evaluate AI agents with specialized techniques and scorers"
    href="/genai/eval-monitor/running-evaluation/agents"
  />
  <TileCard
    icon={BookOpen}
    title="Prompt Registry"
    description="Version control and manage your judge prompts with MLflow Prompt Registry"
    href="/genai/prompt-registry"
  />
  <TileCard
    icon={MessageSquare}
    title="Collect User Feedback"
    description="Integrate user feedback to continuously improve your evaluation criteria and model performance"
    href="/genai/assessments/feedback/"
  />
</TilesGrid>
