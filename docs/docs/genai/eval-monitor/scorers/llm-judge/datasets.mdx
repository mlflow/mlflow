---
title: Judge Dataset Integration
sidebar_label: Dataset Integration
description: Using evaluation datasets with custom LLM judges for systematic testing and improvement
tag: Advanced
keywords: [mlflow genai, mlflow judges, custom scorers, evaluation datasets, model evaluation, llm judge accuracy, alignment]
---

import FeatureHighlights from "@site/src/components/FeatureHighlights";
import DAGLoop from "@site/src/components/DAGLoop";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { ChartBar, Database, Target, RefreshCw, GitBranch, Activity, Users, Brain, FileText } from "lucide-react";

# Judge Dataset Integration

Evaluation datasets enable systematic testing and improvement of your custom LLM judges. By building datasets from traces and adding ground truth labels, you can measure judge accuracy and identify areas for improvement.

<DAGLoop
  steps={[
    {
      icon: Database,
      title: "Build Dataset",
      description: "Collect traces & add labels",
      detailedDescription: "Build evaluation datasets from traces and add ground truth expectations for judge validation.",
      isFocus: true
    },
    {
      icon: Target,
      title: "Create & Run Judges",
      description: "Evaluate with judges",
      detailedDescription: "Design domain-specific judges and run them against the dataset to generate assessments.",
      isFocus: true
    },
    {
      icon: ChartBar,
      title: "Analyze Accuracy",
      description: "Compare with expectations",
      detailedDescription: "Measure judge accuracy by comparing assessments against ground truth expectations in the dataset."
    },
    {
      icon: Users,
      title: "Collect Feedback",
      description: "Gather human assessments",
      detailedDescription: "Collect expert feedback on judge outputs to identify areas for improvement."
    },
    {
      icon: RefreshCw,
      title: "Align & Improve",
      description: "Refine judge accuracy",
      detailedDescription: "Use SIMBA optimizer to align judges with human feedback, improving accuracy."
    }
  ]}
  loopBackIcon={Activity}
  loopBackText="Continuous Refinement"
  loopBackDescription="Iterate to maintain and improve judge accuracy as your application evolves"
  circleSize={500}
/>

## Why Integrate Judges with Datasets?

<FeatureHighlights features={[
  {
    icon: Database,
    title: "Consistent Test Data",
    description: "Evaluation datasets provide reproducible test cases, ensuring consistent judge performance measurement across iterations."
  },
  {
    icon: Target,
    title: "Ground Truth Comparison",
    description: "Expectations in datasets serve as ground truth, enabling automatic accuracy measurement of judge evaluations."
  },
  {
    icon: RefreshCw,
    title: "Systematic Improvement",
    description: "Track judge performance over time, identify weaknesses, and systematically improve through alignment."
  },
  {
    icon: GitBranch,
    title: "Version Control",
    description: "Track datasets for complete evaluation reproducibility."
  }
]} />

:::important SQL Backend Required
Evaluation datasets require a SQL-based tracking backend. Set it up before using datasets:

```python
mlflow.set_tracking_uri("sqlite:///mlflow.db")  # Or PostgreSQL/MySQL
```

:::

## Complete Example: Build → Evaluate → Improve

Here's how to build a dataset from traces, evaluate your judge, and improve accuracy:

```python
import mlflow
from mlflow.genai.judges import make_judge
from mlflow.genai.datasets import create_dataset

# Set up MLflow with SQL backend (required for datasets)
mlflow.set_tracking_uri("sqlite:///mlflow.db")

# Step 1: Build dataset from traces
dataset = create_dataset(
    name="judge_accuracy_test",
    experiment_id="0",
    tags={"purpose": "judge_validation", "version": "1.0"},
)

# Get traces from an experiment and add to dataset
traces_df = mlflow.search_traces(
    experiment_ids=["0"], max_results=100
)  # Returns DataFrame with trace data

# Add traces directly - MLflow extracts inputs automatically
dataset.merge_records(traces_df)
print(f"Added {len(traces_df)} records to evaluation dataset")

# Step 2: Add ground truth expectations for accuracy measurement
edge_cases = [
    {
        "inputs": {"question": ""},  # Empty input
        "expectations": {"quality": "poor", "reason": "empty_input"},
    },
    {
        "inputs": {"question": "How do I reset my password?"},
        "expectations": {"quality": "good", "helpful": "yes"},
    },
    {
        "inputs": {"question": "URGENT!!! HELP!!!"},
        "expectations": {"quality": "poor", "reason": "no_clear_question"},
    },
]
dataset.merge_records(edge_cases)

# Step 3: Create judge and evaluate
quality_judge = make_judge(
    name="answer_quality",
    instructions=(
        "Evaluate if {{ outputs }} properly addresses {{ inputs }}. "
        "Rate as 'good', 'fair', or 'poor'."
    ),
    model="anthropic:/claude-opus-4-1-20250805",
)


def my_app(question):
    # Your application logic
    return {"answer": f"Response to: {question}"}


# Evaluate judge performance
result = mlflow.genai.evaluate(data=dataset, scorers=[quality_judge], predict_fn=my_app)

# Step 4: Iterate and improve
# - Review results in MLflow UI
# - Add more test cases based on errors
# - Collect human feedback on judge outputs
# - Use alignment to improve judge accuracy
```

## Key Integration Points

### Dataset Operations

```python
# Create dataset
dataset = create_dataset(name="my_dataset", experiment_id="0")

# Add traces
traces_df = mlflow.search_traces(experiment_ids=["0"])
dataset.merge_records(traces_df)

# Add manual test cases
test_cases = [
    {"inputs": {...}, "expectations": {...}},
    {"inputs": {...}, "expectations": {...}},
]
dataset.merge_records(test_cases)

# Access dataset records
df = dataset.to_df()
```

### Evaluation with Datasets

```python
# Datasets work seamlessly with mlflow.genai.evaluate
result = mlflow.genai.evaluate(
    data=dataset,  # Pass dataset directly
    scorers=[judge1, judge2],
    predict_fn=my_app,  # Generate outputs at evaluation time
)
```

## Best Practices

1. **Start with Traces**: Bootstrap datasets using traces from development or QA testing
2. **Add Edge Cases**: Include problematic inputs to test judge robustness
3. **Label Strategically**: Focus ground truth labels on critical or ambiguous cases
4. **Iterate Regularly**: Continuously expand datasets as your application evolves
5. **Track Metrics**: Log judge accuracy metrics to monitor improvement over time

## Learn More

<TilesGrid>
  <TileCard
    icon={RefreshCw}
    iconSize={48}
    title="Judge Alignment"
    description="Learn how to align judges with human feedback for improved accuracy."
    link="/genai/eval-monitor/scorers/llm-judge/alignment"
    linkText="Explore alignment →"
    containerHeight={64}
  />
  <TileCard
    icon={FileText}
    iconSize={48}
    title="Workflow Examples"
    description="See complete production patterns for judge development and deployment."
    link="/genai/eval-monitor/scorers/llm-judge/workflow"
    linkText="View workflows →"
    containerHeight={64}
  />
  <TileCard
    icon={Brain}
    iconSize={48}
    title="Custom LLM Judges"
    description="Return to the overview to explore more judge features and capabilities."
    link="/genai/eval-monitor/scorers/llm-judge/"
    linkText="Back to overview →"
    containerHeight={64}
  />
</TilesGrid>
