import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Database, BookOpen, Target } from "lucide-react";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from '@site/src/components/TabsWrapper';

# RetrievalGroundedness judge

The `RetrievalGroundedness` judge assesses whether your application's response is factually supported by the provided context (either from a RAG system or generated by a tool call), helping detect hallucinations or statements not backed by that context.

This built-in LLM judge is designed for evaluating RAG applications that need to ensure responses are grounded in retrieved information.

## Prerequisites for running the examples

1. Install MLflow and required packages

   ```bash
   pip install --upgrade "mlflow>=3.4.0"
   ```

2. Create an MLflow experiment by following the [setup your environment quickstart](/genai/getting-started/connect-environment/).

## Usage examples

The RetrievalGroundedness judge can be invoked directly for single trace assessment or used with MLflow's evaluation framework for batch evaluation.

**Requirements:**

Trace requirements:

- The MLflow Trace must contain at least one span with `span_type` set to `RETRIEVER`
- `inputs` and `outputs` must be on the Trace's root span

<TabsWrapper>
<Tabs groupId="invocation-method">
  <TabItem value="direct" label="Invoke directly" default>

```python
from mlflow.genai.scorers import retrieval_groundedness
import mlflow

# Get a trace from a previous run
trace = mlflow.get_trace("<your-trace-id>")

# Assess if the response is grounded in the retrieved context
feedback = retrieval_groundedness(trace=trace)
print(feedback)
```

  </TabItem>
  <TabItem value="evaluate" label="Invoke with evaluate()">

```python
import mlflow
from mlflow.genai.scorers import RetrievalGroundedness

# Evaluate traces from previous runs
results = mlflow.genai.evaluate(
    data=traces,  # DataFrame or list containing trace data
    scorers=[RetrievalGroundedness()],
)
```

  </TabItem>
</Tabs>
</TabsWrapper>

## RAG example

Here's a complete example showing how to create a RAG application and evaluate if responses are grounded in retrieved context:

1. Initialize an OpenAI client to connect to OpenAI-hosted LLMs. Use the native OpenAI SDK to connect to OpenAI-hosted models. Select a model from the <ins>[available OpenAI models](https://platform.openai.com/docs/models)</ins>.

   ```python
   import mlflow
   import os
   import openai

   # Ensure your OPENAI_API_KEY is set in your environment
   # os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>" # Uncomment and set if not globally configured

   # Enable auto-tracing for OpenAI
   mlflow.openai.autolog()

   # Create an OpenAI client connected to OpenAI SDKs
   client = openai.OpenAI()

   # Select an LLM
   model_name = "gpt-4o-mini"
   ```

2. Define and evaluate your RAG application:

   ```python
   from mlflow.genai.scorers import RetrievalGroundedness
   from mlflow.entities import Document
   from typing import List


   # Define a retriever function with proper span type
   @mlflow.trace(span_type="RETRIEVER")
   def retrieve_docs(query: str) -> List[Document]:
       # Simulated retrieval based on query

       if "mlflow" in query.lower():
           return [
               Document(
                   id="doc_1",
                   page_content="MLflow is an open-source platform for managing the ML lifecycle.",
                   metadata={"source": "mlflow_docs.txt"},
               ),
               Document(
                   id="doc_2",
                   page_content="MLflow provides tools for experiment tracking, model packaging, and deployment.",
                   metadata={"source": "mlflow_features.txt"},
               ),
           ]
       else:
           return [
               Document(
                   id="doc_3",
                   page_content="Machine learning involves training models on data.",
                   metadata={"source": "ml_basics.txt"},
               )
           ]


   # Define your RAG app
   @mlflow.trace
   def rag_app(query: str):
       # Retrieve relevant documents
       docs = retrieve_docs(query)
       context = "\n".join([doc.page_content for doc in docs])

       # Generate response using LLM
       messages = [
           {"role": "system", "content": f"Answer based on this context: {context}"},
           {"role": "user", "content": query},
       ]

       response = client.chat.completions.create(model=model_name, messages=messages)

       return {"response": response.choices[0].message.content}


   # Create evaluation dataset
   eval_dataset = [
       {"inputs": {"query": "What is MLflow used for?"}},
       {"inputs": {"query": "What are the main features of MLflow?"}},
   ]

   # Run evaluation with RetrievalGroundedness judge
   eval_results = mlflow.genai.evaluate(
       data=eval_dataset,
       predict_fn=rag_app,
       scorers=[
           RetrievalGroundedness(
               model="openai:/gpt-4o-mini",  # Optional
           )
       ],
   )
   ```

## Select the LLM that powers the judge

You can change the judge model by using the `model` argument in the judge definition. The model must be specified in the format `<provider>:/<model-name>`, where `<provider>` is a LiteLLM-compatible model provider.

You can customize the judge by providing a different judge model:

```python
from mlflow.genai.scorers import RetrievalGroundedness

# Use a different judge model
groundedness_judge = RetrievalGroundedness(
    model="openai:/gpt-4o-mini"  # Or any LiteLLM-compatible model
)

# Use in evaluation
eval_results = mlflow.genai.evaluate(
    data=eval_dataset, predict_fn=rag_app, scorers=[groundedness_judge]
)
```

For a list of supported models, see [selecting judge models](/genai/eval-monitor/scorers/llm-judge/#selecting-judge-models).

## Interpret results

The judge returns a Feedback object with:

- **value**: "yes" if response is grounded, "no" if it contains hallucinations
- **rationale**: Detailed explanation identifying:
  - Which statements are supported by context
  - Which statements lack support (hallucinations)
  - Specific quotes from context that support or contradict claims

## Next steps

<TilesGrid>
  <TileCard
    icon={BookOpen}
    title="Evaluate context sufficiency"
    description="Check if your retriever provides adequate information"
    href="/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency"
  />
  <TileCard
    icon={Database}
    title="Evaluate context relevance"
    description="Ensure retrieved documents are relevant to queries"
    href="/genai/eval-monitor/scorers/llm-judge/rag/relevance/#retrievalrelevance-judge"
  />
  <TileCard
    icon={Target}
    title="Run comprehensive RAG evaluation"
    description="Combine multiple judges for complete RAG assessment"
    href="/genai/eval-monitor/quickstart/"
  />
</TilesGrid>
