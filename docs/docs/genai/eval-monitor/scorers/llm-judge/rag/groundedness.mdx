import TileCard from '@site/src/components/TileCard';
import { APILink } from "@site/src/components/APILink";
import TilesGrid from '@site/src/components/TilesGrid';
import { Database, BookOpen, Target } from "lucide-react";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from '@site/src/components/TabsWrapper';
import JudgePrerequisites from '@site/src/content/judge_prerequisites.mdx';
import SelectJudgeModel from '@site/src/content/select_judge_model.mdx';

# RetrievalGroundedness judge

The `RetrievalGroundedness` judge assesses whether your application's response is factually supported by the provided context (either from a RAG system or generated by a tool call), helping detect hallucinations or statements not backed by that context.

This built-in LLM judge is designed for evaluating RAG applications that need to ensure responses are grounded in retrieved information.

<JudgePrerequisites />

## Usage examples

The RetrievalGroundedness judge can be invoked directly for single trace assessment or used with MLflow's evaluation framework for batch evaluation.

**Requirements:**

Trace requirements:

- The MLflow Trace must contain at least one span with `span_type` set to `RETRIEVER`
- `inputs` and `outputs` must be on the Trace's root span

<TabsWrapper>
<Tabs groupId="invocation-method">
  <TabItem value="direct" label="Invoke directly" default>

```python
from mlflow.genai.scorers import RetrievalGroundedness
import mlflow

# Get a trace from a previous run
trace = mlflow.get_trace("<your-trace-id>")

# Assess if the response is grounded in the retrieved context
feedback = RetrievalGroundedness()(trace=trace)
print(feedback)
```

  </TabItem>
  <TabItem value="evaluate" label="Invoke with evaluate()">

```python
import mlflow
from mlflow.genai.scorers import RetrievalGroundedness

# Evaluate traces from previous runs
results = mlflow.genai.evaluate(
    data=traces,  # DataFrame or list containing trace data
    scorers=[RetrievalGroundedness()],
)
```

  </TabItem>
</Tabs>
</TabsWrapper>

:::tip
For a complete RAG application example with these judges, see the <ins>[RAG Evaluation guide](/genai/eval-monitor/scorers/llm-judge/rag/)</ins>.
:::

## Interpret results

The RetrievalGroundedness judge evaluates each retriever span separately and returns a separate Feedback object for each retriever span in your trace. Each <APILink fn="mlflow.entities.Feedback">`Feedback`</APILink> object contains:

- **value**: "yes" if response is grounded in the retrieved context, "no" if it contains hallucinations
- **rationale**: Detailed explanation identifying:
  - Which statements are supported by the context
  - Which statements lack support (hallucinations)
  - Specific quotes from context that support or contradict claims

<SelectJudgeModel />

## Next steps

<TilesGrid>
  <TileCard
    icon={BookOpen}
    title="Evaluate context sufficiency"
    description="Check if your retriever provides adequate information"
    href="/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency"
  />
  <TileCard
    icon={Database}
    title="Evaluate context relevance"
    description="Ensure retrieved documents are relevant to queries"
    href="/genai/eval-monitor/scorers/llm-judge/rag/relevance/#retrievalrelevance-judge"
  />
  <TileCard
    icon={Target}
    title="Run comprehensive RAG evaluation"
    description="Combine multiple judges for complete RAG assessment"
    href="/genai/eval-monitor/quickstart/"
  />
</TilesGrid>
