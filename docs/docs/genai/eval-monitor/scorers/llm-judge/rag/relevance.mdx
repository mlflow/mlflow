import { APILink } from "@site/src/components/APILink";
import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Target, Database, Hammer } from "lucide-react";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Answer and Context Relevance Judges

MLflow provides two built-in LLM judges to assess relevance in your GenAI applications. These judges help diagnose quality issues - if context isn't relevant, the generation step cannot produce a helpful response.

- `RelevanceToQuery`: Evaluates if your app's response directly addresses the user's input
- `RetrievalRelevance`: Evaluates if each document returned by your app's retriever(s) is relevant

## Prerequisites for running the examples

1. Install MLflow and required packages

   ```bash
   pip install --upgrade "mlflow>=3.4.0" openai
   ```

2. Create an MLflow experiment by following the [setup your environment quickstart](/genai/getting-started/connect-environment/).

## RelevanceToQuery Judge

This judge evaluates if your app's response directly addresses the user's input without deviating into unrelated topics.

You can invoke the judge directly with a single input for testing, or pass it to <APILink fn="mlflow.genai.evaluate">mlflow.genai.evaluate</APILink> for running full evaluation on a dataset.

**Requirements:**

- **Trace requirements**: `inputs` and `outputs` must be on the Trace's root span

<TabsWrapper>
<Tabs groupId="invocation-method">
  <TabItem value="direct" label="Invoke directly" default>

```python
import mlflow
from mlflow.genai.scorers import RelevanceToQuery

assessment = RelevanceToQuery(name="my_relevance_to_query")(
    inputs={"question": "What is the capital of France?"},
    outputs="The capital of France is Paris.",
)
print(assessment)
```

  </TabItem>
  <TabItem value="evaluate" label="Invoke with evaluate()">

```python
import mlflow
from mlflow.genai.scorers import RelevanceToQuery

data = [
    {
        "inputs": {"question": "What is the capital of France?"},
        "outputs": "The capital of France is Paris.",
    }
]
result = mlflow.genai.evaluate(data=data, scorers=[RelevanceToQuery()])
```

  </TabItem>
</Tabs>
</TabsWrapper>

## RetrievalRelevance Judge

This judge evaluates if each document returned by your app's retriever(s) is relevant to the input request.

**Requirements:**

- **Trace requirements**: The MLflow Trace must contain at least one span with `span_type` set to `RETRIEVER`

<TabsWrapper>
<Tabs groupId="invocation-method">
  <TabItem value="direct" label="Invoke directly" default>

```python
from mlflow.genai.scorers import retrieval_relevance
import mlflow

# Get a trace from a previous run
trace = mlflow.get_trace("<your-trace-id>")

# Assess if each retrieved document is relevant
feedbacks = retrieval_relevance(trace=trace)
print(feedbacks)
```

  </TabItem>
  <TabItem value="evaluate" label="Invoke with evaluate()">

```python
import mlflow
from mlflow.genai.scorers import RetrievalRelevance

# Evaluate traces from previous runs
results = mlflow.genai.evaluate(
    data=traces,  # DataFrame or list containing trace data
    scorers=[RetrievalRelevance()],
)
```

  </TabItem>
</Tabs>
</TabsWrapper>

## RAG example

Here's a complete example showing how to create a RAG application with a retriever and evaluate it:

```python
import mlflow
from mlflow.genai.scorers import RetrievalRelevance
from mlflow.entities import Document
from typing import List


# Define a retriever function with proper span type
@mlflow.trace(span_type="RETRIEVER")
def retrieve_docs(query: str) -> List[Document]:
    # Simulated retrieval - in practice, this would query a vector database
    if "capital" in query.lower() and "france" in query.lower():
        return [
            Document(
                id="doc_1",
                page_content="Paris is the capital of France.",
                metadata={"source": "geography.txt"},
            ),
            Document(
                id="doc_2",
                page_content="The Eiffel Tower is located in Paris.",
                metadata={"source": "landmarks.txt"},
            ),
        ]
    else:
        return [
            Document(
                id="doc_3",
                page_content="Python is a programming language.",
                metadata={"source": "tech.txt"},
            )
        ]


# Define your app that uses the retriever
@mlflow.trace
def rag_app(query: str):
    docs = retrieve_docs(query)
    # In practice, you would pass these docs to an LLM
    return {"response": f"Found {len(docs)} relevant documents."}


# Create evaluation dataset
eval_dataset = [
    {"inputs": {"query": "What is the capital of France?"}},
    {"inputs": {"query": "How do I use Python?"}},
]

# Run evaluation with RetrievalRelevance judge
eval_results = mlflow.genai.evaluate(
    data=eval_dataset,
    predict_fn=rag_app,
    scorers=[
        RetrievalRelevance(
            model="openai:/gpt-4o-mini",  # Optional
        )
    ],
)
```

## Select the LLM that powers the judge

You can change the judge model by using the `model` argument in the judge definition. The model must be specified in the format `<provider>:/<model-name>`, where `<provider>` is a LiteLLM-compatible model provider.

You can customize these judges by providing different judge models:

```python
from mlflow.genai.scorers import RelevanceToQuery, RetrievalRelevance

# Use different judge models
relevance_judge = RelevanceToQuery(
    model="openai:/gpt-4o-mini"  # Or any LiteLLM-compatible model
)

retrieval_judge = RetrievalRelevance(model="anthropic:/claude-4-opus")

# Use in evaluation
eval_results = mlflow.genai.evaluate(
    data=eval_dataset, predict_fn=rag_app, scorers=[relevance_judge, retrieval_judge]
)
```

For a list of supported models, see [selecting judge models](/genai/eval-monitor/scorers/llm-judge/#selecting-judge-models).

## Interpret results

The judge returns a Feedback object with:

- **value**: "yes" if context is relevant, "no" if not
- **rationale**: Explanation of why the judge found the context relevant or irrelevant

## Next steps

<TilesGrid>
  <TileCard
    icon={Target}
    title="Explore other built-in judges"
    description="Learn about groundedness, safety, and correctness judges"
    href="/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges"
  />
  <TileCard
    icon={Hammer}
    title="Create custom judges"
    description="Build specialized judges for your use case"
    href="#"
  />
  <TileCard
    icon={Database}
    title="Evaluate RAG applications"
    description="Apply relevance judges in comprehensive RAG evaluation"
    href="/genai/eval-monitor/quickstart"
  />
</TilesGrid>
