import TileCard from '@site/src/components/TileCard';
import TilesGrid from '@site/src/components/TilesGrid';
import { Bot, Shield, Hammer } from "lucide-react";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Safety Judge

The `Safety` judge assesses the safety of given content (whether generated by the application or provided by a user), checking for harmful, unethical, or inappropriate material.

The Safety judge evaluates text content to identify potentially harmful, offensive, or inappropriate material. It returns a pass/fail assessment along with a detailed rationale explaining the safety concerns (if any).

## Prerequisites for running the examples

1. Install MLflow and required packages

   ```bash
   pip install --upgrade "mlflow>=3.4.0"
   ```

2. Create an MLflow experiment by following the [setup your environment quickstart](/genai/getting-started/connect-environment/).

## Usage examples

The `Safety` judge can be invoked directly for single assessment or used with MLflow's evaluation framework for batch evaluation.

<TabsWrapper>
<Tabs groupId="invocation-method">
  <TabItem value="direct" label="Invoke directly" default>

```python
from mlflow.genai.scorers import Safety

# Assess the safety of a single output
assessment = Safety()(
    outputs="MLflow is an open-source platform for managing the ML lifecycle, including experimentation, reproducibility, and deployment."
)
print(assessment)
```

  </TabItem>
  <TabItem value="evaluate" label="Invoke with evaluate()">

```python
import mlflow
from mlflow.genai.scorers import Safety

# Create evaluation dataset with various safety scenarios
eval_dataset = [
    {
        "inputs": {"query": "Tell me about MLflow"},
        "outputs": {
            "response": "MLflow is an open-source platform for managing the ML lifecycle, including experimentation, reproducibility, and deployment."
        },
    },
    {
        "inputs": {"query": "How can I stay productive?"},
        "outputs": {
            "response": "To stay productive, try these tips: Take regular breaks, prioritize tasks, stay hydrated, get enough sleep, and maintain a healthy work-life balance."
        },
    },
    {
        "inputs": {"query": "What's your opinion on sensitive topics?"},
        "outputs": {
            "response": "I aim to provide helpful, factual information while avoiding potentially harmful or biased content."
        },
    },
]

# Run evaluation with Safety judge
eval_results = mlflow.genai.evaluate(
    data=eval_dataset,
    scorers=[
        Safety(
            model="openai:/gpt-4o-mini",  # Optional.
        ),
    ],
)
```

  </TabItem>
</Tabs>
</TabsWrapper>

## Select the LLM that powers the judge

You can change the judge model by using the `model` argument in the judge definition. The model must be specified in the format `<provider>:/<model-name>`, where `<provider>` is a LiteLLM-compatible model provider.

You can customize the Safety judge by specifying a different model:

```python
from mlflow.genai.scorers import Safety

# Use a different model for safety evaluation
safety_judge = Safety(model="anthropic:/claude-4-opus")  # Use a different model

# Run evaluation with Safety judge
eval_results = mlflow.genai.evaluate(data=eval_dataset, scorers=[safety_judge])
```

For a list of supported models, see [selecting judge models](/genai/eval-monitor/scorers/llm-judge/#selecting-judge-models).

## Next steps

<TilesGrid>
  <TileCard
    icon={Shield}
    title="Explore other built-in judges"
    description="Learn about relevance, groundedness, and correctness judges"
    href="/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges"
  />
  <TileCard
    icon={Hammer}
    title="Create custom safety guidelines"
    description="Define specific safety criteria for your use case with Guidelines judge"
    href="/genai/eval-monitor/scorers/llm-judge/guidelines"
  />
  <TileCard
    icon={Bot}
    title="Evaluate Agents"
    description="Learn how to evaluate AI agents with specialized techniques and scorers"
    href="/genai/eval-monitor/running-evaluation/agents"
  />
</TilesGrid>
