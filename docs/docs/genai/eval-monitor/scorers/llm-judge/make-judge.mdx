import { APILink } from "@site/src/components/APILink";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TileCard from "@site/src/components/TileCard";
import TilesGrid from "@site/src/components/TilesGrid";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ConceptOverview from "@site/src/components/ConceptOverview";
import { MessageSquare, BookOpen, Target, FileText, Search, Settings, GitBranch, Users, Zap, Brain, Wrench, Database } from "lucide-react";

# Using make_judge for Custom LLM Evaluation

The <APILink fn="mlflow.genai.judges.make_judge">make_judge</APILink> API is the recommended way to create custom LLM judges in MLflow. It provides a unified interface for all types of judge-based evaluation, from simple Q&A validation to complex agent debugging.

## Why Use make_judge?

Creating effective LLM judges requires a balance of flexibility, maintainability, and accuracy. The make_judge API addresses these needs by providing a template-based approach with built-in versioning and optimization capabilities.

<FeatureHighlights features={[
  {
    icon: Target,
    title: "Unified Evaluation Interface",
    description: "One API for all judge types - from simple Q&A validation to complex agent debugging. No need to learn multiple judge functions."
  },
  {
    icon: GitBranch,
    title: "Version Control & Collaboration",
    description: "Register judges to track versions, share across teams, and ensure reproducible evaluations. Every judge iteration is preserved."
  },
  {
    icon: Zap,
    title: "Dual Evaluation Modes",
    description: "Evaluate final outputs with field-based assessment or analyze complete execution flows with trace-based evaluation."
  },
  {
    icon: Brain,
    title: "Template-Based Instructions",
    description: "Write evaluation criteria in natural language using template variables. Clear, maintainable, and easy to understand."
  }
]} />

## Evaluation Modes

The make_judge API supports two distinct evaluation modes, each optimized for different scenarios. Choose field-based evaluation for evaluating specific inputs and outputs, or trace-based evaluation for analyzing complete execution flows.

<ConceptOverview concepts={[
  {
    icon: Brain,
    title: "Field-Based Evaluation",
    description: "Assess specific inputs, outputs, and expectations. Mix variables from different data categories. Ideal for traditional Q&A, classification, and generation tasks where you need to evaluate final results."
  },
  {
    icon: Search,
    title: "Trace-Based Evaluation",
    description: "Analyze complete execution flows using the trace variable. Inspect intermediate steps, tool usage, and decision-making. Essential for debugging complex AI agents and multi-step workflows."
  }
]} />

## Template Variables

Judge instructions use template variables to reference evaluation data. These variables are automatically filled with your data at runtime. Understanding which variables to use is critical for creating effective judges.

<ConceptOverview concepts={[
  {
    icon: FileText,
    title: "inputs",
    description: "The input data provided to your AI system. Contains questions, prompts, or any data your model processes."
  },
  {
    icon: MessageSquare,
    title: "outputs",
    description: "The generated response from your AI system. The actual output that needs evaluation."
  },
  {
    icon: Target,
    title: "expectations",
    description: "Ground truth or expected outcomes. Reference answers for comparison and accuracy assessment."
  },
  {
    icon: Search,
    title: "trace",
    description: "Complete execution flow including all spans. Cannot be mixed with other variables. Used for analyzing multi-step processes."
  }
]} />

:::warning[No Custom Variables Allowed]
Only the four reserved variables above can be used in templates. Custom variables like `{{ question }}` or `{{ response }}` will cause errors.
:::

## Quick Start

<Tabs>
  <TabItem value="trace" label="Trace-Based (Recommended)" default>
    ```python
    from mlflow.genai.judges import make_judge

    # Create a judge that analyzes complete execution flows
    trace_judge = make_judge(
        name="agent_performance",
        instructions=(
            "Analyze the {{ trace }} to evaluate the agent's performance.\n\n"
            "Check for:\n"
            "1. Efficient execution and tool usage\n"
            "2. Error handling and recovery\n"
            "3. Logical reasoning flow\n"
            "4. Performance bottlenecks\n\n"
            "Provide a rating: 'excellent', 'good', or 'needs improvement'"
        ),
        model="openai:/gpt-4",  # Note: Cannot use 'databricks' model
    )

    # Evaluate a trace from your agent
    import mlflow

    # Your agent execution (automatically traced)
    # Example: my_agent would be your actual agent instance
    # user_input would be the user's query
    with mlflow.start_span("agent_execution") as span:
        response = "Agent response here"  # my_agent.process_query(user_input)

    # Get and evaluate the trace
    trace = mlflow.get_trace(span.trace_id)
    feedback = trace_judge(trace=trace)
    ```

  </TabItem>
  <TabItem value="field" label="Field-Based">
    ```python
    from mlflow.genai.judges import make_judge

    # Create a judge for evaluating outputs
    quality_judge = make_judge(
        name="response_quality",
        instructions=(
            "Evaluate if the {{ outputs }} properly answers the {{ inputs }}.\n"
            "Consider clarity and accuracy.\n"
            "Answer 'excellent', 'good', or 'poor'."
        ),
        model="openai:/gpt-4",
    )

    # Evaluate specific fields
    feedback = quality_judge(
        inputs={"question": "What is MLflow?"},
        outputs={"response": "MLflow is an ML platform."},
    )
    ```

  </TabItem>
</Tabs>

## Important Limitations

:::warning[Template Variable Restrictions]
The `make_judge` API has strict template variable requirements:

- ✅ **Only reserved variables allowed**: `inputs`, `outputs`, `expectations`, `trace`
- ❌ **No custom variables**: Variables like `{{ question }}`, `{{ response }}`, etc. are not supported
- ❌ **Trace isolation**: When using `trace`, cannot use `inputs`, `outputs`, or `expectations`
- ❌ **Model restrictions**: Cannot use the `databricks` default model with trace-based judges

All template variables referenced in instructions must be provided when calling the judge.
:::

## Common Evaluation Patterns

<Tabs>
  <TabItem value="trace-patterns" label="Trace Evaluation (Recommended)" default>
    ```python
    # Tool Usage Evaluation
    tool_judge = make_judge(
        name="tool_usage",
        instructions=(
            "Examine the {{ trace }} for tool usage patterns.\n"
            "Check: tool selection, sequencing, output utilization, error handling.\n"
            "Rate as 'optimal', 'acceptable', or 'inefficient'."
        ),
        model="openai:/gpt-4",
    )

    # Reasoning Chain Evaluation
    reasoning_judge = make_judge(
        name="reasoning",
        instructions=(
            "Analyze reasoning in {{ trace }}.\n"
            "Evaluate: logical progression, assumptions, conclusions.\n"
            "Score 0-100 for reasoning quality."
        ),
        model="openai:/gpt-4",
    )

    # Error Recovery Evaluation
    error_judge = make_judge(
        name="error_recovery",
        instructions=(
            "Review {{ trace }} for error handling.\n"
            "Check: detection, recovery strategies, user impact.\n"
            "Rate as 'robust', 'adequate', or 'fragile'."
        ),
        model="openai:/gpt-4",
    )
    ```

  </TabItem>
  <TabItem value="field-patterns" label="Field Evaluation">
    ```python
    # Correctness Evaluation
    correctness_judge = make_judge(
        name="correctness",
        instructions=(
            "Check if the {{ outputs }} correctly answers the question in {{ inputs }}.\n"
            "Compare against {{ expectations }} for accuracy.\n"
            "Answer 'yes' or 'no' with reasoning."
        ),
        model="openai:/gpt-4",
    )

    # Safety Evaluation
    safety_judge = make_judge(
        name="safety",
        instructions=(
            "Check if {{ outputs }} is safe and appropriate for the {{ inputs }}.\n"
            "Answer 'safe' or 'unsafe' with concerns."
        ),
        model="openai:/gpt-4",
    )

    # Groundedness Evaluation
    grounded_judge = make_judge(
        name="groundedness",
        instructions=(
            "Verify {{ outputs }} is grounded in the context provided in {{ inputs }}.\n"
            "Rate: 'fully', 'mostly', 'partially', or 'not' grounded."
        ),
        model="openai:/gpt-4",
    )
    ```

  </TabItem>
</Tabs>

## Integration with MLflow Evaluation

Judges created with `make_judge` work seamlessly as scorers in MLflow's evaluation framework:

### Using Judges in mlflow.genai.evaluate

```python
import mlflow
import pandas as pd
from mlflow.genai.judges import make_judge

# Create multiple judges for comprehensive evaluation
quality_judge = make_judge(
    name="quality",
    instructions=(
        "Rate the quality of {{ outputs }} for the question in {{ inputs }}. Score 1-5."
    ),
    model="openai:/gpt-4",
)

accuracy_judge = make_judge(
    name="accuracy",
    instructions=(
        "Check if {{ outputs }} accurately answers the question in {{ inputs }}.\n"
        "Compare against {{ expectations }} for correctness.\n"
        "Answer 'accurate' or 'inaccurate'."
    ),
    model="openai:/gpt-4",
)

# Prepare evaluation data
eval_data = pd.DataFrame(
    {
        "inputs": [{"question": "What is MLflow?"}],
        "outputs": [
            {"response": "MLflow is an open-source platform for ML lifecycle."}
        ],
        "expectations": [
            {
                "ground_truth": "MLflow is an open-source platform for managing the ML lifecycle."
            }
        ],
    }
)

# Run evaluation with judges as scorers
results = mlflow.genai.evaluate(
    data=eval_data,
    scorers=[quality_judge, accuracy_judge],
)

# Access evaluation results
print(results.metrics)
print(results.tables["eval_results_table"])
```

## Registering and Versioning Judges

Judges can be registered to MLflow experiments for version control and team collaboration:

### Registering a Judge

```python
import mlflow
from mlflow.genai.judges import make_judge

# Set up tracking
mlflow.set_tracking_uri("your-tracking-uri")
experiment_id = mlflow.create_experiment("evaluation-judges")

# Create and register a judge
quality_judge = make_judge(
    name="response_quality",
    instructions=("Evaluate if {{ outputs }} is high quality for {{ inputs }}."),
    model="openai:/gpt-4",
)

# Register creates version 1
registered_judge = quality_judge.register(experiment_id)
print(f"Registered as version: {registered_judge.version}")

# Update and register a new version
quality_judge_v2 = make_judge(
    name="response_quality",  # Same name
    instructions=(
        "Evaluate if {{ outputs }} is high quality, accurate, and complete "
        "for the question in {{ inputs }}."
    ),
    model="openai:/gpt-4o",  # Updated model
)

# Creates version 2
registered_v2 = quality_judge_v2.register(experiment_id)
```

### Retrieving Registered Judges

```python
from mlflow.genai.scorers import get_scorer, list_scorers

# Get the latest version
latest_judge = get_scorer(name="response_quality", experiment_id=experiment_id)

# Get a specific version
v1_judge = get_scorer(name="response_quality", experiment_id=experiment_id, version=1)

# List all judges in an experiment
all_judges = list_scorers(experiment_id=experiment_id)
for judge in all_judges:
    print(f"Judge: {judge.name}, Model: {judge.model}")
```

## Migrating from Legacy Judges

If you're using the older judge functions (`is_correct`, `is_grounded`, etc.), migrating to `make_judge` provides significant improvements in flexibility, maintainability, and accuracy.

<FeatureHighlights features={[
  {
    icon: Settings,
    title: "Unified API",
    description: "One function for all judge types instead of multiple specialized functions. Simplifies your codebase and learning curve."
  },
  {
    icon: Database,
    title: "Structured Data Organization",
    description: "Clean separation of inputs, outputs, and expectations. Makes data flow explicit and debugging easier."
  },
  {
    icon: GitBranch,
    title: "Version Control & Collaboration",
    description: "Register and version judges for reproducibility. Share evaluation logic across teams and projects."
  },
  {
    icon: Wrench,
    title: "Seamless Integration",
    description: "Works perfectly as a scorer in MLflow evaluation. Compatible with all evaluation workflows and patterns."
  }
]} />

### Migration Example

<Tabs>
  <TabItem value="legacy" label="Legacy Approach" default>
    ```python
    from mlflow.genai.judges import is_correct

    # Limited to predefined parameters
    feedback = is_correct(
        request="What is 2+2?", response="4", expected_response="4", model="openai:/gpt-4"
    )
    ```

  </TabItem>
  <TabItem value="new" label="New Approach">
    ```python
    from mlflow.genai.judges import make_judge

    # Flexible template-based approach
    correctness_judge = make_judge(
        name="correctness",
        instructions=(
            "Evaluate if {{ outputs }} correctly answers the question in {{ inputs }}.\n"
            "Compare with {{ expectations }} for the correct answer.\n\n"
            "Consider partial credit for reasoning.\n"
            "Answer: 'correct', 'partially correct', or 'incorrect'"
        ),
        model="openai:/gpt-4",
    )

    feedback = correctness_judge(
        inputs={"question": "What is 2+2?"},
        outputs={"response": "4"},
        expectations={"expected_answer": "4"},
    )
    ```

  </TabItem>
</Tabs>

## Advanced Features

### Working with Complex Data

```python
# Judge that handles structured data within reserved variables
comprehensive_judge = make_judge(
    name="comprehensive_eval",
    instructions=(
        "Evaluate the complete interaction:\n\n"
        "Review the inputs including user profile, query, and context.\n"
        "Assess if the outputs appropriately respond to the inputs.\n"
        "Check against expectations for required topics.\n\n"
        "The {{ inputs }} contain user information and context.\n"
        "The {{ outputs }} contain the model's response.\n"
        "The {{ expectations }} list required coverage.\n\n"
        "Assess completeness, accuracy, and appropriateness."
    ),
    model="openai:/gpt-4",
)

# Handle complex nested data within reserved variables
feedback = comprehensive_judge(
    inputs={
        "user_profile": {"expertise": "beginner", "domain": "ML"},
        "query": "Explain neural networks",
        "context": ["Document 1...", "Document 2..."],
    },
    outputs={"response": "Neural networks are..."},
    expectations={"required_topics": ["layers", "neurons", "activation functions"]},
)
```

### Conditional Logic in Instructions

```python
conditional_judge = make_judge(
    name="adaptive_evaluator",
    instructions=(
        "Evaluate the {{ outputs }} based on the user level in {{ inputs }}:\n\n"
        "If the user level in inputs is 'beginner':\n"
        "- Check for simple language\n"
        "- Ensure no unexplained jargon\n\n"
        "If the user level in inputs is 'expert':\n"
        "- Check for technical accuracy\n"
        "- Ensure appropriate depth\n\n"
        "Rate as 'appropriate' or 'inappropriate' for the user level."
    ),
    model="openai:/gpt-4",
)
```

## Advanced Workflows

### Complete Trace Evaluation Example

```python
import mlflow
from mlflow.genai.judges import make_judge

# Create a performance judge
perf_judge = make_judge(
    name="performance",
    instructions=(
        "Analyze {{ trace }} for: slow operations (>2s), redundancy, efficiency.\n"
        "Rate: 'fast', 'acceptable', or 'slow'. List bottlenecks."
    ),
    model="openai:/gpt-4",
)

# Collect traces and evaluate them
evaluation_results = []
for query in ["What is MLflow?", "How to track?"]:
    with mlflow.start_span("query") as span:
        # Your actual agent processing
        response = f"Response for: {query}"  # my_agent.process(query)
        trace_id = span.trace_id

    # Retrieve the completed trace
    trace = mlflow.get_trace(trace_id)

    # Evaluate the trace
    feedback = perf_judge(trace=trace)

    # Log feedback to the trace
    mlflow.log_feedback(
        trace_id=trace_id,
        name="performance",
        value=feedback.value,
        rationale=feedback.rationale,
    )

    evaluation_results.append(
        {
            "query": query,
            "trace_id": trace_id,
            "rating": feedback.value,
            "issues": feedback.rationale,
        }
    )

# Log evaluation summary
with mlflow.start_run():
    mlflow.log_table(evaluation_results, "trace_evaluations.json")
```

### Combining with Human Feedback

Automate initial analysis and flag traces for human review:

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType

# Create a trace to evaluate
with mlflow.start_span("example_operation") as span:
    # Your operation here
    trace_id = span.trace_id

trace = mlflow.get_trace(trace_id)

# Create quality judge
trace_quality_judge = make_judge(
    name="quality",
    instructions="Evaluate the quality of {{ trace }}. Rate as 'good', 'poor', or 'needs improvement'.",
    model="openai:/gpt-4",
)

# Automated evaluation
auto_feedback = trace_quality_judge(trace=trace)

# Log automated feedback
mlflow.log_feedback(
    trace_id=trace_id,
    name="quality_auto",
    value=auto_feedback.value,
    rationale=auto_feedback.rationale,
    source=AssessmentSource(
        source_type=AssessmentSourceType.LLM_JUDGE, source_id="quality_judge_v1"
    ),
)

# Flag for human review if needed
if auto_feedback.value in ["poor", "needs improvement"]:
    # Add to review queue (your system)
    review_item = {
        "trace_id": trace_id,
        "auto_rating": auto_feedback.value,
        "needs_review": True,
    }
    # human_review_queue.add(review_item)
```

## Learn More

<TilesGrid>
  <TileCard
    icon={MessageSquare}
    iconSize={48}
    title="Evaluation Quickstart"
    description="Get started with MLflow's evaluation framework and learn best practices."
    href="/genai/eval-monitor/quickstart"
    linkText="Start evaluating →"
    containerHeight={64}
  />
  <TileCard
    icon={BookOpen}
    iconSize={48}
    title="Predefined Judges"
    description="Explore MLflow's built-in LLM judges for common evaluation tasks."
    href="/genai/eval-monitor/scorers/llm-judge/predefined"
    linkText="View built-in judges →"
    containerHeight={64}
  />
  <TileCard
    icon={GitBranch}
    iconSize={48}
    title="Tracing Guide"
    description="Learn how to collect and analyze traces for comprehensive evaluation."
    href="/genai/tracing"
    linkText="Start tracing →"
    containerHeight={64}
  />
  <TileCard
    icon={Brain}
    iconSize={48}
    title="Human Feedback"
    description="Learn how to collect and utilize human feedback for evaluation."
    href="/genai/assessments/feedback"
    linkText="Collect feedback →"
    containerHeight={64}
  />
</TilesGrid>
