import { APILink } from "@site/src/components/APILink";
import { Users, Brain, Hammer } from "lucide-react";
import TileCard from "@site/src/components/TileCard";
import TilesGrid from "@site/src/components/TilesGrid";

# Custom Judges

Custom LLM judges let you define complex and nuanced judging guidelines for GenAI applications using natural language.

While MLflow [built-in LLM judges](/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges) offer excellent starting points for common quality dimensions, custom judges created using `make_judge()` give you full control over evaluation criteria. You can create these judges using either the **UI** or the **SDK**.

:::tip[Try the Judge Builder UI]
The fastest way to create LLM judges is through the **Judge Builder UI** - no code required. Navigate to your experiment's **Judges** tab to create and test judges visually. See the <ins>[Create a Custom Judge](/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge/#step-2-define-custom-judges)</ins> page for details.
:::

:::note[Version Requirements]

- **UI**: The Judge Builder UI requires **MLflow >= 3.9.0**.
- **SDK**: The `make_judge` API requires **MLflow >= 3.4.0**. For earlier versions, use the deprecated <APILink fn="mlflow.genai.judges.custom_prompt_judge">custom_prompt_judge</APILink> instead.
  :::

## Prompts and template variables

To create a judge, you provide a prompt with natural language instructions on how to assess the quality of your agent. <APILink fn="mlflow.genai.judges.make_jduge">`make_judge()`</APILink> accepts template variables to access the agent's inputs, outputs, expected outputs or behaviors, and even complete traces.

Your instructions must include at least one template variable, but you don't need to use all of them.

- `{{ inputs }}` - Input data provided to the agent
- `{{ outputs }}` - Output data generated by your agent
- `{{ expectations }}` - Ground truths or expected outcomes
- `{{ trace }}` - The complete execution trace of your agent

:::warning[Only Reserved Variables Allowed]
You can only use the reserved template variables shown above (`inputs`, `outputs`, `expectations`, `conversation`, `trace`). Custom variables like `{{ question }}` will cause validation errors. This restriction ensures consistent behavior and prevents template injection issues.

**Note on `conversation` variable:** The `{{ conversation }}` template variable can be used with `{{ expectations }}`, however it cannot be combined with `{{ inputs }}`, `{{ outputs }}`, or `{{ trace }}` variables. This is because conversation history provides complete context, making individual turn data redundant.
:::

## Trace-based judges

Trace-based judges analyze execution traces to understand what happened during agent execution. They autonomously explore traces using Model Context Protocol (MCP) tools and can:

- Validate tool usage patterns
- Identify performance bottlenecks
- Investigate execution failures
- Verify multi-step workflows

The following example defines a judge that assesses tool calling correctness by analyzing traces:

```python
from mlflow.genai.judges import make_judge
from typing import Literal

# Agent judge for tool calling correctness
tool_usage_judge = make_judge(
   name="tool_usage_validator",
   instructions=(
       "Analyze the {{ trace }} to verify correct tool usage.\n\n"
       "Check that the agent selected appropriate tools for the user's request "
       "and called them with correct parameters."
   ),
   feedback_value_type=Literal["correct", "incorrect"],
   model="openai:/gpt-5-mini"  # Required for trace-based judges
)
```

For trace-based judges to analyze the full trace, the `model` argument must be specified in `make_judge()`.

For a complete tutorial, see [Create a custom judge using make_judge()](/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge).

## Selecting Judge Models

By default, MLflow will use **OpenAI's GPT-4o-mini** model as the judge model. You can change the judge model by passing an override to the `model` argument within the scorer definition. The model must be specified in the format of `<provider>:/<model-name>`.

```python
from mlflow.genai.scorers import Correctness

Correctness(model="openai:/gpt-4o-mini")
Correctness(model="anthropic:/claude-4-opus")
Correctness(model="google:/gemini-2.0-flash")
```

### Supported Models

#### AI Gateway Endpoints

[AI Gateway](/genai/governance/ai-gateway/) endpoints are the recommended way to configure judge models, especially when creating judges from the UI. Benefits include:

- **Run judges directly from the UI** - Test and execute judges without leaving the browser
- **Centralized API key management** - No need to configure API keys locally
- **Traffic routing and fallbacks** - Configure load balancing and provider fallbacks

To use AI Gateway endpoints, select the endpoint from the UI dropdown or specify the endpoint name from the SDK with the `gateway:/` prefix, e.g., `gateway:/my-chat-endpoint`.

#### Direct Model Providers

MLflow also supports calling model providers directly:

- OpenAI / Azure OpenAI
- Anthropic
- Amazon Bedrock
- Cohere
- Together AI
- Any other providers supported by [LiteLLM](https://docs.litellm.ai/docs/providers), such as Google Gemini, xAI, Mistral, and more.

:::warning
Judges configured with direct model providers require API keys to be set locally (e.g., `OPENAI_API_KEY`) and **cannot be run from the UI**. Use AI Gateway endpoints if you want to run the judges from the UI.
:::

To use LiteLLM integrated models, install LiteLLM by running `pip install litellm` and specify the provider and model name in the same format as natively supported providers, e.g., `gemini:/gemini-2.0-flash`.

:::info

In Databricks, the default model is set to <ins>[Databricks's research-backed LLM judges](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/judges/)</ins>.

:::

### Choosing the Right LLM for Your Judge

The choice of LLM model significantly impacts judge performance and cost. Here's guidance based on your development stage and use case:

**Early Development Stage (Inner Loop)**

- **Recommended**: Start with powerful models like GPT-4o or Claude Opus
- **Why**: When you're beginning your agent development journey, you typically lack:
  - Use-case-specific grading criteria
  - Labeled data for optimization
- **Benefits**: More intelligent models can deeply explore traces, identify patterns, and help you understand common issues in your system
- **Trade-off**: Higher cost, but lower evaluation volume during development makes this acceptable

**Production & Scaling Stage**

- **Recommended**: Transition to smaller models (GPT-4o-mini, Claude Haiku) with smarter optimizers
- **Why**: As you move toward production:
  - You've collected labeled data and established grading criteria
  - Cost becomes a critical factor at scale
  - You can align smaller judges using more powerful optimizers
- **Approach**: Use a smaller judge model paired with a powerful optimizer model (e.g., GPT-4o-mini judge aligned using Claude Opus optimizer)

## Best practices for writing judge instructions

Be specific about expected output format. Your instructions should clearly specify what format the judge should return:

- **Categorical responses:** List specific values (for example, 'fully_resolved', 'partially_resolved', 'needs_follow_up')
- **Boolean responses:** Explicitly state the judge should return true or false
- **Numeric scores:** Specify the scoring range and what each score means

Break down complex evaluations. For complex evaluation tasks, structure your instructions into clear sections:

- What to evaluate
- What information to examine
- How to make the judgment
- What format to return

## Align judges with human experts

The base judge is a starting point. As you gather expert feedback on your application's outputs, you can align the LLM judges to the feedback to further improve judge accuracy. See [Align judges with humans](/genai/eval-monitor/scorers/llm-judge/alignment).

## Next steps

<TilesGrid>
  <TileCard
    icon={Hammer}
    title="Create a custom judge"
    description="Get a hands-on tutorial that demonstrates both standard and trace-based judges."
    href="/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge"
    linkText="Create custom judge →"
  />
  <TileCard
    icon={Users}
    title="Collect Human Feedback"
    description="Learn how to collect human feedback for evaluation."
    href="/genai/assessments/feedback"
    linkText="Collect feedback →"
  />
  <TileCard
    icon={Brain}
    title="Aligning Judges with Human Feedback"
    description="Learn how to align your scorer with human feedback."
    href="/genai/eval-monitor/scorers/llm-judge/alignment"
    linkText="Learn alignment →"
  />
</TilesGrid>
