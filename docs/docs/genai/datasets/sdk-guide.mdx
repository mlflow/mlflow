import Tabs from "@theme/Tabs"
import TabItem from "@theme/TabItem"
import { APILink } from "@site/src/components/APILink";
import TabsWrapper from "@site/src/components/TabsWrapper";
import ConceptOverview from "@site/src/components/ConceptOverview";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import { Activity, Users, Code, FileText, HelpCircle, Rocket, BarChart3, Target } from "lucide-react";

# Evaluation Datasets SDK Reference

Complete API reference for creating, managing, and querying evaluation datasets programmatically.

:::warning[SQL Backend Required]
Evaluation Datasets require an MLflow Tracking Server with a SQL backend (PostgreSQL, MySQL, SQLite, or MSSQL).
This feature is **not available** with FileStore (local file system-based tracking).
:::

## Quick API Overview

```python
from mlflow.genai.datasets import (
    create_dataset,  # Create new dataset
    get_dataset,  # Retrieve by ID
    search_datasets,  # Search and filter
    set_dataset_tags,  # Update tags
    delete_dataset_tag,  # Remove tags
    delete_dataset,  # Permanent deletion
)
```

## The Active Record Pattern

The <APILink fn="mlflow.entities.EvaluationDataset" text="EvaluationDataset" /> object follows an active record pattern. It's both a data container and provides methods to interact with the contents of the container.
Records are added and updated by interacting with the EvaluationDataset object itself, rather than through a separate API.

```python
# Get a dataset
dataset = get_dataset(dataset_id="d-1a2b3c4d5e6f7890abcdef1234567890")

# The dataset object is "live" - it can fetch and update data
current_record_count = len(dataset.records)  # Lazy loads if needed

# Add new records directly on the object
new_records = [
    {
        "inputs": {"question": "What are your business hours?"},
        "expectations": {"mentions_hours": True, "includes_timezone": True},
    }
]
dataset.merge_records(new_records)  # Updates backend immediately

# Convert to DataFrame for analysis
df = dataset.to_df()
# Access auto-computed properties
schema = dataset.schema  # Field structure
profile = dataset.profile  # Dataset statistics
```

## Record Merging Mechanics

The <APILink fn="mlflow.entities.EvaluationDataset.merge_records" text="merge_records()" /> method handles both new records and updates to existing ones. **Records are matched based on a hash of their inputs** - if a record with identical inputs already exists, its expectations and tags will be merged rather than creating a duplicate record.

<TabsWrapper>
  <Tabs>
    <TabItem value="adding" label="Adding New Records" default>

When you add records for the first time, they're stored with their inputs, expectations, metadata and (optionally) outputs:

```python
# Initial record
record_v1 = {
    "inputs": {"question": "What is MLflow?", "context": "ML platform overview"},
    "expectations": {"accuracy": 0.8, "mentions_tracking": True},
}

dataset.merge_records([record_v1])
# Creates a new record in the dataset
```

    </TabItem>
    <TabItem value="updating" label="Updating Existing Records">

When you merge a record with identical inputs, the existing record is updated by **merging** the new expectations and tags with the existing ones:

```python
# Updated version with same inputs but enhanced expectations
record_v2 = {
    "inputs": {
        "question": "What is MLflow?",  # Same question
        "context": "ML platform overview",  # Same context
    },
    "expectations": {
        "accuracy": 0.95,  # Updates existing value
        "mentions_models": True,  # Adds new expectation
        "clarity": 0.9  # Adds new metric
        # Note: "mentions_tracking": True is preserved from record_v1
    },
    "tags": {"reviewed": "true", "reviewer": "ml_team"},
}

dataset.merge_records([record_v2])
# The record is updated, not duplicated
# Final record has all expectations from both v1 and v2 merged together
```

    </TabItem>
    <TabItem value="uniqueness" label="Input Uniqueness">

Records are considered unique based on their **entire inputs dictionary**. Even small differences create separate records:

```python
# These are treated as different records due to different inputs
record_a = {
    "inputs": {"question": "What is MLflow?", "temperature": 0.7},
    "expectations": {"accuracy": 0.9},
}

record_b = {
    "inputs": {
        "question": "What is MLflow?",
        "temperature": 0.8,
    },  # Different temperature
    "expectations": {"accuracy": 0.9},
}

dataset.merge_records([record_a, record_b])
# Results in 2 separate records due to different temperature values
```

    </TabItem>

  </Tabs>
</TabsWrapper>

## Source Type Inference

MLflow automatically assigns source types before sending records to the backend using these rules:

<ConceptOverview
  title="Source Type Behavior"
  concepts={[
    {
      title: "Automatic Inference",
      description: "MLflow automatically infers source types based on record characteristics when no explicit source is provided."
    },
    {
      title: "Client-Side Processing",
      description: "Source type inference happens in merge_records() before records are sent to the tracking backend."
    },
    {
      title: "Manual Override",
      description: "You can always specify explicit source information to override automatic inference."
    }
  ]}
/>

### Inference Rules

<TabsWrapper>
  <Tabs>
    <TabItem value="trace" label="TRACE Source" default>

Records from MLflow traces are automatically assigned the `TRACE` source type:

```python
# When adding traces directly (automatic TRACE source)
traces = mlflow.search_traces(locations=["0"], return_type="list")
dataset.merge_records(traces)  # All records get TRACE source type

# Or when using DataFrame from search_traces
traces_df = mlflow.search_traces(locations=["0"])  # Returns DataFrame
dataset.merge_records(
    traces_df
)  # Automatically detects traces and assigns TRACE source
```

    </TabItem>
    <TabItem value="human" label="HUMAN Source">

Records with expectations are inferred as `HUMAN` source (subject matter expert annotations):

```python
# Records with expectations indicate human review/annotation
human_curated = [
    {
        "inputs": {"question": "What is MLflow?"},
        "expectations": {"answer": "MLflow is an ML platform", "quality": 0.9}
        # Automatically inferred as HUMAN source due to expectations
    }
]
dataset.merge_records(human_curated)
```

    </TabItem>
    <TabItem value="code" label="CODE Source">

Records with only inputs (no expectations) are inferred as `CODE` source (programmatically generated):

```python
# Records without expectations indicate programmatic generation
generated_tests = [
    {"inputs": {"question": f"Test question {i}"}}
    for i in range(100)
    # Automatically inferred as CODE source (no expectations field)
]
dataset.merge_records(generated_tests)
```

    </TabItem>

  </Tabs>
</TabsWrapper>

### Manual Source Override

You can explicitly specify the source type and metadata for any record:

```python
# Specify HUMAN source with metadata
human_curated = {
    "inputs": {"question": "What are your business hours?"},
    "expectations": {"accuracy": 1.0, "includes_timezone": True},
    "source": {
        "source_type": "HUMAN",
        "source_data": {"curator": "support_team", "date": "2024-11-01"},
    },
}

# Specify DOCUMENT source
from_docs = {
    "inputs": {"question": "How to install MLflow?"},
    "expectations": {"mentions_pip": True},
    "source": {
        "source_type": "DOCUMENT",
        "source_data": {"document_id": "install_guide", "page": 1},
    },
}

dataset.merge_records([human_curated, from_docs])
```

### Available Source Types

<FeatureHighlights features={[
  {
    icon: Activity,
    title: "TRACE",
    description: "Production data captured via MLflow tracing - automatically assigned when adding traces"
  },
  {
    icon: Users,
    title: "HUMAN",
    description: "Subject matter expert annotations - inferred for records with expectations"
  },
  {
    icon: Code,
    title: "CODE",
    description: "Programmatically generated tests - inferred for records without expectations"
  },
  {
    icon: FileText,
    title: "DOCUMENT",
    description: "Test cases from documentation or specs - must be explicitly specified"
  },
  {
    icon: HelpCircle,
    title: "UNSPECIFIED",
    description: "Source unknown or not provided - for legacy or imported data"
  }
]} />

## Search Filter Reference

### Searchable Fields

| Field              | Type      | Example                               |
| ------------------ | --------- | ------------------------------------- |
| `name`             | string    | `name = 'production_tests'`           |
| `tags.<key>`       | string    | `tags.status = 'validated'`           |
| `created_by`       | string    | `created_by = 'alice@company.com'`    |
| `last_updated_by`  | string    | `last_updated_by = 'bob@company.com'` |
| `created_time`     | timestamp | `created_time > 1698800000000`        |
| `last_update_time` | timestamp | `last_update_time > 1698800000000`    |

### Filter Operators

- `=`, `!=`: Exact match
- `LIKE`, `ILIKE`: Pattern matching with `%` wildcard (ILIKE is case-insensitive)
- `>`, `<`, `>=`, `<=`: Numeric/timestamp comparison
- `AND`: Combine conditions (OR is not currently supported)

### Common Filter Examples

<table style={{ width: "100%" }}>
  <thead>
    <tr>
      <th style={{ width: "40%" }}>Filter Expression</th>
      <th style={{ width: "30%" }}>Description</th>
      <th style={{ width: "30%" }}>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>`name = 'production_qa'`</strong></td>
      <td>Exact name match</td>
      <td>Find a specific dataset</td>
    </tr>
    <tr>
      <td><strong>`name LIKE '%test%'`</strong></td>
      <td>Pattern matching</td>
      <td>Find all test datasets</td>
    </tr>
    <tr>
      <td><strong>`tags.status = 'validated'`</strong></td>
      <td>Tag equality</td>
      <td>Find production-ready datasets</td>
    </tr>
    <tr>
      <td><strong>`tags.version = '2.0' AND tags.team = 'ml'`</strong></td>
      <td>Multiple tag conditions</td>
      <td>Find team-specific versions</td>
    </tr>
    <tr>
      <td><strong>`created_by = 'alice@company.com'`</strong></td>
      <td>Creator filter</td>
      <td>Find datasets by author</td>
    </tr>
    <tr>
      <td><strong>`created_time > 1698800000000`</strong></td>
      <td>Time-based filter</td>
      <td>Find recent datasets</td>
    </tr>
  </tbody>
</table>

```python
# Complex filter example
datasets = search_datasets(
    filter_string="""
        tags.status = 'production'
        AND name LIKE '%customer%'
        AND created_time > 1698800000000
    """,
    order_by=["last_update_time DESC"],
)
```

## Client API vs Fluent API

The <APILink fn="mlflow.tracking.MlflowClient" text="MlflowClient" /> provides an object-oriented interface with the same functionality as the fluent API:

<TabsWrapper>
  <Tabs>
    <TabItem value="create" label="Create Dataset" default>

```python
from mlflow import MlflowClient

client = MlflowClient()

# Create a dataset
dataset = client.create_dataset(
    name="customer_support_qa",
    experiment_id=["0"],
    tags={"version": "1.0", "team": "ml-platform"},
)
```

    </TabItem>
    <TabItem value="get" label="Get Dataset">

```python
# Get a dataset by ID
dataset = client.get_dataset(dataset_id="d-7f2e3a9b8c1d4e5f6a7b8c9d0e1f2a3b")

# Access properties
print(f"Dataset: {dataset.name}")
print(f"Records: {len(dataset.records)}")
```

    </TabItem>
    <TabItem value="search" label="Search Datasets">

```python
# Search for datasets
datasets = client.search_datasets(
    experiment_ids=["0"],
    filter_string="tags.status = 'validated'",
    order_by=["created_time DESC"],
    max_results=50,
)

for dataset in datasets:
    print(f"{dataset.name}: {dataset.dataset_id}")
```

    </TabItem>
    <TabItem value="tags" label="Manage Tags">

```python
# Set tags
client.set_dataset_tags(
    dataset_id=dataset.dataset_id, tags={"status": "production", "validated": "true"}
)

# Delete a tag
client.delete_dataset_tag(dataset_id=dataset.dataset_id, key="deprecated")
```

    </TabItem>
    <TabItem value="delete" label="Delete Dataset">

```python
# Delete a dataset
client.delete_dataset(dataset_id=dataset.dataset_id)
```

    </TabItem>

  </Tabs>
</TabsWrapper>

## Next Steps

<TilesGrid>
  <TileCard
    icon={Rocket}
    iconSize={48}
    title="End-to-End Workflow"
    description="Learn the complete evaluation-driven development workflow from app building to production"
    href="/genai/datasets/end-to-end-workflow"
    linkText="View complete workflow →"
    containerHeight={64}
  />
  <TileCard
    icon={BarChart3}
    iconSize={48}
    title="Run Evaluations"
    description="Use your datasets to systematically evaluate and improve your GenAI applications"
    href="/genai/eval-monitor"
    linkText="Start evaluating →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Define Expectations"
    description="Learn how to add ground truth expectations to your test data for quality validation"
    href="/genai/assessments/expectations"
    linkText="Set expectations →"
    containerHeight={64}
  />
</TilesGrid>
