import Tabs from "@theme/Tabs"
import TabItem from "@theme/TabItem"
import TabsWrapper from "@site/src/components/TabsWrapper";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ConceptOverview from "@site/src/components/ConceptOverview";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import DAGLoop from "@site/src/components/DAGLoop";
import { Database, Layers, TestTube, GitBranch, Users, ChartBar, FileText, Code, Activity, Target, BarChart3, TrendingUp, RefreshCw } from "lucide-react";
import useBaseUrl from '@docusaurus/useBaseUrl';

# Evaluation Datasets

## Transform Your GenAI Testing with Structured Evaluation Data

Evaluation datasets are the foundation of systematic GenAI application testing. They provide a centralized way to manage test data, ground truth expectations, and evaluation results—enabling you to measure and improve the quality of your AI applications with confidence.

:::warning[SQL Backend Required]
Evaluation Datasets require an MLflow Tracking Server with a **[SQL backend](/self-hosting/architecture/backend-store/#types-of-backend-stores)** (PostgreSQL, MySQL, SQLite, or MSSQL).
This feature is **not available** in FileStore (local file system-based tracking). If you need
a simple local configuration for MLflow, use the sqlite option when starting MLflow.
:::

## Quickstart: Build Your First Evaluation Dataset

There are several ways to create evaluation datasets, each suited to different stages of your GenAI development process.

The simplest way to create one is through MLflow's UI. Navigate to an Experiment that you want the evaluation dataset to be associated with and you can directly create a new one by supplying a unique name.
After adding records to it, you can view the dataset's entries in the UI.

<video src={useBaseUrl("/images/eval-datasets.mp4")} controls loop autoPlay muted aria-label="Evaluation Datasets Video" />

At its core, evaluation datasets are comprised of **inputs** and **expectations**. **Outputs** are an optional addition that can be added to an evaluation dataset for
post-hoc evaluation with scorers. Adding these elements can be done either directly from traces, dictionaries, or via a Pandas DataFrame.

<TabsWrapper>
<Tabs>
<TabItem value="from-traces" label="Build from Traces" default>

```python
import mlflow
from mlflow.genai.datasets import create_dataset, set_dataset_tags

# Create your evaluation dataset
dataset = create_dataset(
    name="production_validation_set",
    experiment_id=["0"],  # "0" is the default experiment
    tags={"team": "ml-platform", "stage": "validation"},
)

# Optionally, add additional tags to your dataset.
# Tags can be used to search for datasets with search_datasets API
set_dataset_tags(
    dataset_id=dataset.dataset_id,
    tags={"environment": "dev", "validation_version": "1.3"},
)

# First, retrieve traces that will become the basis of the dataset
traces = mlflow.search_traces(
    experiment_ids=["0"],
    max_results=20,
    filter_string="attributes.name = 'chat_completion'",
    return_type="list",  # Returns list[Trace]
)

# Add expectations to the traces
for trace in traces:
    mlflow.log_expectation(
        trace_id=trace.info.trace_id,
        name="expected_answer",
        value=(
            "The correct answer should include step-by-step instructions "
            "for password reset with email verification"
        ),
    )

# Retrieve the traces with added expectations
annotated_traces = mlflow.search_traces(
    experiment_ids=["0"],
    max_results=20,
    return_type="list",
)

# Merge the list of Trace objects directly into your dataset
dataset.merge_records(annotated_traces)
```

</TabItem>
<TabItem value="from-dicts" label="From Dictionaries">

```python
import mlflow
from mlflow.genai.datasets import create_dataset

# Create dataset with manual test cases
dataset = create_dataset(
    name="regression_test_suite",
    experiment_id=["0", "1"],  # Multiple experiments
    tags={"type": "regression", "priority": "critical"},
)

# Define test cases with expected outputs (ground truth)
test_cases = [
    {
        "inputs": {
            "question": "How do I reset my password?",
            "context": "user_support",
        },
        "expectations": {
            "expected_answer": (
                "To reset your password, click 'Forgot Password' on the login page, "
                "enter your email, and follow the link sent to your inbox"
            ),
            "must_contain_steps": True,
            "expected_tone": "helpful",
        },
    },
    {
        "inputs": {
            "question": "What are your refund policies?",
            "context": "customer_service",
        },
        "expectations": {
            "expected_answer": (
                "We offer full refunds within 30 days of purchase. "
                "Refunds after 30 days are subject to approval."
            ),
            "must_include_timeframe": True,
            "must_mention_exceptions": True,
        },
    },
]

dataset.merge_records(test_cases)
```

</TabItem>
<TabItem value="from-pandas" label="From DataFrame">

```python
import pandas as pd
from mlflow.genai.datasets import create_dataset

# Create dataset
dataset = create_dataset(
    name="benchmark_dataset",
    experiment_id=["0"],
    tags={"source": "benchmark", "version": "2024.1"},
)

# Create DataFrame with inputs and expectations (ground truth)
df = pd.DataFrame(
    [
        {
            "inputs": {
                "question": "What is MLflow?",
                "domain": "general",
            },
            "expectations": {
                "expected_answer": "MLflow is an open-source platform for ML",
                "must_mention": ["tracking", "experiments", "models"],
            },
        },
        {
            "inputs": {
                "question": "How do I track experiments?",
                "domain": "technical",
            },
            "expectations": {
                "expected_answer": "Use mlflow.start_run() and mlflow.log_params()",
                "must_mention": ["log_params", "log_metrics"],
            },
        },
        {
            "inputs": {
                "question": "Explain model versioning",
                "domain": "technical",
            },
            "expectations": {
                "expected_answer": "Model Registry provides versioning",
                "must_mention": ["Model Registry", "versions"],
            },
        },
    ]
)

# Add records from DataFrame
dataset.merge_records(df)
```

</TabItem>
</Tabs>
</TabsWrapper>

## Understanding Source Types

Every record in an evaluation dataset has a **source type** that tracks its provenance. This enables you to analyze model performance by data origin and understand which types of test data are most valuable.

<FeatureHighlights features={[
  {
    icon: Activity,
    title: "TRACE",
    description: "Records from production traces - automatically assigned when adding traces via mlflow.search_traces()"
  },
  {
    icon: Users,
    title: "HUMAN",
    description: "Subject matter expert annotations - automatically inferred for records with expectations (ground truth)"
  },
  {
    icon: Code,
    title: "CODE",
    description: "Programmatically generated test cases - automatically inferred for records without expectations"
  },
  {
    icon: FileText,
    title: "DOCUMENT",
    description: "Test cases extracted from documentation or specs - must be explicitly specified with source metadata"
  }
]} />

Source types are automatically inferred based on record characteristics but can be explicitly overridden when needed. See the [SDK Guide](/genai/datasets/sdk-guide#source-type-inference) for detailed inference rules and examples.

## Why Evaluation Datasets?

<FeatureHighlights features={[
  {
    icon: Database,
    title: "Centralized Test Management",
    description: "Store all your test cases, expected outputs, and evaluation criteria in one place. No more scattered CSV files or hardcoded test data."
  },
  {
    icon: RefreshCw,
    title: "Consistent Evaluation Source",
    description: "Maintain a concrete representation of test data that can be used repeatedly as your project evolves. Eliminate manual testing and avoid repeatedly assembling evaluation data for each iteration."
  },
  {
    icon: TestTube,
    title: "Systematic Testing",
    description: "Move beyond ad-hoc testing to systematic evaluation. Define clear expectations and measure performance consistently across deployments."
  },
  {
    icon: Users,
    title: "Collaborative Improvement",
    description: "Enable your entire team to contribute test cases and expectations. Share evaluation datasets across projects and teams."
  }
]} />

## The Evaluation Loop

Evaluation datasets bridge the critical gap between trace generation and evaluation execution in the GenAI development lifecycle. As you test your application and capture traces with expectations, **evaluation datasets transform these individual test cases into a materialized, reusable evaluation suite**. This creates a consistent and evolving collection of evaluation records that grows with your application—each iteration adds new test cases while preserving the historical test coverage. Rather than losing valuable test scenarios after each development cycle, you build a comprehensive evaluation asset that can immediately assess the quality of changes and improvements to your implementation.

<DAGLoop
  title="The Evaluation Loop"
  steps={[
    {
      icon: Code,
      title: "Iterate on Code",
      description: "Build and improve your GenAI application",
      detailedDescription: "Whether starting fresh or improving existing code, implement changes to your GenAI application. Use MLflow's comprehensive tracing to monitor each iteration, capture execution details, and track how your modifications impact performance and quality."
    },
    {
      icon: TestTube,
      title: "Test App",
      description: "Run initial tests and scenarios",
      detailedDescription: "Execute thorough testing with diverse prompts including edge cases, adversarial inputs, and typical user scenarios. Use MLflow's tracing to capture every interaction, response time, and token consumption for analysis."
    },
    {
      icon: Activity,
      title: "Collect Traces",
      description: "Capture real interactions",
      detailedDescription: "Systematically collect traces from both testing environments and production deployments. Each trace contains the complete request-response cycle, intermediate steps, and metadata that forms the foundation of your evaluation data."
    },
    {
      icon: Target,
      title: "Add Expectations",
      description: "Define ground truth outputs",
      detailedDescription: "Annotate traces with expected outputs and quality criteria. Domain experts define what constitutes correct behavior, creating a gold standard for evaluation. These expectations become the benchmark against which all AI responses are measured."
    },
    {
      icon: Database,
      title: "Create Dataset",
      description: "Organize evaluation data",
      detailedDescription: "Transform your annotated traces into structured evaluation datasets. These datasets become reusable test suites that can be versioned, shared across teams, and used to consistently measure performance across different model versions and configurations.",
      isFocus: true
    },
    {
      icon: BarChart3,
      title: "Run Evaluation",
      description: "Execute systematic evaluation",
      detailedDescription: "Run comprehensive evaluations using MLflow's evaluation framework with automated scorers, LLM judges, and custom metrics. Generate detailed reports comparing actual outputs against expectations to quantify your AI's performance."
    },
    {
      icon: TrendingUp,
      title: "Analyze Results",
      description: "Identify improvements",
      detailedDescription: "Deep dive into evaluation results to identify patterns, failure modes, and improvement opportunities. Use MLflow's visualization tools to compare performance across experiments and track progress over time."
    }
  ]}
  loopBackIcon={RefreshCw}
  loopBackText="Iterate & Improve"
  loopBackDescription="After analyzing results, iterate on your application by refining prompts, adjusting model parameters, or enhancing your evaluation criteria. The cycle continues as you progressively improve quality."
  circleSize={600}
/>

## Key Features

<ConceptOverview concepts={[
  {
    icon: Target,
    title: "Ground Truth Management",
    description: "Define and maintain expected outputs for your test cases. Capture expert knowledge about what constitutes correct behavior for your AI system."
  },
  {
    icon: GitBranch,
    title: "Schema Evolution",
    description: "Automatically track the structure of your test data as it evolves. Add new fields and test dimensions without breaking existing evaluations."
  },
  {
    icon: RefreshCw,
    title: "Incremental Updates",
    description: "Continuously improve your test suite by adding new cases from production. Update expectations as your understanding of correct behavior evolves."
  },
  {
    icon: FileText,
    title: "Flexible Tagging",
    description: "Organize datasets with tags for easy discovery and filtering. Track metadata like data sources, annotation guidelines, and quality levels."
  },
  {
    icon: TrendingUp,
    title: "Performance Tracking",
    description: "Monitor how your application performs against the same test data over time. Identify regressions and improvements across deployments."
  },
  {
    icon: Activity,
    title: "Experiment Integration",
    description: "Link datasets to MLflow experiments for complete traceability. Understand which test data was used for each model evaluation."
  }
]} />

## Next Steps

Ready to improve your GenAI testing? Start with these resources:

<TilesGrid>
  <TileCard
    icon={Layers}
    iconSize={48}
    title="Dataset Structure"
    description="Understand how evaluation datasets organize test inputs, expectations, and metadata"
    href="/genai/concepts/evaluation-datasets"
    linkText="Learn the concepts →"
    containerHeight={64}
  />
  <TileCard
    icon={Code}
    iconSize={48}
    title="SDK Guide"
    description="Complete guide to creating and managing evaluation datasets programmatically"
    href="/genai/datasets/sdk-guide"
    linkText="View SDK guide →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Setting Expectations"
    description="Learn how to define ground truth and expected outputs for your AI system"
    href="/genai/assessments/expectations"
    linkText="Define expectations →"
    containerHeight={64}
  />
  <TileCard
    icon={Activity}
    iconSize={48}
    title="Tracing Guide"
    description="Capture detailed execution data from your GenAI applications"
    href="/genai/tracing"
    linkText="Start tracing →"
    containerHeight={64}
  />
  <TileCard
    icon={ChartBar}
    iconSize={48}
    title="Evaluation Framework"
    description="Run systematic evaluations using your datasets with automated scorers"
    href="/genai/eval-monitor"
    linkText="Learn evaluation →"
    containerHeight={64}
  />
</TilesGrid>

# Building MLflow evaluation datasets

To systematically test and improve a GenAI application, you use an evaluation dataset. An evaluation dataset is a selected set of example inputs — either labeled (with known expected outputs) or unlabeled (without ground-truth answers). Evaluation datasets help you improve your app's performance in the following ways:

- Improve quality by testing fixes against known problematic examples from production.
- Prevent regressions. Create a "golden set" of examples that must always work correctly.
- Compare app versions. Test different prompts, models, or app logic against the same data.
- Target specific features. Build specialized datasets for safety, domain knowledge, or edge cases.
- Validate the app across different environments as part of LLMOps.

MLflow evaluation datasets are stored in :re[UC], which provides built-in versioning, lineage, sharing, and governance.

## Requirements

To create an evaluation dataset, you must have `CREATE TABLE` permissions on a :re[UC] schema.

## How to build an evaluation dataset

There are 3 ways to create an evaluation dataset:

- [Creating a dataset from existing traces](#creating-a-dataset-from-existing-traces): If you have already captured traces from a GenAI application, you can use them to create an evaluation dataset based on real-world scenarios.
- [Importing a dataset or building a dataset from scratch](#building-a-dataset-from-scratch): Use an existing dataset or build an evaluation dataset from scratch. This is useful for quick prototyping or for targeted testing of specific features.
- [Seeding an evaluation dataset with synthetic data](#seeding-an-evaluation-dataset-with-synthetic-data): Databricks can automatically generate a representative evaluation set from your documents, allowing you to quickly evaluate your agent with good coverage of test cases.

This page describes how to create an MLflow evaluation dataset. You can also use other types of datasets, such as Pandas DataFrames or a list of dictionaries, to get started quickly. See [\_](/mlflow3/genai/eval-monitor/eval-examples.md) for examples.

## Step 1: Create a dataset

The first step is to create an MLflow-managed evaluation dataset. MLflow-managed evaluation datasets track changes over time and maintain links to individual evaluation results.

::::tabs
:::tab-item[Using the UI]

Follow the recording below to use the UI to create an evaluation dataset

![Create evaluation dataset using the UI](https://assets.docs.databricks.com/_static/images/mlflow3-genai/new-images/create-dataset-alone.gif)

:::
:::tab-item[Using the SDK]

Create an evaluation dataset programmatically by searching for traces and adding them to the dataset.

```python
import mlflow
import mlflow.genai.datasets
import time
from databricks.connect import DatabricksSession

# 0. If you are using a local development environment, connect to Serverless Spark which powers MLflow's evaluation dataset service
spark = DatabricksSession.builder.remote(serverless=True).getOrCreate()

# 1. Create an evaluation dataset

# Replace with a Unity Catalog schema where you have CREATE TABLE permission
uc_schema = "workspace.default"
# This table will be created in the above UC schema
evaluation_dataset_table_name = "email_generation_eval"

eval_dataset = mlflow.genai.datasets.create_dataset(
    name=f"{uc_schema}.{evaluation_dataset_table_name}",
)
print(f"Created evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}")
```

:::
::::

## Step 2: Add records to your dataset

### <a id="creating-a-dataset-from-existing-traces"></a>Approach 1: Create from existing traces

One of the most effective ways to build a relevant evaluation dataset is by curating examples directly from your application's historical interactions captured by MLflow Tracing. You can create datasets from traces using either the MLflow Monitoring UI or the SDK.

::::tabs
:::tab-item[Using the UI]

Follow the recording below to use the UI to add existing production traces to the dataset

![trace](https://assets.docs.databricks.com/_static/images/mlflow3-genai/new-images/eval-dataset-add-traces.gif)

:::
:::tab-item[Using the SDK]

Programmatically search for traces and then add them to the dataset. Refer to the [query traces](/mlflow3/genai/tracing/observe-with-traces/query-via-sdk.md#search-api) reference page for details on how to use filters in `search_traces()`. You can use filters to identify traces by success, failure, use in production, or other properties.

```python
import mlflow

# 2. Search for traces
traces = mlflow.search_traces(
    filter_string="attributes.status = 'OK' AND tags.environment = 'production'",
    order_by=["attributes.timestamp_ms DESC"],
    max_results=10,
)

print(f"Found {len(traces)} successful traces")

# 3. Add the traces to the evaluation dataset
eval_dataset = eval_dataset.merge_records(traces)
print(f"Added {len(traces)} records to evaluation dataset")

# Preview the dataset
df = eval_dataset.to_df()
print(f"\nDataset preview:")
print(f"Total records: {len(df)}")
print("\nSample record:")
sample = df.iloc[0]
print(f"Inputs: {sample['inputs']}")
```

:::
::::

### <a id="creating-a-dataset-from-domain-expert-feedback"></a>Approach 2: Create from domain expert labels

Leverage feedback from domain experts captured in MLflow Labeling Sessions to enrich your evaluation datasets with ground truth labels. Before doing these steps, follow the [collect domain expert feedback](/mlflow3/genai/human-feedback/expert-feedback/label-existing-traces.md) guide to create a labeling session.

```python
import mlflow.genai.labeling as labeling

# Get a labeling sessions
all_sessions = labeling.get_labeling_sessions()
print(f"Found {len(all_sessions)} sessions")

for session in all_sessions:
    print(f"- {session.name} (ID: {session.labeling_session_id})")
    print(f"  Assigned users: {session.assigned_users}")

# Sync from the labeling session to the dataset

all_sessions[0].sync(dataset_name=f"{uc_schema}.{evaluation_dataset_table_name}")
```

### <a id="building-a-dataset-from-scratch"></a>Approach 3: Build from scratch or import existing

You can import an existing dataset or curate examples from scratch. Your data must match (or be transformed to match) the [evaluation dataset schema](/mlflow3/genai/eval-monitor/concepts/eval-datasets.md).

```python
# Define comprehensive test cases
evaluation_examples = [
    {
        "inputs": {"question": "What is MLflow?"},
        "expected": {
            "expected_response": "MLflow is an open source platform for managing the end-to-end machine learning lifecycle.",
            "expected_facts": [
                "open source platform",
                "manages ML lifecycle",
                "experiment tracking",
                "model deployment",
            ],
        },
    },
]

eval_dataset = eval_dataset.merge_records(evaluation_examples)
```

### <a id="seeding-an-evaluation-dataset-with-synthetic-data"></a>Approach 4: Seed using synthetic data

Generating synthetic data can expand your testing efforts by quickly creating diverse inputs and covering edge cases. See [\_](/generative-ai/agent-evaluation/synthesize-evaluation-set.md).

## Step 3: Update existing datasets

```python
import mlflow.genai.datasets
import pandas as pd

# Load existing dataset
dataset = mlflow.genai.datasets.get_dataset(name="catalog.schema.eval_dataset")

# Add new test cases
new_cases = [
    {
        "inputs": {"question": "What are MLflow models?"},
        "expectations": {
            "expected_facts": ["model packaging", "deployment", "registry"],
            "min_response_length": 100,
        },
    }
]

# Merge new cases
dataset = dataset.merge_records(new_cases)
```

## Limitations

- [Customer Managed Keys (CMK)](/security/keys/customer-managed-keys.md) are not supported.
- Maximum of 2,000 rows per evaluation dataset.
- Maximum of 20 expectations per dataset record.

If you need any of these limitations relaxed for your use case, contact your Databricks representative.

## Next steps

- [Evaluate your app](/mlflow3/genai/eval-monitor/evaluate-app.md) - Use your newly created dataset for evaluation
- [Create custom scorers](/mlflow3/genai/eval-monitor/custom-scorers.md) - Build scorers to evaluate against ground truth

## Reference guides

- [Evaluation datasets](/mlflow3/genai/eval-monitor/concepts/eval-datasets.md) - Deep dive into dataset structure and capabilities
- [Evaluation harness](/mlflow3/genai/eval-monitor/concepts/eval-harness.md) - Learn how [`mlflow.genai.evaluate()`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.genai.html#mlflow.genai.evaluate) uses your datasets
- [Tracing data model](/mlflow3/genai/tracing/data-model.md) - Understand traces as a source for evaluation datasets
- [Scorers](/mlflow3/genai/eval-monitor/concepts/scorers.md) - Applies each scorer to evaluate the quality of outputs from running the new app against evaluation dataset
