---
sidebar_label: Assessments API Guide
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Assessments API Guide

This guide walks you through the complete MLflow Assessments API following the natural workflow: from initial setup through advanced evaluation strategies. Each section builds on the previous, showing you how to implement the feedback and expectation patterns described in the [main overview](/genai/assessments).

## Getting Started

### Prerequisites

Before using the Feedback and Expectations API, ensure you have:

- MLflow 3.2.0 or later installed
- An active MLflow tracking server or local tracking setup
- Traces that have been logged from your GenAI application to an MLflow Experiment

### Feedback and Expectation Types

MLflow supports two fundamental types that work together to create a complete evaluation framework:

**Expectations** define the ground truth - what your AI should produce for a given input. These always come from human experts who understand the correct answer (learn more about [expectation concepts](/genai/tracing/concepts/expectations)).

**Feedback** evaluates how well your AI actually performed. This can come from three sources (learn more about [feedback concepts](/genai/tracing/concepts/feedback)):

```python
from mlflow.entities import AssessmentSource, AssessmentSourceType

# Human expert providing ground truth or evaluation
human_source = AssessmentSource(
    source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"
)

# Automated rule-based evaluation
code_source = AssessmentSource(
    source_type=AssessmentSourceType.CODE, source_id="accuracy_checker_v1"
)

# AI-powered evaluation at scale
llm_judge_source = AssessmentSource(
    source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"
)
```

## Logging Feedback and Expectations via the MLflow UI

The MLflow UI provides a visual interface for adding feedback and expectations directly to your traces. This approach is ideal for manual review, collaborative evaluation, and situations where domain experts need to contribute without writing code.

<Tabs>
    <TabItem value="get_started" label="Get Started" default>
### Getting Started

To add feedback or expectations to a trace, select the trace from your experiment's listing and click the "Add Assessment" button. This opens the entry form where you can choose between creating an expectation (ground truth) or feedback (evaluation).

![Add Assessment](/images/assessments/add_assessment_ui.png)

</TabItem>
<TabItem value="add_expectation" label="Add Expectation">

### Adding Expectations

Expectations define the ground truth for what your AI should produce. To add an expectation:

1. Select "Expectation" in the Assessment Type dropdown
2. Add a descriptive name (e.g., "expected_answer", "correct_classification")
3. Choose the appropriate data type for your values
4. Enter the expected values
5. Optionally add rationale to explain the expectation

![Add Expectation](/images/assessments/add_expectation_ui.png)

</TabItem>
<TabItem value="add_feedback" label="Add Feedback">

### Adding Feedback

Feedback captures evaluations of how well the AI performed. To add feedback:

1. Select "Feedback" in the Assessment Type dropdown
2. Add a specific name for the quality aspect (e.g., "accuracy", "helpfulness")
3. Choose the appropriate data type
4. Enter your evaluation value
5. Optionally add rationale to explain your assessment

![Add Feedback](/images/assessments/add_feedback_ui.png)

### Multiple Feedback Entries

You can add multiple feedback entries for the same aspect by clicking the plus (+) icon next to an existing feedback name. This is useful for gathering multiple perspectives or correcting automated evaluations.

![Additional Feedback](/images/assessments/additional_feedback_ui.png)

    </TabItem>
    <TabItem value="edit" label="Editing">

### Editing Feedback and Expectations

To edit an existing entry, click the dropdown menu (⋮) next to it and select "Edit". The UI displays visual indicators when entries have been modified.

![Edit Delete](/images/assessments/edit_delete_ui.png)

![Edit Feedback](/images/assessments/edit_feedback_ui.png)

    </TabItem>
    <TabItem value="view_assessments" label="Viewing">

### Viewing Feedback and Expectations

The trace detail page displays all feedback and expectations associated with a trace, including source attribution, timestamps, and rationale. This view helps you understand the complete evaluation picture for each trace.

![Assessments Trace Detail](/images/assessments/assessments_trace_detail_ui.png)

</TabItem>

</Tabs>

## Logging Feedback and Expectations via API

This section covers the core workflow of adding feedback and expectations to your traces. Start with expectations to establish ground truth, then add feedback from various sources to evaluate performance.

<Tabs>
  <TabItem value="expectations" label="📋 Expectations (Ground Truth)" default>

### Setting Ground Truth Standards

Expectations capture what your AI should produce for a given input. These always come from human experts and establish the reference point for evaluation.

#### Basic Text Expectations

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType

# Log a simple factual expectation
expectation = mlflow.log_expectation(
    trace_id="tr-1234567890abcdef",
    name="expected_answer",
    value="The capital of France is Paris.",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"
    ),
    metadata={
        "question_type": "factual",
        "confidence": "high",
        "annotation_time": "2024-01-15T10:30:00Z",
    },
)
```

#### Structured Expectations

For complex AI outputs, expectations can capture detailed structured data:

```python
# Expected classification result
mlflow.log_expectation(
    trace_id="tr-1234567890abcdef",
    name="expected_classification",
    value={
        "category": "customer_complaint",
        "severity": "high",
        "department": "billing",
        "requires_followup": True,
    },
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="support_lead@company.com"
    ),
)
```

#### Span-Level Expectations

Target specific operations within your AI pipeline:

```python
# Expected retrieval results for RAG systems
mlflow.log_expectation(
    trace_id="tr-1234567890abcdef",
    span_id="QUQIguHd3A4=",  # Retrieval span
    name="expected_documents",
    value=["doc_123", "doc_456", "doc_789"],
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="rag_expert@company.com"
    ),
    metadata={"relevance_threshold": 0.8, "expected_count": 3},
)
```

  </TabItem>
  <TabItem value="human_feedback" label="👥 Human Feedback">

### Expert Quality Assessment

Human feedback captures nuanced quality judgments that automated systems might miss, especially for creative, domain-specific, or sensitive outputs.

#### Quality Ratings

```python
# Comprehensive quality assessment
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="response_quality",
    value={"accuracy": 0.9, "helpfulness": 0.85, "clarity": 0.95, "completeness": 0.8},
    rationale="Response is accurate and clear, but could include more examples",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="domain_expert@company.com"
    ),
    metadata={
        "expertise_level": "senior",
        "review_time_seconds": 120,
        "confidence": "high",
    },
)
```

#### Pass/Fail Assessment

```python
# Safety evaluation by human moderator
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="content_safe",
    value=True,
    rationale="Content follows all safety guidelines and company policies",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="content_moderator_team"
    ),
)
```

#### Creative and Subjective Evaluation

```python
# Evaluating creative outputs that require human judgment
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="creative_quality",
    value=0.7,
    rationale="Story has good character development but pacing could be improved",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN,
        source_id="creative_director@company.com",
    ),
    metadata={
        "evaluation_criteria": ["originality", "engagement", "coherence"],
        "target_audience": "young_adult",
    },
)
```

  </TabItem>
  <TabItem value="automated_feedback" label="🤖 Automated Feedback">

### Scaled Evaluation with LLM Judges and Code

Automated feedback enables evaluation of every trace in production through AI judges and programmatic checks.

#### LLM Judge Evaluation

```python
# AI-powered quality assessment at scale
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="llm_quality_score",
    value={"relevance": 0.9, "coherence": 0.95, "factual_accuracy": 0.85},
    rationale="Response directly addresses the question with coherent reasoning. Minor factual gaps identified.",
    source=AssessmentSource(
        source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"
    ),
    metadata={
        "judge_model": "gpt-4-0125-preview",
        "evaluation_prompt_version": "v2.1",
        "temperature": 0.0,
    },
)

# Safety check with confidence scoring
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="safety_check",
    value=False,
    rationale="Response contains medical advice that requires professional consultation",
    source=AssessmentSource(
        source_type=AssessmentSourceType.LLM_JUDGE, source_id="safety_classifier_claude"
    ),
    metadata={"violation_category": "medical_advice", "confidence_score": 0.92},
)
```

#### Programmatic Code Evaluation

```python
# Rule-based format validation
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="format_compliance",
    value=True,
    rationale="Response follows required JSON schema with all mandatory fields",
    source=AssessmentSource(
        source_type=AssessmentSourceType.CODE, source_id="format_validator_v1.2"
    ),
)

# Business rule compliance
mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="compliance_check",
    value=False,
    rationale="Required disclaimer missing from financial advice response",
    source=AssessmentSource(
        source_type=AssessmentSourceType.CODE, source_id="compliance_checker_v2"
    ),
    metadata={"violated_rules": ["financial_disclaimer_required"], "severity": "high"},
)
```

#### Error Handling for Automated Evaluation

```python
from mlflow.entities import AssessmentError


# Robust evaluation with error logging
def safe_automated_evaluation(trace_id, response_text):
    try:
        # Attempt LLM judge evaluation
        score = call_external_llm_judge(response_text)
        return mlflow.log_feedback(
            trace_id=trace_id,
            name="external_quality_score",
            value=score,
            source=AssessmentSource(
                source_type=AssessmentSourceType.LLM_JUDGE,
                source_id="external_judge_api",
            ),
        )
    except Exception as e:
        # Log evaluation failure for monitoring
        return mlflow.log_feedback(
            trace_id=trace_id,
            name="external_quality_score",
            value=None,
            error=AssessmentError(
                error_code="EVALUATION_FAILED",
                error_message=f"External judge API error: {str(e)}",
            ),
            source=AssessmentSource(
                source_type=AssessmentSourceType.LLM_JUDGE,
                source_id="external_judge_api",
            ),
        )
```

  </TabItem>
</Tabs>

## Managing Feedback and Expectations

Once you have feedback and expectations on your traces, you need to retrieve, update, and sometimes delete them. This section covers the core management operations.

<Tabs>
  <TabItem value="retrieve" label="🔍 Retrieve" default>

### Getting Feedback and Expectation Data

Retrieve specific feedback or expectations to analyze evaluation results:

```python
# Get a specific feedback or expectation by ID
assessment = mlflow.get_assessment(
    trace_id="tr-1234567890abcdef", assessment_id="a-0987654321abcdef"
)

# Access assessment details
name = assessment.name
value = assessment.value
source_type = assessment.source.source_type
rationale = assessment.rationale if hasattr(assessment, "rationale") else None
```

  </TabItem>
  <TabItem value="update" label="✏️ Update">

### Modifying Feedback and Expectations

Update existing feedback or expectations when you need to correct or refine them:

```python
from mlflow.entities import Expectation

# Update an expectation with new information
updated_expectation = Expectation(
    name="expected_answer",
    value="Updated answer with more comprehensive details and examples",
)

mlflow.update_assessment(
    trace_id="tr-1234567890abcdef",
    assessment_id="a-0987654321abcdef",
    assessment=updated_expectation,
)
```

  </TabItem>
  <TabItem value="delete" label="🗑️ Delete">

### Removing Feedback and Expectations

Delete feedback or expectations that were logged incorrectly:

```python
# Delete a specific feedback or expectation
mlflow.delete_assessment(
    trace_id="tr-1234567890abcdef", assessment_id="a-5555666677778888"
)
```

:::note
If deleting a `Feedback` that has been marked as a replacement to an existing feedback using the `override_feedback` API, the original feedback that was marked as invalid will return to a valid state.
:::

  </TabItem>
</Tabs>

## Overriding Feedback

The `override_feedback` function allows you to correct automated feedback while preserving the original for audit trails. This is essential for human-in-the-loop evaluation workflows.

### Basic Override Example

When human expertise reveals that automated evaluation was incorrect:

```python
from mlflow.entities import AssessmentSource, AssessmentSourceType

# Step 1: Original automated feedback (this might have been logged earlier)
llm_feedback = mlflow.log_feedback(
    trace_id="tr-1234567890abcdef",
    name="relevance",
    value=0.6,
    rationale="Response partially addresses the question",
    source=AssessmentSource(
        source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"
    ),
)

# Step 2: Human expert reviews and disagrees
corrected_feedback = mlflow.override_feedback(
    trace_id="tr-1234567890abcdef",
    assessment_id=llm_feedback.assessment_id,
    value=0.9,
    rationale="Response fully addresses the question with comprehensive examples",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="expert_reviewer@company.com"
    ),
    metadata={"override_reason": "LLM underestimated relevance", "confidence": "high"},
)
```

The process of overriding an existing `Feedback` will mark the original feedback as invalid, but preserve the history of the original feedback.

### Override vs Update

Note the difference between override and update:

- **`override_feedback`**: Creates a new feedback that marks the original as invalid but preserves it for analysis
- **`update_assessment`**: Modifies the existing feedback or expectation in-place

Use override when you want to preserve the original automated evaluation for training data or analysis. Use update when you want to simply correct a feedback or expectation.

## Best Practices

Following these practices ensures your feedback and expectation data is reliable, scalable, and valuable for improving your AI systems.

<Tabs>
  <TabItem value="naming" label="📝 Naming" default>

### Consistent Naming

Use clear, descriptive naming conventions that make feedback and expectation data easy to analyze:

```python
# Excellent: Descriptive, hierarchical names
mlflow.log_expectation(trace_id=trace_id, name="expected_sql_query", value=sql)
mlflow.log_feedback(trace_id=trace_id, name="sql_syntax_valid", value=True)
mlflow.log_feedback(trace_id=trace_id, name="sql_execution_time_ms", value=245)

# Poor: Vague, inconsistent names
mlflow.log_expectation(trace_id=trace_id, name="expected", value=sql)
mlflow.log_feedback(trace_id=trace_id, name="good", value=True)
```

  </TabItem>
  <TabItem value="sources" label="🔍 Sources">

### Traceable Source Attribution

Provide specific, traceable source information for audit trails and quality analysis:

```python
# Excellent: Version-specific, environment-aware sources
source = AssessmentSource(
    source_type=AssessmentSourceType.CODE, source_id="response_validator_v2.1_prod"
)

# Good: Individual human attribution
source = AssessmentSource(
    source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"
)

# Poor: Generic, untraceable sources
source = AssessmentSource(source_type=AssessmentSourceType.CODE, source_id="validator")
```

  </TabItem>
  <TabItem value="metadata" label="📊 Metadata">

### Rich Contextual Information

Include metadata that helps with analysis and debugging:

```python
mlflow.log_feedback(
    trace_id=trace_id,
    name="response_quality",
    value=0.85,
    source=human_source,
    metadata={
        "reviewer_expertise": "domain_expert",
        "review_duration_seconds": 45,
        "confidence": "high",
        "criteria_version": "v2.3",
        "evaluation_context": "production_review",
    },
)
```

  </TabItem>
</Tabs>

## Next Steps

Now that you understand the Assessments API, explore how it integrates with other MLflow capabilities:

- [MLflow Tracing Guide](/genai/tracing) - Learn how feedback and expectations integrate with trace data
- [Evaluation Workflows](/genai/eval-monitor) - Discover systematic evaluation strategies using feedback and expectations
