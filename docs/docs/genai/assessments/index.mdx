import { CardGroup, TitleCard } from "@site/src/components/Card";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from "@site/src/components/TabsWrapper";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { MessageSquare, Target, Code, FileText, TrendingUp } from "lucide-react";

# Evaluate and Improve Your GenAI Applications

MLflow provides comprehensive tools for evaluating and improving your GenAI applications through two complementary approaches: collecting feedback on actual performance and establishing ground truth expectations. Together, these capabilities enable you to systematically measure quality, identify improvements, and build better AI systems.

<TilesGrid>
  <TileCard
    icon={MessageSquare}
    iconSize={48}
    title="Collecting Automated and Human Feedback"
    description="Capture quality evaluations from LLM judges, programmatic checks, and human reviewers to understand how well your AI performs"
    href="/genai/assessments/feedback"
    linkText="Start collecting →"
    containerHeight={120}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Annotating Ground Truth with Expectations"
    description="Define expected outputs and correct answers to establish quality baselines and enable objective accuracy measurement"
    href="/genai/assessments/expectations"
    linkText="Start annotating →"
    containerHeight={120}
  />
</TilesGrid>

## Quick Start Examples

<TabsWrapper>
  <Tabs>
    <TabItem value="feedback" label="Feedback" default>

### Collect Feedback on AI Performance

#### Via the MLflow UI

From a trace page, you can directly provide feedback to any trace.

![Create Feedback](/images/assessments/add_feedback_ui.png)

#### Via API

```python
import mlflow
from mlflow.entities import AssessmentSource
from mlflow.entities.assessment_source import AssessmentSourceType

# Log feedback from an LLM judge
mlflow.log_feedback(
    trace_id=trace_id,
    name="response_quality",
    value=0.85,
    rationale="Clear and helpful response with minor verbosity",
    source=AssessmentSource(
        source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"
    ),
)
```

    </TabItem>
    <TabItem value="expectations" label="Expectations">

### Define Expected Outputs

#### Via the MLflow UI

From a trace page, you can add expectations directly in the UI.

![Create Expectation](/images/assessments/add_expectation_ui.png)

#### Via API

```python
import mlflow
from mlflow.entities import AssessmentSource
from mlflow.entities.assessment_source import AssessmentSourceType

# Log ground truth expectation
mlflow.log_expectation(
    trace_id=trace_id,
    name="expected_answer",
    value="The capital of France is Paris",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"
    ),
)
```

    </TabItem>

  </Tabs>
</TabsWrapper>

## Learn More

<TilesGrid>
  <TileCard
    icon={MessageSquare}
    title="Feedback Documentation"
    description="Complete guide to collecting and managing feedback"
    href="/genai/assessments/feedback"
    linkText="Learn feedback →"
  />
  <TileCard
    icon={FileText}
    title="MLflow Tracing"
    description="Understand how evaluation integrates with traces"
    href="/genai/tracing"
    linkText="Learn tracing →"
  />
  <TileCard
    icon={TrendingUp}
    title="Evaluation Workflows"
    description="Build systematic evaluation strategies"
    href="/genai/eval-monitor"
    linkText="Explore workflows →"
  />
</TilesGrid>
