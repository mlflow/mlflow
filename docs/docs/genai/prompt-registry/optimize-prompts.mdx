---
sidebar_position: 5
sidebar_label: Optimize Prompts üÜï
---

import { APILink } from "@site/src/components/APILink";

# Optimize Prompts (Experimental)

MLflow provides the <APILink fn="mlflow.genai.optimize_prompts" /> API to automatically improve your prompts using evaluation metrics and training data. This feature helps you enhance prompt quality by leveraging the [GEPA](https://arxiv.org/abs/2507.19457) optimization algorithm, which uses iterative refinement and LLM-based reflection to systematically improve prompts.

:::tip Key Benefits

- **Automatic Improvement**: Optimize prompts based on evaluation metrics without manual tuning
- **Data-Driven**: Uses your training data and custom scorers to guide optimization
- **Flexible Evaluation**: Support for custom scorers and objective functions
- **Version Control**: Automatically registers optimized prompts in MLflow Prompt Registry
- **Extensible**: Plug in custom optimization algorithms by extending base classes

:::

:::note[Version Requirements]
The `optimize_prompts` API requires **MLflow >= 3.5.0**.
:::

## Quick Start

Here's a simple example of optimizing a prompt for better accuracy:

```python
import mlflow
import openai
from mlflow.genai.optimize import LLMParams

# Register initial prompt
prompt = mlflow.genai.register_prompt(
    name="qa",
    template="Answer this question: {{question}}",
)


# Define your prediction function
def predict_fn(question: str) -> str:
    prompt = mlflow.genai.load_prompt("prompts:/qa/latest")
    completion = openai.OpenAI().chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt.format(question=question)}],
    )
    return completion.choices[0].message.content


# Training data with inputs and expected outputs
dataset = [
    {"inputs": {"question": "What is the capital of France?"}, "outputs": "Paris"},
    {"inputs": {"question": "What is the capital of Germany?"}, "outputs": "Berlin"},
    {"inputs": {"question": "What is the capital of Japan?"}, "outputs": "Tokyo"},
    {"inputs": {"question": "What is the capital of Italy?"}, "outputs": "Rome"},
]

# Optimize the prompt
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o"),
)

# Use the optimized prompt
optimized_prompt = result.optimized_prompts[0]
print(f"Optimized template: {optimized_prompt.template}")
```

The API will produce an improved prompt that performs better on your evaluation criteria:

```
Optimized template:
Answer the following question: {{question}}

Provide accurate, concise answers based on well-established facts.
For questions about capital cities, respond with only the city name.

Examples:
- France ‚Üí Paris
- Germany ‚Üí Berlin
- Japan ‚Üí Tokyo

Ensure responses are factual and directly address the question asked.
```

## Components

The <APILink fn="mlflow.genai.optimize_prompts" /> API requires the following components:

<table style={{ width: "100%" }}>
  <thead>
    <tr>
      <th style={{ width: "30%" }}>Component</th>
      <th style={{ width: "70%" }}>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Predict Function</strong></td>
      <td>A callable that takes inputs as keyword arguments and returns outputs. Must use prompts from MLflow Prompt Registry.</td>
    </tr>
    <tr>
      <td><strong>Training Data</strong></td>
      <td>Dataset with <code>inputs</code> (dict) and <code>outputs</code> (expected results). Supports pandas DataFrame, list of dicts, or MLflow EvaluationDataset.</td>
    </tr>
    <tr>
      <td><strong>Target Prompt URIs</strong></td>
      <td>List of prompt URIs to optimize (e.g., <code>["prompts:/qa/1"]</code>)</td>
    </tr>
    <tr>
      <td><strong>Optimizer LM Params</strong></td>
      <td>LLM configuration for the optimization algorithm using <APILink fn="mlflow.genai.optimize.LLMParams" /></td>
    </tr>
  </tbody>
</table>

### 1. Predict Function

Your `predict_fn` must:
- Accept inputs as keyword arguments matching your prompt template variables
- Load and use prompts from MLflow Prompt Registry
- Return outputs in the same format as your training data

```python
def predict_fn(question: str) -> str:
    # Load prompt from registry
    prompt = mlflow.genai.load_prompt("prompts:/qa/latest")

    # Format the prompt with input variables
    formatted_prompt = prompt.format(question=question)

    # Call your LLM
    response = your_llm_call(formatted_prompt)

    return response
```

### 2. Training Data

Provide a dataset with `inputs` and `outputs`:

```python
# List of dictionaries
dataset = [
    {"inputs": {"question": "What is AI?"}, "outputs": "Artificial Intelligence"},
    {"inputs": {"question": "What is ML?"}, "outputs": "Machine Learning"},
]

# Or pandas DataFrame
import pandas as pd

dataset = pd.DataFrame(
    {
        "inputs": [
            {"question": "What is AI?"},
            {"question": "What is ML?"},
        ],
        "outputs": ["Artificial Intelligence", "Machine Learning"],
    }
)
```

### 3. Target Prompt URIs

Specify which prompts to optimize:

```python
target_prompt_uris = [
    "prompts:/qa/1",  # Specific version
    "prompts:/instruction/latest",  # Latest version
]
```

### 4. Optimizer LM Parameters

Configure the LLM used for optimization:

```python
from mlflow.genai.optimize import LLMParams

optimizer_lm_params = LLMParams(
    model_name="openai:/gpt-4o",  # Powerful model for optimization
    temperature=0.7,
)
```

## Advanced Usage

### Using Custom Scorers

Define custom evaluation metrics to guide optimization:

```python
from mlflow.genai.scorers import scorer


@scorer
def accuracy_scorer(outputs, expectations):
    """Check if output matches expected value."""
    return 1.0 if outputs.lower() == expectations.lower() else 0.0


@scorer
def brevity_scorer(outputs):
    """Prefer shorter outputs (max 50 chars)."""
    return min(1.0, 50 / max(len(outputs), 1))


# Combine scorers with a weighted objective
def weighted_objective(scores):
    return 0.7 * scores["accuracy_scorer"] + 0.3 * scores["brevity_scorer"]


# Use custom scorers
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o"),
    scorers=[accuracy_scorer, brevity_scorer],
    objective=weighted_objective,
)
```

### Custom Optimization Algorithm

Implement your own optimizer by extending <APILink fn="mlflow.genai.optimize.BasePromptOptimizer" />:

```python
from mlflow.genai.optimize import BasePromptOptimizer, PromptAdapterOutput


class MyCustomOptimizer(BasePromptOptimizer):
    def optimize(self, eval_fn, train_data, target_prompts, optimizer_lm_params):
        # Your custom optimization logic
        optimized_prompts = {}
        for prompt_name, prompt_template in target_prompts.items():
            # Implement your algorithm
            optimized_prompts[prompt_name] = your_optimization_algorithm(
                prompt_template, train_data, optimizer_lm_params
            )

        return PromptAdapterOutput(optimized_prompts=optimized_prompts)


# Use custom optimizer
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o"),
    optimizer=MyCustomOptimizer(),
)
```

### Multi-Prompt Optimization

Optimize multiple prompts together:

```python
# Register multiple prompts
system_prompt = mlflow.genai.register_prompt(
    name="system_instruction",
    template="You are a helpful assistant.",
)
user_prompt = mlflow.genai.register_prompt(
    name="user_query",
    template="Question: {{question}}",
)

# Optimize both
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[system_prompt.uri, user_prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o"),
)

# Access optimized prompts
optimized_system = result.optimized_prompts[0]
optimized_user = result.optimized_prompts[1]
```

## Result Object

The API returns a <APILink fn="mlflow.genai.optimize.PromptAdaptationResult" /> object:

```python
result = mlflow.genai.optimize_prompts(...)

# Access optimized prompts
for prompt in result.optimized_prompts:
    print(f"Name: {prompt.name}")
    print(f"Version: {prompt.version}")
    print(f"Template: {prompt.template}")
    print(f"URI: {prompt.uri}")

# Check optimizer used
print(f"Optimizer: {result.optimizer_name}")

# View evaluation scores (if available)
print(f"Initial score: {result.initial_eval_score}")
print(f"Final score: {result.final_eval_score}")
```

## Common Use Cases

### Improving Accuracy

Optimize prompts to produce more accurate outputs:

```python
@scorer
def exact_match(outputs, expectations):
    return 1.0 if outputs == expectations else 0.0


result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o"),
    scorers=[exact_match],
)
```

### Optimizing for Format

Ensure outputs follow a specific format:

```python
import json


@scorer
def json_format_scorer(outputs):
    try:
        json.loads(outputs)
        return 1.0
    except:
        return 0.0


result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o"),
    scorers=[json_format_scorer],
)
```

## Troubleshooting

### Issue: Optimization Takes Too Long

**Solution**: Reduce dataset size or use a faster optimizer model:

```python
# Use fewer examples
small_dataset = dataset[:20]

# Use faster model for optimization
optimizer_lm_params = LLMParams(model_name="openai:/gpt-4o-mini")
```

### Issue: No Improvement Observed

**Solution**: Check your evaluation metrics and increase dataset diversity:

- Ensure scorers accurately measure what you care about
- Increase training data size and diversity
- Try a more powerful optimizer model
- Verify outputs format matches expectations

### Issue: Prompts Not Being Used

**Solution**: Ensure `predict_fn` loads prompts from registry:

```python
# ‚úÖ Correct - loads from registry
def predict_fn(question: str):
    prompt = mlflow.genai.load_prompt("prompts:/qa/latest")
    return llm_call(prompt.format(question=question))


# ‚ùå Incorrect - hardcoded prompt
def predict_fn(question: str):
    return llm_call(f"Answer: {question}")
```

## See Also

- [Rewrite Prompts for New Models](/genai/prompt-registry/adapt-prompts): Adapt prompts when switching between language models
- [Create and Edit Prompts](/genai/prompt-registry/create-and-edit-prompts): Basic Prompt Registry usage
- [Evaluate Prompts](/genai/eval-monitor/running-evaluation/prompts): Evaluate prompt performance
