---
sidebar_position: 7
sidebar_label: LangChain Optimization
---

import { APILink } from "@site/src/components/APILink";
import useBaseUrl from '@docusaurus/useBaseUrl';

# Optimizing Prompts for LangChain

<p style={{display: 'flex', justifyContent: 'center', margin: '1em 0'}}>
  <img src={useBaseUrl("/images/logos/langchain-logo.png")} alt="LangChain Logo" style={{width: 300, objectFit: 'contain'}} />
</p>

This guide demonstrates how to leverage <APILink fn="mlflow.genai.optimize_prompts" /> alongside [LangChain](https://python.langchain.com/) to enhance your chain's prompts automatically. The <APILink fn="mlflow.genai.optimize_prompts" /> API is framework-agnostic, enabling you to perform end-to-end prompt optimization of your chains from any framework using state-of-the-art techniques. For more information about the API, please visit [Optimize Prompts](/genai/prompt-registry/optimize-prompts).

## Prerequisites

```bash
pip install langchain langchain-openai mlflow gepa
```

Set your OpenAI API key:

```bash
export OPENAI_API_KEY="your-api-key"
```

Set tracking server and MLflow experiment:

```python
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("LangChain Optimization")
```

## Basic Example

Here's a complete example of optimizing a translation chain. The example shows how easy it is to optimize prompts in a LangChain workflow with minimal changes. Note that we call <APILink fn="mlflow.entities.model_registry.PromptVersion.to_single_brace_format" >`PromptVersion.to_single_brace_format()`</APILink> instead of <APILink fn="mlflow.entities.model_registry.PromptVersion.format" >`PromptVersion.format()`</APILink> inside `predict_fn`.

```python
import mlflow
from mlflow.genai.scorers import Correctness
from mlflow.genai.optimize.optimizers import GepaPromptOptimizer
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from langchain.chains import LLMChain

# Step 1: Register your initial prompt
prompt = mlflow.genai.register_prompt(
    name="translation-prompt",
    template="Translate the following text from {{input_language}} to {{output_language}}: {{text}}",
)


# Step 2: Create a prediction function
def predict_fn(input_language, output_language, text):
    # Load prompt from registry
    prompt = mlflow.genai.load_prompt("prompts:/translation-prompt@latest")

    template = PromptTemplate(
        input_variables=["input_language", "output_language", "text"],
        template=prompt.to_single_brace_format(),  # call to_single_brace_format
    )

    llm = OpenAI(temperature=0.7)

    chain = LLMChain(llm=llm, prompt=template)

    result = chain.run(
        input_language=input_language, output_language=output_language, text=text
    )

    return result


# Step 3: Prepare training data
dataset = [
    {
        "inputs": {
            "input_language": "English",
            "output_language": "French",
            "text": "Hello, how are you?",
        },
        "expectations": {"expected_response": "Bonjour, comment allez-vous?"},
    },
    {
        "inputs": {
            "input_language": "English",
            "output_language": "Spanish",
            "text": "Good morning",
        },
        "expectations": {"expected_response": "Buenos d√≠as"},
    },
    {
        "inputs": {
            "input_language": "English",
            "output_language": "German",
            "text": "Thank you very much",
        },
        "expectations": {"expected_response": "Vielen Dank"},
    },
    # more data...
]

# Step 4: Optimize the prompt
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    prompt_uris=[prompt.uri],
    optimizer=GepaPromptOptimizer(reflection_model="openai:/gpt-5"),
    scorers=[Correctness(model="openai:/gpt-5")],
)

# Step 5: Use the optimized prompt
optimized_prompt = result.optimized_prompts[0]
print(f"Optimized prompt URI: {optimized_prompt.uri}")
print(f"Optimized template: {optimized_prompt.template}")

# Since your chain already uses @latest, it will automatically use the optimized prompt
predict_fn(
    input_language="English",
    output_language="Japanese",
    text="Welcome to MLflow",
)
```

## Key Points for LangChain Integration

### Using `to_single_brace_format()`

LangChain uses single-brace syntax for variables (`{variable}`), while MLflow Prompt Registry uses double-brace syntax (`{{variable}}`). Use <APILink fn="mlflow.entities.model_registry.PromptVersion.to_single_brace_format" >`to_single_brace_format()`</APILink> to convert MLflow prompts to LangChain-compatible format:

```python
# MLflow format: "Hello {{name}}"
# LangChain format: "Hello {name}"

prompt = mlflow.genai.load_prompt("prompts:/my-prompt@latest")
langchain_template = prompt.to_single_brace_format()
```
