---
title: Link Production Traces to App Versions
description: Configure production deployments to link traces to specific LoggedModel versions for version-aware monitoring.
last_update:
  date: 2025-05-23
---

# Link Production Traces to App Versions

When deploying a `LoggedModel` to production, you need to link the traces it generates back to the specific version for monitoring and debugging. This guide shows how to configure your deployment to include version information in production traces.

:::tip[Databricks Model Serving]
**Deploying on Databricks Model Serving?** Trace linking is automatically configured for you. Skip to [Trace Linking on Databricks Model Serving](#trace-linking-on-databricks-model-serving) for details.
:::

## Prerequisites

For production deployments **outside of Databricks Model Serving**, install the `mlflow-tracing` package:

```bash
pip install --upgrade mlflow-tracing
```

This package is specifically optimized for production environments, offering:

- **Minimal dependencies** for faster, leaner deployments
- **Performance optimizations** for high-volume tracing

:::note
MLflow 3 (specifically `mlflow-tracing` for production) is required for production tracing. MLflow 2.x is not supported for production deployments due to performance limitations and missing features essential for production use.
:::

## Environment Variable Configuration

Add the `LoggedModel` ID to your production environment configuration alongside the standard MLflow tracing variables:

```bash
# Standard MLflow tracing configuration (see prod-tracing guide for details)
export DATABRICKS_HOST="https://your-workspace.databricks.com"
export DATABRICKS_TOKEN="your-databricks-token"
export MLFLOW_TRACKING_URI=databricks
export MLFLOW_EXPERIMENT_NAME="/Shared/production-genai-app"

# Add LoggedModel version tracking by specifying your LoggedModel ID
export MLFLOW_ACTIVE_MODEL_ID="m-930352f2b0c246c9a8ac7df98a03cede"
```

## Automatic Trace Linking

:::important
When you set the `MLFLOW_ACTIVE_MODEL_ID` environment variable, **all traces are automatically linked to that LoggedModel**. You don't need to manually tag traces - MLflow handles this for you!
:::

Your application code remains exactly the same as during development:

```python
import mlflow
from fastapi import FastAPI, Request

app = FastAPI()


@mlflow.trace
@app.post("/chat")
def handle_chat(request: Request, message: str):
    # Your traces are automatically linked to the LoggedModel
    # specified in MLFLOW_ACTIVE_MODEL_ID

    # Your application logic here
    response = process_message(message)
    return {"response": response}
```

To add additional context to your traces (such as user IDs, session IDs, or custom metadata), see [Adding context to production traces](/genai/tracing/prod-tracing#adding-context-to-production-traces) in the production tracing guide.

## Deployment Examples

### Docker

When deploying with Docker, pass all necessary environment variables through your container configuration:

```dockerfile
# Dockerfile
FROM python:3.9-slim

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application code
COPY . /app
WORKDIR /app

# Declare required environment variables (no defaults)
ENV DATABRICKS_HOST
ENV DATABRICKS_TOKEN
ENV MLFLOW_TRACKING_URI
ENV MLFLOW_EXPERIMENT_NAME
ENV MLFLOW_ACTIVE_MODEL_ID

CMD ["python", "app.py"]
```

Run the container with environment variables:

```bash
docker run -d \
  -e DATABRICKS_HOST="https://your-workspace.databricks.com" \
  -e DATABRICKS_TOKEN="your-databricks-token" \
  -e MLFLOW_TRACKING_URI=databricks \
  -e MLFLOW_EXPERIMENT_NAME="/Shared/production-genai-app" \
  -e MLFLOW_ACTIVE_MODEL_ID="m-930352f2b0c246c9a8ac7df98a03cede" \
  -e APP_VERSION="1.0.0" \
  your-app:latest
```

### Kubernetes

For Kubernetes deployments, use ConfigMaps and Secrets to manage configuration:

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mlflow-config
data:
  DATABRICKS_HOST: "https://your-workspace.databricks.com"
  MLFLOW_TRACKING_URI: "databricks"
  MLFLOW_EXPERIMENT_NAME: "/Shared/production-genai-app"
  MLFLOW_ACTIVE_MODEL_ID: "m-930352f2b0c246c9a8ac7df98a03cede"

---
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: databricks-secrets
type: Opaque
stringData:
  DATABRICKS_TOKEN: "your-databricks-token"

---
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: genai-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: genai-app
  template:
    metadata:
      labels:
        app: genai-app
    spec:
      containers:
        - name: app
          image: your-app:latest
          ports:
            - containerPort: 8000
          envFrom:
            - configMapRef:
                name: mlflow-config
            - secretRef:
                name: databricks-secrets
          env:
            - name: APP_VERSION
              value: "1.0.0"
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
```

## Querying Version-Specific Traces

Once deployed, you can query traces by model version:

```python
import mlflow
from mlflow.client import MlflowClient

client = MlflowClient()

# Get the experiment ID
experiment = client.get_experiment_by_name("/Shared/production-genai-app")

# Find all traces from a specific model version
traces = mlflow.search_traces(
    experiment_ids=[experiment.experiment_id],
    model_id="m-930352f2b0c246c9a8ac7df98a03cede",
)
```

## Trace Linking on Databricks Model Serving

When you [deploy a `LoggedModel` to Databricks Model Serving](/genai/prompt-version-mgmt/version-tracking/optionally-package-app-code-and-files-for-databricks-model-serving), trace linking is automatically configured:

- **mlflow-tracing is preinstalled** - No need to add it to your requirements
- **Authentication is automatic** - The serving environment handles all Databricks authentication
- **Model version is tracked** - Traces are automatically tagged with the deployed model version
- **No environment variables needed** - Everything is configured by the platform

To view traces from your Databricks Model Serving endpoint:

1. Navigate to your Model Serving endpoint in the Databricks UI
2. Click on the "Monitoring" tab to view traces and metrics
3. All traces are automatically linked to the specific model version serving the requests

<!-- TODO: add screenshot: Databricks Model Serving endpoint Monitoring tab; Traces linked to the deployed LoggedModel version; Model version information displayed in the UI -->

The only requirement is that your application code uses MLflow tracing (either through autologging or manual instrumentation with `@mlflow.trace`).

## Next Steps

For complete production tracing setup including authentication, monitoring, and feedback collection for deployments outside Databricks Model Serving, see **[Production observability with tracing](/genai/tracing/prod-tracing)**.
