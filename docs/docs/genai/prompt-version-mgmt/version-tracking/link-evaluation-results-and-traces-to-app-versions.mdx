---
title: Link Evaluation Results and Traces to App Versions
description: Use mlflow.genai.evaluate to evaluate specific LoggedModel versions and automatically link the results.
---

# Link Evaluation Results and Traces to App Versions

After [tracking application versions with MLflow](/genai/prompt-version-mgmt/version-tracking/track-application-versions-with-mlflow), you can evaluate specific versions using `mlflow.genai.evaluate()`. This guide shows how to link evaluation results to your `LoggedModel` versions.

## Why Link Evaluations to Versions?

When you evaluate a specific `LoggedModel` version:

- **All evaluation metrics, traces, and results are automatically linked** to that version
- **Compare versions objectively** with consistent evaluation datasets
- **Track quality over time** as you iterate on prompts and code

## Quick Example: Evaluating Your Customer Support Agent

Continuing from the previous guide where we versioned our customer support agent, let's evaluate it using the `model_id` we obtained:

```python
import mlflow
import pandas as pd
from mlflow.genai.scorers import relevance_to_query, guideline_adherence

# Use the model_id from the previous guide's active_model_info
model_id = active_model_info.model_id  # e.g., "m-930352f2b0c246c9a8ac7df98a03cede"

# Define evaluation dataset with customer support queries
eval_data = pd.DataFrame(
    {
        "inputs": [
            {"question": "How do I reset my password?"},
            {"question": "What are your business hours?"},
            {"question": "Can I get a refund for my last order?"},
            {"question": "Where can I find the user manual for product X?"},
            {"question": "I'm having trouble with the payment page, can you help?"},
        ]
    }
)

# Define scorers for customer support quality
# 1. Check if response is relevant to the question
relevance_scorer = relevance_to_query

# 2. Ensure responses follow support guidelines
support_guidelines = guideline_adherence.with_config(
    name="support_guidelines",
    global_guidelines=[
        "Be polite and professional in all responses",
        "Provide clear, actionable steps when possible",
        "Acknowledge the customer's concern before providing a solution",
    ],
)

# Run evaluation linked to the specific model version
evaluation_results = mlflow.genai.evaluate(
    data=eval_data,
    model_id=model_id,  # Links results to this specific LoggedModel version
    scorers=[relevance_scorer, support_guidelines],
)

# View results
print(f"Evaluation completed for model {model_id}")
print(
    f"Average relevance: {evaluation_results.metrics.get('relevance_to_query_mean', 'N/A'):.2f}"
)
print(
    f"Support guidelines adherence: {evaluation_results.metrics.get('support_guidelines_mean', 'N/A'):.2f}"
)
```

## Accessing Evaluation Results

The evaluation results are automatically linked to your `LoggedModel` version. Here's how to access them:

### 1. View Evaluation Metrics on the LoggedModel

The evaluation metrics are stored directly on the `LoggedModel`:

```python
# Fetch the LoggedModel to see evaluation metrics
logged_model = mlflow.get_logged_model(model_id=model_id)

# The evaluation metrics are available in the model's metrics
print("\n=== Evaluation Metrics on LoggedModel ===")
for metric_key, metric_value in logged_model.metrics.items():
    if metric_key.startswith(("relevance_to_query", "support_guidelines")):
        print(f"{metric_key}: {metric_value}")
```

### 2. Find Evaluation Traces Linked to the LoggedModel

Each evaluation run generates traces that are automatically linked to the `LoggedModel`:

```python
# Search for traces linked to this specific model version
# Return as list of Trace objects instead of DataFrame
traces = mlflow.search_traces(
    experiment_ids=[logged_model.experiment_id],
    model_id=model_id,  # Find traces for this specific LoggedModel version
    return_type="list",  # Get Trace objects instead of DataFrame
)

print(f"\n=== Traces linked to model {model_id} ===")
print(f"Found {len(traces)} traces")

# Access properties through the trace.info object
for i, trace in enumerate(traces[:3]):
    print(f"\nTrace {i+1}:")
    print(f"  Trace ID: {trace.info.trace_id}")
    print(f"  State: {trace.info.state}")
    print(f"  Duration: {trace.info.execution_duration}ms")
    if trace.info.request_preview:
        print(f"  Request preview: {trace.info.request_preview[:100]}...")
    if trace.info.response_preview:
        print(f"  Response preview: {trace.info.response_preview[:100]}...")
```

### 3. Access Detailed Evaluation Results

The evaluation also returns detailed results you can analyze:

```python
# Access the evaluation results table
results_df = evaluation_results.tables["eval_results_table"]

print("\n=== Detailed Evaluation Results ===")
# Show individual scores for each test case
for idx, row in results_df.iterrows():
    question = row["inputs"]["question"]
    relevance_score = row.get("relevance_to_query", "N/A")
    guidelines_score = row.get("support_guidelines", "N/A")
    print(f"\nQ: {question}")
    print(f"   Relevance: {relevance_score}")
    print(f"   Guidelines: {guidelines_score}")
```

### 4. View in MLflow UI

In the MLflow UI:

1. Navigate to your experiment and find the `LoggedModel`
2. Click on the model to see evaluation metrics in the metrics tab
3. View linked traces in the traces tab
4. Compare evaluation results across different model versions

<!-- TODO: add screenshots: LoggedModel with evaluation metrics in the MLflow UI; Linked traces tab showing evaluation traces; Model comparison view with evaluation results -->

## Next Steps

- **[Compare multiple app versions](/genai/prompt-version-mgmt/version-tracking/compare-app-versions)** side-by-side using evaluation results
- **[Deploy your evaluated version](/genai/prompt-version-mgmt/version-tracking/optionally-package-app-code-and-files-for-databricks-model-serving)** to Databricks Model Serving
- **[Create evaluation datasets](/genai/eval-monitor/concepts/eval-datasets)** for consistent testing across versions

For a complete guide to evaluation including custom scorers and advanced features, see **[Evaluating a version of your app](/genai/eval-monitor/evaluate-app)**.
