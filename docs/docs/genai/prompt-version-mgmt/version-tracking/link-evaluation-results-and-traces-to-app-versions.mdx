---
title: Link Evaluation Results and Traces to App Versions
description: Use mlflow.genai.evaluate to evaluate specific LoggedModel versions and automatically link the results.
---

# Link Evaluation Results and Traces to App Versions

After [tracking application versions with MLflow](/genai/prompt-version-mgmt/version-tracking/track-application-versions-with-mlflow), you can evaluate specific versions using `mlflow.genai.evaluate()`. This guide shows how to link evaluation results to your `LoggedModel` versions.

## Why Link Evaluations to Versions?

When you evaluate a specific `LoggedModel` version:

- **All evaluation metrics, traces, and results are automatically linked** to that version
- **Compare versions objectively** with consistent evaluation datasets
- **Track quality over time** as you iterate on prompts and code

## Quick Example: Evaluating Your Customer Support Agent

:::note Only on Databricks
The GenAI Evaluate feature is only available on Databricks Managed MLflow.
:::

Continuing from the previous guide where we versioned our customer support agent, let's evaluate it using the `model_id` we obtained:

```python
import mlflow
import pandas as pd
from mlflow.genai.scorers import RelevanceToQuery, Guidelines

# Define evaluation dataset with customer support queries
eval_data = pd.DataFrame(
    {
        "inputs": [
            {"question": "How do I reset my password?"},
            {"question": "What are your business hours?"},
            {"question": "Can I get a refund for my last order?"},
            {"question": "Where can I find the user manual for product X?"},
            {"question": "I'm having trouble with the payment page, can you help?"},
        ]
    }
)

# Define scorers for customer support quality
# 1. Check if response is relevant to the question
relevance_scorer = RelevanceToQuery()

# 2. Ensure responses follow support guidelines
support_guidelines = Guidelines(
    name="support_guidelines",
    guidelines=[
        "Be polite and professional in all responses",
        "Provide clear, actionable steps when possible",
        "Acknowledge the customer's concern before providing a solution",
    ],
)

model_id = active_model_info.model_id
# Run evaluation linked to the specific model version
evaluation_results = mlflow.genai.evaluate(
    data=eval_data,
    model_id=model_id,  # Links results to this specific LoggedModel version
    scorers=[relevance_scorer, support_guidelines],
)

# View results
print(f"Evaluation completed for model {model_id}")
print(f"Metrics: {evaluation_results.metrics}")
```

## Accessing Evaluation Results

The evaluation results are automatically linked to your `LoggedModel` version. Here's how to access them:

### 1. View Evaluation Metrics on the LoggedModel

The evaluation metrics are stored directly on the `LoggedModel`:

```python
# Fetch the LoggedModel to see evaluation metrics
logged_model = mlflow.get_logged_model(model_id=model_id)

# The evaluation metrics are available in the model's metrics
print("\n=== Evaluation Metrics on LoggedModel ===")
for metric in logged_model.metrics:
    print(f"{metric.key}: {metric.value}")
```

### 2. Find Evaluation Traces Linked to the LoggedModel

Each evaluation run generates traces that are automatically linked to the `LoggedModel`:

```python
# Search for traces linked to this specific model version
# Return as list of Trace objects instead of DataFrame
traces = mlflow.search_traces(
    experiment_ids=[logged_model.experiment_id],
    model_id=model_id,  # Find traces for this specific LoggedModel version
    return_type="list",  # Get Trace objects instead of DataFrame
)

print(f"\n=== Traces linked to model {model_id} ===")
print(f"Found {len(traces)} traces")

# Access properties through the trace.info object
for i, trace in enumerate(traces[:3]):
    print(f"\nTrace {i+1}:")
    print(f"  Trace ID: {trace.info.trace_id}")
    print(f"  Status: {trace.info.status}")
    print(f"  Duration: {trace.info.execution_duration}ms")
    if trace.info.request_preview:
        print(f"  Request preview: {trace.info.request_preview[:100]}...")
    if trace.info.response_preview:
        print(f"  Response preview: {trace.info.response_preview[:100]}...")
```

### 3. Access Detailed Evaluation Results

The evaluation also returns detailed results you can analyze:

```python
# Access the evaluation results table
results_df = evaluation_results.tables["eval_results"]

print("\n=== Detailed Evaluation Results ===")
# Show individual scores for each test case
for idx, row in results_df.iterrows():
    question = row["request"]["question"]
    assessments = row["assessments"]
    results = {assessment["name"]: assessment["value"] for assessment in assessments}
    print(f"\nQ: {question}")
    print(f"   Eval result: {results}")
```

### 4. View in Databricks UI

In the Databricks UI:

1. Navigate to your experiment and find the `LoggedModel`
2. Click on the model to see evaluation metrics in the metrics tab
3. View linked traces in the traces tab
4. Compare evaluation results across different model versions

<!-- TODO: add screenshots: LoggedModel with evaluation metrics in the MLflow UI; Linked traces tab showing evaluation traces; Model comparison view with evaluation results -->

## Next Steps

- **[Compare multiple app versions](/genai/prompt-version-mgmt/version-tracking/compare-app-versions)** side-by-side using evaluation results
- **[Create evaluation datasets](/genai/eval-monitor/concepts/eval-datasets)** for consistent testing across versions

For a complete guide to evaluation including custom scorers and advanced features, see **[Evaluating a version of your app](/genai/eval-monitor/evaluate-app)**.
