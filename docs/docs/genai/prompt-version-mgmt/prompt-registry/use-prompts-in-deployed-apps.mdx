---
title: Use Prompts in Deployed Apps
description: Learn how to effectively use prompts from the MLflow Prompt Registry in your deployed GenAI applications, leveraging aliases for dynamic updates and A/B testing.
---

# Use Prompts in Deployed Apps

:::danger Placeholder
TODO: add coming soon for databricks docs
:::

Integrating versioned prompts from the MLflow Prompt Registry into your deployed GenAI applications requires careful consideration of versioning strategies, dynamic updates, and risk management. This page focuses on best practices for using prompts in a production environment, with a strong emphasis on leveraging aliases for flexibility and control.

You will learn about:

- Configuring production applications to use registry prompts, particularly via aliases.
- Strategies for prompt versioning in deployment (pinning vs. aliases).
- How to update prompts referenced by aliases without redeploying your application.
- Monitoring prompt usage in production (conceptually).
- Utilizing aliases for A/B testing different prompt versions and for facilitating rollbacks.

## Configuring Production Applications to Use Registry Prompts

When your GenAI application is deployed to a production environment, it should load its prompts from the MLflow Prompt Registry. The recommended way to do this is by referencing a **prompt alias** rather than a specific version number in your application's configuration or code.

Recall that an alias (e.g., `production`, `live-summary-prompt`) is a mutable pointer to a specific, immutable prompt version. By using an alias, you can change the underlying prompt version that your deployed application uses _without needing to redeploy the application itself_.

**Example: Application Configuration**

Your application might have a configuration file (e.g., YAML, JSON, or environment variables) that specifies which prompt alias to use:

```yaml
# Example application_config.yaml
# ... other configurations ...
prompt_uris:
  customer_greeting: prompts:/chat-greetings/production
  issue_summarizer: prompts:/ticket-summaries/production
# ... other configurations ...
```

Your application code would then load this configuration and use the specified URIs with `mlflow.load_prompt()`:

```python
# Placeholder for Python code example
# Goal: Show application code loading a prompt URI from a config and using it.
# Steps:
# 1. Import mlflow and a config loading library (e.g., yaml).
# 2. Conceptually load a configuration that contains a prompt URI with an alias.
# 3. Use mlflow.load_prompt() with the URI from the config.
# 4. (Optional) Format and use the prompt.
# Example:
# import mlflow
# import yaml # Assuming a yaml config file

# def load_app_config(config_path="application_config.yaml"):
#     # In a real app, load this securely and robustly
#     # For this example, assume it returns a dict like the YAML above
#     # This is a conceptual placeholder for config loading
#     mock_config = {
#         'prompt_uris': {
#             'customer_greeting': "prompts:/chat-greetings/production"
#         }
#     }
#     return mock_config

# app_config = load_app_config()
# greeting_prompt_uri = app_config['prompt_uris']['customer_greeting']

# try:
#     greeting_prompt = mlflow.load_prompt(uri=greeting_prompt_uri)
#     print(f"Successfully loaded prompt for customer greeting: '{greeting_prompt.name}' version {greeting_prompt.version} (via alias)")
#     # resolved_greeting = greeting_prompt.format(customer_name="Alex")
#     # print(f"Formatted greeting: {resolved_greeting}")
# except Exception as e:
#     print(f"Error loading/using production prompt: {e}")
```

## Prompt Versioning Strategies for Deployment

You have two main strategies for managing which prompt versions your deployed application uses:

1.  **Pinning to Specific Versions**: Your application code or configuration explicitly references a specific version number (e.g., `prompts:/my-prompt/3`).
    - **Pros**: Maximum stability and predictability. You know exactly which version is running.
    - **Cons**: Updating the prompt requires a code change and redeployment of your application.
2.  **Using Aliases (Recommended for most production scenarios)**: Your application code or configuration references an alias (e.g., `prompts:/my-prompt/production`).
    - **Pros**: Flexibility. You can update the prompt version by simply changing where the alias points in the MLflow Prompt Registry, without any application code changes or redeployment. This allows for rapid iteration and hotfixes.
    - **Cons**: Requires careful management of aliases to ensure the correct version is live. Changes to aliases should be audited.

For most production use cases, **using aliases is the preferred method** due to the operational flexibility it offers.

## Updating Prompts via Aliases Without Redeployment

This is one of the most powerful features of using aliases with the Prompt Registry.

**Scenario**: You have a prompt `invoice-parser` and the alias `production` currently points to version `2`. Your application is configured to use `prompts:/invoice-parser/production`.

1.  **Develop and Test a New Version**: You create and thoroughly test version `3` of `invoice-parser`, which significantly improves accuracy.
2.  **Update the Alias**: Using the MLflow UI or the `mlflow.set_prompt_alias()` Python API, you change the `production` alias for `invoice-parser` to now point to version `3`.

    ```python
    # Placeholder for Python code example
    # Goal: Show how to update an alias to point to a new prompt version.
    # Steps:
    # 1. Import mlflow.
    # 2. Get an MlflowClient instance.
    # 3. Specify the prompt name, the new version, and the alias name.
    # 4. Call client.set_prompt_alias().
    # Example:
    # import mlflow
    # client = mlflow.tracking.MlflowClient()
    # prompt_name = "invoice-parser"
    # new_production_version = "3"
    # alias_name = "production"
    # try:
    #     client.set_prompt_alias(name=prompt_name, alias=alias_name, version=new_production_version)
    #     print(f"Alias '{alias_name}' for prompt '{prompt_name}' now points to version '{new_production_version}'.")
    # except Exception as e:
    #     print(f"Error updating alias: {e}")
    ```

3.  **Application Behavior**: The next time your deployed application instances (which might have caching, see below) load the prompt `prompts:/invoice-parser/production`, they will automatically fetch version `3` without any code changes or restarts.

**Considerations for Caching**: Deployed applications might cache loaded prompts for performance. Ensure your application has a strategy to periodically refresh cached prompts or a mechanism to trigger a refresh if immediate updates are critical.

## Monitoring Prompt Usage in Production

While the Prompt Registry manages the prompts themselves, monitoring their usage and performance in production relies on your application's observability stack, ideally integrated with MLflow Tracing.

- **Log Prompt Identifiers**: Ensure your application logs include the specific prompt name and version (or the alias used and the version it resolved to) for each transaction.
- **Trace Integration**: If using MLflow Tracing, the `mlflow.load_prompt()` call can automatically add metadata to your traces about the prompt URI used. This allows you to correlate application behavior, LLM outputs, and performance metrics (latency, cost) with specific prompt versions in the MLflow Tracing UI.
- **Performance Metrics**: Track LLM response times, token counts, and error rates. Segment these metrics by prompt version to identify if a new prompt version has impacted performance.
- **Quality Metrics**: Collect user feedback (e.g., thumbs up/down, explicit feedback) and run offline evaluations or online monitoring with LLM judges on production traffic, associating results back to the prompt versions used.

<!-- TODO(image): Conceptual diagram showing a deployed app, requests coming in, app loading prompt via alias from Registry, interaction with LLM, and traces/logs capturing prompt version info. -->

## Utilizing Aliases for A/B Testing and Rollbacks

Aliases are also invaluable for advanced deployment strategies:

- **A/B Testing**: You can introduce a new prompt version to a subset of users or traffic. For example, have a separate alias like `experimental-parser` pointing to version `4`. Your application or load balancer could route a percentage of requests to use this experimental alias, allowing you to compare its performance against the main `production` alias before a full rollout.
- **Easy Rollbacks**: If a newly promoted prompt version (e.g., version `3` pointed to by `production`) is found to have issues, you can quickly roll back by simply updating the `production` alias to point back to the previously stable version (e.g., version `2`). This can be done in seconds without a stressful redeployment.

<!-- TODO(image): Diagram illustrating A/B testing with two aliases (`production` and `experimental`) pointing to different prompt versions, with traffic split between them. -->

## Key Takeaways

- Using **aliases** (e.g., `prompts:/my-prompt/production`) in deployed applications is highly recommended for dynamic prompt updates without redeployment.
- Updating an alias in the MLflow Prompt Registry allows for hot-swapping prompt versions in live applications.
- Careful alias management and auditing are crucial.
- Integrate prompt version information into your production logging and MLflow Tracing for effective monitoring.
- Aliases facilitate A/B testing of new prompt versions and enable rapid rollbacks if issues arise.
- Be mindful of prompt caching strategies in your deployed applications.

## Prerequisites

- A production-ready GenAI application.
- An established deployment pipeline.
- MLflow Prompt Registry populated with versioned prompts and aliases.
- (Recommended) MLflow Tracing implemented in your application for monitoring.

By strategically using prompts from the registry, especially with aliases, you can significantly improve the agility, reliability, and maintainability of your deployed GenAI applications.
