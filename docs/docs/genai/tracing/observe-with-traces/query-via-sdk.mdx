import DatabricksCallout from "@site/src/components/DatabricksCallout"
import { APILink } from "@site/src/components/APILink";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Query Traces via SDK

<DatabricksCallout docsPath="/mlflow3/genai/tracing/observe-with-traces/query-via-sdk" />

This guide shows you how to programmatically query traces using the MLflow SDK for debugging, monitoring, and analysis. While the [Search Traces](/genai/tracing/search-traces) guide covers comprehensive search functionality, this page focuses on practical SDK usage patterns for observability workflows.

## Why Query Traces Programmatically?

The MLflow UI is great for interactive exploration, but programmatic access enables automation and integration with your existing workflows. You can analyze error patterns across thousands of traces, monitor performance trends over time, create evaluation datasets from real production data, and build custom alerts for your monitoring systems.

## Basic Query Methods

MLflow provides two main ways to query traces programmatically:

<Tabs>
  <TabItem value="search-traces" label="mlflow.search_traces()" default>
    Returns a pandas DataFrame that's perfect for data analysis and quick insights:

    ```python
    import mlflow

    # Get recent error traces as a DataFrame
    error_traces = mlflow.search_traces(
        experiment_ids=["1"], filter_string="status = 'ERROR'", max_results=100
    )

    # Easy analysis with pandas
    print(f"Found {len(error_traces)} errors")
    print(f"Average execution time: {error_traces['execution_time_ms'].mean():.1f}ms")
    ```

    **Best for:** Quick analysis, data exploration, creating charts and reports
  </TabItem>
  <TabItem value="client-api" label="MlflowClient.search_traces()" default>
    Returns full trace objects with detailed span information:

    ```python
    from mlflow import MlflowClient

    client = MlflowClient()
    traces = client.search_traces(
        experiment_ids=["1"], filter_string="status = 'ERROR'", max_results=100
    )

    # Access detailed trace information
    for trace in traces:
        print(f"Trace {trace.info.trace_id}: {trace.info.status}")
        for span in trace.data.spans:
            if span.status.status_code == "ERROR":
                print(f"  Error in span: {span.name}")
    ```

    **Best for:** Detailed inspection, accessing span data, building monitoring tools
  </TabItem>
</Tabs>

## Common Use Cases

### Finding and Analyzing Errors

When you need to understand what's going wrong in your application, start with simple queries to identify patterns:

```python
import mlflow
from datetime import datetime, timedelta

# Get errors from the last 24 hours
yesterday = datetime.now() - timedelta(days=1)
timestamp_ms = int(yesterday.timestamp() * 1000)

error_traces = mlflow.search_traces(
    experiment_ids=["1"],
    filter_string=f"status = 'ERROR' AND timestamp_ms > {timestamp_ms}",
    order_by=["timestamp_ms DESC"],
)

print(f"Found {len(error_traces)} errors in the last 24 hours")

# Look at error distribution by tags
if not error_traces.empty:
    # Count errors by user if user tags exist
    if "tags" in error_traces.columns:
        print("\nError patterns:")
        tag_analysis = {}
        for tags in error_traces["tags"].dropna():
            if isinstance(tags, dict):
                user = tags.get("user_id", "unknown")
                tag_analysis[user] = tag_analysis.get(user, 0) + 1

        for user, count in sorted(
            tag_analysis.items(), key=lambda x: x[1], reverse=True
        )[:5]:
            print(f"  {user}: {count} errors")
```

### Performance Monitoring

Track how your application performs over time and identify bottlenecks:

```python
# Get successful traces to analyze performance
recent_traces = mlflow.search_traces(
    experiment_ids=["1"],
    filter_string="status = 'OK'",
    order_by=["timestamp_ms DESC"],
    max_results=1000,
)

if not recent_traces.empty:
    # Basic performance metrics
    avg_time = recent_traces["execution_time_ms"].mean()
    p95_time = recent_traces["execution_time_ms"].quantile(0.95)

    print(f"Average execution time: {avg_time:.1f}ms")
    print(f"95th percentile: {p95_time:.1f}ms")

    # Find slowest traces
    slow_traces = recent_traces[recent_traces["execution_time_ms"] > p95_time]
    print(f"Found {len(slow_traces)} slow traces (>{p95_time:.1f}ms)")
```

### User Session Analysis

Understand how users interact with your application by analyzing their sessions:

```python
# Analyze traces for a specific user
user_traces = mlflow.search_traces(
    experiment_ids=["1"],
    filter_string="tags.user_id = 'user123'",
    order_by=["timestamp_ms ASC"],
)

if not user_traces.empty:
    print(f"User has {len(user_traces)} interactions")

    # Calculate session metrics
    total_time = user_traces["execution_time_ms"].sum()
    error_count = len(user_traces[user_traces["status"] == "ERROR"])

    print(f"Total execution time: {total_time:.1f}ms")
    print(
        f"Error rate: {error_count}/{len(user_traces)} ({error_count/len(user_traces)*100:.1f}%)"
    )

    # Show recent activity
    print("\nRecent traces:")
    for _, trace in user_traces.tail(3).iterrows():
        status = "âœ…" if trace["status"] == "OK" else "âŒ"
        print(f"  {status} {trace['execution_time_ms']:.1f}ms")
```

## Building Evaluation Datasets

Use real production traces to create evaluation datasets that reflect actual user interactions:

```python
# Get successful traces from recent production usage
production_traces = mlflow.search_traces(
    experiment_ids=["1"],
    filter_string="status = 'OK'",
    extract_fields=["llm_call.inputs.messages", "llm_call.outputs.response"],
    max_results=500,
)

# Convert to evaluation format
eval_examples = []
for _, trace in production_traces.iterrows():
    messages = trace.get("llm_call.inputs.messages")
    response = trace.get("llm_call.outputs.response")

    if messages and response:
        # Extract user message
        user_message = ""
        if isinstance(messages, list):
            for msg in messages:
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")
                    break

        if user_message:
            eval_examples.append(
                {
                    "input": user_message,
                    "output": response,
                    "trace_id": trace.get("request_id"),
                }
            )

print(f"Created evaluation dataset with {len(eval_examples)} examples")

# Save for later use
import pandas as pd

eval_df = pd.DataFrame(eval_examples)
eval_df.to_csv("evaluation_dataset.csv", index=False)
```

## Simple Monitoring Setup

Create basic monitoring that alerts you to issues:

```python
def check_app_health(experiment_ids, alert_threshold_error_rate=0.05):
    """Simple health check for trace data"""

    # Get traces from last hour
    one_hour_ago = int((datetime.now() - timedelta(hours=1)).timestamp() * 1000)

    recent_traces = mlflow.search_traces(
        experiment_ids=experiment_ids,
        filter_string=f"timestamp_ms > {one_hour_ago}",
        max_results=1000,
    )

    if recent_traces.empty:
        print("âš ï¸  No traces found in the last hour")
        return

    # Calculate basic metrics
    total_traces = len(recent_traces)
    error_count = len(recent_traces[recent_traces["status"] == "ERROR"])
    error_rate = error_count / total_traces

    print(f"Last hour: {total_traces} traces, {error_count} errors ({error_rate:.1%})")

    # Alert on high error rate
    if error_rate > alert_threshold_error_rate:
        print(
            f"ðŸš¨ HIGH ERROR RATE: {error_rate:.1%} (threshold: {alert_threshold_error_rate:.1%})"
        )
    else:
        print("âœ… Error rate within normal range")

    # Show average performance
    successful_traces = recent_traces[recent_traces["status"] == "OK"]
    if not successful_traces.empty:
        avg_time = successful_traces["execution_time_ms"].mean()
        print(f"Average response time: {avg_time:.1f}ms")


# Run health check
check_app_health(["1"])
```

## Best Practices

**Start Simple**: Begin with basic queries and gradually add complexity as needed. Most monitoring can be done with simple filters and pandas operations.

**Use Time Windows**: Always filter by timestamp when analyzing recent data to keep queries fast and relevant.

**Handle Missing Data**: Production traces may have missing fields, so always check if data exists before using it.

**Keep Queries Focused**: Use specific filters to get only the data you need rather than retrieving everything and filtering in memory.

## Error Handling

Add basic error handling to make your scripts more robust:

```python
def safe_trace_query(experiment_ids, filter_string=None):
    """Query traces with basic error handling"""
    try:
        return mlflow.search_traces(
            experiment_ids=experiment_ids, filter_string=filter_string
        )
    except Exception as e:
        print(f"Error querying traces: {e}")
        return pd.DataFrame()  # Return empty DataFrame on error


# Usage
traces = safe_trace_query(["1"], "status = 'ERROR'")
if not traces.empty:
    print(f"Found {len(traces)} traces")
else:
    print("No traces found or query failed")
```

## Summary

Programmatic trace querying with the MLflow SDK enables powerful automation for debugging, monitoring, and analysis. Start with simple queries to understand your data, then build up to more sophisticated monitoring and evaluation workflows.

The key is to focus on actionable insights that help you understand and improve your application's behavior in production.

## Next Steps

**[MLflow Tracing UI](/genai/tracing/observe-with-traces/ui)**: Learn the web interface for interactive trace exploration

**[Search Traces](/genai/tracing/search-traces)**: Master advanced search and filtering techniques

**[Delete and Manage Traces](/genai/tracing/observe-with-traces/delete-traces)**: Understand trace lifecycle management