import FeatureHighlights from '@site/src/components/FeatureHighlights';
import ImageBox from '@site/src/components/ImageBox';
import useBaseUrl from '@docusaurus/useBaseUrl';
import { CircleCheck, CircleArrowRight, CircleArrowLeft } from "lucide-react";

# OpenTelemetry Integration

[OpenTelemetry](https://opentelemetry.io/) is a CNCF-backed project that provides a vendor-neutral observability APIs and SDKs to instrument your applications and collect telemetry data in a consistent way. MLflow Tracing is fully compatible with OpenTelemetry, making it free from vendor lock-in.

<div style={{ justifyContent: 'center', display: 'flex', margin: '2rem 0'}}>
  <img src={useBaseUrl('/images/llms/tracing/opentelemetry/hero.png')} alt="OpenTelemetry" width="90%" />
</div>

<FeatureHighlights col={1} features={[
  {
    icon: CircleArrowRight,
    title: "Ingest OpenTelemetry Traces into MLflow",
    description: "MLflow Server exposes /v1/trace OTLP endpoint. This endpoint allows you to collect traces from your applications written in any language that supports OpenTelemetry protocol, such as Java, Go, Rust, etc."
  },
  {
    icon: CircleArrowLeft,
    title: "Export MLflow Traces to OpenTelemetry Backends",
    description: "Traces generated by MLflow SDK are fully compatible with OpenTelemetry traces spec, allowing you to export traces to any observability platform that supports OpenTelemetry protocol, such as Datadog, Grafana, Prometheus, etc."
  },
  {
    icon: CircleCheck,
    title: "Understand Semantic Conventions",
    description: "MLflow understands the popular OpenTelemetry semantic conventions for GenAI, such as GenAI Semantic Conventions, OpenInference, OpenLLMetry, etc. Traces generated with these conventions will be treated as first-class citizens in MLflow and can be pipelined to other MLflow features."
  },
]} />

## OpenTelemetry native MLflow Tracing SDK

To get started with vendor-neutral tracing quickly, your can use the OpenTelemetry native MLflow Tracing SDK. The SDK provides a convenient one-line auto-tracing experience for [popular GenAI libraries](/genai/tracing/integrations) and enhance the general OpenTelemetry traces with rich AI-specific metadata such as prompts, token usage, model name, etc. See [Quickstart](/genai/tracing/quickstart/python-openai) for getting started with MLflow Tracing SDK.

```python
import mlflow
from openai import OpenAI

mlflow.openai.autolog()

client = OpenAI()
response = client.responses.create(model="gpt-5", input="Hello, world!")
```

MLflow Tracing SDK also works seamlessly with existing applications that are already instrumented with OpenTelemetry. Enhance your existing telemetry for HTTP frameworks, databases, network calls, etc, with MLflow's AI tracing capabilities.

## Ingest OpenTelemetry Traces into MLflow

MLflow Server exposes `/v1/trace` ([OTLP](https://opentelemetry.io/docs/specs/otlp/)) endpoint. This endpoint allows you to collect traces from your applications written in any language that supports OpenTelemetry protocol, such as Java, Go, Rust, etc.

See [Collect OpenTelemetry Traces into MLflow](/genai/tracing/opentelemetry/ingest) for more details on how to collect traces into MLflow Server.

## Export MLflow Traces/Metrics via OTLP

MLflow Traces and Metrics can be exported to other OpenTelemetry-compatible backends such as Datadog, Grafana, Prometheus, etc, to integrate with your existing observability platform. You can also use [dual export](/genai/tracing/opentelemetry/export#dual-export) to send traces to both MLflow and OpenTelemetry simultaneously.

See [Export MLflow Traces/Metrics via OTLP](/genai/tracing/opentelemetry/export) for more details.
