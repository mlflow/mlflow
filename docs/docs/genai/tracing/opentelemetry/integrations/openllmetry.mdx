import ImageBox from '@site/src/components/ImageBox';

# Ingesting OpenLLMetry Traces into MLflow

[OpenLLMetry](https://github.com/traceloop/openllmetry/tree/main/packages) is a set of instrumentation libraries for LLMs, vector databases, and GenAI frameworks. With MLflow OpenTelemetry endpoint, you can ingest OpenLLMetry traces info MLflow and visualize, evaluate, and optimize your GenAI applications.

:::tip

For tracing frameworks and libraries that are already supported by MLflow, use <ins>[native integrations](/genai/tracing/integrations)</ins> for easier setup.

:::

## Step 1. Start MLflow tracking Server

Install the [mlflow](https://pypi.org/project/mlflow/) Python package and start MLflow Tracking Server with SQL-based backend store. The following command starts MLflow Server with SQLite backend store:

```bash
mlflow server --backend-store-uri sqlite:///mlflow.db --port 5000
```

Alternatively, you can use the [Docker Compose](/self-hosting#docker-compose) file to set up MLflow Tracking Server with PostgreSQL backend store.

To use other types of SQL databases such as PostgreSQL, MySQL, and MSSQL, change the store URI according to the [backend store documentation](/self-hosting/architecture/backend-store).

## Step 2. Install Dependencies

```bash
%pip install openai opentelemetry-instrumentation-openai -qU
```

## Step 3: Configure Environment Variables

```python
import os

# Replace with your own MLflow tracking URI and experiment ID.
MLFLOW_TRACKING_URI = "http://localhost:5000/v1/trace"
MLFLOW_EXPERIMENT_ID = "123"

os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = f"{MLFLOW_TRACKING_URI}/v1/trace"
os.environ[
    "OTEL_EXPORTER_OTLP_TRACES_HEADERS"
] = f"x-mlflow-experiment-id={MLFLOW_EXPERIMENT_ID}"

# OpenAI API Key (or other LLM provider API key)
os.environ["OPENAI_API_KEY"] = "<your-api-key>"
```

## Step 4: Instrument Your Application

The following example shows how to instrument the OpenAI SDK. You can also use other [frameworks](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation) supported by OpenInference.

```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.openai import OpenAIInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor


# # Setup OpenTelemetry
tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))
trace.set_tracer_provider(tracer_provider)

# Instrument OpenAI
OpenAIInstrumentor().instrument()
```

## Step 5: Run your application

Let's run a simple example to generate a trace.

```python
import openai

response = openai.OpenAI().responses.create(
    input="What is MLflow Tracing?",
    model="gpt-5",
)
```

## Step 6: View your traces

You can view your traces in MLflow UI. Access to the tracking server URL (e.g., `http://localhost:5000`) in your browser and open the experiment you specified in Step 3.

## Next Steps

- [Collect Human Feedback on Traces](/genai/tracing/collect-user-feedback)
- [Evaluate LLMs and GenAI Applications](/genai/eval-monitor)
- [Production Monitoring](/genai/tracing/prod-tracing)
