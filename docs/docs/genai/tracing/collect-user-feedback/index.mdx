# Collect User Feedback with Assessments

Captuing user feedback is critical for understanding the real-world quality of your GenAI application. MLflow's **Assessments APIs** provide a structured, standardized approach to collecting, storing, and analyzing both user feedback and ground truth labels directly within your traces.

## Why Use Assessments for Feedback?

Assessments APIs offer significant advantages over traditional feedback approaches:

1. **Structured data model** - Standardized format with clear attribution and rationale
2. **Direct trace integration** - Feedback is linked directly to specific application executions
3. **Multiple source types** - Support for human feedback, automated evaluations, and ground truth
4. **Audit trail** - Complete history of feedback changes with override capabilities
5. **Production ready** - Available in OSS MLflow 3.2.0+ with no external dependencies

## Assessment Types for Feedback

Assessments support various feedback formats through a unified API:

| Assessment Value Type    | Description                                | Example Use Cases                    |
| ------------------------ | ------------------------------------------ | ------------------------------------ |
| **Boolean values**       | Simple `True`/`False` assessments         | Thumbs up/down, correct/incorrect    |
| **Numeric scores**       | Integer or float ratings                   | 1-5 star ratings, confidence scores  |
| **Categorical labels**   | String classifications                     | "Helpful", "Neutral", "Unhelpful"    |
| **Structured data**      | Complex objects with multiple dimensions   | Detailed quality breakdowns          |

## Assessments-Based Feedback Collection

### Core Concepts

**Expectations**: Ground truth labels provided by domain experts that represent the "correct" or "desired" outcome for specific interactions.

**Feedback**: Quality evaluations from any source (users, automated systems, or human reviewers) that assess various aspects of application performance.

### Direct Trace Integration

Assessments are directly linked to MLflow traces using trace IDs, creating an immediate connection between application execution and quality assessment:

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType

# Your traced application execution
with mlflow.start_span(name="user_query") as span:
    response = your_llm_application("What is MLflow?")
    trace_id = span.trace_id

# Collect user feedback directly on the trace
mlflow.log_feedback(
    trace_id=trace_id,
    name="user_satisfaction",
    value=True,  # Boolean feedback
    rationale="Response was helpful and accurate",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="user_456"
    ),
)
```

## Implementation Examples

### Simple Boolean Feedback

Collect basic thumbs up/down feedback from users:

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType


def collect_user_feedback(trace_id, is_helpful, user_id):
    """Collect simple boolean feedback from users."""
    mlflow.log_feedback(
        trace_id=trace_id,
        name="is_helpful",
        value=is_helpful,
        rationale="User indicated response was helpful"
        if is_helpful
        else "User indicated response was not helpful",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )


# Usage in your application
trace_id = "your_trace_id_here"
collect_user_feedback(trace_id, True, "user_123")
```

### Detailed Feedback with Ratings

Capture more nuanced feedback with numeric scores:

```python
def collect_detailed_feedback(trace_id, rating, comments, user_id):
    """Collect detailed feedback with ratings and comments."""
    # Log the rating
    mlflow.log_feedback(
        trace_id=trace_id,
        name="quality_rating",
        value=rating,  # 1-5 scale
        rationale=f"User provided rating: {rating}/5. Comments: {comments}",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )

    # Log binary helpfulness based on rating
    is_helpful = rating >= 4
    mlflow.log_feedback(
        trace_id=trace_id,
        name="is_helpful",
        value=is_helpful,
        rationale=f"Derived from rating: {rating}/5",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )


# Usage
collect_detailed_feedback(
    trace_id="your_trace_id",
    rating=4,
    comments="Good response but could be more specific",
    user_id="user_123",
)
```

## Feedback Collection Best Practices

### 1. Minimize User Friction

- **Start simple**: Begin with boolean assessments (thumbs up/down)
- **Progressive enhancement**: Offer optional detailed feedback for engaged users
- **Contextual collection**: Gather feedback immediately after user interactions

### 2. Structure Your Assessments

- **Consistent naming**: Use standardized assessment names across your application
- **Clear rationale**: Always provide meaningful explanations for assessment values
- **Appropriate sources**: Use `HUMAN` source type for all user-generated feedback

### 3. Privacy and Security

- **User anonymization**: Use hashed or pseudonymous identifiers in source_id
- **Data minimization**: Only collect necessary feedback information
- **Retention policies**: Implement appropriate data lifecycle management

### 4. Production Considerations

- **Error handling**: Gracefully handle assessment API failures
- **Rate limiting**: Prevent feedback spam or abuse
- **Async processing**: Consider background processing for non-critical feedback




## Next Steps

- **[Assessments API Guide](/genai/assessments/api-guide)**: Comprehensive guide to all assessment APIs
- **[Assessments Overview](/genai/assessments)**: Understanding expectations, feedback, and sources
- **[Search Traces](/genai/tracing/search-traces)**: Query traces with assessment data
- **[Quality Evaluation](/genai/tracing/quality-with-traces)**: Analyze assessment patterns for quality insights

:::tip
Start with simple boolean assessments and expand to more detailed feedback as you understand your users' needs. The assessments API scales with your requirements while maintaining data consistency.
:::

## Summary

MLflow Assessments provide a modern, structured approach to feedback collection:

- **Structured data model**: Standardized assessments with clear attribution and rationale
- **Direct integration**: Feedback linked directly to trace execution data
- **Production ready**: Available in OSS MLflow 3.2.0+ with comprehensive API support
- **Multiple sources**: Support for human feedback, automated evaluations, and ground truth
- **Quality insights**: Rich data for understanding and improving application performance

<<<<<<< HEAD
By using assessments for feedback collection, you gain a robust foundation for continuous quality improvement that integrates seamlessly with MLflow's tracing and evaluation ecosystem.
=======
User feedback, combined with comprehensive tracing, provides the observability foundation needed to build and maintain high-quality GenAI applications that truly serve user needs.
>>>>>>> assessments
