import FeatureHighlights from "@site/src/components/FeatureHighlights";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { ThumbsUp, BarChart3, Shield, Target, MessageSquare, FileText, Search, TrendingUp, Clock, Users, Zap, Database, Lock } from "lucide-react";

# Collect User Feedback

Capturing user feedback is critical for understanding the real-world quality of your GenAI application. MLflow's **Feedback API** provides a structured, standardized approach to collecting, storing, and analyzing user feedback directly within your traces.

## Why Use MLflow Feedback for User Feedback?

<FeatureHighlights features={[
  {
    icon: Target,
    title: "Direct Trace Integration",
    description: "Feedback is linked directly to specific application executions, creating an immediate connection between user reactions and system performance."
  },
  {
    icon: Shield,
    title: "Structured Data Model",
    description: "Standardized format with clear attribution and rationale ensures consistent feedback collection across your entire application."
  },
  {
    icon: BarChart3,
    title: "Production Ready",
    description: "Available in OSS MLflow 3.2.0+ with no external dependencies, designed for high-throughput production environments."
  },
  {
    icon: ThumbsUp,
    title: "Complete Audit Trail",
    description: "Track every feedback change with timestamps and user attribution, enabling comprehensive quality analysis over time."
  }
]} />

## Step-by-Step Guide: Collecting User Feedback

### 1. Set Up Your GenAI Application with Tracing

First, create a simple application that automatically generates traces using MLflow's OpenAI autologging:

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType
import openai

# Enable automatic tracing for OpenAI calls
mlflow.openai.autolog()

# Initialize your LLM client
client = openai.OpenAI()


def ask_question(question):
    """Simple Q&A application with automatic tracing."""
    # This call is automatically traced by MLflow
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant. Answer questions clearly and concisely.",
            },
            {"role": "user", "content": question},
        ],
        temperature=0.7,
    )

    answer = response.choices[0].message.content
    return answer


# Generate some traces - each call creates a trace automatically
question = "What is machine learning?"
answer = ask_question(question)
print(f"Question: {question}")
print(f"Answer: {answer}")

# You can get the trace ID from the MLflow UI or search API
# For this example, we'll show how to collect feedback programmatically
```

### 2. Collect Simple Thumbs Up/Down Feedback

Implement basic boolean feedback collection. In a real application, you'd get the trace_id from your tracing system:

```python
def collect_thumbs_feedback(trace_id, is_helpful, user_id):
    """Collect simple thumbs up/down feedback from users."""
    mlflow.log_feedback(
        trace_id=trace_id,
        name="user_satisfaction",
        value=is_helpful,
        rationale="User indicated response was helpful"
        if is_helpful
        else "User indicated response was not helpful",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )
    print(f"✓ Feedback recorded: {'👍' if is_helpful else '👎'}")


# Example: Collect feedback on a trace
# In practice, you'd get this trace_id from your application's tracing
trace_id = "tr-abc123def456"  # Replace with actual trace ID
collect_thumbs_feedback(trace_id, True, "user_123")
```

### 3. Add More Detailed Quality Feedback

Collect structured feedback for deeper insights:

```python
def collect_detailed_feedback(trace_id, rating, comments, user_id):
    """Collect detailed feedback with ratings and comments."""
    # Log the numeric rating
    mlflow.log_feedback(
        trace_id=trace_id,
        name="quality_rating",
        value=rating,  # 1-5 scale
        rationale=f"User provided rating: {rating}/5. Comments: {comments}",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )

    # Log structured feedback for analysis
    mlflow.log_feedback(
        trace_id=trace_id,
        name="quality_breakdown",
        value={
            "rating": rating,
            "is_helpful": rating >= 4,
            "category": "excellent" if rating >= 4 else "needs_improvement",
            "user_comments": comments,
        },
        rationale=f"Structured feedback from user rating: {rating}/5",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )

    print(f"✓ Detailed feedback recorded: {rating}/5 stars")


# Example detailed feedback using the same trace ID
collect_detailed_feedback(
    trace_id="tr-abc123def456",  # Same trace from previous step
    rating=4,
    comments="Good explanation, but could use more examples",
    user_id="user_123",
)
```

### 4. View Feedback in MLflow UI

After collecting feedback, you can view it in the MLflow UI:

![Feedback in MLflow UI](/images/assessments/assessments_trace_detail_ui.png)

The trace detail page shows all feedback attached to your traces, making it easy to analyze user satisfaction and identify patterns in your application's performance.

### 5. Adding and Updating Feedback via UI

Users can also provide feedback directly through the MLflow UI:

**Creating New Feedback:**
![Create Feedback](/images/assessments/add_feedback_ui.png)

**Adding Additional Feedback:**
![Additional Feedback](/images/assessments/additional_feedback_ui.png)

This collaborative approach enables both programmatic feedback collection and manual review workflows.

## Feedback Value Types

MLflow feedback supports various formats to match your application's needs:

| Feedback Type   | Description                              | Example Use Cases                   |
| --------------- | ---------------------------------------- | ----------------------------------- |
| **Boolean**     | Simple `True`/`False` feedback           | Thumbs up/down, correct/incorrect   |
| **Numeric**     | Integer or float ratings                 | 1-5 star ratings, confidence scores |
| **Categorical** | String classifications                   | "Helpful", "Neutral", "Unhelpful"   |
| **Structured**  | Complex objects with multiple dimensions | Detailed quality breakdowns         |

## MLflow Feedback Collection Best Practices

<FeatureHighlights features={[
  {
    icon: TrendingUp,
    title: "Start with Boolean Feedback",
    description: "Use MLflow's boolean feedback type for simple thumbs up/down collection. Once you analyze patterns with MLflow's search APIs, expand to numeric ratings or structured feedback types."
  },
  {
    icon: Clock,
    title: "Link Feedback to Fresh Traces",
    description: "Collect feedback immediately after trace generation when the interaction context is available. MLflow's direct trace-feedback linkage ensures you always have the full execution context."
  },
  {
    icon: Database,
    title: "Use Consistent Naming Conventions",
    description: "Standardize feedback names like 'user_satisfaction' or 'quality_rating' across traces. This enables MLflow's search and aggregation features to provide meaningful insights across your application."
  },
  {
    icon: Zap,
    title: "Leverage Structured Feedback",
    description: "Use MLflow's structured feedback values to capture multiple dimensions in a single log call. This reduces API calls while maintaining rich context for analysis."
  },
  {
    icon: Lock,
    title: "Use Source Attribution Properly",
    description: "Set meaningful source_id values in AssessmentSource objects for tracking feedback providers. MLflow preserves complete audit trails with timestamps and source attribution."
  },
  {
    icon: Users,
    title: "Combine Programmatic and UI Collection",
    description: "Use MLflow's API for automated collection and the UI for manual review. Both methods integrate seamlessly, allowing different teams to contribute feedback through their preferred interface."
  }
]} />

## Next Steps

<TilesGrid>
  <TileCard
    icon={MessageSquare}
    iconSize={48}
    title="Feedback API Guide"
    description="Comprehensive guide to all feedback APIs with advanced patterns and examples"
    href="/genai/assessments/api-guide"
    linkText="View API reference →"
    containerHeight={120}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Feedback Concepts"
    description="Deep dive into feedback architecture, schema, and best practices"
    href="/genai/tracing/concepts/feedback"
    linkText="Learn concepts →"
    containerHeight={120}
  />
  <TileCard
    icon={Search}
    iconSize={48}
    title="Search and Analyze Traces"
    description="Query traces with feedback data and analyze patterns for quality insights"
    href="/genai/tracing/search-traces"
    linkText="Start analyzing →"
    containerHeight={120}
  />
</TilesGrid>

:::tip
Start with simple boolean feedback and expand to more detailed feedback as you understand your users' needs. The feedback API scales with your requirements while maintaining data consistency.
:::

## Summary

MLflow Feedback provides a robust, production-ready solution for user feedback collection:

- **Structured data model**: Standardized feedback with clear attribution and rationale
- **Direct trace integration**: Feedback linked directly to application execution data
- **Production ready**: Available in OSS MLflow 3.2.0+ with comprehensive API support
- **Flexible formats**: Support for boolean, numeric, categorical, and structured feedback
- **Quality insights**: Rich data for understanding and improving application performance

By using MLflow Feedback for user feedback collection, you gain a robust foundation for continuous quality improvement that integrates seamlessly with MLflow's tracing ecosystem. User feedback, combined with comprehensive tracing, provides the observability foundation needed to build and maintain high-quality GenAI applications that truly serve user needs.
