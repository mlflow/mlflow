import { APILink } from "@site/src/components/APILink";

# Quality Assessment with Traces and Assessments

MLflow Tracing and Assessments work together to provide comprehensive quality understanding for your GenAI applications. This guide shows how assessments attached to traces enable systematic quality improvements through structured feedback collection and analysis.

## Why Combine Traces with Assessments?

**Complete Context**: Traces capture how your application processes requests, while assessments capture quality judgments about those processes.

**Structured Feedback**: Assessments provide standardized quality signals with clear attribution and rationale.

**Actionable Insights**: The combination enables understanding both what happened (traces) and how well it worked (assessments).

**Quality Improvement**: Use assessment patterns to identify specific areas for improvement in your application logic.

## Assessment-Driven Quality Analysis

### Understanding Quality Patterns

Assessments attached to traces provide structured insights into application performance across different scenarios:

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType


# Example: Collecting user satisfaction assessment
def collect_user_satisfaction(trace_id, satisfaction_rating, user_comments, user_id):
    """Collect structured user satisfaction data."""
    mlflow.log_feedback(
        trace_id=trace_id,
        name="user_satisfaction",
        value=satisfaction_rating,  # 1-5 scale
        rationale=f"User feedback: {user_comments}",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=user_id
        ),
    )


# Example: Automated quality assessment
def automated_quality_check(trace_id, response_text):
    """Perform automated quality checks on responses."""
    # Check response length
    appropriate_length = 50 <= len(response_text) <= 500

    # Check for required elements
    has_greeting = any(
        word in response_text.lower() for word in ["hello", "hi", "greetings"]
    )

    # Combined quality score
    quality_pass = appropriate_length and has_greeting

    mlflow.log_feedback(
        trace_id=trace_id,
        name="automated_quality_check",
        value=quality_pass,
        rationale=f"Length check: {appropriate_length}, Greeting check: {has_greeting}",
        source=AssessmentSource(
            source_type=AssessmentSourceType.CODE, source_id="quality_validator_v1.0"
        ),
    )
```

### Expert Ground Truth Collection

Use assessments to capture expert expectations for important interactions:

```python
def log_expert_ground_truth(trace_id, correct_answer, expert_id):
    """Log expert-provided ground truth for evaluation."""
    mlflow.log_expectation(
        trace_id=trace_id,
        name="correct_answer",
        value=correct_answer,
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=expert_id
        ),
    )


def log_expert_quality_assessment(trace_id, quality_dimensions, expert_id):
    """Log comprehensive expert quality assessment."""
    mlflow.log_feedback(
        trace_id=trace_id,
        name="expert_quality_evaluation",
        value=quality_dimensions,  # Dictionary with multiple scores
        rationale="Expert evaluation across accuracy, helpfulness, and safety dimensions",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id=expert_id
        ),
    )
```

## Quality Improvement Workflows

### Identifying Quality Issues

Use assessment data to systematically identify patterns of quality issues:

**1. Collect Diverse Assessments**: Gather feedback from users, automated systems, and expert reviewers.

**2. Pattern Recognition**: Look for common characteristics in traces with negative assessments.

**3. Root Cause Analysis**: Examine trace execution details for traces with poor quality assessments.

**4. Targeted Improvements**: Make specific changes to address identified quality issues.

### Measuring Improvement Impact

Track the effectiveness of your quality improvements:

```python
def measure_quality_improvement(before_date, after_date, assessment_name):
    """Compare assessment patterns before and after improvements."""

    # Note: This is conceptual - actual implementation would depend
    # on your data storage and retrieval mechanisms

    print(f"Measuring improvement for {assessment_name}")
    print("1. Identify traces from before improvement period")
    print("2. Identify traces from after improvement period")
    print("3. Compare assessment distributions")
    print("4. Calculate statistical significance of improvement")

    # Example metrics to track:
    metrics = {
        "satisfaction_improvement": "Change in average user satisfaction",
        "error_rate_reduction": "Decrease in negative assessments",
        "consistency_improvement": "Reduced variance in quality scores",
    }

    return metrics
```

## Assessment Types for Quality Monitoring

### User Experience Assessments

Track how users perceive your application's outputs:

```python
# Binary satisfaction
mlflow.log_feedback(
    trace_id=trace_id,
    name="is_helpful",
    value=True,
    rationale="User found the response helpful",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="user_789"
    ),
)

# Numeric rating
mlflow.log_feedback(
    trace_id=trace_id,
    name="response_quality",
    value=4,  # 1-5 scale
    rationale="Good response but could be more specific",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="user_789"
    ),
)
```

### Technical Quality Assessments

Monitor technical aspects of your application's performance:

```python
# Safety assessment
mlflow.log_feedback(
    trace_id=trace_id,
    name="content_safety",
    value=True,
    rationale="Content passed all safety filters",
    source=AssessmentSource(
        source_type=AssessmentSourceType.CODE, source_id="safety_pipeline_v2.1"
    ),
)

# Accuracy assessment from LLM judge
mlflow.log_feedback(
    trace_id=trace_id,
    name="factual_accuracy",
    value=0.85,
    rationale="Response contains mostly accurate information with minor factual errors",
    source=AssessmentSource(
        source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-fact-checker"
    ),
)
```

## Quality Analysis Best Practices

### Comprehensive Assessment Strategy

**Multi-dimensional Evaluation**: Assess various aspects of quality (accuracy, helpfulness, safety, relevance).

**Multiple Sources**: Combine human feedback, automated checks, and LLM judge evaluations.

**Consistent Methodology**: Use standardized assessment names and scales across your application.

**Clear Attribution**: Maintain detailed source information for all assessments.

### Quality Monitoring Over Time

**Baseline Establishment**: Set initial quality benchmarks using assessment data.

**Trend Detection**: Monitor assessment patterns to identify quality drift or improvements.

**Regression Prevention**: Use assessment history to detect when changes negatively impact quality.

**Continuous Improvement**: Use assessment insights to guide iterative application improvements.

## Integration with Development Workflow

### Development Phase Quality Assessment

Use assessments during development to validate improvements:

1. **Create Test Cases**: Use traces from development testing with expert assessments
2. **Validate Changes**: Compare assessment patterns before and after code changes
3. **Quality Gates**: Establish minimum assessment thresholds for deployment
4. **Iterative Improvement**: Use assessment feedback to guide further development

### Production Quality Monitoring

Deploy assessment collection in production for ongoing quality assurance:

1. **User Feedback Collection**: Implement user satisfaction assessment collection
2. **Automated Quality Checks**: Run continuous automated assessments on production traces
3. **Expert Review Sampling**: Periodically collect expert assessments on sampled interactions
4. **Alert Systems**: Monitor assessment trends for quality degradation

## Next Steps

- **[Assessments Overview](/genai/assessments)**: Complete guide to MLflow Assessments
- **[Collect User Feedback](/genai/tracing/collect-user-feedback)**: Implement assessment-based feedback collection
- **[Search Traces](/genai/tracing/search-traces)**: Query and analyze traces for quality insights
- **[Production Monitoring](/genai/tracing/prod-tracing)**: Set up comprehensive production observability

:::tip
Quality assessment is most effective when it's systematic and multi-dimensional. Use assessments to capture both objective and subjective quality signals, and leverage the structured data for continuous improvement of your GenAI applications.
:::

## Summary

MLflow Assessments transform quality evaluation from ad-hoc feedback collection into systematic quality intelligence:

- **Structured Quality Data**: Assessments provide standardized quality signals with clear attribution
- **Complete Context**: Traces with assessments offer full visibility into application performance
- **Actionable Insights**: Assessment patterns guide specific quality improvements
- **Continuous Monitoring**: Ongoing assessment collection enables proactive quality management

By combining traces with assessments, you build a comprehensive quality evaluation system that scales from development through production, enabling data-driven quality improvements for your GenAI applications.