---
sidebar_position: 28
sidebar_label: xAI / Grok
---

import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { Activity, FileText, Gauge } from "lucide-react";
import ImageBox from "@site/src/components/ImageBox";
import StepHeader from "@site/src/components/StepHeader";
import ServerSetup from "@site/src/content/setup_server_slim.mdx";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TabsWrapper from "@site/src/components/TabsWrapper";
import { Users, BookOpen, Scale } from "lucide-react";

# Tracing xAI / Grok

[MLflow Tracing](../../) provides automatic tracing capability for Grok LLMs hosted by xAI through the OpenAI SDK integration. Since xAI platform provides an OpenAI-compatible API for Grok models, you can use `mlflow.openai.autolog()` to trace interactions with Grok models.

<ImageBox src="/images/llms/tracing/openai-function-calling.png" alt="Tracing via autolog" />

MLflow trace automatically captures the following information about Grok calls:

- Prompts and completion responses
- Latencies
- Token usage
- Model name
- Additional metadata such as `temperature`, `max_completion_tokens`, if specified.
- Function calling if returned in the response
- Built-in tools such as web search, file search, computer use, etc.
- Any exception if raised

## Getting Started

<StepHeader number={1} title="Install dependencies" />

<TabsWrapper>
<Tabs>
  <TabItem value="python" label="Python" default>

```bash
pip install mlflow openai
```

  </TabItem>
  <TabItem value="typescript" label="JS / TS">

```bash
npm install mlflow-openai openai
```

  </TabItem>
</Tabs>
</TabsWrapper>

<StepHeader number={2} title="Start MLflow server" />

<ServerSetup />

<StepHeader number={3} title="Enable tracing and call Grok" />

<TabsWrapper>
<Tabs>
  <TabItem value="python" label="Python" default>

    ```python
    import openai
    import mlflow

    # Enable auto-tracing for OpenAI (works with Grok)
    mlflow.openai.autolog()

    # Optional: Set a tracking URI and an experiment
    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment("Grok")

    # Initialize the OpenAI client with Grok API endpoint
    client = openai.OpenAI(
        base_url="https://api.x.ai/v1",
        api_key="<your_grok_api_key>",
    )

    response = client.chat.completions.create(
        model="grok-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of France?"},
        ],
    )
    ```

  </TabItem>
  <TabItem value="typescript" label="JS / TS">

    ```typescript
    import { OpenAI } from "openai";
    import { tracedOpenAI } from "mlflow-openai";

    // Wrap the OpenAI client and point to Grok endpoint
    const client = tracedOpenAI(
      new OpenAI({
        baseURL: "https://api.x.ai/v1",
        apiKey: "<your_grok_api_key>",
      })
    );

    const response = await client.chat.completions.create({
      model: "grok-4",
      messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: "What is the capital of France?" },
      ],
      temperature: 0.1,
      max_tokens: 100,
    });
    ```

  </TabItem>
</Tabs>
</TabsWrapper>

<StepHeader number={4} title="View traces in MLflow UI" />

Browse to your MLflow UI (for example, http://localhost:5000) and open the `Grok` experiment to see traces for the calls above.

<ImageBox src="/images/llms/tracing/basic-openai-trace.png" alt="Grok Tracing" />

-> View <u>[Next Steps](#next-steps)</u> for learning about more MLflow features like user feedback tracking, prompt management, and evaluation.

## Streaming and Async Support

MLflow supports tracing for streaming and async Grok APIs. Visit the [OpenAI Tracing documentation](../openai) for example code snippets for tracing streaming and async calls through OpenAI SDK.

## Combine with frameworks or manual tracing

The automatic tracing capability in MLflow is designed to work seamlessly with the [Manual Tracing SDK](/genai/tracing/app-instrumentation/manual-tracing) or multi-framework integrations. The examples below show Python (manual span) and JS/TS (manual span) at the same level of complexity.

<TabsWrapper>
<Tabs>
  <TabItem value="python" label="Python" default>

    ```python
    import json
    from openai import OpenAI
    import mlflow
    from mlflow.entities import SpanType

    # Initialize the OpenAI client with Grok API endpoint
    client = OpenAI(
        base_url="https://api.x.ai/v1",
        api_key="<your_grok_api_key>",
    )


    # Create a parent span for the Grok call
    @mlflow.trace(span_type=SpanType.CHAIN)
    def answer_question(question: str):
        messages = [{"role": "user", "content": question}]
        response = client.chat.completions.create(
            model="grok-4",
            messages=messages,
        )

        # Attach session/user metadata to the trace
        mlflow.update_current_trace(
            metadata={
                "mlflow.trace.session": "session-12345",
                "mlflow.trace.user": "user-a",
            }
        )
        return response.choices[0].message.content


    answer = answer_question("What is the capital of France?")
    ```

  </TabItem>
  <TabItem value="typescript" label="JS / TS">

    ```typescript
    import * as mlflow from "mlflow-tracing";
    import { OpenAI } from "openai";
    import { tracedOpenAI } from "mlflow-openai";

    mlflow.init({
      trackingUri: "http://localhost:5000",
      experimentId: "<your_experiment_id>",
    });

    // Wrap the OpenAI client and point to Grok endpoint
    const client = tracedOpenAI(
      new OpenAI({
        baseURL: "https://api.x.ai/v1",
        apiKey: "<your_grok_api_key>",
      })
    );

    // Create a traced function that wraps the Grok call
    const answerQuestion = mlflow.trace(
      async (question: string) => {
        const resp = await client.chat.completions.create({
          model: "grok-4",
          messages: [{ role: "user", content: question }],
        });
        return resp.choices[0].message?.content;
      },
      { name: "answer-question" }
    );

    await answerQuestion("What is the capital of France?");
    ```

  </TabItem>
</Tabs>
</TabsWrapper>

Running either example will produce a trace that includes the Grok LLM span; the traced function creates the parent span automatically.

<ImageBox src="/images/llms/tracing/openai-trace-with-manual-span.png" alt="Grok Tracing with Manual Tracing" />

## Next steps

<TilesGrid>
  <TileCard
    icon={Users}
    iconSize={48}
    title="Track User Feedback"
    description="Record user feedback on traces for tracking user satisfaction."
    href="/genai/tracing/collect-user-feedback"
    linkText="Learn about feedback ->"
    containerHeight={64}
  />
  <TileCard
    icon={BookOpen}
    iconSize={48}
    title="Manage Prompts"
    description="Learn how to manage prompts with MLflow's prompt registry."
    href="/genai/prompt-registry"
    linkText="Manage prompts ->"
    containerHeight={64}
  />
  <TileCard
    icon={Scale}
    iconSize={48}
    title="Evaluate Traces"
    description="Evaluate traces with LLM judges to understand and improve your AI application's behavior."
    href="/genai/eval-monitor/running-evaluation/traces"
    linkText="Evaluate traces ->"
    containerHeight={64}
  />
</TilesGrid>
