import DatabricksCallout from "@site/src/components/DatabricksCallout"

# Auto Tracing Integrations

<DatabricksCallout docsPath="/mlflow3/genai/tracing/integrations/index" />

MLflow Tracing is integrated with 20+ popular Generative AI libraries and frameworks, offering **one-line automatic tracing** experience. This allows you to gain immediate observability into your GenAI applications with minimal setup.

import { APILink } from "@site/src/components/APILink";
import { Card, CardGroup, SmallLogoCard } from "@site/src/components/Card";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

## Supported Integrations

<CardGroup isSmall>
  <SmallLogoCard link="/genai/tracing/integrations/listing/openai">
    <span>![OpenAI Logo](/images/logos/openai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/langchain">
    <span>![LangChain Logo](/images/logos/langchain-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/langgraph">
    <span>![LangGraph Logo](/images/logos/langgraph-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/llama_index">
    <span>![LlamaIndex Logo](/images/logos/llamaindex-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/anthropic">
    <span>![Anthropic Logo](/images/logos/anthropic-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/dspy">
    <span>![DSPy Logo](/images/logos/dspy-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/bedrock">
    <span>![Bedrock Logo](/images/logos/bedrock-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/autogen">
    <span>![AutoGen Logo](/images/logos/autogen-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/ag2">
    <span>![AG2 Logo](/images/logos/ag2-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/gemini">
    <span>![Gemini Logo](/images/logos/google-gemini-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/litellm">
    <span>![LiteLLM Logo](/images/logos/litellm-logo.jpg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/crewai">
    <span>![CrewAI Logo](/images/logos/crewai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/ollama">
    <span>![Ollama Logo](/images/logos/ollama-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/groq">
    <span>![Groq Logo](/images/logos/groq-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/mistral">
    <span>![Mistral Logo](/images/logos/mistral-ai-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/instructor">
    <span>![Instructor Logo](/images/logos/instructor-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/openai-agent">
    <span>![OpenAI Agent Logo](/images/logos/openai-agent-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/swarm">
    <span>![OpenAI Swarm Logo](/images/logos/openai-swarm-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/deepseek">
    <span>![DeepSeek Logo](/images/logos/deepseek-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/txtai">
    <span>![txtai Logo](/images/logos/txtai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/smolagents">
    <span>![Smolagents Logo](/images/logos/smolagents-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/genai/tracing/integrations/listing/pydantic_ai">
    <span>![PydanticAI Logo](/images/logos/pydanticai-logo.png)</span>
  </SmallLogoCard>
</CardGroup>
<br />

:::info Missing an Integration?
Is your favorite library missing? Consider [contributing](/genai/tracing/integrations/contribute) or [submitting a feature request](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=enhancement&projects=&template=feature_request_template.yaml&title=%5BFR%5D).
:::

<Tabs>
<TabItem value="openai" label="OpenAI" default>

```python
import mlflow
import openai

# Enable auto-tracing for OpenAI
mlflow.openai.autolog()

# Set up MLflow tracking
mlflow.set_experiment("openai-tracing-demo")

openai_client = openai.OpenAI()

messages = [
    {
        "role": "user",
        "content": "What is the capital of France?",
    }
]

response = openai_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    temperature=0.1,
    max_tokens=100,
)
# View trace in MLflow UI
```

[Full OpenAI Integration Guide →](/genai/tracing/integrations/listing/openai)

</TabItem>

<TabItem value="langchain" label="LangChain">

```python
import mlflow
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

mlflow.langchain.autolog()

mlflow.set_experiment("langchain-tracing-demo")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, max_tokens=1000)
prompt = PromptTemplate.from_template("Tell me a joke about {topic}.")
chain = prompt | llm | StrOutputParser()

chain.invoke({"topic": "artificial intelligence"})
# View trace in MLflow UI
```

[Full LangChain Integration Guide →](/genai/tracing/integrations/listing/langchain)

</TabItem>

<TabItem value="langgraph" label="LangGraph">

```python
import mlflow
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

mlflow.langchain.autolog()  # LangGraph uses LangChain's autolog

mlflow.set_experiment("langgraph-tracing-demo")


@tool
def get_weather(city: str):
    """Use this to get weather information."""
    return f"It might be cloudy in {city}"


llm = ChatOpenAI(model="gpt-4o-mini")
graph = create_react_agent(llm, [get_weather])
result = graph.invoke({"messages": [("user", "what is the weather in sf?")]})
# View trace in MLflow UI
```

[Full LangGraph Integration Guide →](/genai/tracing/integrations/listing/langgraph)

</TabItem>

<TabItem value="anthropic" label="Anthropic">

```python
import mlflow
import anthropic
import os

mlflow.anthropic.autolog()

mlflow.set_experiment("anthropic-tracing-demo")

client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello, Claude"}],
)
# View trace in MLflow UI
```

[Full Anthropic Integration Guide →](/genai/tracing/integrations/listing/anthropic)

</TabItem>

<TabItem value="dspy" label="DSPy">

```python
import mlflow
import dspy

mlflow.dspy.autolog()

mlflow.set_experiment("dspy-tracing-demo")

lm = dspy.LM("openai/gpt-4o-mini")  # Assumes OPENAI_API_KEY is set
dspy.configure(lm=lm)


class SimpleSignature(dspy.Signature):
    input_text: str = dspy.InputField()
    output_text: str = dspy.OutputField()


program = dspy.Predict(SimpleSignature)
result = program(input_text="Summarize MLflow Tracing.")
# View trace in MLflow UI
```

[Full DSPy Integration Guide →](/genai/tracing/integrations/listing/dspy)

</TabItem>

<TabItem value="bedrock" label="Bedrock">

```python
import mlflow
import boto3

mlflow.bedrock.autolog()

mlflow.set_experiment("bedrock-tracing-demo")

bedrock = boto3.client(
    service_name="bedrock-runtime", region_name="us-east-1"  # Replace with your region
)
response = bedrock.converse(
    modelId="anthropic.claude-3-5-sonnet-20241022-v2:0",
    messages=[{"role": "user", "content": "Hello World in one line."}],
)
# View trace in MLflow UI
```

[Full Bedrock Integration Guide →](/genai/tracing/integrations/listing/bedrock)

</TabItem>

<TabItem value="autogen" label="AutoGen">

```python
import mlflow
from autogen import ConversableAgent
import os

mlflow.autogen.autolog()

mlflow.set_experiment("autogen-tracing-demo")

config_list = [{"model": "gpt-4o-mini", "api_key": os.environ.get("OPENAI_API_KEY")}]
assistant = ConversableAgent("assistant", llm_config={"config_list": config_list})
user_proxy = ConversableAgent(
    "user_proxy", human_input_mode="NEVER", code_execution_config=False
)

user_proxy.initiate_chat(assistant, message="What is 2+2?")
# View trace in MLflow UI
```

[Full AutoGen Integration Guide →](/genai/tracing/integrations/listing/autogen)

</TabItem>

<TabItem value="llama-index" label="LlamaIndex">

```python
import mlflow
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI

mlflow.llama_index.autolog()

mlflow.set_experiment("llama-index-tracing-demo")

# Initialize LLM
llm = OpenAI(model="gpt-4o-mini")

# Create a simple query engine
documents = SimpleDirectoryReader("data").load_data()  # Replace with your data path
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine(llm=llm)

response = query_engine.query("What is the main topic of these documents?")
# View trace in MLflow UI
```

[Full LlamaIndex Integration Guide →](/genai/tracing/integrations/listing/llama_index)

</TabItem>

<TabItem value="combining" label="Multiple Libraries">

## Enabling Multiple Auto Tracing Integrations

As GenAI applications often combine multiple libraries, MLflow Tracing allows you to enable auto-tracing for several integrations simultaneously, providing a unified tracing experience.

For example, to enable both LangChain and direct OpenAI tracing:

```python
import mlflow

# Enable MLflow Tracing for both LangChain and OpenAI
mlflow.langchain.autolog()
mlflow.openai.autolog()

# Your code using both LangChain and OpenAI directly...
```

:::note Example
An example of using both LangChain and OpenAI with MLflow tracing can be [found here](/genai/tracing/app-instrumentation/automatic#multi-framework-example).
:::

MLflow will generate a single, cohesive trace that combines steps from both LangChain and direct OpenAI LLM calls, allowing you to inspect the complete flow.

### Advanced Example: LangChain + OpenAI

```python
import mlflow
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# Enable MLflow Tracing for both LangChain and OpenAI
mlflow.langchain.autolog()
mlflow.openai.autolog()

mlflow.set_experiment("LangChain")

# Define a chain that uses OpenAI as an LLM provider
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, max_tokens=1000)

prompt_template = PromptTemplate.from_template(
    "Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. "
    "Emulate their quirks and mannerisms to the best of your ability, embracing their traits—even if they aren't entirely "
    "constructive or inoffensive. The question is: {question}"
)

chain = prompt_template | llm | StrOutputParser()

chain.invoke(
    {
        "person": "Linus Torvalds",
        "question": "Can I just set everyone's access to sudo to make things easier?",
    }
)
```

MLflow will generate a single trace that combines LangChain steps and OpenAI LLM call, allowing you to inspect the raw input and output passed to the OpenAI LLM.

## Disabling Auto Tracing

Auto tracing for each library can be disabled by calling `mlflow.<library>.autolog(disable=True)`. Moreover, you can disable tracing for all integrations using `mlflow.autolog(disable=True)`.

```python
import mlflow

# Disable for a specific library
mlflow.openai.autolog(disable=True)

# Disable all autologging
mlflow.autolog(disable=True)
```

</TabItem>
</Tabs>

## Next Steps

Ready to start tracing? Choose your path:

- **[Quickstart Guide](/genai/tracing/quickstart)** - Get tracing working in 10 minutes
- **[Manual Instrumentation](/genai/tracing/app-instrumentation/manual-tracing)** - Add custom tracing
- **[Search Traces](/genai/tracing/search-traces)** - Find and analyze your traces
- **[Production Deployment](/genai/tracing/prod-tracing)** - Scale for production workloads

## Summary

MLflow's automatic tracing provides:
- **One-line setup** for 20+ AI libraries
- **Zero code changes** for basic observability
- **Unified traces** across multi-framework apps
- **Rich metadata** including inputs, outputs, and performance
- **Production-ready** scaling and monitoring

Click any integration above to get started with detailed setup instructions.