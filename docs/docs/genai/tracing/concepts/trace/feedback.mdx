import Tabs from "@theme/Tabs"
import TabItem from "@theme/TabItem"
import { APILink } from "@site/src/components/APILink";

# Feedback Concepts

This guide introduces the core concepts of feedback and assessment in MLflow's GenAI evaluation framework. Understanding these concepts is essential for effectively measuring and improving the quality of your GenAI applications.

## What is Feedback?

**Feedback** in MLflow represents the result of any quality assessment performed on your GenAI application outputs. It provides a standardized way to capture evaluations, whether they come from automated systems, LLM judges, or human reviewers.

Feedback serves as the bridge between running your application and understanding its quality, enabling you to systematically track performance across different dimensions like correctness, relevance, safety, and adherence to guidelines.

## Core Concepts

### Feedback Object
The **Feedback object** (also referred to as an **Assessment** in some contexts) is the fundamental building block of MLflow's evaluation system. It serves as a standardized container for the result of any quality check, providing a common language for assessment across different evaluation methods.

<Tabs>
  <TabItem value="structure" label="Feedback Structure" default>
    Every Feedback object contains three core components:

    **Name**: A string identifying the specific quality aspect being assessed

    Examples: `"correctness"`, `"relevance_to_query"`, `"is_safe"`, `"guideline_adherence_politeness"`

    **Value**: The actual result of the assessment, which can be:
    - Numeric scores (e.g., `0.0` to `1.0`, `1` to `5`)
    - Boolean values (`True`/`False`)
    - Categorical labels (e.g., `"PASS"`, `"FAIL"`, `"EXCELLENT"`)
    - Structured data (e.g., `{"score": 0.8, "confidence": 0.9}`)

    **Rationale**: A string explaining why the assessment resulted in the given value

    This explanation is crucial for transparency, debugging, and understanding evaluation behavior, especially for LLM-based assessments.
  </TabItem>
  <TabItem value="sources" label="Feedback Sources">
    Feedback can originate from multiple sources, each with different characteristics:

    **LLM-based Evaluations**: Automated assessments using language models as judges
    - Fast and scalable
    - Can evaluate complex, subjective criteria
    - Provide detailed reasoning in rationale

    **Programmatic Checks**: Rule-based or algorithmic evaluations
    - Deterministic and consistent
    - Fast execution
    - Good for objective, measurable criteria

    **Human Reviews**: Manual assessments from human evaluators
    - Highest quality for subjective evaluations
    - Slower and more expensive
    - Essential for establishing ground truth

    All feedback types are treated equally in MLflow and can be combined to provide comprehensive quality assessment.
  </TabItem>
  <TabItem value="attachment" label="Trace Attachment">
    Feedback objects are attached to **MLflow Traces**, creating a direct connection between application execution and quality assessment:

    **Execution + Assessment**: Each trace captures how your application processed a request, while feedback captures how well it performed

    **Multi-dimensional Quality**: A single trace can have multiple feedback objects assessing different quality dimensions

    **Historical Analysis**: Attached feedback enables tracking quality trends over time and across different application versions

    **Debugging Context**: When quality issues arise, you can examine both the execution trace and the assessment rationale
  </TabItem>
</Tabs>

### Assessment Dimensions

Feedback can evaluate various aspects of your GenAI application's performance:

<Tabs>
  <TabItem value="correctness" label="Correctness & Accuracy" default>
    **Factual Accuracy**: Whether the generated content contains correct information

    **Answer Completeness**: How thoroughly the response addresses the user's question

    **Logical Consistency**: Whether the reasoning and conclusions are sound

    Example assessment:
    ```python
    import mlflow
    from mlflow.entities import AssessmentSource, AssessmentSourceType

    # Log factual accuracy assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="factual_accuracy",
        value=0.85,
        rationale="The response correctly identifies 3 out of 4 key facts about MLflow, but incorrectly states the founding year.",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id="expert_reviewer_001"
        ),
    )
    ```
  </TabItem>
  <TabItem value="relevance" label="Relevance & Context">
    **Query Relevance**: How well the response addresses the specific user question

    **Context Utilization**: Whether retrieved documents or provided context were used effectively

    **Topic Adherence**: Staying on-topic and avoiding irrelevant tangents

    Example assessment:
    ```python
    # Log relevance assessment with categorical value
    mlflow.log_feedback(
        trace_id="trace_123",
        name="relevance_to_query",
        value="HIGH",
        rationale="Response directly answers the user's question about MLflow features and provides relevant examples.",
        source=AssessmentSource(
            source_type=AssessmentSourceType.LLM_JUDGE,
            source_id="gpt-4-relevance-evaluator",
        ),
    )
    ```
  </TabItem>
  <TabItem value="safety" label="Safety & Guidelines">
    **Content Safety**: Detecting harmful, inappropriate, or toxic content

    **Guideline Adherence**: Following specific organizational or ethical guidelines

    **Bias Detection**: Identifying unfair bias or discrimination in responses

    Example assessment:
    ```python
    # Log safety assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="is_safe",
        value=True,
        rationale="Content contains no harmful, toxic, or inappropriate material.",
        source=AssessmentSource(
            source_type=AssessmentSourceType.CODE, source_id="safety_filter_v2.1"
        ),
    )
    ```
  </TabItem>
  <TabItem value="quality" label="Quality & Style">
    **Writing Quality**: Grammar, clarity, and coherence of the response

    **Tone Appropriateness**: Whether the tone matches the intended context

    **Helpfulness**: How useful the response is to the user

    Example assessment:
    ```python
    # Log helpfulness assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="helpfulness",
        value=4,
        rationale="Response provides clear, actionable information but could include more specific examples.",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id="user_456"
        ),
    )
    ```
  </TabItem>
</Tabs>

## Feedback Lifecycle

Understanding how feedback flows through your evaluation process:

<Tabs>
  <TabItem value="generation" label="Generation" default>
    **During Application Execution**: Traces are created as your GenAI application processes requests

    **Post-Execution Evaluation**: Feedback is generated by evaluating the trace data (inputs, outputs, intermediate steps)

    **Multiple Evaluators**: Different evaluation methods can assess the same trace, creating multiple feedback objects

    **Batch or Real-time**: Feedback can be generated immediately or in batch processes
  </TabItem>
  <TabItem value="attachment" label="Attachment">
    **Trace Association**: Each feedback object is linked to a specific trace using trace IDs

    **Persistent Storage**: Feedback is stored alongside trace data in MLflow's backend

    **Metadata Preservation**: All context about the evaluation method and timing is maintained

    **Version Tracking**: Changes to feedback or re-evaluations are tracked over time
  </TabItem>
  <TabItem value="aggregation" label="Aggregation">
    **Quality Metrics**: Individual feedback objects can be aggregated into overall quality scores

    **Trend Analysis**: Historical feedback enables tracking quality changes over time

    **Comparative Analysis**: Compare feedback across different model versions, prompts, or configurations

    **Reporting**: Generate quality reports and dashboards from aggregated feedback data
  </TabItem>
</Tabs>

## Assessment Value Types

MLflow Assessments support different value types to accommodate various evaluation needs:

<Tabs>
  <TabItem value="numeric" label="Numeric & Boolean Values" default>
    **Continuous Scores**: Floating-point values representing quality on a scale
    - Range: Often 0.0 to 1.0 or 1 to 5
    - Use case: Measuring degrees of quality like relevance or accuracy

    **Boolean Values**: Binary assessments for pass/fail criteria
    - Values: `True` or `False`
    - Use case: Safety checks, guideline compliance

    Examples:
    ```python
    # Continuous score assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="relevance_score",
        value=0.87,
        rationale="High relevance to user query with comprehensive coverage",
        source=AssessmentSource(
            source_type=AssessmentSourceType.LLM_JUDGE, source_id="relevance-evaluator-v2"
        ),
    )

    # Boolean assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="contains_pii",
        value=False,
        rationale="No personally identifiable information detected",
        source=AssessmentSource(
            source_type=AssessmentSourceType.CODE, source_id="pii-detector-v1.3"
        ),
    )
    ```
  </TabItem>
  <TabItem value="categorical" label="Categorical Values">
    **Quality Labels**: Discrete categories representing quality levels
    - Values: Predefined labels like "EXCELLENT", "GOOD", "POOR"
    - Use case: Human-like quality ratings

    **Classification Labels**: Specific category assignments
    - Values: Domain-specific categories
    - Use case: Content classification, intent recognition

    Examples:
    ```python
    # Quality rating assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="overall_quality",
        value="GOOD",
        rationale="Response is helpful and accurate but could be more comprehensive",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id="quality_reviewer_001"
        ),
    )

    # Classification assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="response_type",
        value="INFORMATIONAL",
        rationale="Response provides factual information without recommendations",
        source=AssessmentSource(
            source_type=AssessmentSourceType.CODE, source_id="content-classifier-v2.0"
        ),
    )
    ```
  </TabItem>
  <TabItem value="structured" label="Structured Values">
    **Complex Objects**: Rich data structures containing multiple assessment aspects
    - Format: JSON objects with nested properties
    - Use case: Comprehensive evaluations with multiple dimensions

    Example:
    ```python
    # Multi-dimensional structured assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="comprehensive_quality",
        value={
            "overall_score": 0.85,
            "accuracy": 0.9,
            "fluency": 0.8,
            "confidence": 0.75,
            "safety": 1.0,
        },
        rationale="Comprehensive evaluation across multiple quality dimensions",
        source=AssessmentSource(
            source_type=AssessmentSourceType.LLM_JUDGE,
            source_id="multi-dimensional-evaluator-v1",
        ),
    )
    ```
  </TabItem>
</Tabs>


## Assessment Sources and Methods

MLflow Assessments support three source types, each with specific use cases and characteristics:

<Tabs>
  <TabItem value="llm-judges" label="LLM_JUDGE Sources" default>
    **AssessmentSourceType.LLM_JUDGE**: Evaluations performed by language models

    **Characteristics**:
    - Scalable to large volumes of data
    - Can evaluate subjective, complex criteria
    - Provide detailed reasoning in rationale
    - Consistent evaluation methodology

    **Implementation Example**:
    ```python
    # LLM judge evaluation
    def llm_judge_evaluation(trace_id, response_text):
        # Your LLM evaluation logic here
        score = evaluate_with_llm_judge(response_text)

        mlflow.log_feedback(
            trace_id=trace_id,
            name="llm_judge_quality",
            value=score,
            rationale="LLM judge evaluated based on coherence, accuracy, and helpfulness criteria",
            source=AssessmentSource(
                source_type=AssessmentSourceType.LLM_JUDGE,
                source_id="gpt-4-quality-judge-v2",
            ),
        )
    ```
  </TabItem>
  <TabItem value="programmatic" label="CODE Sources">
    **AssessmentSourceType.CODE**: Programmatic, rule-based evaluations

    **Characteristics**:
    - Deterministic and reproducible
    - Fast execution and low latency
    - Objective, measurable criteria
    - Easy to debug and maintain

    **Implementation Example**:
    ```python
    # Simple programmatic evaluation
    response_text = "Hello! MLflow is a platform for managing machine learning lifecycles."

    # Basic length check
    meets_length_requirement = len(response_text) >= 50

    # Check for greeting
    has_greeting = response_text.lower().startswith(("hello", "hi", "greetings"))

    # Log assessment
    mlflow.log_feedback(
        trace_id="trace_123",
        name="basic_quality_check",
        value=meets_length_requirement and has_greeting,
        rationale=f"Length adequate: {meets_length_requirement}, Has greeting: {has_greeting}",
        source=AssessmentSource(
            source_type=AssessmentSourceType.CODE, source_id="basic_validator_v1.0"
        ),
    )
    ```
  </TabItem>
  <TabItem value="human-review" label="HUMAN Sources">
    **AssessmentSourceType.HUMAN**: Manual assessments from human evaluators

    **Characteristics**:
    - Highest quality for subjective criteria
    - Nuanced understanding of context
    - Ground truth establishment
    - Complex reasoning evaluation

    **Implementation Example**:
    ```python
    # Human evaluation interface
    def collect_human_feedback(trace_id, rating, comments, evaluator_id):
        mlflow.log_feedback(
            trace_id=trace_id,
            name="human_quality_rating",
            value=rating,  # 1-5 scale
            rationale=f"Human evaluator feedback: {comments}",
            source=AssessmentSource(
                source_type=AssessmentSourceType.HUMAN, source_id=evaluator_id
            ),
        )


    # For expectations (ground truth)
    # Example: Log what the correct response should be
    mlflow.log_expectation(
        trace_id="trace_123",
        name="expected_response",
        value="The answer should explain MLflow's core components: tracking, projects, models, and registry.",
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN, source_id="domain_expert_001"
        ),
    )
    ```
  </TabItem>
</Tabs>

## Integration with MLflow

MLflow's **Assessments APIs** provide a modern, structured approach to capturing and managing feedback that integrates seamlessly with the tracing ecosystem.

### Assessments: The Modern Feedback Standard

**Structured Data Model**: Unlike traditional tag-based approaches, assessments provide a standardized format with:
- Clear value types (boolean, numeric, categorical, structured)
- Attribution through source tracking (human reviewers, automated systems, LLM judges)
- Rationale explanations for transparency and debugging
- Audit trails for tracking changes over time

**Direct Trace Integration**: Assessments are first-class citizens in MLflow tracing:
- Linked directly to trace IDs for immediate context
- Searchable through trace query APIs
- Visualized alongside execution data in the MLflow UI
- Preserved with complete metadata for analysis

### Assessment Types in MLflow

**Expectations**: Ground truth labels provided by domain experts
```python
mlflow.log_expectation(
    trace_id=trace_id,
    name="correct_answer",
    value="MLflow is an open-source platform for managing machine learning lifecycles, including experimentation, reproducibility, deployment, and model registry.",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"
    ),
)
```

**Feedback**: Quality evaluations from any source
```python
mlflow.log_feedback(
    trace_id=trace_id,
    name="user_satisfaction",
    value=4,  # 1-5 scale
    rationale="Response was helpful but could be more specific",
    source=AssessmentSource(
        source_type=AssessmentSourceType.HUMAN, source_id="user_123"
    ),
)
```

### Source Attribution and Tracking

Each assessment captures complete attribution information:

- **Source Type**: Clearly identifies whether feedback came from humans, code, or LLM judges
- **Source ID**: Specific identifier for the evaluator (user ID, system name, model version)
- **Rationale**: Explanation of the assessment reasoning
- **Timestamp**: When the assessment was created

This attribution enables you to:
- Understand the reliability and context of each assessment
- Track evaluator performance and consistency
- Filter assessments by source type for analysis
- Maintain audit trails for quality decisions

## Best Practices for Assessments

<Tabs>
  <TabItem value="naming" label="Assessment Design" default>
    **Descriptive Names**: Use clear, consistent naming conventions across your application
    - Examples: `is_helpful`, `relevance_score`, `safety_check`, `user_satisfaction`
    - Why: Consistent naming enables easier querying and analysis across teams
    - Tracking: Standard names allow aggregation and comparison over time

    **Appropriate Value Types**: Choose the right data type for your assessment needs
    - Boolean for binary decisions (`True`/`False`) - simple yes/no evaluations
    - Numeric for scores and ratings (`1-5`, `0.0-1.0`) - measurable quality levels
    - Categorical for classifications (`"GOOD"`, `"FAIR"`, `"POOR"`) - discrete quality levels
    - Structured for complex evaluations (JSON objects) - multi-dimensional assessment
    - Why: Proper types enable meaningful aggregation and statistical analysis

    **Meaningful Rationale**: Always provide explanations that aid in debugging and improvement
    - Why: Rationale enables understanding of assessment reasoning and quality patterns
    - Usability: Helps developers understand specific improvement areas
  </TabItem>
  <TabItem value="sources" label="Source Management">
    **Clear Attribution**: Use descriptive source IDs that help identify assessment origin
    - User IDs for human feedback: `"user_123"`, `"expert@company.com"`
    - System IDs for automated checks: `"safety_filter_v2.1"`, `"format_validator"`
    - Model IDs for LLM judges: `"gpt-4-quality-evaluator"`, `"claude-3-relevance-judge"`
    - Why: Clear attribution enables filtering by evaluator reliability and expertise
    - Tracking: Allows monitoring evaluator consistency and identifying drift

    **Source Type Accuracy**: Use the correct `AssessmentSourceType` for proper categorization
    - `HUMAN` for all user and expert feedback - enables filtering for ground truth
    - `CODE` for programmatic evaluations - identifies deterministic assessments
    - `LLM_JUDGE` for language model-based assessments - separates AI from human judgment
    - Why: Accurate typing enables proper analysis and reliability assessment
    - Usability: Teams can focus on specific feedback types for different purposes
  </TabItem>
  <TabItem value="strategy" label="Quality Strategy">
    **Multi-dimensional Assessment**: Evaluate various quality aspects simultaneously
    - Why: Comprehensive quality understanding requires multiple perspectives
    - Tracking: Enables identification of specific quality dimensions that need improvement
    - Example: Combine accuracy, helpfulness, safety, and relevance assessments

    **Balanced Sources**: Combine human feedback, automated checks, and LLM judges
    - Human feedback: Ground truth and subjective quality assessment
    - Automated checks: Consistent, scalable objective measures
    - LLM judges: Scalable subjective evaluation with reasoning
    - Why: Different sources provide complementary insights into application quality
    - Usability: Enables both real-time monitoring and detailed quality analysis

    **Temporal Tracking**: Monitor assessment trends to detect quality changes over time
    - Why: Quality can degrade due to model updates, data drift, or changing user expectations
    - Tracking: Historical assessment data enables quality regression detection
    - Usability: Proactive identification of quality issues before user impact
  </TabItem>
</Tabs>

## Getting Started with Assessments

To begin using MLflow's modern assessment system:

**[Assessments Overview](/genai/assessments)**: Comprehensive introduction to expectations, feedback, and sources

**[Assessments API Guide](/genai/assessments/api-guide)**: Complete API reference with examples and best practices

**[Collect User Feedback](/genai/tracing/collect-user-feedback)**: Implementation patterns for user feedback collection

**[Search Traces](/genai/tracing/search-traces)**: Query traces using assessment data for analysis

**[Quality Evaluation](/genai/tracing/quality-with-traces)**: Advanced techniques for quality assessment

---

*MLflow Assessments represent the evolution of feedback concepts, providing a structured, production-ready approach to quality evaluation. By adopting assessments, you gain a robust foundation for systematic quality improvement that scales with your GenAI applications.*