import { APILink } from "@site/src/components/APILink";

# Propagate Trace Context Across Services

When your application spans multiple services, propagate the active trace context over HTTP so spans recorded in different services are stitched into a single trace.

If you set up MLflow tracking to Databricks, to make distributed tracing work, the trace destination must be set to Unity Catalog. Please refer to [Store MLflow traces in Unity Catalog](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/trace-unity-catalog) for details.

- Use the <APILink fn="mlflow.tracing.get_tracing_context_headers_for_http_request" /> API on the client to inject headers.
- Use the <APILink fn="mlflow.tracing.set_tracing_context_from_http_request_headers" /> API on the server to extract and set the current context for the duration of the request.

## Client example

```python
import requests
import mlflow
from mlflow.tracing import get_tracing_context_headers_for_http_request

with mlflow.start_span("client-root"):
    headers = get_tracing_context_headers_for_http_request()
    requests.post(
        "https://your.service/handle", headers=headers, json={"input": "hello"}
    )
```

## Server handler example

```python
import mlflow
from flask import Flask, request
from mlflow.tracing import set_tracing_context_from_http_request_headers

app = Flask(__name__)


@app.post("/handle")
def handle():
    headers = dict(request.headers)
    with set_tracing_context_from_http_request_headers(headers):
        with mlflow.start_span("server-handler") as span:
            # ... your logic ...
            span.set_attribute("status", "ok")
    return {"ok": True}
```
