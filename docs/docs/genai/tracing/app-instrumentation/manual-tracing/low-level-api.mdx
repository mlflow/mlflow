import DatabricksCallout from "@site/src/components/DatabricksCallout"
import Tabs from "@theme/Tabs"
import TabItem from "@theme/TabItem"
import { APILink } from "@site/src/components/APILink";

# Low-level Client APIs (Advanced)

<DatabricksCallout docsPage="/mlflow3/genai/tracing/manual-tracing/low-level-api" />

The MLflow Client APIs provide direct, fine-grained control over trace lifecycle management. While the [high-level APIs](/tracing/app-instrumentation/manual-tracing/fluent-apis) handle most use cases elegantly, client APIs are essential for advanced scenarios requiring explicit control over trace creation, custom trace IDs, or integration with existing observability systems.

:::warning
**Before You Begin**: We recommend using client APIs only when high-level APIs don't meet your requirements:

No automatic parent-child relationship detection

Manual exception handling required

Incompatible with auto-tracing integrations

Full control over trace lifecycle

Custom trace ID management

Integration with existing systems
:::

## Core Concepts

### Trace Lifecycle

Every trace follows a strict lifecycle that must be managed explicitly:

1. **Start Trace** - Create the root span
2. **Start Span(s)** - Add child spans as needed
3. **End Span(s)** - Close spans in reverse order (LIFO)
4. **End Trace** - Complete the root span

:::important
**Golden Rule**: Every `start_trace` or `start_span` call must have a corresponding `end_trace` or `end_span` call. Failing to close spans will result in incomplete traces.
:::

### Key Identifiers

Understanding these identifiers is crucial for client API usage:

| Identifier   | Description             | Usage                           |
| ------------ | ----------------------- | ------------------------------- |
| `request_id` | Unique trace identifier | Links all spans in a trace      |
| `span_id`    | Unique span identifier  | Identifies specific span to end |
| `parent_id`  | Parent span's ID        | Creates span hierarchy          |

## Getting Started

### Initialize the Client

```python
from mlflow import MlflowClient

# Initialize client with default tracking URI
client = MlflowClient()

# Or specify a custom tracking URI
client = MlflowClient(tracking_uri="http://localhost:5000")
```

### Starting a Trace

Unlike high-level APIs, you must explicitly start a trace before adding spans:

```python
# Start a new trace - this creates the root span
root_span = client.start_trace(
    name="my_application_flow",
    inputs={"user_id": "123", "action": "generate_report"},
    attributes={"environment": "production", "version": "1.0.0"}
)

# Extract the request_id for subsequent operations
request_id = root_span.request_id
print(f"Started trace with ID: {request_id}")
```

### Adding Child Spans

Create a hierarchy of spans to represent your application's workflow:

```python
# Create a child span for data retrieval
data_span = client.start_span(
    name="fetch_user_data",
    request_id=request_id,  # Links to the trace
    parent_id=root_span.span_id,  # Creates parent-child relationship
    inputs={"user_id": "123"},
    attributes={"database": "users_db", "query_type": "select"}
)

# Create a sibling span for processing
process_span = client.start_span(
    name="process_data",
    request_id=request_id,
    parent_id=root_span.span_id,  # Same parent as data_span
    inputs={"data_size": "1024KB"},
    attributes={"processor": "gpu", "batch_size": 32}
)
```

### Ending Spans

End spans in reverse order of creation (LIFO - Last In, First Out):

```python
# End the data retrieval span
client.end_span(
    request_id=data_span.request_id,
    span_id=data_span.span_id,
    outputs={"record_count": 42, "cache_hit": True},
    attributes={"duration_ms": 150}
)

# End the processing span
client.end_span(
    request_id=process_span.request_id,
    span_id=process_span.span_id,
    outputs={"processed_records": 42, "errors": 0},
    status="OK"
)
```

### Ending a Trace

Complete the trace by ending the root span:

```python
# End the root span (completes the trace)
client.end_trace(
    request_id=request_id,
    outputs={"report_url": "https://example.com/report/123"},
    attributes={"total_duration_ms": 1250, "status": "success"}
)
```

## Practical Examples

<Tabs>
  <TabItem value="error-handling" label="Error Handling" default>
    Proper error handling ensures traces are completed even when exceptions occur:

    ```python
    def traced_operation():
        client = MlflowClient()
        root_span = None

        try:
            # Start trace
            root_span = client.start_trace("risky_operation")

            # Start child span
            child_span = client.start_span(
                name="database_query",
                request_id=root_span.request_id,
                parent_id=root_span.span_id
            )

            try:
                # Risky operation
                result = perform_database_query()

                # End child span on success
                client.end_span(
                    request_id=child_span.request_id,
                    span_id=child_span.span_id,
                    outputs={"result": result},
                    status="OK"
                )
            except Exception as e:
                # End child span on error
                client.end_span(
                    request_id=child_span.request_id,
                    span_id=child_span.span_id,
                    status="ERROR",
                    attributes={"error": str(e)}
                )
                raise

        except Exception as e:
            # Log error to trace
            if root_span:
                client.end_trace(
                    request_id=root_span.request_id,
                    status="ERROR",
                    attributes={"error_type": type(e).__name__, "error_message": str(e)}
                )
            raise
        else:
            # End trace on success
            client.end_trace(
                request_id=root_span.request_id,
                outputs={"status": "completed"},
                status="OK"
            )
    ```
  </TabItem>
  <TabItem value="custom-management" label="Custom Trace Management">
    Implement custom trace ID generation and management for integration with existing systems:

    ```python
    import uuid
    from datetime import datetime

    class CustomTraceManager:
        """Custom trace manager with business-specific trace IDs"""

        def __init__(self):
            self.client = MlflowClient()
            self.active_traces = {}

        def generate_trace_id(self, user_id: str, operation: str) -> str:
            """Generate custom trace ID based on business logic"""
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            return f"{user_id}_{operation}_{timestamp}_{uuid.uuid4().hex[:8]}"

        def start_custom_trace(self, user_id: str, operation: str, **kwargs):
            """Start trace with custom ID format"""
            trace_name = self.generate_trace_id(user_id, operation)

            root_span = self.client.start_trace(
                name=trace_name,
                attributes={
                    "user_id": user_id,
                    "operation": operation,
                    "custom_trace_id": trace_name,
                    **kwargs
                }
            )

            self.active_traces[trace_name] = root_span
            return root_span

        def get_active_trace(self, trace_name: str):
            """Retrieve active trace by custom name"""
            return self.active_traces.get(trace_name)

    # Usage
    manager = CustomTraceManager()
    trace = manager.start_custom_trace(
        user_id="user123",
        operation="report_generation",
        report_type="quarterly"
    )
    ```
  </TabItem>
  <TabItem value="batch-processing" label="Batch Processing">
    Track complex workflows with multiple levels of nesting:

    ```python
    def batch_processor(items):
        client = MlflowClient()

        # Start main trace
        root = client.start_trace(
            name="batch_processing",
            inputs={"batch_size": len(items)}
        )

        results = []

        # Process each item
        for i, item in enumerate(items):
            # Create span for each item
            item_span = client.start_span(
                name=f"process_item_{i}",
                request_id=root.request_id,
                parent_id=root.span_id,
                inputs={"item_id": item["id"]}
            )

            try:
                # Validation span
                validation_span = client.start_span(
                    name="validate",
                    request_id=root.request_id,
                    parent_id=item_span.span_id
                )

                is_valid = validate_item(item)

                client.end_span(
                    request_id=validation_span.request_id,
                    span_id=validation_span.span_id,
                    outputs={"is_valid": is_valid}
                )

                if is_valid:
                    # Processing span
                    process_span = client.start_span(
                        name="transform",
                        request_id=root.request_id,
                        parent_id=item_span.span_id
                    )

                    result = transform_item(item)
                    results.append(result)

                    client.end_span(
                        request_id=process_span.request_id,
                        span_id=process_span.span_id,
                        outputs={"transformed": result}
                    )

                # End item span
                client.end_span(
                    request_id=item_span.request_id,
                    span_id=item_span.span_id,
                    status="OK"
                )

            except Exception as e:
                # Handle errors gracefully
                client.end_span(
                    request_id=item_span.request_id,
                    span_id=item_span.span_id,
                    status="ERROR",
                    attributes={"error": str(e)}
                )

        # End main trace
        client.end_trace(
            request_id=root.request_id,
            outputs={
                "processed_count": len(results),
                "success_rate": len(results) / len(items)
            }
        )

        return results
    ```
  </TabItem>
</Tabs>

## Best Practices

<Tabs>
  <TabItem value="context-managers" label="Context Managers" default>
    Create custom context managers to ensure spans are always closed:

    ```python
    from contextlib import contextmanager

    @contextmanager
    def traced_span(client, name, request_id, parent_id=None, **kwargs):
        """Context manager for safe span management"""
        span = client.start_span(
            name=name,
            request_id=request_id,
            parent_id=parent_id,
            **kwargs
        )
        try:
            yield span
        except Exception as e:
            client.end_span(
                request_id=span.request_id,
                span_id=span.span_id,
                status="ERROR",
                attributes={"error": str(e)}
            )
            raise
        else:
            client.end_span(
                request_id=span.request_id,
                span_id=span.span_id,
                status="OK"
            )

    # Usage
    with traced_span(client, "my_operation", request_id, parent_id) as span:
        # Your code here
        result = perform_operation()
    ```
  </TabItem>
  <TabItem value="state-management" label="State Management">
    Manage trace state for complex applications:

    ```python
    class TraceStateManager:
        """Manage trace state across application components"""

        def __init__(self):
            self.client = MlflowClient()
            self._trace_stack = []

        @property
        def current_trace(self):
            """Get current active trace"""
            return self._trace_stack[-1] if self._trace_stack else None

        def push_trace(self, name: str, **kwargs):
            """Start a new trace and push to stack"""
            if self.current_trace:
                # Create child span if trace exists
                span = self.client.start_span(
                    name=name,
                    request_id=self.current_trace.request_id,
                    parent_id=self.current_trace.span_id,
                    **kwargs
                )
            else:
                # Create new trace
                span = self.client.start_trace(name=name, **kwargs)

            self._trace_stack.append(span)
            return span

        def pop_trace(self, **kwargs):
            """End current trace and pop from stack"""
            if not self._trace_stack:
                return

            span = self._trace_stack.pop()

            if self._trace_stack:
                # End child span
                self.client.end_span(
                    request_id=span.request_id,
                    span_id=span.span_id,
                    **kwargs
                )
            else:
                # End root trace
                self.client.end_trace(
                    request_id=span.request_id,
                    **kwargs
                )
    ```
  </TabItem>
  <TabItem value="attributes" label="Meaningful Attributes">
    Enrich your traces with context that aids debugging:

    **Good Examples:**
    ```python
    # Good: Specific, actionable attributes
    client.start_span(
        name="llm_call",
        request_id=request_id,
        parent_id=parent_id,
        attributes={
            "model": "gpt-4",
            "temperature": 0.7,
            "max_tokens": 1000,
            "prompt_template": "rag_v2",
            "user_tier": "premium"
        }
    )
    ```

    **Bad Examples:**
    ```python
    # Bad: Generic, unhelpful attributes
    client.start_span(
        name="process",
        request_id=request_id,
        parent_id=parent_id,
        attributes={"step": 1, "data": "some data"}
    )
    ```
  </TabItem>
</Tabs>

## Common Pitfalls

:::warning
**Avoid these common mistakes:**

**Forgetting to end spans** - Always use try/finally or context managers

**Incorrect parent-child relationships** - Double-check span IDs

**Mixing high-level and low-level APIs** - They don't interoperate

**Hardcoding trace IDs** - Always generate unique IDs

**Ignoring thread safety** - Client APIs are not thread-safe by default
:::

## Advanced Patterns

<Tabs>
  <TabItem value="distributed" label="Distributed Tracing" default>
    For distributed systems, you can propagate trace context across services:

    ```python
    def service_a_operation():
        client = MlflowClient()
        
        # Start trace in service A
        root_span = client.start_trace("distributed_operation")
        
        # Pass trace context to service B
        trace_context = {
            "request_id": root_span.request_id,
            "parent_span_id": root_span.span_id
        }
        
        # Call service B with context
        result = call_service_b(trace_context)
        
        # End trace in service A
        client.end_trace(
            request_id=root_span.request_id,
            outputs={"result": result}
        )

    def service_b_operation(trace_context):
        client = MlflowClient()
        
        # Create child span in service B
        span = client.start_span(
            name="service_b_processing",
            request_id=trace_context["request_id"],
            parent_id=trace_context["parent_span_id"]
        )
        
        # Do work in service B
        result = process_in_service_b()
        
        # End span
        client.end_span(
            request_id=span.request_id,
            span_id=span.span_id,
            outputs={"processed": result}
        )
        
        return result
    ```
  </TabItem>
  <TabItem value="conditional" label="Conditional Tracing">
    Implement intelligent tracing that adapts based on conditions:

    ```python
    class ConditionalTracer:
        def __init__(self, sample_rate=0.1, error_always_trace=True):
            self.client = MlflowClient()
            self.sample_rate = sample_rate
            self.error_always_trace = error_always_trace
            self.active_traces = {}

        def should_trace(self, operation_type: str, user_tier: str = None) -> bool:
            """Decide whether to trace based on conditions"""
            import random
            
            # Always trace for premium users
            if user_tier == "premium":
                return True
                
            # Always trace critical operations
            if operation_type in ["payment", "security"]:
                return True
                
            # Sample other operations
            return random.random() < self.sample_rate

        def conditional_trace(self, name: str, operation_type: str, **kwargs):
            """Start trace only if conditions are met"""
            if self.should_trace(operation_type, kwargs.get("user_tier")):
                trace = self.client.start_trace(name, **kwargs)
                self.active_traces[trace.request_id] = trace
                return trace
            return None

        def conditional_end_trace(self, request_id: str, **kwargs):
            """End trace if it was started"""
            if request_id in self.active_traces:
                self.client.end_trace(request_id=request_id, **kwargs)
                del self.active_traces[request_id]
    ```
  </TabItem>
  <TabItem value="performance" label="Performance Optimization">
    Optimize tracing for high-performance applications:

    ```python
    class PerformantTracer:
        def __init__(self, batch_size=100, flush_interval=5.0):
            self.client = MlflowClient()
            self.batch_size = batch_size
            self.flush_interval = flush_interval
            self.span_queue = []
            self.last_flush = time.time()

        def add_span_to_queue(self, span_data):
            """Queue spans for batch processing"""
            self.span_queue.append(span_data)
            
            # Flush if batch is full or time interval passed
            if (len(self.span_queue) >= self.batch_size or 
                time.time() - self.last_flush > self.flush_interval):
                self.flush_spans()

        def flush_spans(self):
            """Process queued spans in batch"""
            if not self.span_queue:
                return
                
            # Process all queued spans
            for span_data in self.span_queue:
                # Create and end span immediately
                span = self.client.start_span(**span_data["start_params"])
                self.client.end_span(**span_data["end_params"])
            
            # Clear queue
            self.span_queue.clear()
            self.last_flush = time.time()

        def traced_operation(self, name: str, operation, **kwargs):
            """Execute operation with deferred tracing"""
            start_time = time.time()
            
            try:
                result = operation()
                status = "OK"
                error_info = {}
            except Exception as e:
                status = "ERROR"
                error_info = {"error": str(e)}
                raise
            finally:
                # Queue span data instead of creating immediately
                span_data = {
                    "start_params": {
                        "name": name,
                        "request_id": kwargs.get("request_id"),
                        "parent_id": kwargs.get("parent_id"),
                        "inputs": kwargs.get("inputs", {})
                    },
                    "end_params": {
                        "outputs": {"result": result} if "result" in locals() else {},
                        "status": status,
                        "attributes": {
                            "duration_ms": (time.time() - start_time) * 1000,
                            **error_info
                        }
                    }
                }
                self.add_span_to_queue(span_data)
    ```
  </TabItem>
</Tabs>

## When to Use Client APIs

<Tabs>
  <TabItem value="use-for" label="Use Client APIs for" default>
    **Custom trace ID generation schemes**

    **Integration with existing trace systems**

    **Complex trace lifecycle management**

    **Advanced span hierarchies**

    **Custom trace state management**

    **Distributed tracing across services**

    **Conditional or sampling-based tracing**
  </TabItem>
  <TabItem value="avoid-for" label="Avoid Client APIs for">
    **Simple function tracing** (use `@mlflow.trace`)

    **Local Python applications** (use context managers)

    **Quick prototyping** (use high-level APIs)

    **Integration with auto-tracing**

    **Standard workflow patterns** (use decorators)
  </TabItem>
</Tabs>

## Performance Considerations

**Batch Operations**: When creating many spans, consider batching operations to reduce overhead.

**Memory Management**: Be mindful of keeping references to span objects - clean them up when done.

**Network Calls**: Each start/end operation may result in network calls to the tracking server.

**Thread Safety**: Use locks or thread-local storage when using client APIs in multi-threaded environments.

## Next Steps

**[High-Level APIs](/tracing/app-instrumentation/manual-tracing/fluent-apis)** - Simpler alternative for most use cases

**[Automatic Tracing](/tracing/app-instrumentation/automatic)** - One-line tracing for supported frameworks

**[Trace Data Model](/tracing/data-model)** - Understanding trace structure and components

**[Querying Traces](/tracing/api/search)** - Programmatically search and analyze your traces