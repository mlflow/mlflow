import Tabs from "@theme/Tabs"
import TabItem from "@theme/TabItem"
import { APILink } from "@site/src/components/APILink";
import TabsWrapper from "@site/src/components/TabsWrapper";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { GitMerge } from "lucide-react";

# Instrument Your App with MLflow Tracing

:::tip
New to MLflow Tracing? Checkout the <ins>[Quick Start Guide](/genai/tracing/quickstart/python-openai)</ins> to get started.
:::

This page provides a comprehensive guide for instrumenting your GenAI applications with MLflow Tracing.

## 1. Installation

<TabsWrapper>
<Tabs>
  <TabItem value="python" label="Python" default>
    Add [`mlflow-tracing`](https://pypistats.org/packages/mlflow-tracing) to your Python environment to enable tracing features with minimum set of dependencies.
    ```bash
    pip install mlflow-tracing
    ```
  </TabItem>
  <TabItem value="js" label="JS/TS">
    ```bash
    npm install mlflow-tracing
    ```
  </TabItem>
</Tabs>
</TabsWrapper>

:::info

The [`mlflow-tracing`](https://pypistats.org/packages/mlflow-tracing) Python package does not include other MLflow features, such as the self-host tracking server, evaluation capabilities, prompt management, etc. If you want to use the full set of MLflow's features, please install the full MLflow package. We recommend using the [`mlflow-tracing`](https://pypistats.org/packages/mlflow-tracing) package for production applications to keep the dependencies and package footprint minimal.

```bash
pip install mlflow
```

:::

## 2. Connect Your Application with MLflow

<TabsWrapper>
<Tabs>
  <TabItem value="local" label="Self-Host MLflow (Local)" default>
    For local development or personal use, locally hosting MLflow is a good option. While this is not recommended for production use, it is the fastest way to get started. For logging traces, we recommend using a [SQL Backend](https://mlflow.org/docs/latest/tracking.html#sql-backend) for better performance.

    To start a local MLflow server, run the following command in your terminal. Alternatively, you can use the official [Docker image](https://github.com/mlflow/mlflow/pkgs/container/mlflow) to run the server.

    ```bash
    # Replace sqlite:/// part with your preferred database URI.
    mlflow server --host 127.0.0.1 --port 5000 --backend-store-uri sqlite:///mlruns.db
    ```

    Then, in your application, configure the tracking URI and set an active experiment using the following code:

    **Python:**

    ```python
    import mlflow

    mlflow.set_tracking_uri("http://127.0.0.1:5000")
    mlflow.set_experiment("<your-experiment-name>")
    ```

    **JS/TS:**

    ```typescript
    import * as mlflow from "mlflow-tracing";

    mlflow.init(
      trackingUri: "http://127.0.0.1:5000",
      experimentId: "<your-experiment-id>",  // If no experiment is created yet, create it on UI first.
    );
    ```

  </TabItem>
  <TabItem value="remote" label="Remote MLflow">
    When you have a remote MLflow server already running (either self-hosted or a cloud-based managed MLflow), you can connect your application to it by configuring the tracking URI.

    **Python**

    ```python
    import mlflow

    mlflow.set_tracking_uri("https://<your-mlflow-server>:<port>")
    mlflow.set_experiment("<your-experiment-name>")
    ```

    **JS/TS:**

    ```typescript
    import * as mlflow from "mlflow-tracing";

    mlflow.init(
      trackingUri: "https://<your-mlflow-server.com>:<port>",
      experimentId: "<your-experiment-id>",  // If no experiment is created yet, create it within the MLflow UI first.
    );
    ```

  </TabItem>
  <TabItem value="databricks" label="Databricks">

    :::info
    These steps describe using a Databricks Personal Access Token. MLflow also works with the other <ins>[Databricks-supported authentication methods](https://mlflow.org/docs/latest/tracking.html#databricks)</ins>.
    :::

    If you're working on tracing with Databricks, configure your authentication with the Databricks CLI or by using the following environment variables:

    ```bash
    export DATABRICKS_HOST="https://your-workspace.databricks.com"
    export DATABRICKS_TOKEN="your-databricks-token"
    ```

    Then, set the tracking URI and the target experiment to store the traces in MLflow from your application.

    **Python:**
    ```python
    mlflow.set_tracking_uri("databricks")
    mlflow.set_experiment("<your-experiment-name>")
    ```

    **JS/TS:**

    ```typescript
    import * as mlflow from "mlflow-tracing";

    mlflow.init(
      trackingUri: "databricks",
      experimentId: "<your-experiment-id>",
    );
    ```

  </TabItem>
  <TabItem value="otel" label="OpenTelemetry">
    MLflow traces are compatible with OpenTelemetry, allowing you to export traces to OpenTelemetry-compatible monitoring backends.

    Install the OpenTelemetry Protocol (OTLP) exporter:

    ```bash
    pip install opentelemetry-exporter-otlp
    ```


    Then, configure the following environment variables:

    ```bash
    export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="http://your-otel-collector.com:4317/v1/traces"
    ```

    Your monitoring backends may have additional setup to enable trace exporting. Please refer to [OpenTelemetry Integration](/genai/tracing/prod-tracing#opentelemetry-integration) for additional configurations and links to the each vendor's setup documentation.

    :::note
      OpenTelemetry export is only supported for the Python SDK right now.
    :::

  </TabItem>
</Tabs>
</TabsWrapper>

## 3. Instrumenting Your Application Logic

MLflow offers different ways to instrument your application logic. Follow the links below to learn more about each approach to instrument your application:

<TilesGrid>
  <TileCard
    image="/images/llms/tracing/app-instrumentation/autolog-logos.png"
    title="Automatic Tracing"
    description="Instrument your application with a single line of code."
    href="/genai/tracing/app-instrumentation/automatic"
    containerHeight={96}
  />
  <TileCard
    image="/images/llms/tracing/app-instrumentation/manual-tracing2.png"
    title="Manual Tracing"
    description="Instrument any Python code with a few lines of code, with full control and flexibility."
    href="/genai/tracing/app-instrumentation/manual-tracing"
    containerHeight={96}
  />
  <TileCard
    image="/images/logos/javascript-typescript-logo.png"
    title="Typescript SDK"
    description="Instrument your Node.js applications with MLflow Tracing Typescript SDK."
    href="/genai/tracing/app-instrumentation/typescript-sdk"
    containerHeight={96}
  />
</TilesGrid>

## ⚙️ Common Patterns

### Production Considerations

MLflow Tracing is production ready, but in order to ensure the scalability and reliability of the tracing system, we recommend the following best practices:

1. Enable [Async Logging](/genai/tracing/prod-tracing#asynchronous-trace-logging) and set up appropriate queue size and timeout.
2. Use the lightweight [`mlflow-tracing`](https://pypistats.org/packages/mlflow-tracing) package for minimizing the package footprint and dependencies.
3. Use [managed MLflow services](/genai/tracing/prod-tracing#managed-monitoring-with-databricks) for reducing the operational overhead and ensure the scalability of the tracing system.
4. When using self-hosted MLflow, make sure to use the **SQL Backend** with a scalable database like PostgreSQL. The default file-based backend has scalability limitations and is not recommended for production use.

### Async Applications

Async programming is an effective tool for improving the throughput of your application, particularly for LLM-based applications that are typically I/O bound. MLflow Tracing natively [supports instrumenting async applications](/genai/tracing/app-instrumentation/manual-tracing#async-support).

### Multi-Threaded Applications

Multi-threading is a common strategy for parallelizing IO-bound operations in applications. MLflow Tracing supports [multi-threaded applications](/genai/tracing/app-instrumentation/manual-tracing#multi-threading) using context propagation.

### Managing User sessions

Many LLM-based applications are deployed as chat-based applications, where each user session is a separate thread. Grouping traces by user session is a common practice. MLflow Tracing supports [managing user sessions](/genai/tracing/track-users-sessions).

### Redacting PII Data

Traces can contain sensitive data such as raw user inputs, internal document contents, etc. MLflow Tracing supports [redacting PII data](/genai/tracing/observe-with-traces/masking) using flexible masking rules, custom functions, and integration with external PII masking libraries.

### Collecting User Feedbacks

User feedback is a valuable source of information for improving the user experience of your application. MLflow Tracing supports [collecting user feedback on traces](/genai/tracing/collect-user-feedback) to track and analyze the feedbacks effectively.
