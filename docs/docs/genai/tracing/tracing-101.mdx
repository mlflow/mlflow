import DatabricksCallout from "@site/src/components/DatabricksCallout"
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Tracing Concepts

<DatabricksCallout docsPath="/mlflow3/genai/tracing/tracing-101" />

Understanding the core concepts of tracing is essential for effectively monitoring and debugging GenAI applications. This guide explains the fundamental building blocks of MLflow Tracing and how they apply to different types of applications.

:::tip
A good companion to the explanations in this guide is the [Tracing Data Model](/genai/tracing/data-model) guide, which shows how MLflow Tracing represents the concepts discussed here.
:::

## What is Tracing?

Tracing in the context of machine learning (ML) refers to the detailed tracking and recording of the data flow and processing steps during the execution of an ML model. It provides transparency and insights into each stage of the model's operation, from data input to prediction output. This detailed tracking is crucial for debugging, optimizing, and understanding the performance of ML models.

![MLflow Tracing](/images/llms/tracing/tracing-top.gif)

<Tabs>
<TabItem value="core-concepts" label="Core Concepts" default>

## Concept of a Span

In the context of tracing, a **span** represents a single operation within the system. It captures metadata such as the start time, end time, and other contextual information about the operation. Along with the metadata, the inputs that are provided to a unit of work (such as a call to a GenAI model, a retrieval query from a vector store, or a function call), as well as the output from the operation, are recorded.

The diagram below illustrates a call to a GenAI model and the collection of relevant information within a span:

![Span Structure](/images/llms/tracing/span-anatomy.png)

### Key Span Components

- **Operation Name**: Identifies what the span represents (e.g., "openai_chat_completion", "vector_search")
- **Start/End Times**: When the operation began and completed
- **Inputs**: The data sent to the operation
- **Outputs**: The data returned from the operation
- **Attributes**: Additional metadata about the operation
- **Status**: Whether the operation succeeded or failed

## Concept of a Trace

A **trace** is a collection of spans organized in a Directed Acyclic Graph (DAG) structure that represents the complete execution flow of a request through your system. Each span represents a single operation, and these spans are linked together to form a comprehensive view of the end-to-end process.

### Key Trace Characteristics

- **DAG-like Structure**: Ensures no cycles in the sequence of operations, making execution flow clear
- **Span Relationships**: Parent-child relationships between spans show how operations are nested
- **Hierarchical Association**: Spans mirror your application's structure, showing component interactions
- **Complete Context**: Full visibility from initial request to final response

### Example: Simple Chat Application

```python
import mlflow
from openai import OpenAI

# This creates a trace with multiple spans
@mlflow.trace  # Root span: "process_chat_request"
def process_chat_request(user_message):
    # Child span: automatically created for OpenAI call
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": user_message}]
    )
    return response.choices[0].message.content
```

This simple function creates a trace with:
1. **Root span**: `process_chat_request` (your function)
2. **Child span**: `openai_chat_completion` (automatically captured)

</TabItem>

<TabItem value="genai-applications" label="GenAI Applications">

## GenAI ChatCompletions Use Case

In Generative AI (GenAI) applications, such as chat completions, tracing becomes essential for developers building GenAI-powered applications. These applications involve generating human-like text based on input prompts, and tracing provides visibility into the entire interaction context.

![GenAI ChatCompletions Architecture](/images/llms/tracing/chat-completions-architecture.png)

### What Tracing Captures

Enabling tracing on chat interfaces allows you to evaluate:

- **Full Contextual History**: Complete conversation context
- **Prompt Engineering**: How prompts are constructed and modified
- **Input Processing**: User input validation and preprocessing
- **Configuration Parameters**: Model settings and their effects
- **Output Generation**: Response quality and characteristics

### Key Metadata for ChatCompletions

Additional metadata surrounding the inference process is useful for various reasons:

- **Token Counts**: Number of tokens processed (affects billing and performance)
- **Model Name**: Specific model used for inference
- **Provider Type**: Service or platform providing the model (OpenAI, Anthropic, etc.)
- **Query Parameters**: Settings like temperature, top-k, max_tokens
- **Query Input**: The request input (user question)
- **Query Response**: System-generated response
- **Latency**: Time taken for each operation
- **Cost**: API costs associated with the request

### Example: Enhanced Chat Application

```python
import mlflow
from openai import OpenAI
import time

@mlflow.trace
def enhanced_chat_completion(user_message, conversation_history=None):
    start_time = time.time()
    
    # Add context to the trace
    mlflow.update_current_trace(
        tags={
            "application": "customer_support_chat",
            "user_type": "premium",
            "conversation_length": len(conversation_history or [])
        }
    )
    
    # Prepare messages with history
    messages = conversation_history or []
    messages.append({"role": "user", "content": user_message})
    
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.7,
        max_tokens=500
    )
    
    # Add performance metrics
    mlflow.update_current_trace(
        attributes={
            "response_time_seconds": time.time() - start_time,
            "token_count": response.usage.total_tokens,
            "model_used": response.model
        }
    )
    
    return response.choices[0].message.content
```

</TabItem>

<TabItem value="rag-applications" label="RAG Applications">

## Advanced Retrieval-Augmented Generation (RAG) Applications

In more complex applications like Retrieval-Augmented Generation (RAG), tracing becomes essential for effective debugging and optimization. RAG involves multiple stages, including document retrieval, embedding, and interaction with GenAI models.

![RAG Architecture](/images/llms/tracing/rag-architecture.png)

### The Challenge Without Tracing

When only the input and output are visible, it becomes challenging to identify the source of issues. If a GenAI system generates an unsatisfactory response, the problem might lie in:

- **Vector Store Optimization**: Efficiency and accuracy of document retrieval
- **Embedding Model**: Quality of the model used to encode and search documents
- **Reference Material**: Content and quality of the documents being queried
- **Query Processing**: How user queries are transformed for retrieval
- **Context Assembly**: How retrieved documents are combined with the prompt

### Critical Steps in RAG (Often Hidden)

Without tracing, these steps are effectively a "black box":

1. **Embedding of the input query**
2. **The return of the encoded query vector**
3. **The vector search input**
4. **The retrieved document chunks from the Vector Database**
5. **The final input to the GenAI model**

### Example: Traced RAG Application

```python
import mlflow
from openai import OpenAI
import numpy as np

@mlflow.trace
def rag_chat_completion(user_question, vector_store):
    # Tag the trace for RAG-specific monitoring
    mlflow.update_current_trace(
        tags={
            "application_type": "rag",
            "vector_store_type": "chromadb",
            "retrieval_strategy": "semantic_search"
        }
    )
    
    # Step 1: Embed the user question
    embedded_question = embed_query(user_question)
    
    # Step 2: Retrieve relevant documents
    relevant_docs = retrieve_documents(embedded_question, vector_store)
    
    # Step 3: Generate response with context
    response = generate_with_context(user_question, relevant_docs)
    
    return response

@mlflow.trace
def embed_query(query):
    """Convert user query to vector embedding."""
    # Embedding logic here
    mlflow.update_current_trace(
        attributes={
            "query_length": len(query),
            "embedding_model": "text-embedding-ada-002"
        }
    )
    # Return embedding vector
    return np.random.rand(1536)  # Placeholder

@mlflow.trace
def retrieve_documents(query_embedding, vector_store, top_k=5):
    """Retrieve relevant documents from vector store."""
    # Vector search logic here
    mlflow.update_current_trace(
        attributes={
            "top_k": top_k,
            "search_type": "cosine_similarity"
        }
    )
    
    # Simulate document retrieval
    documents = [
        {"content": "Document 1 content...", "score": 0.85},
        {"content": "Document 2 content...", "score": 0.82},
    ]
    
    mlflow.update_current_trace(
        attributes={
            "documents_found": len(documents),
            "avg_relevance_score": sum(d["score"] for d in documents) / len(documents)
        }
    )
    
    return documents

@mlflow.trace
def generate_with_context(question, documents):
    """Generate answer using retrieved context."""
    # Prepare context from documents
    context = "\n".join([doc["content"] for doc in documents])
    
    mlflow.update_current_trace(
        attributes={
            "context_length": len(context),
            "num_context_docs": len(documents)
        }
    )
    
    # Generate response with context
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": f"Answer based on this context: {context}"},
            {"role": "user", "content": question}
        ]
    )
    
    return response.choices[0].message.content
```

### Benefits of RAG Tracing

With this tracing setup, you can:

- **Debug Retrieval Issues**: See exactly which documents were retrieved and their relevance scores
- **Optimize Embedding**: Monitor embedding quality and performance
- **Tune Context Assembly**: Understand how context affects generation quality
- **Monitor Performance**: Track latency at each stage of the pipeline
- **Analyze Failures**: Identify which component caused issues

</TabItem>

<TabItem value="traditional-ml" label="Traditional ML">

## Beyond GenAI: Tracing for Traditional Machine Learning

:::note
While this documentation focuses on GenAI applications where tracing provides the most value, MLflow Tracing can also be applied to traditional ML workflows for monitoring and performance analysis.
:::

In traditional ML, the inference process is relatively straightforward compared to GenAI applications. When a request is made, input data is fed into the model, which processes the data and generates a prediction.

![Traditional ML Inference Architecture](/images/llms/tracing/tracing-traditional-ml.png)

### Traditional ML Characteristics

- **Transparent Process**: Both input and output are clearly defined and understandable
- **Deterministic**: Same input typically produces the same output
- **Single Model**: Usually involves one primary model for prediction
- **Structured Data**: Often works with tabular or well-defined data formats

### Example: Spam Detection Model

```python
# Input: Email text
email_text = "Congratulations! You've won $1000. Click here to claim..."

# Output: Binary classification
is_spam = True  # or False
```

This process is wholly visible - the email content and spam/not-spam label are both interpretable.

### When Tracing Helps in Traditional ML

While qualitative model performance assessment may not require tracing, it can still provide value for:

#### 1. Performance Monitoring
```python
import mlflow
import time

@mlflow.trace
def predict_fraud(transaction_data):
    start_time = time.time()
    
    # Preprocess the data
    processed_data = preprocess_transaction(transaction_data)
    
    # Make prediction
    prediction = fraud_model.predict(processed_data)
    
    # Log performance metrics
    mlflow.update_current_trace(
        attributes={
            "prediction_time_ms": (time.time() - start_time) * 1000,
            "model_version": "v2.1.0",
            "confidence_score": prediction.probability
        }
    )
    
    return prediction
```

#### 2. API Access Logging
```python
@mlflow.trace
def ml_api_endpoint(request_data, user_id):
    mlflow.update_current_trace(
        tags={
            "user_id": user_id,
            "api_version": "v1",
            "endpoint": "/predict"
        }
    )
    
    # Process request and return prediction
    result = make_prediction(request_data)
    
    mlflow.update_current_trace(
        attributes={
            "request_size_bytes": len(str(request_data)),
            "response_size_bytes": len(str(result))
        }
    )
    
    return result
```

#### 3. Multi-Model Pipelines
```python
@mlflow.trace
def ensemble_prediction(input_data):
    # Use multiple models in sequence
    preprocessed = preprocessing_model(input_data)
    features = feature_extraction_model(preprocessed)
    prediction = final_model(features)
    
    return prediction
```

### Differences from GenAI Tracing

| Aspect | Traditional ML | GenAI Applications |
|--------|---------------|-------------------|
| **Complexity** | Simple input â†’ output | Multi-step, contextual processes |
| **Transparency** | High (interpretable I/O) | Low (complex internal processing) |
| **Debugging Need** | Performance & infrastructure | Quality, relevance, hallucinations |
| **Trace Value** | Operational monitoring | Essential for development & debugging |

</TabItem>
</Tabs>

<details>
    <summary>Key Benefits of Tracing</summary>

Regardless of the application type, tracing provides several key benefits:

1. **Debugging and Troubleshooting**
- Identify where failures occur in complex pipelines
- Understand the root cause of unexpected outputs
- Debug performance bottlenecks

2. **Performance Optimization**
- Monitor latency at each stage
- Identify slow components in your pipeline
- Track resource usage and costs

3. **Quality Assurance**
- Evaluate the quality of each component
- Monitor drift in model performance
- Ensure consistent behavior across deployments

4. **Operational Monitoring**
- Track API usage and user behavior
- Monitor system health and availability
- Generate alerts based on trace data

5. **Development Insights**
- Understand how different configurations affect outcomes
- A/B test different approaches
- Collect data for model improvements
</details>

## Getting Started with Tracing

Ready to implement tracing in your applications? Check out these resources:

- **[Quickstart Guide](/genai/tracing/quickstart)**: Get tracing working in 10 minutes
- **[Automatic Tracing](/genai/tracing/integrations/)**: One-line setup for popular libraries
- **[Manual Instrumentation](/genai/tracing/app-instrumentation/manual-tracing)**: Custom tracing for complex applications
- **[Production Deployment](/genai/tracing/prod-tracing)**: Scale tracing for production workloads

## Summary

Tracing is a powerful technique for understanding and optimizing ML applications:

- **Spans** capture individual operations with their inputs, outputs, and metadata
- **Traces** provide end-to-end visibility into request processing
- **GenAI applications** benefit most from tracing due to their complexity
- **Traditional ML** can use tracing for performance and operational monitoring
- **RAG applications** require tracing to debug multi-step retrieval and generation processes

Whether you're building a simple chatbot or a complex RAG system, tracing provides the observability needed to build reliable, high-quality ML applications.