import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { APILink } from "@site/src/components/APILink";
import ImageBox from '@site/src/components/ImageBox';
import TabsWrapper from '@site/src/components/TabsWrapper';

# Tracing Quickstart (TS/JS)

:::info

The Python quickstart is available <ins>[here](/genai/tracing/quickstart/python-openai)</ins>.

:::

This quickstart guide will walk you through setting up a simple GenAI application with MLflow Tracing. In less than 10 minutes, you'll enable tracing, run a basic application, and explore the generated traces in the MLflow UI.

## Prerequisites

Install the required packages by running the following command:

```bash
npm install mlflow-openai
```

:::info

The code example in this guide uses OpenAI SDK, however, the contents of this guide are applicable to any other LLM providers, such as Anthropic, Google, Bedrock, and more.

For other LLM providers that are not natively integrated with auto-tracing, install the `mlflow-tracing` package and wrap the LLM call with the `mlflow.trace` wrapper function instead.

:::

## Step 1: Set up your environment

### Connect to MLflow

MLflow logs Traces in a tracking server. Connect your application to the tracking server by one of the following methods.

<TabsWrapper>
<Tabs>
<TabItem value="python" label="Self Host (pip) " default>

The Typescript package does not contain the tracking server. To start a self-hosted tracking server from Python:

```bash
# Install mlflow python package
pip install mlflow -U

# Start MLflow tracking server locally
mlflow ui --port 5000

# This will start the server at http://127.0.0.1:5000
```

</TabItem>

<TabItem value="docker" label="Self Host (Docker) ">

The Typescript package does not contain the tracking server. To start a self-hosted tracking server using Docker:

# TODO: Update this

```bash
# Pull the latest mlflow docker image
docker pull mlflow/mlflow:latest

# Start MLflow tracking server locally
docker run -p 5000:5000 mlflow/mlflow:latest

# This will start the server at http://127.0.0.1:5000
```

</TabItem>

<TabItem value="remote" label="Remove MLflow Server">

If you have a remote MLflow tracking server already running, skip this step.


</TabItem>

<TabItem value="databricks" label="Databricks">

If you use [Databricks managed MLflow](https://docs.databricks.com/aws/en/mlflow3/genai/), you can skip this step.

</TabItem>

</Tabs>
</TabsWrapper>

:::tip

If you are unsure about how to set up MLflow tracking server, you can start with the cloud-based MLflow powered by Databricks: <ins>[Sign up for free â†’](https://login.databricks.com/?destination_url=%2Fml%2Fexperiments-signup%3Fsource%3DTRY_MLFLOW&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW&utm_source=mlflow_org&tuuid=a9534f33-78bf-4b81-becc-4334e993251d&rl_aid=e6685d78-9f85-4fed-b64f-08e247f53547&intent=SIGN_UP)</ins>

:::

### Create a new MLflow Experiment

Create a new MLflow experiment in the MLflow UI, or choose an existing experiment.

### Initialize the Tracing SDK

<TabsWrapper>
  <Tabs>
    <TabItem value="self-hosted" label="Self Host" default>

    ```typescript
    import { init } from "mlflow-tracing";

    init({
        trackingUri: "<your-tracking-server-uri>",
        experimentId: "<your-experiment-id>",
    });
    ```
    </TabItem>

    <TabItem value="remote" label="Remote MLflow Server">

    ```typescript
    import { init } from "mlflow-tracing";

    init({
        trackingUri: "<remote-tracking-server-uri>",
        experimentId: "<your-experiment-id>",
    });
    ```

    </TabItem>

    <TabItem value="databricks" label="Databricks">

    Method 1: Explicitly set the host and token in the init function.

    ```typescript
    import { init } from "mlflow-tracing";

    init({
        trackingUri: "databricks",
        experimentId: "<your-experiment-id>",
        host: "<your-databricks-host>",
        token: "<your-databricks-personal-access-token>",
    });
    ```

    Method 2: Use Databricks config file for authentication.

    ```typescript
    import { init } from "mlflow-tracing";

    init({
        trackingUri: "databricks",
        experimentId: "<your-experiment-id>",
        // Optional: Set the Databricks config file path if it is not in the default location
        configPath: "<your-databricks-config-file-path>",
    });
    ```

    Method 3: Use environment variables

    ```bash
    export DATABRICKS_HOST=<your-databricks-host>
    export DATABRICKS_TOKEN=<your-databricks-personal-access-token>
    ```

    ```typescript
    import { init } from "mlflow-tracing";

    init({
        trackingUri: "databricks",
        experimentId: "<your-experiment-id>",
    });
    ```

    </TabItem>
  </Tabs>
</TabsWrapper>

### Set OpenAI API Key (or other LLM providers)

```bash
export OPENAI_API_KEY="your-api-key-here"  # Replace with your actual API key
```

## Step 2: Trace a single LLM call

Let's start with a simple example of tracing a single LLM call. We first wrap the OpenAI client with the `tracedOpenAI` function. After that, every calls to OpenAI API will generate as a trace span, capturing the input, output, latency, token counts, and other metadata.

```typescript
import { OpenAI } from "openai";
import { tracedOpenAI } from "mlflow-openai";

// Wrap the OpenAI client with the tracedOpenAI function
const client = tracedOpenAI(new OpenAI());

// Invoke the client as usual
const response = await client.chat.completions.create({
    model: "o4-mini",
    messages: [
        {"role": "system", "content": "You are a helpful weather assistant."},
        {"role": "user", "content": "What's the weather like in Seattle?"},
    ],
})
```

After running the code above, go to the MLflow UI and select "Traces" tab. It should show the newly created trace.

<ImageBox src="/images/llms/tracing/quickstart/single-openai-trace-list.png" alt="Single Trace" />

The table view shows primary metadata of the trace, such as the trace ID, execution duration, token count, source system, status. You can add/remove columns by selecting the columns in the drop down.

By clicking on the request row (the linked request text), you can view the detailed spans in the trace.

<ImageBox src="/images/llms/tracing/quickstart/single-openai-trace-detail.png" alt="Single Trace Detail" width="80%" />

The "Chat" view in the above screenshot shows the full chat messages exchanged between the user and the model. By clicking other table like "Inputs / Outputs", "Attributes", you can see different aspects of the trace, such as the raw input payload, token usage breakdown, and more.


## Step 3: Trace a tool calling agent

Next, let's add a bit more complexity to the application. To get the real-time weather information, we will use an external weather API as a tool. The application will include a tool calling flow, not only a simple LLM call. To instrument that custom Python flow, we will use the `mlflow.trace` wrapper function.

```typescript
import * as mlflow from "mlflow-tracing";

// Wrap the tool function with the `mlflow.trace` wrapper. The wrapped function will be automatically traced and logged to MLflow.
const getWeather = mlflow.trace(
    // A tool function that fetches the weather information from a weather API
    async function getWeather(latitude: number, longitude: number) {
        const response = await fetch(`https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m`);
        const data = await response.json();
        return data['current']['temperature_2m'];
    },
     // Set the span type to TOOL. You can also set other span configuration here.
    { spanType: mlflow.SpanType.TOOL }
);
```

To pass the function as a tool to the LLM, we need to define the JSON schema for the function.

```typescript
const tools = [{
    type: "function",
    function: {
        name: "get_weather" as const,
        description: "Get current temperature for provided coordinates in celsius.",
        parameters: {
            type: "object",
            properties: {
                latitude: { type: "number" },
                longitude: { type: "number" }
            },
            required: ["latitude", "longitude"],
            additionalProperties: false
        },
        strict: true
    }
}];
```

Lastly, define a simple flow that first ask LLM to get instruction for calling the tool, invoke the tool function, and return the result to the LLM. This example uses the `mlflow.withSpan` API to create a parent span for the agent flow, but you can also achieve the same by using the `mlflow.trace` API like the previous example.

```python
async function runToolAgent(question: string) {
    console.log(`\nðŸ¤– Running tool agent with question: "${question}"`);

    return await mlflow.withSpan(
        async () => {
            const client = tracedOpenAI(new OpenAI());
            const messages: any[] = [{"role": "user", "content": question}];

            // First LLM call to get tool instructions
            const response = await client.chat.completions.create({
                model: "o4-mini",
                messages: messages,
                tools: tools,
            });

            const aiMsg = response.choices[0].message;
            messages.push(aiMsg);

            // If the model requests tool call(s), invoke the function
            if (aiMsg.tool_calls && aiMsg.tool_calls.length > 0) {
                for (const toolCall of aiMsg.tool_calls) {
                    const functionName = toolCall.function.name;

                    if (functionName === "get_weather") {
                        // Invoke the tool function with the provided arguments
                        const args = JSON.parse(toolCall.function.arguments);
                        const toolResult = await getWeather(args.latitude, args.longitude);
                        messages.push({
                            "role": "tool",
                            "tool_call_id": toolCall.id,
                            "content": String(toolResult),
                        });
                    } else {
                        throw new Error(`Invalid tool returned: ${functionName}`);
                    }
                }

                // Second LLM call with tool results
                const finalResponse = await client.chat.completions.create({
                    model: "o4-mini",
                    messages: messages
                });
                return finalResponse.choices[0].message.content;
            }
            return aiMsg.content;
        },
        {
            name: "run_tool_agent",
            spanType: mlflow.SpanType.AGENT,
        }
    );
}

```

Now we can run the application.

```python
# Run the tool calling agent
const question = "What's the weather like in Seattle?";
await runToolAgent(question);
```

## Step 4: Explore Traces in the UI

After running the application, you can explore the traces in the MLflow UI.

<ImageBox src="/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png" alt="Tool Calling Trace" width="90%" />

The trace shows all LLM invocations and tool calls, organized in a tree structure. You can also inspect the timeline breakdown by clicking the timeline icon next to the tree view. This helps you understand where the time is spent in the application.

<ImageBox src="/images/llms/tracing/quickstart/openai-tool-calling-trace-timeline.png" alt="Tool Calling Trace Timeline" width="90%" />

## Step 5: Attach Feedbacks on Traces

As a last step of this quickstart, let's attach a feedback on the generated traces. In the real world development, human feedback is critical to improve the quality of the application.

To add a feedback on a trace, you can open the trace detail page and click the "Add new Assessment" button on the top right. It will open an input form where you can provide a various feedback values and metadata. For example, we can add a feedback called " Quality" with a integer value (1~5), indicating how good the answer is. Then we can also put the detailed rationale behind the score for future reference.

<ImageBox src="/images/llms/tracing/quickstart/openai-trace-feedback-input.png" alt="Feedback Input Form" width="90%" />

When you submit the form with "Create" button, the feedback will be attached to the trace.

<ImageBox src="/images/llms/tracing/quickstart/openai-trace-feedback-record.png" alt="Feedback List" width="90%" />

The aggregated score in the experiment can be seen in the Trace list. You can do slice-and-dice by various criteria, such as timestamp, source, tags, and it will update the aggregated score in real-time.

<ImageBox src="/images/llms/tracing/quickstart/openai-trace-feedback-aggregate.png" alt="Feedback Aggregated" width="90%" />


## Summary

Congratulations! You've successfully:

- âœ… Set up MLflow Tracing for a GenAI application
- âœ… Enabled automatic tracing for OpenAI API calls
- âœ… Generated and explored traces in the MLflow UI
- âœ… Learned how to add custom tracing using decorators
- âœ… Learned how to attach feedback on traces

MLflow Tracing provides powerful observability for your GenAI applications, helping you monitor performance, debug issues, and understand user interactions. Continue exploring the advanced features to get the most out of your tracing setup!


## Next Steps

Now that you have basic tracing working, explore these advanced features:

- [Integration with Other Libraries](/genai/tracing/integrations/): Use MLflow Tracing with other LLM providers and frameworks, such as LangGraph, Pydantic AI.
- [Automatic Evaluation](/genai/eval-monitor): Learn how to set up automatic evaluation for traces using MLflow's GenAI evaluation feature.
- [Production Monitoring](/genai/tracing/prod-tracing): Learn how to use MLflow Tracing in production set up with the optimized SDK.

