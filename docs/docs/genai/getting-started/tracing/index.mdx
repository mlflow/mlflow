---
description: "Learn how to build your first GenAI application using MLflow and Tracing."
title: Overview
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Tracing a GenAI App

MLflow Tracing provides comprehensive visibility into your GenAI application's execution,
helping you debug, optimize, and understand your app's behavior. With tracing, you can see
exactly what happens inside your application - from user inputs to model outputs, including
all intermediate steps, latencies, and token usage.

## Quick Example

Here's how easy it is to add tracing to your GenAI application:

<Tabs>
  <TabItem value="basic" label="Basic Setup">
    ```python
    import mlflow

    @mlflow.trace
    def ask_question(question: str) -> str:
        """Simple traced function that processes a question."""
        response = call_llm(question)
        return response


    ask_question("What is MLflow?")
    ```

  </TabItem>

  <TabItem value="autolog" label="Autologging Usage">
    ```python
    import openai
    import mlflow

    # enable openai autologging
    mlflow.openai.autolog()

    openai_client = openai.OpenAI()
    messages = [
        {
            "role": "user",
            "content": "What is MLflow?",
        }
    ]
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.1,
        max_tokens=100,
    )
    ```

  </TabItem>

  <TabItem value="advanced" label="Advanced Usage">
    ```python
    import mlflow

    @mlflow.trace(name="qa_pipeline", span_type="CHAIN")
    def qa_pipeline(question: str):
        with mlflow.start_span(name="preprocess") as span:
            span.set_attribute("question_length", len(question))
            processed_question = preprocess_question(question)

        with mlflow.start_span(name="llm_call") as span:
            response = call_llm(processed_question)
            span.set_attribute("tokens_used", response.usage.total_tokens)

        return response.content


    qa_pipeline("What is MLflow?")
    ```

  </TabItem>
</Tabs>

## Choose your development environment

Select the quickstart guide that matches your development environment:

| Development Environment                                                                         | Use this guide if...                                                                             |
| ----------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| [\*Notebook\*\*](./tracing-notebook)<br/>Get started with MLflow Tracing directly in a Notebook | You develop in Notebooks and want the simplest setup with no authentication configuration needed |
| [**Local IDE**](./tracing-ide)<br/>Set up MLflow Tracing in your local development environment  | You develop in VS Code, PyCharm, or any other local IDE and need to connect to MLflow            |

## What you'll build

In either quickstart, you'll create a simple GenAI application that:

- Automatically captures detailed traces of each request
- Provides insights into token usage, latency, and application flow
- Enables debugging and optimization of your GenAI pipeline

Ready to get started? Choose your development environment above to begin building your first traced GenAI application.
