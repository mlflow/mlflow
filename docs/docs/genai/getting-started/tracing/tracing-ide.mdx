---
description: "Learn how to build your first GenAI application using MLflow OSS and Tracing in your IDE."
last_update:
  date: 2025-05-18
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Tracing a GenAI App (IDE)

This quickstart helps you integrate your GenAI app with [MLflow Tracing](/genai/tracing) using MLflow OSS in your local IDE development environment.

## What you'll achieve

By the end of this tutorial, you will have:

- A local MLflow Experiment for your GenAI app
- Set up your local development environment with MLflow OSS
- Used MLflow Tracing to instrument your app.

## Prerequisites

- **Python 3.9+**: Local Python environment
- **OpenAI API Key**: For accessing OpenAI models (or other LLM provider)

## Step 1: Install MLflow

When working in your local IDE, you need to install MLflow OSS.

<Tabs>
  <TabItem value="mlflow3" label="MLflow 3">
    ```bash
    pip install --upgrade "mlflow>=3.1"
    ```
  </TabItem>

  <TabItem value="mlflow2" label="MLflow 2.x">
    ```bash
    pip install --upgrade "mlflow>=2.15.0,<3.0.0"
    ```
  </TabItem>
</Tabs>

:::note[MLflow Version Recommendation]
While tracing features are available in MLflow 2.15.0+, **it is strongly recommended to install MLflow 3 (specifically 3.1 or newer)** for the latest GenAI capabilities, including expanded tracing features and robust support.
:::

## Step 2: Start MLflow Tracking Server

:::note
An MLflow Experiment is the container for your GenAI application. With MLflow OSS, you can run a local tracking server to store your experiments and traces.
:::

1. Start the MLflow tracking server in your terminal:

   ```bash
   mlflow server --host 127.0.0.1 --port 8080
   ```

2. Open your browser and navigate to `http://127.0.0.1:8080` to access the MLflow UI

3. You'll see the MLflow tracking interface where experiments and traces will be displayed

## Step 3: Set up your environment

:::note
For MLflow OSS, you'll need to configure your environment to connect to your local tracking server and set up API keys for your LLM provider.
:::

<Tabs>
  <TabItem value="env-vars" label="Use environment variables">
    Set the following environment variables in your terminal:

    ```bash
    export MLFLOW_TRACKING_URI=http://127.0.0.1:8080
    export OPENAI_API_KEY=<your-openai-api-key>
    ```

  </TabItem>

  <TabItem value="env-file" label="Use a .env file">
    1. Create a `.env` file in your project root:
       ```bash
       MLFLOW_TRACKING_URI=http://127.0.0.1:8080
       OPENAI_API_KEY=<your-openai-api-key>
       ```
    2. Install the `python-dotenv` package:
       ```bash
       pip install python-dotenv
       ```
    3. Load environment variables in your code:

       ```python
       # At the beginning of your Python script
       from dotenv import load_dotenv

       # Load environment variables from .env file
       load_dotenv()
       ```

  </TabItem>
</Tabs>

## Step 4: Instrument your application

Select the appropriate integration for your application:

<Tabs>
  <TabItem value="openai" label="OpenAI">
    1. Install the required Python packages:

       ```bash
       pip install -U mlflow openai
       ```

    2. Create a Python file named `app.py` in your project directory:

       Here, we use the [`@mlflow.trace` decorator](/genai/tracing/app-instrumentation/manual-tracing) that makes it easy to trace any Python application combined with the [OpenAI automatic instrumentation](/genai/tracing/integrations/listing/openai) to capture the details of the call to the OpenAI SDK.

       ```python
       import mlflow
       import openai
       import os

       # Set the MLflow tracking URI
       mlflow.set_tracking_uri("http://127.0.0.1:8080")

       # Enable MLflow's autologging to instrument your application with Tracing
       mlflow.openai.autolog()

       # Create an OpenAI client
       client = openai.OpenAI()


       # Use the trace decorator to capture the application's entry point
       @mlflow.trace
       def my_app(input: str):
           # This call is automatically instrumented by `mlflow.openai.autolog()`
           response = client.chat.completions.create(
               model="gpt-4o-mini",
               messages=[
                   {
                       "role": "system",
                       "content": "You are a helpful assistant.",
                   },
                   {
                       "role": "user",
                       "content": input,
                   },
               ],
           )
           return response.choices[0].message.content


       my_app(input="What is MLflow?")
       ```

    3. Run the application:

       ```bash
       python app.py
       ```

  </TabItem>

  <TabItem value="anthropic" label="Anthropic Claude">
    1. Install the required Python packages:

       ```bash
       pip install -U mlflow anthropic
       ```

    2. Create a Python file named `app.py` in your project directory:

       ```python
       import anthropic
       import mlflow

       # Enable auto-tracing for Anthropic
       mlflow.anthropic.autolog()

       # Set the MLflow tracking URI
       mlflow.set_tracking_uri("http://127.0.0.1:8080")
       mlflow.set_experiment("Anthropic")

       # Configure your API key.
       client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

       # Use the create method to create new message.
       message = client.messages.create(
           model="claude-3-5-sonnet-20241022",
           max_tokens=1024,
           messages=[
               {"role": "user", "content": "Hello, Claude"},
           ],
       )
       ```

    3. Set your Anthropic API key and run:

       ```bash
       export ANTHROPIC_API_KEY=<your-anthropic-api-key>
       python app.py
       ```

  </TabItem>
</Tabs>

## Step 5: View the Trace in MLflow

1. Navigate to your MLflow UI at `http://127.0.0.1:8080`
2. You will now see the generated trace in the **Traces** tab
3. Click on the trace to view its details

### Understanding the Trace

The trace you've just created shows:

- **Root Span**: Represents the inputs to the `my_app(...)` function
  - **Child Span**: Represents the LLM completion request
- **Attributes**: Contains metadata like model name, token counts, and timing information
- **Inputs**: The messages sent to the model
- **Outputs**: The response received from the model

This simple trace already provides valuable insights into your application's behavior, such as:

- What was asked
- What response was generated
- How long the request took
- How many tokens were used (affecting cost)

## Next Steps

Congratulations! You've successfully built your first GenAI application with MLflow OSS Tracing!

- **Learn more about Tracing**: Learn about [tracing concepts](/genai/tracing), [manual tracing](/genai/tracing/app-instrumentation/manual-tracing) for more control, [tracking users & sessions](/genai/tracing/track-users-sessions), and [tracking environments & context](/genai/tracing/track-environments-context) to your traces.
- **Evaluate your application**: Use MLflow's [evaluation capabilities](/genai/eval-monitor) to measure the quality of your GenAI app.

:::tip
For more complex applications like RAG systems or multi-step agents, MLflow Tracing provides even more value by revealing the inner workings of each component and step. You can also experiment with different LLM providers and compare their performance using MLflow's tracking capabilities.
:::
