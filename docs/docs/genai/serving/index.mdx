import Link from "@docusaurus/Link";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { APILink } from "@site/src/components/APILink";
import TabsWrapper from "@site/src/components/TabsWrapper";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import { Server, Globe, Zap, Settings, Code, Users, BarChart3, Shield } from "lucide-react";

# MLflow Model Serving

Transform your trained models into production-ready inference servers with MLflow's comprehensive serving capabilities. Deploy locally, in the cloud, or through managed endpoints with standardized REST APIs.

<FeatureHighlights features={[
  {
    icon: Globe,
    title: "REST API Endpoints",
    description: "Automatic generation of standardized REST endpoints for model inference with consistent request/response formats."
  },
  {
    icon: Code,
    title: "Multi-Framework Support",
    description: "Serve models from any ML framework through MLflow's flavor system with unified deployment patterns."
  },
  {
    icon: Settings,
    title: "Custom Applications",
    description: "Build sophisticated serving applications with custom logic, preprocessing, and business rules."
  },
  {
    icon: Zap,
    title: "Scalable Deployment",
    description: "Deploy to various targets from local development servers to cloud platforms and Kubernetes clusters."
  }
]} />

## Quick Start

Get your model serving in minutes with these simple steps:

<TabsWrapper>
<Tabs>
<TabItem value="serve" label="1. Serve Model" default>

Choose your serving approach:

```bash
# Serve a logged model
mlflow models serve -m "models:/<model-id>" -p 5000

# Serve a registered model
mlflow models serve -m "models:/<model-name>/<model-version>" -p 5000

# Serve a model from local path
mlflow models serve -m ./path/to/model -p 5000
```

Your model will be available at `http://localhost:5000`

</TabItem>
<TabItem value="predict" label="2. Make Predictions">

Send prediction requests via HTTP:

```bash
curl -X POST http://localhost:5000/invocations \
  -H "Content-Type: application/json" \
  -d '{"inputs": [[1, 2, 3, 4]]}'
```

**Using Python:**
```python
import requests
import json

data = {
    "dataframe_split": {
        "columns": ["feature1", "feature2", "feature3", "feature4"],
        "data": [[1, 2, 3, 4]],
    }
}

response = requests.post(
    "http://localhost:5000/invocations",
    headers={"Content-Type": "application/json"},
    data=json.dumps(data),
)

print(response.json())
```

</TabItem>
</Tabs>
</TabsWrapper>

## How It Works

MLflow serving provides a standardized serving infrastructure that works consistently across different ML frameworks:

<TabsWrapper>
<Tabs>
<TabItem value="architecture" label="Architecture" default>

**Request Flow:**
1. **Model Artifact Loading**: MLflow loads the model using the appropriate flavor (sklearn, pytorch, tensorflow, etc.)
2. **Server Initialization**: FastAPI server starts with standardized endpoints (`/invocations`, `/health`, `/version`)
3. **Request Processing**: Incoming HTTP requests are validated and transformed to the model's expected input format
4. **Model Inference**: The model's `predict()` method is called with the processed input
5. **Response Serialization**: Results are serialized to JSON using MLflow's standardized response format

</TabItem>
<TabItem value="flavors" label="Model Flavors">

**MLflow Flavor System:**
- Each ML framework has a dedicated MLflow flavor (e.g., `mlflow.sklearn`, `mlflow.pytorch`)
- Flavors handle framework-specific loading, preprocessing, and prediction logic
- The serving layer provides a unified interface regardless of the underlying framework
- Custom flavors can be created for proprietary or specialized models

**Supported Input Formats:**
- `dataframe_split`: Pandas DataFrame format with separate columns and data
- `dataframe_records`: List of dictionaries (row-oriented)
- `instances`: Simple list format for basic models
- `inputs`: Flexible format for various input types

</TabItem>
<TabItem value="endpoints" label="Standard Endpoints">

**Built-in Endpoints:**
- **`POST /invocations`**: Main prediction endpoint accepting JSON requests
- **`GET /ping`**: Health check endpoint for monitoring and load balancers
- **`GET /health`**: Alternative health check endpoint (same as /ping)
- **`GET /version`**: Returns MLflow version information
- **`GET /docs`**: Interactive API documentation (Swagger UI)
- **`GET /redoc`**: Alternative API documentation (ReDoc)
- **`GET /openapi.json`**: OpenAPI specification in JSON format

**Request/Response Format:**
```json
// Request
{
  "dataframe_split": {
    "columns": ["feature1", "feature2"],
    "data": [[1.0, 2.0], [3.0, 4.0]]
  }
}

// Response
{
  "predictions": [0.85, 0.92]
}
```

</TabItem>
</Tabs>
</TabsWrapper>

## MLflow Serving Best Practices

<TabsWrapper>
<Tabs>
<TabItem value="model-management" label="Model Management" default>

**Leverage MLflow's model management features:**

- **Use Model Registry**: Register models with versions and aliases for better lifecycle management
- **Model Metadata**: Include comprehensive metadata during model logging for better serving context
- **Environment Consistency**: Use `conda.yaml` or `requirements.txt` to ensure consistent dependencies
- **Model Signatures**: Define input/output schemas to enable request validation and better error handling

```python
import mlflow
from mlflow.models.signature import infer_signature
from mlflow.tracking import MlflowClient

# Log model with signature for better serving
signature = infer_signature(X_train, model.predict(X_train))
mlflow.sklearn.log_model(
    sk_model=model,
    name="my_model",
    signature=signature,
    registered_model_name="production_model",
)

# Set production alias for easy deployment
client = MlflowClient()
client.set_registered_model_alias(
    name="production_model", alias="champion", version="1"
)
```

</TabItem>
<TabItem value="serving-config" label="Serving Configuration">

**Optimize MLflow serving configuration:**

- **Input Formats**: Use `dataframe_split` format for structured data to leverage pandas optimizations
- **Batch Processing**: Send multiple samples in a single request to improve throughput
- **Server Settings**: Configure workers and timeout settings based on model complexity
- **Environment Variables**: Use MLflow environment variables for configuration

```bash
# Configure serving with appropriate settings
export MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT=60
export GUNICORN_CMD_ARGS="--timeout 60 --workers 4"

mlflow models serve \
  --model-uri models:/my_model@champion \
  --port 5000 \
  --enable-mlserver  # For advanced serving features
```

</TabItem>
<TabItem value="custom-pyfunc" label="Custom PyFunc">

**Build efficient custom PyFunc models:**

- **Simplified API**: Modern PyFunc models can omit `context` parameter if not needed (MLflow 2.20.0+)
- **Optional `load_context()`**: Only implement if you need to load artifacts during initialization
- **Vectorized Predictions**: Process batches efficiently in the `predict()` method
- **Type Hints**: Use type hints for better validation and documentation

```python
import joblib
import mlflow
import pandas as pd
from typing import Union


# Simple PyFunc model (no context needed)
class SimpleModel(mlflow.pyfunc.PythonModel):
    def predict(self, model_input: pd.DataFrame) -> pd.DataFrame:
        # Direct prediction logic
        return model_input * 2


# PyFunc model with artifacts (uses load_context)
class CustomModel(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        # Only needed if you have artifacts to load
        self.model = joblib.load(context.artifacts["model"])
        self.preprocessor = joblib.load(context.artifacts["preprocessor"])

    def predict(self, model_input: pd.DataFrame) -> list:
        # No context parameter needed in modern MLflow
        processed = self.preprocessor.transform(model_input)
        return self.model.predict(processed).tolist()


# Log model with artifacts
artifacts = {
    "model": "path/to/model.joblib",
    "preprocessor": "path/to/preprocessor.joblib",
}

mlflow.pyfunc.log_model(
    name="custom_model", python_model=CustomModel(), artifacts=artifacts
)
```

</TabItem>
<TabItem value="monitoring" label="MLflow Integration">

**Integrate with MLflow tracking and monitoring:**

- **Request Logging**: Use MLflow's built-in request logging for audit trails
- **Performance Metrics**: Track serving metrics alongside training metrics
- **Model Performance**: Use MLflow's model evaluation integration for monitoring
- **Experiment Tracking**: Link serving deployments to training experiments

```python
# Enable request logging
import mlflow
from mlflow.tracking import MlflowClient

# Track serving metrics
with mlflow.start_run():
    mlflow.log_metric("requests_per_second", rps)
    mlflow.log_metric("average_latency", latency)
    mlflow.log_metric("error_rate", error_rate)

# Use Model Registry for deployment tracking
client = MlflowClient()
client.set_registered_model_alias(name="my_model", alias="production", version="1")
```

</TabItem>
</Tabs>
</TabsWrapper>

## Complete Example: Train to Production

Follow this step-by-step guide to go from model training to a deployed REST API:

<TabsWrapper>
<Tabs>
<TabItem value="train" label="1. Train & Log" default>

Train a simple model with automatic logging:

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import pandas as pd

# Load sample data
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Enable sklearn autologging with model registration
mlflow.sklearn.autolog(registered_model_name="iris_classifier")

# Train model - MLflow automatically logs everything
with mlflow.start_run() as run:
    model = RandomForestClassifier(n_estimators=10, random_state=42)
    model.fit(X_train, y_train)

    # Autologging automatically captures:
    # - Model artifacts
    # - Training parameters (n_estimators, random_state, etc.)
    # - Training metrics (score on training data)
    # - Model signature (inferred from training data)
    # - Input example

    # Optional: Log additional custom metrics
    accuracy = model.score(X_test, y_test)
    mlflow.log_metric("test_accuracy", accuracy)

    print(f"Run ID: {run.info.run_id}")
    print("Model automatically logged and registered!")
```

</TabItem>
<TabItem value="register" label="2. Promote Model">

Set up model alias for production:

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Get the latest registered version (autologging creates version 1)
model_version = client.get_registered_model("iris_classifier").latest_versions[0]

# Set production alias (replaces deprecated stages)
client.set_registered_model_alias(
    name="iris_classifier", alias="production", version=model_version.version
)

print(f"Model version {model_version.version} tagged as 'production'")

# Model URI for serving (using alias)
model_uri = "models:/iris_classifier@production"
print(f"Production model URI: {model_uri}")
```

</TabItem>
<TabItem value="serve" label="3. Start Server">

Serve the registered model:

```bash
# Serve using model alias (MLflow 3.x way)
mlflow models serve \
  --model-uri "models:/iris_classifier@production" \
  --port 5000 \
  --env-manager local

# Server will start at http://localhost:5000
# Available endpoints:
# - POST /invocations (predictions)
# - GET /ping (health check)
# - GET /version (model info)
```

**Alternative serving approaches:**
```bash
# Serve by specific version number
mlflow models serve \
  --model-uri "models:/iris_classifier/1" \
  --port 5000

# Serve from run URI
mlflow models serve \
  --model-uri "runs:/<run-id>/model" \
  --port 5000
```

</TabItem>
<TabItem value="predict" label="4. Make Predictions">

Send requests to your served model:

```python
import requests
import json
import pandas as pd

# Prepare test data (same format as training)
test_data = {
    "dataframe_split": {
        "columns": [
            "sepal length (cm)",
            "sepal width (cm)",
            "petal length (cm)",
            "petal width (cm)",
        ],
        "data": [
            [5.1, 3.5, 1.4, 0.2],  # setosa
            [6.2, 2.9, 4.3, 1.3],  # versicolor
            [7.3, 2.9, 6.3, 1.8],  # virginica
        ],
    }
}

# Make prediction request
response = requests.post(
    "http://localhost:5000/invocations",
    headers={"Content-Type": "application/json"},
    data=json.dumps(test_data),
)

# Parse response
if response.status_code == 200:
    predictions = response.json()
    print("Predictions:", predictions)
    # Output: {"predictions": [0, 1, 2]}
else:
    print(f"Error: {response.status_code}, {response.text}")

# Health check
health = requests.get("http://localhost:5000/ping")
print("Health status:", health.status_code)  # Should be 200

# Model info
info = requests.get("http://localhost:5000/version")
print("Model version info:", info.json())
```

</TabItem>
</Tabs>
</TabsWrapper>

## Next Steps

Ready to build more advanced serving applications? Explore these specialized topics:

<TilesGrid>
  <TileCard
    icon={Code}
    title="Custom Applications"
    description="Build sophisticated serving logic with custom preprocessing, routing, and business rules"
    href="./custom-apps"
    linkText="Build custom apps →"
  />
  <TileCard
    icon={Users}
    title="Responses Agents"
    description="Handle complex response patterns and multi-step inference workflows"
    href="./responses-agent"
    linkText="Learn about agents →"
  />
</TilesGrid>

:::tip Get Started
The examples in each section are designed to be practical and ready-to-use. Start with the Quick Start above, then explore the use cases that match your deployment needs.
:::
