# Tutorial: Debug and Instrument Your GenAI App

## Overview

In this tutorial, you'll transform a GenAI application from a black box into a fully observable system using MLflow Tracing.

**Scenario**: You work for a fictional retailer called TechFlow Electronics. TechFlow has deployed a GenAI-based customer support chatbot that is experiencing performance issues: customers are complaining the bot is slow and sometimes fails to respond.

**Your mission**: Instrument the bot with [MLflow Tracing](/genai/tracing) so you can debug and fix the performance issues.

## Learning Objectives

By the end of this tutorial, you'll be able to:

1. Instrument an existing GenAI app with [MLflow Tracing](/genai/tracing)
2. Debug latency, errors, and logic issues using the Trace UI
3. Analyze cost metrics and token usage
4. Add metadata and user feedback to traces

## Prerequisites

- Python 3.8 or higher
- MLflow 2.14+ installed
- Basic Python knowledge
- Familiarity with FastAPI (helpful but not required)
- Access to an LLM API (OpenAI, Anthropic, or similar)

## Tutorial

### 1. Clone Repository and Configure Environment

1. **Install required dependencies**:
   ```bash
   pip install --upgrade mlflow openai fastapi uvicorn python-dotenv
   ```

2. **Clone the tutorial repository**:
   ```bash
   git clone https://github.com/epec254/genai-tutorial.git
   cd genai-tutorial/chatbot
   ```

3. **Set up environment variables**. Create a `.env` file:
   ```bash
   # MLflow Configuration
   MLFLOW_TRACKING_URI=http://localhost:5000  # Or your MLflow server
   MLFLOW_EXPERIMENT_NAME=techflow-chatbot

   # LLM Configuration (choose one)
   OPENAI_API_KEY=your-api-key  # For OpenAI
   # ANTHROPIC_API_KEY=your-api-key  # For Anthropic

   # API Configuration
   API_HOST=0.0.0.0
   API_PORT=8000
   ```

4. **Test the application locally**:
   ```bash
   # Start MLflow server (in a separate terminal)
   mlflow server --host 0.0.0.0 --port 5000

   # Run the app
   python backend/main.py
   ```

   Visit `http://localhost:8000` to see the chatbot interface.

### 2. Add Instrumentation with MLflow Tracing

To debug the performance issues, we need observability into our app's execution. Let's add MLflow Tracing to see exactly what's happening.

#### Understanding the Application Architecture

Before we instrument, let's understand the chatbot's structure:

- **Backend** (`/backend/`)
  - `main.py`: FastAPI server
  - `services/chat_service.py`: Chatbot logic using LLM APIs
  - `services/data_store.py`: Mock data for customers, orders, and products
- **Frontend** (`/frontend/`): Simple HTML/JS chat interface

#### Step 1: Initialize MLflow in the ChatService

Edit `/backend/services/chat_service.py` to add MLflow initialization and OpenAI autologging:

```python
# Add at the top of the file
import mlflow
import os
from typing import List, Dict, Optional
from openai import OpenAI


class ChatService:
    """Handles chat interactions with LLM"""

    def __init__(self, data_store: DataStore):
        self.data_store = data_store

        # Initialize MLflow
        mlflow.set_experiment(os.getenv("MLFLOW_EXPERIMENT_NAME", "techflow-chatbot"))

        # Enable OpenAI autologging
        mlflow.openai.autolog()

        # Initialize OpenAI client
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # System prompt for the support bot
        self.system_prompt = """You are a helpful customer support assistant for TechFlow Electronics.
You help customers with:
- Order tracking and shipping inquiries
- Product information and recommendations
- Return and refund requests
- Technical support for electronics
- Warranty inquiries

Be professional, empathetic, and solution-focused.
When customers ask about orders, always ask for their order number if not provided.
For product recommendations, ask clarifying questions about their needs and budget.
For returns, check the order date and our 30-day return policy."""
```

#### Step 2: Add Manual Tracing to Key Functions

Now let's add the `@mlflow.trace` decorator to instrument our application's functions:

**In `/backend/services/chat_service.py`**:

```python
@mlflow.trace
def process_chat_request(
    self, messages: List[Dict[str, str]], customer_id: str, session_id: str
) -> ChatResponse:
    """Process a chat request"""
    # Implementation...


@mlflow.trace
def _generate_response(self, messages: List[Dict[str, str]], customer) -> str:
    """Generate response using LLM with context"""
    # Implementation...


@mlflow.trace
def _detect_intent(self, message: str) -> str:
    """Use LLM to detect customer intent from their message"""
    # Implementation...


@mlflow.trace
def _add_order_context(self, messages: list, customer_id: Optional[str]):
    """Add order context to messages"""
    # Implementation...


@mlflow.trace
def _add_product_context(self, messages: list):
    """Add product catalog context with detailed product search"""
    # Implementation...
```

**In `/backend/services/data_store.py`**:

```python
import mlflow


# Add @mlflow.trace to these methods:
@mlflow.trace
def get_product(self, product_id: str) -> Optional[Product]:
    """Get product by ID"""
    # Implementation...


@mlflow.trace
def get_order(self, order_id: str) -> Optional[Order]:
    """Get order by ID"""
    # Implementation...


@mlflow.trace
def get_customer_orders(self, customer_id: str) -> List[Order]:
    """Get all orders for a customer"""
    # Implementation...


@mlflow.trace
def search_products(self, query: str, category: Optional[str] = None) -> List[Product]:
    """Search products by query and optional category"""
    # Implementation...


@mlflow.trace
def get_product_specs(self, product_id: str) -> Optional[Dict[str, Any]]:
    """Simulate slow API call to get product specifications"""
    # Implementation...


@mlflow.trace
def get_product_inventory(self, product_id: str) -> Optional[Dict[str, Any]]:
    """Simulate slow API call to get product inventory"""
    # Implementation...


@mlflow.trace
def search_products_with_details(
    self,
    query: str,
    category: Optional[str] = None,
    include_discontinued: bool = True,
) -> List[Dict[str, Any]]:
    """Search products and fetch their detailed specs and inventory"""
    # Implementation...
```

#### Step 3: Test the Instrumentation

Run the application and make a test request:

```python
# test_instrumentation.py
import requests

response = requests.post(
    "http://localhost:8000/api/chat",
    json={
        "messages": [{"role": "user", "content": "I need help with my order"}],
        "customer_id": "cust_001",
        "session_id": "session_123",
    },
)
print(f"Response: {response.json()}")
print(f"Trace ID: {response.json().get('turn_id')}")
```

View the trace in MLflow UI at `http://localhost:5000`.

### 3. Add Metadata and Feedback to Traces

Let's enhance our traces with metadata about users, sessions, and collect user feedback.

#### Step 1: Add User and Session Metadata

Update `process_chat_request` in `chat_service.py`:

```python
@mlflow.trace
def process_chat_request(
    self, messages: List[Dict[str, str]], customer_id: str, session_id: str
) -> ChatResponse:
    """Process a chat request"""

    # Validate input
    if not messages or len(messages) == 0:
        raise ValueError("No messages provided")

    last_message = messages[-1]
    if last_message.get("role") != "user":
        raise ValueError("Last message must be from user")

    # Get customer info
    customer = self.data_store.get_customer(customer_id)

    # Add metadata to trace
    mlflow.update_current_trace(
        tags={
            "mlflow.trace.user": customer_id,
            "customer_tier": customer.tier.value if customer else "unknown",
            "mlflow.trace.session": session_id,
        }
    )

    # Generate response
    response_content = self._generate_response(messages, customer)

    # Get trace ID for feedback tracking
    trace_id = mlflow.get_current_active_span().trace_id

    return ChatResponse(
        messages=[{"role": "assistant", "content": response_content}],
        turn_id=trace_id,
    )
```

#### Step 2: Tag Detected Intent

Update `_detect_intent` to add the detected intent as a tag:

```python
@mlflow.trace
def _detect_intent(self, message: str) -> str:
    """Use LLM to detect customer intent from their message"""

    intent_prompt = f"""Classify the following customer message into exactly one of these categories:
- order_tracking: Questions about order status, shipping, delivery
- return_request: Requests for returns, refunds, or complaints
- product_recommendation: Asking for suggestions or recommendations
- warranty_inquiry: Questions about warranty coverage
- product_info: Questions about product features or specifications
- general_inquiry: Any other customer service question

Customer message: "{message}"
Respond with only the category name, nothing else."""

    response = self.openai_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a customer intent classifier."},
            {"role": "user", "content": intent_prompt},
        ],
        temperature=0.0,
        max_tokens=20,
    )

    intent = response.choices[0].message.content.strip().lower()

    # Validate and tag the intent
    valid_intents = [
        "order_tracking",
        "return_request",
        "product_recommendation",
        "warranty_inquiry",
        "product_info",
        "general_inquiry",
    ]

    if intent not in valid_intents:
        intent = "unknown"

    mlflow.update_current_trace(tags={"detected_intent": intent})
    return intent
```

#### Step 3: Implement User Feedback

Add feedback handling to `chat_service.py`:

```python
def submit_feedback(self, feedback: FeedbackRequest) -> FeedbackResponse:
    """Submit feedback for a specific message"""
    try:
        # Log feedback to MLflow
        mlflow.log_feedback(
            trace_id=feedback.turn_id,
            name="user_feedback",
            value=True if feedback.rating == "thumbs_up" else False,
            source=mlflow.entities.AssessmentSource(
                source_type="HUMAN",
                source_id=feedback.customer_id,
            ),
        )

        return FeedbackResponse(success=True, message="Feedback submitted successfully")

    except Exception as e:
        return FeedbackResponse(
            success=False, message=f"Error submitting feedback: {str(e)}"
        )
```

Update the feedback endpoint in `main.py`:

```python
@app.post("/api/feedback", response_model=FeedbackResponse)
async def submit_feedback(feedback: FeedbackRequest):
    """Submit user feedback linked to trace"""
    response = chat_service.submit_feedback(feedback)
    if not response.success:
        raise HTTPException(status_code=400, detail=response.message)
    return response
```

### 4. Simulate Production Traffic

To test our instrumented application, let's simulate user traffic:

```python
# simulator.py
import random
import time
import requests
from concurrent.futures import ThreadPoolExecutor

# Sample customer queries
SAMPLE_QUERIES = [
    "Where is my order #12345?",
    "I need a laptop for gaming under $1500",
    "My wireless headphones stopped working",
    "Can I return an item I bought 45 days ago?",
    "What's the warranty on the Samsung TV?",
    "Do you have any iPhone 15 cases in stock?",
    "The screen on my tablet is cracked",
    "I want to cancel my order",
]


def simulate_conversation(customer_id: str, session_id: str):
    """Simulate a customer conversation"""
    query = random.choice(SAMPLE_QUERIES)

    response = requests.post(
        "http://localhost:8000/api/chat",
        json={
            "messages": [{"role": "user", "content": query}],
            "customer_id": customer_id,
            "session_id": session_id,
        },
    )

    # Simulate feedback (80% positive, 20% negative)
    if response.status_code == 200:
        trace_id = response.json().get("turn_id")
        feedback_rating = "thumbs_up" if random.random() > 0.2 else "thumbs_down"

        requests.post(
            "http://localhost:8000/api/feedback",
            json={
                "turn_id": trace_id,
                "rating": feedback_rating,
                "customer_id": customer_id,
            },
        )


# Run simulations
with ThreadPoolExecutor(max_workers=5) as executor:
    for i in range(50):
        customer_id = f"cust_{random.randint(1, 10):03d}"
        session_id = f"session_{int(time.time())}_{i}"
        executor.submit(simulate_conversation, customer_id, session_id)
        time.sleep(0.5)
```

### 5. Debug Issues Using Traces

Now let's use our traces to identify and fix performance issues.

#### Identify Latency Bottlenecks

1. **Filter for slow traces** in MLflow UI:
   - Look for traces with duration > 5 seconds
   - Examine the timeline view to find bottlenecks

2. **Common findings**:
   - Product search API calls take 3-5 seconds
   - Redundant calls to `get_product_specs` and `get_product_inventory`
   - Sequential API calls that could be parallelized

3. **Solutions**:
   ```python
   # Add caching for frequently accessed products
   from functools import lru_cache


   @mlflow.trace
   @lru_cache(maxsize=128)
   def get_product_specs_cached(self, product_id: str):
       return self.get_product_specs(product_id)
   ```

#### Debug Quality Issues

1. **Filter by negative feedback**:
   - Find traces where `user_feedback = false`
   - Look for patterns in failed interactions

2. **Common issues**:
   - Bot recommends discontinued products
   - Incorrect return policy information
   - Missing order context

3. **Solutions**:
   ```python
   # Add product availability check
   @mlflow.trace
   def filter_available_products(self, products: List[Product]) -> List[Product]:
       return [p for p in products if p.status == "available"]
   ```

#### Analyze Cost and Usage

1. **Token usage analysis**:
   - Calculate average tokens per request type
   - Identify expensive conversation patterns

2. **Optimization opportunities**:
   - Shorter system prompts for simple queries
   - Cached responses for FAQ-style questions
   - Model selection based on query complexity

### 6. Best Practices and Wrap-up

#### What We Accomplished

- ✅ Added comprehensive tracing to a GenAI application
- ✅ Captured user and session metadata
- ✅ Implemented user feedback collection
- ✅ Identified and fixed performance bottlenecks
- ✅ Reduced average response time from 5s to 1.2s
- ✅ Cut token usage by 40% with intelligent caching

#### Best Practices for Production

1. **Sampling Strategy**:
   ```python
   # Sample 20% of production traffic
   if random.random() < 0.2:
       mlflow.start_trace()
   ```

2. **Performance Monitoring**:
   - Set up alerts for traces > 5 seconds
   - Monitor token usage trends
   - Track feedback sentiment over time

3. **Security Considerations**:
   - Don't log sensitive customer data in traces
   - Use trace tags for metadata, not PII
   - Implement retention policies for traces

## Next Steps

- Continue to [Tutorial: Iterate on Quality During Development](/genai/tutorials/dev-quality-iteration) where we'll measure and improve response quality
- Explore [Advanced Tracing Features](/genai/tracing/concepts/trace-instrumentation)
- Review [Production Tracing Best Practices](/genai/tracing/prod-tracing)

## FAQ

**Q: How much overhead does tracing add?**
A: MLflow tracing typically adds less than 5% overhead. For most applications, this is negligible.

**Q: Should I trace every request in production?**
A: Use sampling (e.g., 10-20% of requests) to balance observability with performance.

**Q: How do I handle high-volume periods?**
A: Implement adaptive sampling that reduces trace percentage during traffic spikes.

**Q: Can I trace other LLM providers besides OpenAI?**
A: Yes! MLflow supports 15+ providers including Anthropic, Cohere, and Hugging Face.