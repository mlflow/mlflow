# Tutorial: Prototype and Debug AI Agents with MLflow Tracing

This is the first part of a tutorial series on developing and iterating on AI agents using MLflow.

There are several unique challenges associated with AI agents and agent systems:

- Different combinations of models, prompts, tools, and inference parameters can have huge impacts on an agent's performance. Keeping track of tests and experiments comparing performance across different combinations of these elements is challenging.
- It can be difficult to define what makes an agent's responses "good" or "bad," making it challenging to evaluate agent performance.
- Identifying sources of errors or breakdowns in application logic is very difficult when dealing with complex agent systems that include a mix of AI models calls, tool calls, handoffs, human input, and other complex interactions.

MLflow helps to solve these problems by providing a suite of tools for tracing and visualizing all of your AI agent calls, evaluating your models and applications, building application logic into custom models, tracking and versioning your models, and deploying your models to production.

## Use Case: Building a Food Delivery Support Chat Agent

In this tutorial, we will be building a food delivery support chat agent that can help customers with their orders. We will build tools enabling the agent to search for the status of an order, to cancel an order, and to escalate the order to a human agent. This is a simple but highly representative use case for an AI agent: it handles human input, retrieves data, and takes actions on behalf of users.

It's also a use case where mistakes can be costly and can lead to customer churn, so iterating on the agent's quality is critical. It should be friendly and polite when interacting with customers; the information it provides must be accurate; and escalations should be handled gracefully. This is where MLflow comes in: it will help us iterate on the agent's quality during prototyping with MLflow tracing, define our evaluation criteria with MLflow scorers, and compare different agent configurations with MLflow evaluation.

### Simplifying Assumptions

The goal of this tutorial is to show you how to use MLflow to prototype, debug, evaluate, and iterate on your AI agents; _not_ to build a complete production-ready system. To that end, we will make a number of simplifications for the sake of brevity and clarity:

- We will only consider single-turn interactions between the agent and the user.
- Unlike in a real-world application, chats will not be scoped to a single authenticated user.
- Tool calls that are intended to take an action on behalf of the user, such as canceling an order, will only trigger simulated API calls and responses.
- We will use a static dataset of order statuses. In a real-world application, your agent would likely retrieve ordert statuses from a database or API.

## Setup

Before we build our agent, let's install the dependencies we'll need and prepare our dataset of order statuses.
{/* What messaging are we going with here? */}
:::info[Managed MLflow]

For the best experience with all the features in this tutorial, we recommend using managed MLflow in Databricks. You can sign up for free [here](https://login.databricks.com/?destination_url=%2Fml%2Fexperiments-signup%3Fsource%3DTRY_MLFLOW&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS). Not all of the features in this tutorial are currently available in the open-source version of MLflow.

:::

### Install Dependencies

{/* Any notes about MLflow3/upgrading/migrating, or are we assuming users will be following along after the stable release? */}
For this tutorial, we will need to install the latest version of MLflow (we will be using some features only available in MLflow 3). We will be defining our agent using LlamaIndex, so we will also need to install the LlamaIndex and LlamaIndex-LLMs-OpenAI packages. The [databricks-agents](https://pypi.org/project/databricks-agents/) SDK is used by managed MLflow on Databricks for some evaluation-related functionality.

```bash
%pip install openai python-dotenv llama-index llama-index-llms-openai databricks-agents pandas
%pip install mlflow --upgrade --pre -q
%restart_python
```

### Set up MLflow

If you are using open-source MLflow, run the following code to start the MLflow tracking server and view the MLflow UI: `mlflow ui`.

Additionally, it is a good idea to create a new experiment to organize all of the traces, models, and other materials we will be creating.

```python
import mlflow
mlflow.set_experiment("mlflow-agent-tutorial")
```

If you are using managed MLflow, you can skip this step.

### Prepare the Data

Next, we will load a dataset of order statuses for our delivery service. For demonstration purposes, we will use a static dataset of order statuses. In a real-world application, your agent would likely retrieve order statuses from a database or API.

Each order has a unique ID, a customer name, a brand, a list of items, an order total, a timestamp for when the order was placed, a timestamp for when the order was estimated to be delivered, a current status, a driver location, and a timestamp for when the order was last updated.

<details>
<summary>Show full code for loading the order statuses (click to expand)</summary>

```python
orders_data = [
    {
        "order_id": "ORD-001",
        "customer_name": "Sarah Chen",
        "brand": "McDoodles",
        "items": ["Big Stack", "Famous Frites"],
        "order_total": 11.18,
        "placed_at": "2025-05-21T09:00:00Z",
        "estimated_delivery": "2025-05-21T09:30:00Z",
        "current_status": "Out for Delivery",
        "driver_location": "0.3 miles away",
        "last_updated": "2025-05-21T09:22:00Z",
    },
    {
        "order_id": "ORD-002",
        "customer_name": "Mike Johnson",
        "brand": "ChipoLot",
        "items": ["Chicken Burrito", "Chips & Guac"],
        "order_total": 14.43,
        "placed_at": "2025-05-21T09:10:00Z",
        "estimated_delivery": "2025-05-21T09:45:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:15:00Z",
    },
    {
        "order_id": "ORD-003",
        "customer_name": "Lisa Park",
        "brand": "Pando Dash",
        "items": ["Orange Chickadee", "Fried Rice"],
        "order_total": 10.39,
        "placed_at": "2025-05-21T08:45:00Z",
        "estimated_delivery": "2025-05-21T09:20:00Z",
        "current_status": "Delivered",
        "driver_location": None,
        "last_updated": "2025-05-21T09:18:00Z",
    },
    {
        "order_id": "ORD-004",
        "customer_name": "David Kim",
        "brand": "Yo! Sushii",
        "items": ["California Roll", "Spicy Tuna Roll", "Miso Soup"],
        "order_total": 22.68,
        "placed_at": "2025-05-21T08:30:00Z",
        "estimated_delivery": "2025-05-21T09:15:00Z",
        "current_status": "Delivered",
        "driver_location": None,
        "last_updated": "2025-05-21T09:12:00Z",
    },
    {
        "order_id": "ORD-005",
        "customer_name": "Emma Wilson",
        "brand": "GreenSprout",
        "items": ["Harvest Bowl"],
        "order_total": 10.18,
        "placed_at": "2025-05-21T09:15:00Z",
        "estimated_delivery": "2025-05-21T09:50:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:20:00Z",
    },
    {
        "order_id": "ORD-006",
        "customer_name": "Carlos Rodriguez",
        "brand": "Taco Ring",
        "items": ["Doritos Locos Taco x2", "Baja Blast"],
        "order_total": 20.68,
        "placed_at": "2025-05-21T09:05:00Z",
        "estimated_delivery": "2025-05-21T09:40:00Z",
        "current_status": "Out for Delivery",
        "driver_location": "0.8 miles away",
        "last_updated": "2025-05-21T09:28:00Z",
    },
    {
        "order_id": "ORD-007",
        "customer_name": "Jennifer Lee",
        "brand": "Shack Stack",
        "items": ["ShackBurger Double", "Vanilla Shake"],
        "order_total": 13.62,
        "placed_at": "2025-05-21T08:50:00Z",
        "estimated_delivery": "2025-05-21T09:25:00Z",
        "current_status": "Delivered",
        "driver_location": None,
        "last_updated": "2025-05-21T09:23:00Z",
    },
    {
        "order_id": "ORD-008",
        "customer_name": "Robert Taylor",
        "brand": "Jinya Noodle Bar",
        "items": ["Tonkotsu Ramen", "Pork Gyoza"],
        "order_total": 22.43,
        "placed_at": "2025-05-21T09:20:00Z",
        "estimated_delivery": "2025-05-21T09:55:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:25:00Z",
    },
    {
        "order_id": "ORD-009",
        "customer_name": "Ashley Davis",
        "brand": "Starbrews",
        "items": ["Caramel Frappuccino", "Plain Bagel"],
        "order_total": 11.01,
        "placed_at": "2025-05-21T09:25:00Z",
        "estimated_delivery": "2025-05-21T10:00:00Z",
        "current_status": "Order Received",
        "driver_location": None,
        "last_updated": "2025-05-21T09:25:00Z",
    },
    {
        "order_id": "ORD-010",
        "customer_name": "Kevin Zhang",
        "brand": "Five Gals",
        "items": ["Bacon Cheeseburger", "Cajun Fries"],
        "order_total": 9.21,
        "placed_at": "2025-05-21T08:40:00Z",
        "estimated_delivery": "2025-05-21T09:20:00Z",
        "current_status": "Delivered",
        "driver_location": None,
        "last_updated": "2025-05-21T09:38:00Z",
    },
    {
        "order_id": "ORD-011",
        "customer_name": "Maria Gonzalez",
        "brand": "Pho House",
        "items": ["Pho Combo", "Vietnamese Iced Coffee"],
        "order_total": 16.13,
        "placed_at": "2025-05-21T09:12:00Z",
        "estimated_delivery": "2025-05-21T09:47:00Z",
        "current_status": "Out for Delivery",
        "driver_location": "0.5 miles away",
        "last_updated": "2025-05-21T09:30:00Z",
    },
    {
        "order_id": "ORD-012",
        "customer_name": "James Brown",
        "brand": "Dominni Pizza",
        "items": ["MeatZZa Pizza", "Coke"],
        "order_total": 16.57,
        "placed_at": "2025-05-21T09:18:00Z",
        "estimated_delivery": "2025-05-21T09:53:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:23:00Z",
    },
    {
        "order_id": "ORD-013",
        "customer_name": "Amanda Clark",
        "brand": "PokeCraft",
        "items": ["Spicy Ahi Bowl"],
        "order_total": 10.68,
        "placed_at": "2025-05-21T09:22:00Z",
        "estimated_delivery": "2025-05-21T09:57:00Z",
        "current_status": "Order Received",
        "driver_location": None,
        "last_updated": "2025-05-21T09:22:00Z",
    },
    {
        "order_id": "ORD-014",
        "customer_name": "Ryan O'Connor",
        "brand": "Kava Kitchen",
        "items": ["Falafel Crunch Bowl", "Hot Harissa Chips"],
        "order_total": 14.10,
        "placed_at": "2025-05-21T08:55:00Z",
        "estimated_delivery": "2025-05-21T09:30:00Z",
        "current_status": "Out for Delivery",
        "driver_location": "1.2 miles away",
        "last_updated": "2025-05-21T09:38:00Z",
    },
    {
        "order_id": "ORD-015",
        "customer_name": "Nicole Torres",
        "brand": "MaruNoodle",
        "items": ["Chicken Katsu Udon", "Sweet Potato Tempura"],
        "order_total": 17.60,
        "placed_at": "2025-05-21T09:08:00Z",
        "estimated_delivery": "2025-05-21T09:43:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:18:00Z",
    },
    {
        "order_id": "ORD-016",
        "customer_name": "Steven Adams",
        "brand": "ChipoLot",
        "items": ["Steak Burrito", "Barbacoa Burrito"],
        "order_total": 16.71,
        "placed_at": "2025-05-21T09:13:00Z",
        "estimated_delivery": "2025-05-21T09:48:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:18:00Z",
    },
    {
        "order_id": "ORD-017",
        "customer_name": "Rachel Martinez",
        "brand": "McDoodles",
        "items": ["McChicklet", "Apple Slice Bites", "Iced Brew"],
        "order_total": 10.99,
        "placed_at": "2025-05-21T09:26:00Z",
        "estimated_delivery": "2025-05-21T10:01:00Z",
        "current_status": "Order Received",
        "driver_location": None,
        "last_updated": "2025-05-21T09:26:00Z",
    },
    {
        "order_id": "ORD-018",
        "customer_name": "Alex Thompson",
        "brand": "GreenSprout",
        "items": ["Kale Caesar", "Hummus & Focaccia"],
        "order_total": 12.21,
        "placed_at": "2025-05-21T08:35:00Z",
        "estimated_delivery": "2025-05-21T09:10:00Z",
        "current_status": "Delivered",
        "driver_location": None,
        "last_updated": "2025-05-21T09:18:00Z",
    },
    {
        "order_id": "ORD-019",
        "customer_name": "Samantha Lopez",
        "brand": "Yo! Sushii",
        "items": ["Salmon Nigiri x2", "Tuna Sashimi"],
        "order_total": 20.50,
        "placed_at": "2025-05-21T09:17:00Z",
        "estimated_delivery": "2025-05-21T09:52:00Z",
        "current_status": "Being Prepared",
        "driver_location": None,
        "last_updated": "2025-05-21T09:22:00Z",
    },
    {
        "order_id": "ORD-020",
        "customer_name": "Marcus Johnson",
        "brand": "Shack Stack",
        "items": ["Chicken Shack", "Crinkle Fries", "Chocolate Shake"],
        "order_total": 16.56,
        "placed_at": "2025-05-21T08:25:00Z",
        "estimated_delivery": "2025-05-21T09:00:00Z",
        "current_status": "Delivered",
        "driver_location": None,
        "last_updated": "2025-05-21T08:58:00Z",
    },
]
```

</details>

```python
import pandas as pd

orders_df = pd.DataFrame(orders_data)

orders_df
```

Here are the first few rows of our order statuses table:

| order_id | customer_name | brand      | items                              | order_total | placed_at            | estimated_delivery   | current_status   | driver_location | last_updated         |
| -------- | ------------- | ---------- | ---------------------------------- | ----------- | -------------------- | -------------------- | ---------------- | --------------- | -------------------- |
| ORD-001  | Sarah Chen    | McDoodles  | ["Big Stack","Famous Frites"]      | 11.18       | 2025-05-21T09:00:00Z | 2025-05-21T09:30:00Z | Out for Delivery | 0.3 miles away  | 2025-05-21T09:22:00Z |
| ORD-002  | Mike Johnson  | ChipoLot   | ["Chicken Burrito","Chips & Guac"] | 14.43       | 2025-05-21T09:10:00Z | 2025-05-21T09:45:00Z | Being Prepared   | null            | 2025-05-21T09:15:00Z |
| ORD-003  | Lisa Park     | Pando Dash | ["Orange Chickadee","Fried Rice"]  | 10.39       | 2025-05-21T08:45:00Z | 2025-05-21T09:20:00Z | Delivered        | null            | 2025-05-21T09:18:00Z |

## Prototyping a simple order status agent

Suppose, at this phase, we have a very general sense of what we want to build—an agent for a food delivery service that can help customers with their orders—but we have not yet decided on the specifics. In many cases, the first step is building a very simple agent, trying out different prompts and tools, and iterating in an unstructured and informal way on the agent's behavior.

At this phase, we are often working in a notebook environment, and are not particularly systematic about structuring our code or recording formal experiments. Still, we might stumble upon some interesting ideas we want to apply later in the project, so having some system for recording our tests is helpful. To that end, we can use MLflow's autologging and tracing features to record our experiments.

We will start by building a very simple agent that can answer questions about the status of an order.

### Configure the OpenAI Client

First, we will configure the OpenAI client. Make sure to set your OpenAI API key, either as an environment variable or in a `.env` file. In this case, we saved our API key in a `.env` file with a single line: `OPENAI_API_KEY=sk-...`. We then load the API key from the `.env` file using the `dotenv` package, as shown below.

```python
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
```

### Define the Order Status Tool

Next, we will define a simple tool that can retrieve the status of an order based on the order ID. Following OpenAI's [function calling documentation](https://platform.openai.com/docs/guides/function-calling), we will write a function that takes an order ID as input and returns the order status. We will also define a schema for the function, which tells the model what the function does and what arguments it takes.

```python
def get_order_by_id(order_id: str):
    """
    Retrieves a single order by its unique order ID.
    Useful when a user asks for a specific order by its ID.
    """
    result = orders_df[orders_df['order_id'] == order_id]
    if not result.empty:
        return result.iloc[0].to_dict()
    return None

get_order_by_id_schema = schema = {
    "type": "function",
    "function": {
        "name": "get_order_by_id",
        "description": "Retrieves a single order by its unique order ID. Useful when a user asks for a specific order by its ID.",
        "parameters": {
            "type": "object",
            "properties": {
                "order_id": {
                    "type": "string",
                    "description": "The unique identifier for the order"
                }
            },
            "required": ["order_id"]
        }
    }
}
```

### Manually Define a Simple Tool Calling Agent

With most LLM providers, tool calling works as follows:

1. The user asks a question that requires the use of a tool.
2. The model's response includes a `tool_calls` field that includes the name of the function to call and the arguments to pass to the function.
3. The application—_not_ the model itself—calls the function with the arguments and, optionally, passes the result back to the model as a new message with the `role` set to `tool`.

In the code sample below, we define a simple function that implements this process. It takes a single query from the user, calls the model, and then calls the function with the arguments. It then appends the result of the function call to the message history and calls the model again with the updated message history, enabling the model to provide a final response that includes the result of the function call.

```python
import json

user_query = [{"role": "user", "content": "Hello tell me about order ORD-018"}]
tools = [get_order_by_id_schema]

def request_order_status(client, messages, tools):
    message_history = messages.copy()
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=message_history,
        tools=tools
    )

    tool_call = completion.choices[0].message.tool_calls[0]
    args = json.loads(tool_call.function.arguments)

    order_info = get_order_by_id(args["order_id"])

    message_history.append(completion.choices[0].message)
    message_history.append({
        "role": "tool",
        "tool_call_id": tool_call.id,
        "content": str(order_info)
    })

    final_completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=message_history,
        tools=tools,
    )

    return final_completion

print(request_order_status(client, user_query, tools).choices[0].message.content)
```

Which returns:

> Order **ORD-018** details are as follows:
>
> - **Customer Name**: Alex Thompson
> - **Brand**: GreenSprout
> - **Items Ordered**:
>   - Kale Caesar
>   - Hummus & Focaccia
> - **Order Total**: $12.21
> - **Placed At**: May 21, 2025, 08:35 AM (UTC)
> - **Estimated Delivery**: May 21, 2025, 09:10 AM (UTC)
> - **Current Status**: Delivered
> - **Last Updated**: May 21, 2025, 09:18 AM (UTC)
>
> If you have any more questions or need further information, feel free to ask.

Even in this simple example, we begin to see the complexity of building an agent. We need to manage the message history, invoke the AI model API multiple times, call on the tool based on the model's response, and manage the flow of control between the model and the tool. Furthermore, this isn't even close to the kind of response would want to provide to the user. We will ultimately want it to respond in a friendly and personal tone. At present, we do not have an obvious way to identify any points of failure or opportunities for improvement.

This is where MLflow tracing comes in.

## MLflow Tracing: Observability with One Line of Code

MLflow tracing provides a record of every call to a model, including the inputs, outputs, errors, and other information. Tracing makes it much easier to:

- Debug issues with your model calls
- Build datasets for downstream tasks like evaluation and model training
- Understand every execution step in your agent system

Enabling tracing requires just one line of code: `mlflow.<provider>.autolog()`. In this case, we are using OpenAI, so we will use `mlflow.openai.autolog()`. Let's see how it works.

```python
import mlflow
mlflow.openai.autolog()

request_order_status(client, user_query, tools)
```

![MLflow Trace](/images/genai/tutorials/debug-tracing/1_trace.png)

:::tip[View Traces in Jupyter Notebooks]

You can view traces right in a Jupyter notebook (or Databricks notebook if you are using managed MLflow).

In open-source MLflow, to view traces in Jupyter notebooks, make sure you have started the tracking server with `mlflow ui` and set the tracking URI to the URL of the tracking server with `mlflow.set_tracking_uri("<URL>")` (`http://localhost:5000` by default).

In both settings, you can also view traces in the MLflow UI.

:::

Now, whenever we call the `request_order_status` function or otherwise call a model using the OpenAI client, MLflow will automatically log the trace, keeping a complete record of every model call.

This is very useful throughout AI agent development. At this early phase, we are often working in a notebook environment, and are not particularly systematic about structuring our code or recording formal experiments. Still, we might stumble upon some interesting ideas we want to apply later in the project, so having some system for recording our tests is helpful. MLflow's autologging and tracing features make it very straightforward to record all of our experiments.

### Associating Traces with Models

Developing agents often involves a lot of experimentation, evaluation, and iteration. It's easy to lose track of all of the different experiments we've run on a give model, especially if all of the information we've recorded is spread across many different runs and mixed in with traces of other models. To that end, MLflow provides a way to set an active model with which all of the traces and evaluations will be associated.

You can create and set the active model with:

```python
mlflow.set_active_model(name="simple-agent")
```

Now, when we log a trace, it will be associated with the model. You can find your model in the MLflow UI by clicking on the "Models" tab in your experiment.

![MLflow Model](/images/genai/tutorials/debug-tracing/2_model_tab.png)

### Manual Tracing: Add Tracing to our Agent's Tool

One thing you might notice if you investigate the trace we generated above is that our agent workflow actually resulted in _two_ traces: one for the initial model call (where the model responded with the tool call parameters) and one for the final output from the model. There are two key issues here:

1. These two calls are part of the same workflow, so we would like them to be recorded as one trace.
2. The execution of `get_order_by_id` was _not_ traced, leaving out a critical step in our workflow.

We can manually configure our workflow to record the trace exactly as we want it. Autologging is a great way to start with tracing, but MLflow also provides a rich set of tracing tools that can be used to fine-tune the tracing behavior.

First, we will use the `@mlflow.trace` decorator to manually trace the execution of our tool:

```python
@mlflow.trace(span_type="TOOL")
def get_order_by_id(order_id: str):
    """
    Retrieves a single order by its unique order ID.
    Useful when a user asks for a specific order by its ID.
    """
    result = orders_df[orders_df['order_id'] == order_id]
    if not result.empty:
        return result.iloc[0].to_dict()
    return None
```

Now, whenever this function is called, MLflow will automatically log a trace—tracing is not just limited to AI model calls!

Next, we will modify our `request_order_status` function to nest the model and tool calls under the `with mlflow.start_span(span_type="CHAIN") as span:` context manager:

```python
def request_order_status(client, messages, tools):
    message_history = messages.copy()

    with mlflow.start_span(span_type="CHAIN") as span:
        span.set_inputs(inputs=message_history)
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=message_history,
            tools=tools
        )

        tool_call = completion.choices[0].message.tool_calls[0]
        args = json.loads(tool_call.function.arguments)

        order_info = get_order_by_id(args["order_id"])

        message_history.append(completion.choices[0].message)
        message_history.append({
            "role": "tool",
            "tool_call_id": tool_call.id,
            "content": str(order_info)
        })

        final_completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=message_history,
            tools=tools,
        )

        span.set_outputs(outputs=final_completion)

        return final_completion

request_order_status(client, user_query, tools)
```

![MLflow Trace](/images/genai/tutorials/debug-tracing/3_manual_trace.png)

Now we can see the _entire_ execution flow in a single trace, including tool calls and their inputs and outputs. This also shows how we can combine automatic and manual tracing: the model calls are still automatically traced; we just nested them under a parent span and added tracing to the tool call.

### Using Tracing to Debug Errors

Complex agents may involve dozens of different tool calls, retrieval steps, model calls, and more, not to mention all of the orchestration logic that ties it all together. When something goes wrong, it's not always obvious where the problem lies. MLflow tracing helps with this as well. Errors are logged in the "Events" tab of a trace and clearly marked in the trace breakdown. Suppose, for instance, we had a bug in our tool call and tried to get data from `order_df` instead of `orders_df`:

![Debug with Tracing](/images/genai/tutorials/debug-tracing/4_debug.png)

The trace makes the error clear and obvious immediately and records all of the relevant information in the Events tab of the trace.

## Using and Tracing an Agent Framework

After experimenting with different prompts and tools, we might decide that it is time to build a more complete agent by adding additional tools, further iterating on our prompt, and so on. We could certainly continue to build on our simple agent, but we might decide instead to use an existing agent authoring framework. These frameworks, such as [LlamaIndex](https://docs.llamaindex.ai/en/stable/), [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/), and the [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/) provide a lot of the scaffolding we need to build a complete agent without having to start from scratch.

In the rest of this tutorial, we will build a [LlamaIndex agent](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/) with some additional tools and trace it with MLflow. We will use this agent in the rest of this tutorial series.

### Convert our Simple Agent to LlamaIndex

First, let's replicate our existing agent—with one tool—to a LlamaIndex `ReActAgent`.

```python
from llama_index.core.agent.workflow import ReActAgent
from llama_index.core.tools import FunctionTool
from llama_index.llms.openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

def get_order_by_id(order_id: str):
    """
    Retrieves a single order by its unique order ID.
    Useful when a user asks for a specific order by its ID.
    """
    result = orders_df[orders_df['order_id'] == order_id]
    if not result.empty:
        return result.iloc[0].to_dict()
    return None

llm = OpenAI(model="gpt-4o-mini")

tool = FunctionTool.from_defaults(
    get_order_by_id,
)
```

This is very similar to our previous approach, except that we did not need to manually define a tool schema or orchestrate the model and tool calling logic.

### Enable Tracing and Query the Agent

MLflow supports autologging for LlamaIndex agents, so we can enable tracing with just one line of code. This is also a good time to set a new active model as we want to keep traces and metrics for this agent separate from our previous agent.

```python
mlflow.set_active_model(name="llamaindex-agent")
mlflow.llama_index.autolog()
```

Now, whenever we call the agent, MLflow will automatically log the trace and associate it with the `llamaindex-agent` model.

```python
agent = ReActAgent(llm=llm, tools=[tool])
await agent.run("Hello tell me about order ORD-018")
```

![Tracing LlamaIndex Agent](/images/genai/tutorials/debug-tracing/5_llamaindex_trace.png)

:::info[Async Agents]

LlamaIndex agents are asynchronous by default, so we use the `await` keyword to wait for the agent to complete and get the results of the `run` method. If you are new to asynchronous programming, you can read more about it in the context of LlamaIndex [here](https://docs.llamaindex.ai/en/stable/getting_started/async_python/).

:::

### Iterate on the Agent

Now that we have a framework and tracing, we are in a great place to iterate on our prototype agent. Tracing provides instant feedback and a record of our experiments, and the LlamaIndex framework provides easy-to-use abstractions for adding tools and updating the agent's behavior.

First, let's add a couple of new tools: one to escalate the chat to a human agent and one to cancel an order. In both cases, the tools will simulate an API call and response with further information the agent can use to respond to the user. The `escalate_to_human` tool tell the agent how long it will take to get back to the user and the `cancel_order` tool will tell the agent whether it was possible to cancel the order based on the order's status.

```python
import json
from datetime import datetime

def escalate_to_human(issue_summary: str) -> str:
    """
    Escalate the issue to a human agent. This triggers (simulated) backend API calls to create
    a support ticket and notify human agents.

    Args:
        issue_summary: Brief summary of the customer's issue
    """

    # Create the API payload
    escalation_payload = {
        "timestamp": datetime.now().isoformat(),
        "customer_id": "CUST-12345",
        "issue_summary": issue_summary,
        "source": "chatbot_escalation",
        "requires_human": True
    }

    # simulate API call
    def send_api_request(payload):
        ticket_id = f"ESCALATION-{hash(payload['issue_summary']) % 10000:04d}"
        return {
            "status": "success",
            "ticket_id": ticket_id,
            "estimated_response_time_minutes": 15,
            "assigned_agent_id": "AGENT-456"
        }

    api_response = send_api_request(escalation_payload)
    ticket_id = api_response["ticket_id"]

    # Return natural description for the LLM to work with
    return str(api_response)


def cancel_order(order_id: str, reason: str = "customer_request") -> str:
    """
    Cancel a food delivery order if possible.

    Args:
        order_id: The order ID to cancel (e.g., ORD-009)
        reason: Reason for cancellation
    """

    # Check order status in the dataframe
    order_row = orders_df[orders_df['order_id'] == order_id]

    if order_row.empty:
        return f"Order {order_id} not found."

    order_status = order_row['current_status'].iloc[0]
    order_total = order_row['order_total'].iloc[0]

    if order_status != "Order Received":
        return f"Cannot cancel - order status is '{order_status}'"

    # Simulate cancellation API call
    cancellation_id = f"CANCEL-{hash(order_id) % 10000:04d}"

    return f"Cancellation successful. ID: {cancellation_id}, Refund: ${order_total}, ETA: 3 business days"
```

We will also write a more detailed system prompt that tells the agent when to use tools and how to respond to the user. Note that we also specify the "current time" (in this case, May 21, 2025, 09:40 AM, which corresponds to the times in the sample data) so the agent can, for example, reason about whether orders are running late.

```python
CURRENT_DATETIME = "2025-05-21T09:40:00Z"

system_prompt = f"""You are a helpful customer service agent for a food delivery platform. Your goal is to efficiently resolve customer inquiries while maintaining a positive experience.

The current date and time is {CURRENT_DATETIME}.

## Communication Style
- Be friendly, clear, and direct. Avoid overly formal or robotic responses
- Keep responses concise unless more detail is specifically requested
- Use natural, conversational language
- Reply in the language used by the customer

## Order Status Inquiries
- Focus on delivery timing and current status when customers ask about their orders
- Only mention order contents if explicitly asked ("what did I order?")
- Prioritize the information customers care most about: "when will it arrive?" and "where is it now?"

## Escalation Guidelines
- Only escalate to human agents when customers express genuine frustration, anger, or dissatisfaction
- Do NOT escalate for simple questions, normal delays, or routine requests
- Escalate when customers use words like "furious," "terrible," "awful," demand refunds angrily, or express they want to speak to a manager
- Mild disappointment or concern does not require escalation

## Tool Usage
- Use get_order_by_id for any order-related inquiries
- Use cancel_order only when customers explicitly request cancellation
- Use escalate_to_human only when customers are genuinely upset or dissatisfied

## Problem Solving
- Try to resolve issues directly using available tools before considering escalation
- Provide clear next steps and realistic timelines
- Acknowledge concerns without over-apologizing

Focus on being helpful and efficient while maintaining a human touch in your responses.
"""
```

Initializing the agent with these tools and the new prompt is straightforward:

```python
tools = [FunctionTool.from_defaults(get_order_by_id),
         FunctionTool.from_defaults(escalate_to_human),
         FunctionTool.from_defaults(cancel_order),
]

agent = ReActAgent(llm=llm, tools=tools, system_prompt=system_prompt)

await agent.run("I would like to cancel order ORD-008.")
```

The agent responds with:

> I'm unable to cancel order ORD-008 because its status is currently 'Being Prepared'.

We can click through the trace to see the full execution flow detailing how the agent reached this answer. We can see that it called the `cancel_order` tool, which returned a message indicating that the order could not be canceled because the order was already being prepared.

![Trace the Full Workflow](/images/genai/tutorials/debug-tracing/6_trace_full.png)

We can also see that there is some clear room for improvement in the agent's response. It provided the correct information, but in a tone that was a bit too brusque and did not provide any additional context or information. Now that we have a framework and a set of tools, we can start to define the key quality criteria for our agent and use MLflow to evaluate and iterate on it. That will be the focus of the next tutorial.

## Conclusion and Next Steps

In this tutorial, we have seen how MLflow tracing can be used to debug and iterate on AI agents. We experimented with a simple agent, instrumented it with tracing, and then built a more complete agent using LlamaIndex. We saw how autologging offers one-line agent observability for a range of LLM providers and frameworks, and how tracing can be used to understand and debug the whole agent execution flow.

In the next tutorial, we will use MLflow to define and evaluate the critical quality criteria for our agent, and how to use MLflow evaluation to iterate on the agent's behavior during development.

Before moving on to the next tutorial, here are a few suggested next steps to help you get the most out of this tutorial:

- Try to replicate and trace the agent we built in this tutorial using a different LLM provider (like Anthropic) or framework (like LangGraph).
- Add more tools to the agent and see how they are captured in the trace.
- Use some of the insights gathered through tracing to try to improve the agent's response.

We'll see you in the next tutorial!
