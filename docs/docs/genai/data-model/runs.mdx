# MLflow Runs Data Model for GenAI

In MLflow 3, **Runs** represent evaluation iterations of your GenAI application and are attached directly to Models as part of the model's evolution. Unlike earlier MLflow versions where runs lived under experiments, runs now capture specific evaluation sessions that test and validate model performance.

## Overview

A Run represents a single evaluation iteration of your GenAI model - think of it as a snapshot of how your model or application performed during a specific testing session.

```mermaid
graph TB
    subgraph MODEL[" "]
        direction TB
        TITLE[ğŸ¤– GenAI Model: Customer Support Bot]

        subgraph RUNS[ğŸš€ Model Runs - Evaluation Iterations]
            direction TB
            R1[ğŸ“Š Run 1: Initial Evaluation]
            R2[ğŸ“Š Run 2: Post-Training Eval]
            R3[ğŸ“Š Run 3: Production Validation]
            R4[ğŸ“Š Run 4: A/B Test Results]
        end

        R1 -.-> R2
        R2 -.-> R3
        R3 -.-> R4
    end

    classDef modelStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000
    classDef runStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef titleStyle fill:#f5f5f5,stroke:#424242,stroke-width:2px,color:#000

    class MODEL modelStyle
    class RUNS,R1,R2,R3,R4 runStyle
    class TITLE titleStyle
```

## Runs as Model Evolution Snapshots

Each Run captures a specific moment in your model's development lifecycle:

```mermaid
graph TB
    subgraph RUN[ğŸ“Š Individual Run Contains]
        direction TB
        RC1[ğŸ“ˆ Evaluation Metrics]
        RC2[ğŸ“‹ Test Dataset Used]
        RC3[âš™ï¸ Model Configuration]
        RC4[ğŸ“ Generated Traces]
        RC5[ğŸ¯ Quality Scores]
    end

    subgraph PURPOSE[ğŸ¯ Run Purpose]
        direction TB
        P1[ğŸ” Performance Validation]
        P2[ğŸ“Š Quality Assessment]
        P3[ğŸ†š Comparative Analysis]
        P4[âœ… Release Readiness]
    end

    RUN --> PURPOSE

    classDef runStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef purposeStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000

    class RUN,RC1,RC2,RC3,RC4,RC5 runStyle
    class PURPOSE,P1,P2,P3,P4 purposeStyle
```

## Relationship to Other Entities

Runs connect your model development to systematic evaluation:

```mermaid
graph TD
    subgraph ECOSYSTEM[ğŸŒŸ Run Ecosystem]
        direction TB

        M[ğŸ¤– Model] --> R[ğŸ“Š Run]
        R --> T[ğŸ“ Traces]
        R --> A[ğŸ“Š Assessments]

        ED[ğŸ“‹ Evaluation Dataset] --> R
        S[ğŸ¯ Scorers] --> R

        R --> RM[ğŸ“ˆ Run Metrics]
        R --> RR[ğŸ“‹ Run Results]
    end

    classDef modelStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef runStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:3px,color:#000
    classDef entityStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef resultStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000

    class M modelStyle
    class R runStyle
    class T,A,ED,S entityStyle
    class RM,RR resultStyle
```

**Key relationships:**

- **Runs can link to Models**: Iterative model or app development is tracked through runs
- **Runs generate Traces**: Evaluation execution creates trace records
- **Runs produce Assessments**: Quality judgments on model performance
- **Runs use Datasets**: Systematic testing against curated examples
- **Runs apply Scorers**: Automated evaluation functions

## Benefits of Model-Attached Runs

The MLflow 3 approach of attaching runs to models provides:

- **ğŸ¯ Model-Centric Organization**: Evaluation history travels with the model
- **ğŸ“ˆ Evolution Tracking**: Clear progression of model performance over time
- **ğŸ” Focused Analysis**: Evaluation results directly tied to specific logged model
- **ğŸš€ Simplified Workflows**: Natural connection between model development and testing

This model-centric approach makes it easier to understand how your GenAI application has evolved and which evaluation iterations led to improvements or regressions.
