# Version Tracking Data Model

MLflow's version tracking data model provides a structured approach to managing and analyzing different versions of your GenAI applications across their entire lifecycle. By organizing version metadata within MLflow's core entities, you can systematically track performance, debug regressions, and validate deployments across development, staging, and production environments.

## Overview

Version tracking in MLflow integrates seamlessly with the core data model through strategic use of tags and metadata. This approach enables comprehensive version management while maintaining the flexibility to adapt to your specific deployment and development workflows.

```mermaid
graph TD
    subgraph VM[ğŸ“Š Version Management]
        direction TB
        V1[ğŸ·ï¸ Version Tags]
        V2[ğŸŒ Environment Context]
        V3[ğŸ“ˆ Version Metrics]
        V4[ğŸ”„ Deployment Tracking]
    end

    subgraph DM[ğŸ—ƒï¸ Core Data Model]
        direction TB
        A[ğŸ§ª Experiment] --> B[ğŸ“ Trace]
        B --> C[ğŸ“Š Assessment]
        C --> D[ğŸ‘ Feedback]
        E[ğŸš€ Evaluation Run] --> F[ğŸ“ˆ Scored Traces]
        G[ğŸ“‹ Evaluation Dataset] --> H[ğŸ“„ Version Comparisons]
    end

    VM --> DM

    classDef versionStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    classDef coreStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef containerStyle fill:#f5f5f5,stroke:#424242,stroke-width:3px,color:#000

    class V1,V2,V3,V4 versionStyle
    class A,B,C,D,E,F,G,H coreStyle
    class VM,DM containerStyle
```

## Core Entities for Version Tracking

### ğŸ§ª Experiment: The Version Container

An **Experiment** serves as the root container for all versions of your GenAI application. Within a single experiment, you can track multiple application versions, environments, and deployment states while maintaining a unified view of your application's evolution.

**Key characteristics:**

- **Single namespace**: One experiment contains all versions of your application
- **Cross-version analysis**: Compare performance across different versions within the same container
- **Historical continuity**: Maintain complete version history in one location
- **Unified metadata**: Consistent tagging and organization across all versions

```mermaid
graph TD
    subgraph EXP[" "]
        direction TB
        TITLE[ğŸ§ª Customer Support Chatbot Experiment]
        V1[ğŸ“± v1.0.0 - Initial Release]
        V2[ğŸ”§ v1.1.0 - Bug Fixes]
        V3[âœ¨ v2.0.0 - New Features]
        V4[ğŸš€ v2.1.0 - Performance Update]

        TITLE -.-> V1
        V1 --> V2
        V2 --> V3
        V3 --> V4
    end

    classDef versionStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef titleStyle fill:#f5f5f5,stroke:#424242,stroke-width:3px,color:#000
    classDef containerStyle fill:#ffffff,stroke:#424242,stroke-width:1px,color:#000

    class V1,V2,V3,V4 versionStyle
    class TITLE titleStyle
    class EXP containerStyle
```

### ğŸ“ Traces: Version-Aware Execution Records

Each **Trace** represents a single execution of your application and carries version-specific metadata through tags. This enables granular tracking of how different versions perform in various contexts.

**Version metadata captured in traces:**

```mermaid
graph TB
    subgraph TR[ğŸ“ Single Trace Execution]
        direction TB
        A[ğŸ“Š Execution Data]
        B[ğŸ·ï¸ Version Tags]
        C[â±ï¸ Performance Metrics]
        D[ğŸ”— Context Information]
    end

    subgraph TAGS[ğŸ·ï¸ Version Tag Examples]
        direction TB
        T1[app_version: 2.1.0]
        T2[environment: production]
        T3[deployment_id: prod-20240201]
        T4[model_version: claude-3-sonnet]
        T5[feature_flags: new_ui_enabled]
    end

    TR --> TAGS

    classDef traceStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    classDef tagStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000

    class TR,A,B,C,D traceStyle
    class TAGS,T1,T2,T3,T4,T5 tagStyle
```

**Standard vs Custom Version Tags:**

| Tag Type      | Purpose                        | Examples                                         |
| ------------- | ------------------------------ | ------------------------------------------------ |
| **Automatic** | MLflow-populated metadata      | `mlflow.source.git.commit`, `mlflow.source.name` |
| **Standard**  | Reserved for specific meanings | `mlflow.trace.session`, `mlflow.trace.user`      |
| **Custom**    | Application-specific context   | `app_version`, `environment`, `deployment_id`    |

### ğŸ“Š Assessments: Version-Specific Quality Judgments

**Assessments** enable version-specific quality analysis by attaching evaluations to traces. This creates a foundation for comparing quality metrics across different versions and deployment contexts.

```mermaid
graph TD
    subgraph TRACE[ğŸ“ Trace v2.1.0]
        direction TB
        INPUT[ğŸ”¤ Input: What are your hours?]
        OUTPUT[ğŸ’¬ Output: 9 AM - 5 PM EST]
        META[ğŸ·ï¸ Version: 2.1.0, Env: production]
    end

    subgraph ASSESS[ğŸ“Š Assessments]
        direction TB
        F1[ğŸ‘ Feedback: Relevance = 4.8/5]
        F2[âš¡ Feedback: Latency = 250ms]
        F3[âœ… Feedback: Accuracy = 5/5]
        E1[ğŸ¯ Expectation: Standard hours response]
    end

    TRACE --> ASSESS

    classDef traceStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef assessStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000

    class TRACE,INPUT,OUTPUT,META traceStyle
    class ASSESS,F1,F2,F3,E1 assessStyle
```

**Assessment types for version tracking:**

- **Performance Feedback**: Latency, throughput, resource usage
- **Quality Feedback**: Relevance, accuracy, helpfulness scores
- **User Experience**: Satisfaction ratings, usability metrics
- **Regression Testing**: Expected outputs for version validation

### ğŸ¯ Scorers: Automated Version Analysis

**Scorers** provide automated evaluation functions that can detect version-specific performance patterns, regressions, and improvements. They transform raw trace data into actionable version insights.

```mermaid
graph TD
    subgraph SCORERS[ğŸ¯ Version-Aware Scorers]
        direction TB
        S1[âš¡ Performance Scorer]
        S2[ğŸ“ˆ Regression Detector]
        S3[ğŸ›ï¸ Feature Flag Analyzer]
        S4[ğŸŒ Environment Comparator]
    end

    subgraph ANALYSIS[ğŸ“Š Version Analysis]
        direction TB
        A1[ğŸ“‰ v2.0 â†’ v2.1: 15% latency improvement]
        A2[âš ï¸ Staging shows 2% error rate increase]
        A3[âœ… New feature flag performs 8% better]
        A4[ğŸ” Production stability maintained]
    end

    SCORERS --> ANALYSIS

    classDef scorerStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef analysisStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000

    class SCORERS,S1,S2,S3,S4 scorerStyle
    class ANALYSIS,A1,A2,A3,A4 analysisStyle
```

### ğŸ“‹ Evaluation Datasets: Version Testing Collections

**Evaluation Datasets** support systematic version testing by providing curated collections of inputs and expected outputs. These datasets enable consistent comparison across versions and deployment validation.

```mermaid
graph TD
    subgraph DATASETS[ğŸ“‹ Version Testing]
        direction TB
        D1[ğŸ§ª Regression Test Suite]
        D2[âš¡ Performance Benchmark]
        D3[ğŸ†• New Feature Validation]
        D4[ğŸ”„ A/B Test Scenarios]
    end

    subgraph CONTENT[ğŸ“„ Dataset Content]
        direction TB
        C1[ğŸ“ Input Examples]
        C2[âœ… Expected Outputs]
        C3[ğŸ¯ Success Criteria]
        C4[ğŸ·ï¸ Test Categories]
    end

    DATASETS --> CONTENT

    classDef datasetStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px,color:#000
    classDef contentStyle fill:#fff8e1,stroke:#f57c00,stroke-width:2px,color:#000

    class DATASETS,D1,D2,D3,D4 datasetStyle
    class CONTENT,C1,C2,C3,C4 contentStyle
```

**Dataset organization for version management:**

- **Regression Testing**: Core functionality validation across versions
- **Performance Benchmarking**: Standardized performance measurement
- **Feature Validation**: New capability testing and verification
- **Environment Testing**: Deployment-specific scenario validation

### ğŸš€ Evaluation Runs: Version Comparison Engine

**Evaluation Runs** orchestrate systematic version comparisons by running different application versions against the same datasets and collecting scored results for analysis.

```mermaid
graph TD
    subgraph VERSIONS[ğŸ“± Application Versions]
        direction TB
        V1[v2.0.0 Current]
        V2[v2.1.0 Candidate]
    end

    subgraph DATASET[ğŸ“‹ Test Dataset]
        direction TB
        T1[100 Test Cases]
        T2[Expected Outputs]
        T3[Success Criteria]
    end

    subgraph EVALUATION[ğŸš€ Evaluation Run]
        direction TB
        E1[Run v2.0.0 â†’ Results A]
        E2[Run v2.1.0 â†’ Results B]
        E3[Apply Scorers]
        E4[Generate Comparison]
    end

    subgraph RESULTS[ğŸ“Š Comparison Results]
        direction TB
        R1[Performance Metrics]
        R2[Quality Scores]
        R3[Regression Detection]
        R4[Deployment Readiness]
    end

    VERSIONS --> EVALUATION
    DATASET --> EVALUATION
    EVALUATION --> RESULTS

    classDef versionStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef dataStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px,color:#000
    classDef evalStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:3px,color:#000
    classDef resultStyle fill:#fff8e1,stroke:#f57c00,stroke-width:2px,color:#000

    class VERSIONS,V1,V2 versionStyle
    class DATASET,T1,T2,T3 dataStyle
    class EVALUATION,E1,E2,E3,E4 evalStyle
    class RESULTS,R1,R2,R3,R4 resultStyle
```

### ğŸ·ï¸ Labeling Sessions: Human Version Review

**Labeling Sessions** organize traces from specific versions for human expert review, enabling qualitative assessment of version changes and edge case identification.

```mermaid
graph TD
    subgraph SESSION[ğŸ·ï¸ Version Review Session]
        direction TB
        S1[ğŸ“ v2.1.0 Traces]
        S2[ğŸ‘¥ Expert Reviewers]
        S3[ğŸ“‹ Review Criteria]
        S4[â­ Quality Ratings]
    end

    subgraph INSIGHTS[ğŸ’¡ Review Insights]
        direction TB
        I1[ğŸ¯ Quality Improvements]
        I2[âš ï¸ Edge Case Issues]
        I3[ğŸ” User Experience Changes]
        I4[ğŸ“ˆ Performance Feedback]
    end

    SESSION --> INSIGHTS

    classDef sessionStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    classDef insightStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000

    class SESSION,S1,S2,S3,S4 sessionStyle
    class INSIGHTS,I1,I2,I3,I4 insightStyle
```

## Version Tracking Workflow

The complete version tracking workflow integrates all data model entities to provide comprehensive version lifecycle management:

```mermaid
graph TD
    subgraph DEV[ğŸ”¨ Development Phase]
        direction TB
        D1[ğŸ“ Code Changes]
        D2[ğŸ·ï¸ Version Tagging]
        D3[ğŸ“Š Development Traces]
    end

    subgraph TEST[ğŸ§ª Testing Phase]
        direction TB
        T1[ğŸ“‹ Evaluation Datasets]
        T2[ğŸš€ Evaluation Runs]
        T3[ğŸ¯ Automated Scoring]
        T4[ğŸ‘¥ Human Review]
    end

    subgraph DEPLOY[ğŸš€ Deployment Phase]
        direction TB
        DP1[ğŸŒ Environment Deployment]
        DP2[ğŸ“ Production Traces]
        DP3[ğŸ“Š Performance Monitoring]
    end

    subgraph ANALYZE[ğŸ“ˆ Analysis Phase]
        direction TB
        A1[ğŸ“Š Version Comparison]
        A2[ğŸ” Regression Detection]
        A3[ğŸ’¡ Improvement Insights]
        A4[ğŸ¯ Next Version Planning]
    end

    DEV --> TEST
    TEST --> DEPLOY
    DEPLOY --> ANALYZE
    ANALYZE --> DEV

    classDef devStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef testStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000
    classDef deployStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef analyzeStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000

    class DEV,D1,D2,D3 devStyle
    class TEST,T1,T2,T3,T4 testStyle
    class DEPLOY,DP1,DP2,DP3 deployStyle
    class ANALYZE,A1,A2,A3,A4 analyzeStyle
```

## Advanced Version Management Patterns

### Multi-Environment Version Progression

Track the same version as it progresses through different environments:

```mermaid
graph LR
    subgraph ENV[ğŸŒ Environment Progression]
        direction LR
        E1[ğŸ”§ Development] --> E2[ğŸ§ª Staging] --> E3[ğŸš€ Production]
    end

    subgraph TRACKING[ğŸ“Š Version Tracking]
        direction TB
        T1[ğŸ“ Dev Traces<br/>v2.1.0-dev]
        T2[ğŸ“ Staging Traces<br/>v2.1.0-staging]
        T3[ğŸ“ Prod Traces<br/>v2.1.0-prod]
    end

    E1 -.-> T1
    E2 -.-> T2
    E3 -.-> T3

    classDef envStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef trackStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000

    class ENV,E1,E2,E3 envStyle
    class TRACKING,T1,T2,T3 trackStyle
```

### Feature Flag Version Analysis

Understand how feature flags impact different versions:

```mermaid
graph TD
    subgraph VERSION[ğŸ“± Version 2.1.0]
        direction TB
        V1[ğŸ›ï¸ Feature Flag A: ON]
        V2[ğŸ›ï¸ Feature Flag B: OFF]
        V3[ğŸ›ï¸ Feature Flag C: A/B Test]
    end

    subgraph TRACES[ğŸ“ Trace Analysis]
        direction TB
        T1[ğŸ“Š Flag A Impact: +12% performance]
        T2[ğŸ“Š Flag B Impact: Baseline performance]
        T3[ğŸ“Š Flag C Impact: Split testing results]
    end

    VERSION --> TRACES

    classDef versionStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000
    classDef traceStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000

    class VERSION,V1,V2,V3 versionStyle
    class TRACES,T1,T2,T3 traceStyle
```

### Version Rollback Tracking

Monitor the impact of version rollbacks:

```mermaid
timeline
    title Version Deployment Timeline

    section v2.0.0
        Production Stable : Normal performance
                          : Low error rate

    section v2.1.0 Deploy
        Initial Deploy    : Performance monitoring
                         : Quality assessment

    section Issue Detection
        Performance Drop  : Latency increase detected
                         : Error rate spike

    section Rollback
        v2.0.0 Restored  : Performance recovery
                        : System stability

    section Analysis
        Root Cause       : Version comparison
                        : Issue identification
```

## Data Relationships and Dependencies

Understanding how version tracking entities relate to each other:

```mermaid
erDiagram
    EXPERIMENT ||--o{ TRACE : contains
    TRACE ||--o{ ASSESSMENT : receives
    ASSESSMENT ||--|| FEEDBACK : "implements as"
    ASSESSMENT ||--|| EXPECTATION : "implements as"

    EVALUATION_DATASET ||--o{ EVALUATION_RUN : "used in"
    EVALUATION_RUN ||--o{ TRACE : generates
    SCORER ||--o{ FEEDBACK : produces

    LABELING_SESSION ||--o{ TRACE : organizes
    LABELING_SESSION ||--o{ FEEDBACK : "collects human"

    TRACE {
        string trace_id
        string app_version
        string environment
        string deployment_id
        timestamp execution_time
        string status
    }

    ASSESSMENT {
        string assessment_id
        string trace_id
        string assessment_type
        float score
        string rationale
    }

    EVALUATION_RUN {
        string run_id
        string dataset_id
        string app_version
        timestamp created_at
        json results_summary
    }
```

## Key Benefits of the Version Tracking Data Model

#### ğŸ” Comprehensive Observability

- **Cross-version visibility**: Compare performance across all application versions
- **Environment-specific insights**: Understand how versions behave in different deployment contexts
- **Historical analysis**: Track application evolution over time

#### ğŸ“Š Data-Driven Decision Making

- **Regression detection**: Automatically identify performance or quality regressions
- **Improvement validation**: Confirm that new versions deliver expected benefits
- **Deployment confidence**: Make informed decisions about production deployments

#### ğŸ”„ Efficient Development Workflow

- **Systematic testing**: Consistent evaluation processes across version changes
- **Quick iteration**: Rapid feedback on version performance and quality
- **Risk mitigation**: Early detection of issues before production deployment

#### ğŸ¯ Quality Assurance

- **Automated evaluation**: Consistent quality measurement across versions
- **Human validation**: Expert review processes for critical version changes
- **Continuous monitoring**: Ongoing assessment of production version performance

## Integration with MLflow Ecosystem

The version tracking data model seamlessly integrates with MLflow's broader ecosystem:

```mermaid
graph TD
    subgraph MLFLOW[ğŸŒŸ MLflow Ecosystem]
        direction TB

        subgraph CORE[ğŸ—ƒï¸ Core Data Model]
            C1[ğŸ§ª Experiments]
            C2[ğŸ“ Traces]
            C3[ğŸ“Š Assessments]
        end

        subgraph TOOLS[ğŸ› ï¸ MLflow Tools]
            T1[ğŸ–¥ï¸ MLflow UI]
            T2[ğŸ” Search & Query]
            T3[ğŸ“Š Evaluation Framework]
            T4[ğŸ¯ Custom Scorers]
        end

        subgraph INTEGRATIONS[ğŸ”— External Integrations]
            I1[ğŸš€ CI/CD Pipelines]
            I2[ğŸ“Š Monitoring Systems]
            I3[â˜ï¸ Cloud Platforms]
            I4[ğŸ“ˆ Analytics Tools]
        end
    end

    CORE --> TOOLS
    TOOLS --> INTEGRATIONS

    classDef coreStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    classDef toolStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef integrationStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000

    class CORE,C1,C2,C3 coreStyle
    class TOOLS,T1,T2,T3,T4 toolStyle
    class INTEGRATIONS,I1,I2,I3,I4 integrationStyle
```

## Next Steps

To implement comprehensive version tracking using MLflow's data model:

1. **[Track Versions & Environments](/genai/tracing/track-environments-context)**: Learn to attach version metadata to traces
2. **[Evaluation Workflows](/genai/eval-monitor)**: Create systematic version comparison processes
3. **[Query and Analysis](/genai/tracing/search-traces)**: Master advanced querying for version analysis
4. **[MLflow UI](/genai/tracing/observe-with-traces/ui)**: Use the interface for version-specific trace exploration

MLflow's version tracking data model provides the conceptual foundation for systematic application lifecycle management, enabling confident deployments, quick regression detection, and data-driven version management decisions across your GenAI application's evolution.
