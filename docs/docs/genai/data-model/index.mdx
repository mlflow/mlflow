# MLflow Data Model

MLflow's data model provides a structured approach to developing and managing GenAI applications by organizing how you log, debug, and evaluate them to achieve quality, cost, and latency goals. This structured approach addresses key challenges in reproducibility, quality assessment, and iterative development.

## Overview

The MLflow data model consists of several interconnected entities that work together to support your GenAI application development workflow:

🧪 **Experiment** - The root container for your GenAI application

🔍 **Trace** - A log of inputs, outputs, and intermediate steps from a single application execution

📊 **Assessments** - Quality judgments on a Trace, categorized as Feedback or Expectations

🎯 **Scorers** - Definitions of automated evaluation functions that produce Feedback

📋 **Evaluation Datasets** - Curated sets of inputs (and optional Expectations) for offline testing

🚀 **Evaluation Runs** - Results from running app versions against Evaluation Datasets, containing new, scored Traces

🏷️ **Labeling Sessions** - Collections of Traces organized for human review

## MLflow Experiment

An **Experiment** is the top-level container for each distinct application or use case. It contains all Traces from development and production alongside all other entities in the data model. We recommend creating a single Experiment for each application.

### Setting up an Experiment

```python
import mlflow

# Create or set an experiment
mlflow.set_experiment("my-genai-app")

# Or create explicitly
experiment = mlflow.create_experiment("my-genai-app")
```

## MLflow Traces: The Foundation

The foundational concept is the **Trace**: a single, complete execution of your GenAI application (e.g., a user request or API call).

### How Traces are Generated

Traces are generated through:

**Automatic Instrumentation**: Enable with a single line of code for 20+ popular LLM SDKs:

```python
import mlflow

# Enable automatic tracing for OpenAI
mlflow.openai.autolog()

# Your existing code works unchanged
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4", messages=[{"role": "user", "content": "Hello!"}]
)
```

**Custom Instrumentation**: Use MLflow's SDK for fine-grained control:

```python
@mlflow.trace
def my_custom_function(input_data):
    # Your custom logic here
    result = process_data(input_data)
    return result
```

### Purpose of Traces

Traces enable:

- **Observability**: Gain insights into application performance
- **Debugging**: Understand execution flow to resolve issues
- **Quality Evaluation**: Assess response quality over time
- **Human Review**: Provide data for expert annotation



```mermaid
graph LR
    A[👤 User Request] --> B[🤖 GenAI App]
    B --> C[📝 MLflow Trace]
    C --> D[👁️ Observability]
    C --> E[⭐ Quality Evaluation]
    C --> F[👥 Human Review]

    classDef userStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000
    classDef appStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000
    classDef traceStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:3px,color:#000
    classDef outputStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000

    class A userStyle
    class B appStyle
    class C traceStyle
    class D,E,F outputStyle
```

## Assessments: Quality Judgments

**Assessments** are qualitative or quantitative judgments attached to Traces to understand and improve GenAI application quality. A Trace can have multiple Assessments, primarily Feedback or Expectations.

### Feedback Assessments

**Feedback** captures evaluations of a Trace and includes:

- 🏷️ **Name** - Developer-defined category (e.g., `relevance`, `correctness`)
- ⭐ **Score** - Evaluation (e.g., thumbs up/down, numerical rating)
- 💭 **Rationale** - Optional textual explanation for the score

```python
# Log feedback programmatically
mlflow.log_feedback(
    request_id="trace-123",
    feedback_name="relevance",
    feedback_score=4,
    feedback_rationale="Response was highly relevant to the question",
)
```

### Expectation Assessments

**Expectations** are ground truth labels for a Trace (e.g., `expected_facts`, `expected_response`). These are primarily used in offline evaluation to compare app output against known correct answers.

## Scorers: Automated Quality Measurement

**Scorers** are functions that programmatically assess Trace quality, producing Feedback. They can be:

### Code-based Heuristics

```python
def check_response_length(trace):
    """Custom scorer to check if response is appropriate length"""
    response = trace.outputs.get("response", "")
    if 50 <= len(response) <= 500:
        return {"score": 1, "rationale": "Response length is appropriate"}
    else:
        return {"score": 0, "rationale": "Response too short or too long"}
```

### LLM Judges

```python
from mlflow.metrics import genai

# Use built-in LLM judge
relevance_metric = genai.relevance()

# Evaluate traces with the metric
results = mlflow.evaluate(
    model=your_model, data=evaluation_data, extra_metrics=[relevance_metric]
)
```

## Evaluation Datasets and Runs

MLflow provides systematic offline testing through Evaluation Datasets and Evaluation Runs.

### Evaluation Datasets

An **Evaluation Dataset** is a curated collection of example inputs used to evaluate and improve app performance:

```python
# Create an evaluation dataset
dataset = mlflow.data.from_dict(
    {
        "inputs": ["What is MLflow?", "How do I log metrics?"],
        "expected_outputs": ["MLflow is...", "To log metrics..."],
    }
)

# Register the dataset
mlflow.log_input(dataset, context="evaluation")
```

### Evaluation Runs

An **Evaluation Run** stores results from running a new app version against an Evaluation Dataset:

```python
# Run evaluation
results = mlflow.evaluate(
    model=your_model,
    data=evaluation_dataset,
    targets="expected_outputs",
    extra_metrics=[relevance_metric, accuracy_metric],
)
```

The evaluation process:

1. New app version processes inputs from Evaluation Dataset
2. MLflow generates a new Trace for each input
3. Configured Scorers annotate Traces with Feedback
4. All annotated Traces are stored in the Evaluation Run

```mermaid
graph TD
    A[🚀 Production Traces] --> B[🔍 Curate Examples]
    B --> C[📋 Evaluation Dataset]
    C --> D[✅ Add Expectations]
    E[🔄 New App Version] --> F[⚙️ MLflow Evaluation]
    C --> F
    G[🎯 Scorers] --> F
    F --> H[📊 New Traces]
    H --> I[📈 Evaluation Run]

    classDef sourceStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000
    classDef processStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px,color:#000
    classDef dataStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000
    classDef resultStyle fill:#fff8e1,stroke:#f57c00,stroke-width:2px,color:#000
    classDef coreStyle fill:#e8eaf6,stroke:#3f51b5,stroke-width:3px,color:#000

    class A,E sourceStyle
    class B,G processStyle
    class C,D dataStyle
    class H,I resultStyle
    class F coreStyle
```

## Labeling Sessions: Human Review

**Labeling Sessions** organize Traces for human review, typically through the MLflow UI. Domain experts can browse these Traces and attach Feedback as Assessments.

```python
# Create a labeling session
session = mlflow.create_labeling_session(
    name="quality-review-session", trace_ids=["trace-1", "trace-2", "trace-3"]
)
```

## Complete Data Model Structure

All components operate within an Experiment, forming a comprehensive hierarchy:

```mermaid
graph TD
    subgraph EXP[🧪 Experiment Container]
        direction TB
        A[📝 Trace] --> B[📊 Assessment]
        B --> C[👍 Feedback]
        B --> D[🎯 Expectation]
        E[⚙️ Scorer]
        F[🚀 Evaluation Run] --> G[📈 Scored Traces]
        H[📋 Evaluation Dataset] --> I[📄 Input Data + Expectations]
        J[🏷️ Labeling Session] --> K[📝 Curated Traces]
    end

    classDef traceStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    classDef assessmentStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000
    classDef feedbackStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    classDef expectationStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef toolStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000
    classDef dataStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px,color:#000
    classDef containerStyle fill:#f5f5f5,stroke:#424242,stroke-width:3px,color:#000

    class A,K,G traceStyle
    class B assessmentStyle
    class C feedbackStyle
    class D expectationStyle
    class E,F toolStyle
    class H,I,J dataStyle
    class EXP containerStyle
```

## Getting Started

To begin using the MLflow data model:

1. 🧪 **Set up an experiment** for your GenAI application
2. 🔄 **Enable automatic tracing** for your LLM library
3. 🎯 **Define custom scorers** for your quality metrics
4. 📋 **Create evaluation datasets** from representative examples
5. 🚀 **Run evaluations** to compare different versions
6. 👥 **Review traces** and add human feedback as needed

```python
import mlflow

# 1. Set up experiment
mlflow.set_experiment("my-genai-app")

# 2. Enable tracing
mlflow.openai.autolog()

# 3. Your app code runs normally
# Traces are automatically captured

# 4. Evaluate and iterate
results = mlflow.evaluate(
    model=your_model, data=evaluation_data, extra_metrics=[your_custom_scorers]
)
```

## Next Steps

- 🔄 **[Automatic Tracing](/genai/tracing/app-instrumentation/automatic)**: Learn how to enable automatic tracing for your LLM library
- 🛠️ **[Custom Tracing](/genai/tracing/app-instrumentation/manual-tracing)**: Add manual instrumentation to your application
- 📊 **[Evaluation Guide](/genai/evaluation)**: Dive deeper into evaluation workflows
- 🖥️ **[MLflow UI](/genai/tracing/observe-with-traces/ui)**: Explore traces and results in the web interface

MLflow's comprehensive data model empowers systematic observation, debugging, evaluation, and improvement of GenAI applications, providing the foundation for building high-quality, reliable, and maintainable GenAI systems.