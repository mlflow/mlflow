---
sidebar_label: MLflow 3.0
sidebar_position: 1
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { APILink } from "@site/src/components/APILink";
import { CardGroup, PageCard } from "@site/src/components/Card";

# MLflow 3.0 (Preview)

Discover the next generation of MLflow, designed to streamline your AI experimentation and accelerate your journey from idea to production. MLflow 3.0 brings cutting-edge support for GenAI workflows, enabling seamless integration of generative AI models into your projects.

## What is MLflow 3.0?

MLflow 3.0 delivers best-in-class experiment tracking, observability, and performance evaluation for machine learning models, AI applications, and generative AI agents!
MLflow 3.0 introduces a variety of exciting improvements, including:
- Centrally track and analyze the performance of your models and generative AI applications **across all environments**, from interactive queries in a development notebook through production batch or real-time serving deployments.
- Gain insights into model performance with **comprehensive metrics and lineage tracking**, enabling better decision-making for production readiness.
- Streamline the handling of model artifacts and evaluation results with a **cohesive structure** that improves observability and traceability.

Model Tracking helps you create and evaluate different model/agent configurations in your experiments, <APILink fn="mlflow.entities.LoggedModel">LoggedModel</APILink> helps you find the best ones for production usage, and tracing helps enhance observability in your model or GenAI applications.


## Enhanced Model Tracking

In MLflow 3.0, we introduce a refined architecture along with revamped APIs and UI, tailored to enhance generative AI and deep learning workflows. 
With GenAI agents, there are multiple rounds of offline evaluation via batch jobs and interactive queries from human beta testers.
In deep learning, training often generates multiple model checkpoints, where the best candidates are further evaluated before production deployment. 

We are introducing a new first-class object, the <APILink fn="mlflow.entities.LoggedModel">LoggedModel</APILink> entity, into MLflow Tracking to streamline these processes. As you define and evaluate your GenAI agents in code, 
or your deep learning jobs create and evaluate models, they will be automatically stored as MLflow Logged Models in your MLflow Experiment. 

Each GenAI agent consolidates its evaluation metrics from both offline jobs and online interactions if you use managed MLflow with Databricks. 
This feature removes the need to query Runs for traces from different environments (batch jobs, different notebook sessions etc.) to gain a full understanding of the agentâ€™s performance.
Similarly, every deep learning checkpoint is stored as a <APILink fn="mlflow.entities.LoggedModel">LoggedModel</APILink> with direct links to its own metrics. There is no longer any need to carefully log evaluation metrics for each checkpoint
at its epoch all within the same Run, or to later query all metrics at a certain epoch just to retrieve metrics for one checkpoint. 

Specifically, you will be able to:
- View Logged Models alongside key information, including code and weights as artifacts and configurable parameters that affect model behavior
- Holistically track all performance metrics related to your logged models with links to training and evaluation jobs
- Search for and rank existing models based on a set of certain criteria, such as performance metrics on training or evaluation datasets

### Quickstart

This quickstart example demonstrates how to use the LoggedModel lineage feature in MLflow 3.0 with openai sdk, showcasing its integration with runs and traces.

Prerequisite: ``pip install mlflow==3.0.0.rc0 openai``

```python
from openai import OpenAI

import mlflow
from mlflow.metrics.genai import answer_correctness, answer_similarity, faithfulness

# turn on autologging for automatic tracing
mlflow.openai.autolog()

client = OpenAI()

mlflow_ground_truth = (
    "MLflow is an open-source platform for managing "
    "the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, "
    "a company that specializes in big data and machine learning solutions. MLflow is "
    "designed to address the challenges that data scientists and machine learning "
    "engineers face when developing, training, and deploying machine learning models."
)
# Define evaluation metrics
metrics = {
    "answer_similarity": answer_similarity(model="openai:/gpt-4o"),
    "answer_correctness": answer_correctness(model="openai:/gpt-4o"),
    "faithfulness": faithfulness(model="openai:/gpt-4o"),
}
question = "What is MLflow?"

with mlflow.start_run():
    response = (
        client.chat.completions.create(
            messages=[{"role": "user", "content": question}],
            model="gpt-4o-mini",
            temperature=0.1,
            max_tokens=2000,
        )
        .choices[0]
        .message.content
    )

    # Calculate metrics based on the input, response and ground truth
    # The evaluation metrics are callables that can be invoked directly
    answer_similarity_score = metrics["answer_similarity"](
        predictions=response, inputs=question, targets=mlflow_ground_truth
    ).scores[0]
    answer_correctness_score = metrics["answer_correctness"](
        predictions=response, inputs=question, targets=mlflow_ground_truth
    ).scores[0]
    faithfulness_score = metrics["faithfulness"](
        predictions=response, inputs=question, context=mlflow_ground_truth
    ).scores[0]

    # Fetch the LoggedModel that's automatically created during autologging
    logged_model = mlflow.last_logged_model()
    # Log metrics and pass model_id to link the metrics
    mlflow.log_metrics(
        {
            "answer_similarity": answer_similarity_score,
            "answer_correctness": answer_correctness_score,
            "faithfulness": faithfulness_score,
        },
        model_id=logged_model.model_id,
    )


print(f"LoggedModel model id: {logged_model.model_id}")
# LoggedModel model id: a208e70b-e80b-4f8e-b210-20737faadd20

traces = mlflow.search_traces(model_id=logged_model.model_id)
print(traces)
#                          request_id                                              trace  ...                                               tags assessments
# 0  5882df1240cf4dbf845fdc9fa26c4168  Trace(request_id=5882df1240cf4dbf845fdc9fa26c4...  ...  {'mlflow.artifactLocation': 'file:///Users/ser...          []
# [1 rows x 11 columns]
```

Navigate to the run's page to view the LoggedModel:

<div className="center-div" style={{ width: "100%" }}>
    ![The MLflow UI showing the run and logged model](/images/mlflow-3/simple_example_run_page.png)
</div>

Clicking on the logged model takes you to the model page, where you can view its details, including the model ID, source run, associated parameters, metrics and traces:

<div className="center-div" style={{ width: "100%" }}>
    ![The MLflow UI showing the logged model](/images/mlflow-3/simple_example_model.png)
</div>
<div className="center-div" style={{ width: "100%" }}>
    ![The MLflow UI showing the logged model traces](/images/mlflow-3/simple_example_traces.png)
</div>

## MLflow 3.0 Showcases

Explore the examples below to see how MLflow 3.0's powerful features can be applied across various domains.

<CardGroup>
  <PageCard headerText="GenAI with MLflow 3.0" link="/mlflow-3/genai-agent" text="Discover how to log, evaluate, and trace GenAI agents using MLflow 3.0." />
  <PageCard headerText="Deep Learning with MLflow 3.0" link="/mlflow-3/deep-learning" text="Learn how to leverage MLflow 3.0 to identify the best models in deep learning workflows." />
</CardGroup>

## Migration Guide

MLflow 3.0 introduces some key API changes while also removes some outdated features. This guide will help you transition smoothly to the latest version.

### Key changes

- `mlflow.<flavor>.log_model` API usage: `artifact_path` parameter is deprecated, use **`name`** instead
<Tabs>
    <TabItem label="MLflow 2.x" value="mlflow_2" default>
        ```python
        with mlflow.start_run():
            mlflow.pyfunc.log_model(artifact_path="model", python_model=python_model, ...)
        ```
    </TabItem>
    <TabItem label="MLflow 3.0" value="mlflow_3">
        Pass `name` when logging a model. This allows you to later search for LoggedModels using this name.
        ```python
        with mlflow.start_run():
            mlflow.pyfunc.log_model(name="python_model", python_model=python_model, ...)
        ```
    </TabItem>
</Tabs>

- Model artifacts storage location change: In MLflow 2.x, [model artifacts](../model/#storage-format) are stored as run artifacts. 
Since MLflow 3.0, those artifacts will be stored into models artifacts location. Note: this impacts the behavior of <APILink fn="mlflow.client.MlflowClient.list_artifacts">``list_artifacts``</APILink> API.

### Removed Features

- MLflow Recipes
- Flavors: the following model flavors are no longer supported
    - fastai
    - h2o
    - mleap
- AI gateway client APIs: use deployments APIs instead

