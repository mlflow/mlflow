---
sidebar_custom_props: { hide: true }
---

import Link from "@docusaurus/Link";

# Quickstart options and troubleshooting

{/** Eventually, these H2s will probably all be separate articles. For now, I'm
avoiding that so as not to create a bunch of super-skinny pages. **/}

## Customize and troubleshoot MLflow installation \{#quickstart_drilldown_install}

### Python library options

Rather than the default MLflow library, you can install the following variations:

<table>
  <thead>
    <tr>
      <th>**Name**</th>
      <th>**`pip install` command**</th>
      <th>**Description**</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mlflow-skinny</td>
      <td>`pip install mlflow-skinny`</td>
      <td>Lightweight MLflow package without SQL storage, server, UI, or data science dependencies.</td>
    </tr>
    <tr>
      <td>mlflow[extras]</td>
      <td>`pip install mlflow[extras]`</td>
      <td>MLflow package with all dependencies needed to run various MLflow flavors. These dependencies are listed in [this document](https://github.com/mlflow/mlflow/blob/master/requirements/extra-ml-requirements.txt).</td>
    </tr>
    <tr>
      <td>In-development version</td>
      <td>`pip install git+https://github.com/mlflow/mlflow.git@master`</td>
      <td>This is the latest version of MLflow, which may be useful for getting hot-fixes or new features.</td>
    </tr>
  </tbody>
</table>

### Python and Mac OS X

We strongly recommend using a virtual environment manager on Macs. We always recommend
using virtual environments, but they are especially important on Mac OS X because the system
`python` version varies depending on the installation and whether you've installed the Xcode
command line tools. The default environment manager for MLflow is `virtualenv`.
Other popular options are `conda` and `venv`.

### Python

We release MLflow on:

- PyPI (`pip install mlflow`)
- conda-forge (`conda install -c conda-forge mlflow`)

### R and Java

We release MLflow on:

- CRAN (`install.packages("mlflow")`)
- Maven Central (`mlflow-client`, `mlflow-parent`, `mlflow-scoring`, `mlflow-spark`)

For R, see <Link to="/api_reference/R-api.html" target='_blank'>installing MLflow for R</Link>.
For Java, see <Link to="/api_reference/java_api/index.html" target='_blank'>Java API</Link>.

## Save and serve models \{#quickstart_drilldown_log_and_load_model}

MLflow includes a generic `MLmodel` format for saving **models** from a variety of tools in diverse
**flavors**. For example, many models can be served as Python functions, so an `MLmodel` file can
declare how each model should be interpreted as a Python function in order to let various tools
serve it. MLflow also includes tools for running such models locally and exporting them to Docker
containers or commercial serving platforms.

To illustrate this functionality, the `mlflow.sklearn` flavor can log scikit-learn models as
MLflow artifacts and then load them again for serving. There is an example training application in
[sklearn_logistic_regression/train.py](https://github.com/mlflow/mlflow/tree/master/examples/sklearn_logistic_regression).
To run it, switch to the MLflow repository root and run:

```bash
python examples/sklearn_logistic_regression/train.py
```

When you run the example, it outputs an MLflow run ID for that experiment. If you look at the
`mlflow ui`, you will also see that the run saved a **model** folder containing an `MLmodel`
description file and a pickled scikit-learn model. You can pass the run ID and the path of the model
within the artifacts directory (here **model/**) to various tools. For example, MLflow includes a
simple REST server for python-based models:

```bash
mlflow models serve -m --env-manager local runs:/<RUN_ID>/model
```

:::note
By default the server runs on port 5000. If that port is already in use, use the `--port` option to
specify a different port. For example: `mlflow models serve -m runs:/<RUN_ID>/model --port 1234`
:::

Once you have started the server, you can pass it some sample data and see the
predictions.

The following example uses `curl` to send a JSON-serialized pandas DataFrame with the `split`
orientation to the model server. For more information about the input data formats accepted by
the pyfunc model server, see the [MLflow deployment tools documentation](/deployment/deploy-model-locally).

```bash
curl -d '{"dataframe_split": {"columns": ["x"], "data": [[1], [-1]]}}' -H 'Content-Type: application/json' -X POST localhost:5000/invocations
```

which returns:

```bash
[1, 0]
```

For more information, see [MLflow Models](/models).
