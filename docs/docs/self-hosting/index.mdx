---
title: Self Hosting Overview
sidebar_position: 1
---

# Self-Hosting MLflow

> #### ***The most vendor-neutral MLOps/LLMOps platform in the world.***

MLflow is fully open-source. Thousands of users and organizations run their own MLflow instances to meet their specific needs. Being open-source and trusted by the popular cloud providers, MLflow is the best choice for teams/organizations that worry about vendor lock-in.

## The Quickest Path: Running `mlflow` Command

The easiest way to start self-hosted MLflow is to run the `mlflow` command in your terminal. This is suitable for personal use or small teams.

First, install MLflow with:

```bash
pip install mlflow
```

Then, start the server with:

```bash
mlflow server --backend-store-uri sqlite:///mlflow.db --port 5000
```

This will start the server and UI at `http://localhost:5000`. You can connect the client to the server by setting the tracking URI:

```python
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")

# Start tracking!
```

:::tip

The `--backend-store-uri` option is added here to use a database backend. MLflow by default uses a local filesystem for storing the metadata, but a database backend provides much better performance and reliability in general. The file backend works perfectly fine for small use cases, but if you plan to log >1000 runs, metrics, traces, etc, we highly recommend using a database. For different database options such as PostgreSQL, check out <ins>[Backend Store](./architecture/backend-store)</ins>.

:::

## Other Deployment Options

### Docker Compose

The MLflow repository includes a ready-to-run Compose project under `docker-compose/` that provisions MLflow, PostgreSQL, and MinIO.

```bash
git clone https://github.com/mlflow/mlflow.git
cd docker-compose
cp .env.dev.example .env
docker compose up -d
```

Read the instructions [here](https://github.com/mlflow/mlflow/tree/master/docker-compose) for more details and configuration options for the docker compose bundle.

### Kubernetes

To deploy on Kubernetes, use the MLflow Helm chart provided by [Bitnami](https://artifacthub.io/packages/helm/bitnami/mlflow) or [Community Helm Charts](https://artifacthub.io/packages/helm/community-charts/mlflow).

### Cloud Services

If you are looking for production-scale deployments without maintenance costs, MLflow is also available as managed services from popular cloud providers.

* [Databricks](https://www.databricks.com/product/managed-mlflow)
* [AWS Sagemaker](https://aws.amazon.com/sagemaker/ai/experiments/)
* [Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
* [Nebius](https://nebius.com/services/managed-mlflow)
* [GCP (GKE)](https://gke-ai-labs.dev/docs/tutorials/frameworks-and-pipelines/mlflow/)

## Architecture

MLflow, at a high level, consists of the following components:

1. **Tracking Server**: The lightweight FastAPI server that serves the MLflow UI and API.
2. **Backend Store**: The Backend Store is relational database (or file system) that stores the metadata of the experiments, runs, traces, etc.
3. **Artifact Store**: The Artifact Store is responsible for storing the large artifacts such as model weights, images, etc.

Each component is designed to be pluggable, so you can customize it to meet your needs. For example, you can start with a single host mode with SQLite backend and local file system for storing artifacts. To scale up, you can switch backend store to PostgreSQL cluster and point artifact store to cloud storage such as S3, GCS, or Azure Blob Storage.

To learn more about the architecture and available backend options, see [Architecture](./architecture/index).

## Access Control

MLflow support [basic username/password login](./authentication/basic-http-auth) via HTTP authentication, [SSO (Single Sign-On)](./authentication/sso), and [custom authentication functions](./authentication/custom).

## FAQs

See [Troubleshooting & FAQs](./troubleshooting) for more information.