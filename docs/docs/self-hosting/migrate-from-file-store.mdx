---
title: Migrate from File Store
---

# Migrate from File Store to Database

MLflow's default backend store changed from file-based storage (`./mlruns`) to SQLite database in MLflow 3.6. If you have existing data in a file-based backend, you can migrate it to a database using the built-in migration command.

## Prerequisites

- MLflow 3.x or later installed
- Access to the `mlruns` directory (or wherever your FileStore data lives)

## Quick Start

```bash
mlflow migrate-filestore --source ./mlruns --target sqlite:///mlflow.db
```

This reads all data from the `./mlruns` directory and writes it to a new SQLite database at `mlflow.db`.

## What Gets Migrated

The migration tool transfers **all metadata** stored in the file-based backend:

| Category           | Entities                                                          |
| ------------------ | ----------------------------------------------------------------- |
| Experiments        | Experiments, experiment tags                                      |
| Runs               | Runs, params, metrics, latest metrics, tags                       |
| Datasets           | Datasets, inputs, input tags                                      |
| Traces             | Trace info, trace tags, trace request metadata                    |
| Assessments        | Assessments (feedback, expectations)                              |
| Logged Models      | Logged models, params, tags, metrics                              |
| Model Registry     | Registered models, model versions, tags, aliases                  |

### What Is NOT Migrated

- **Artifacts** (model files, images, etc.) — these stay in their original location. The database stores the same artifact URIs that point to the existing files.
- **Trace spans** — stored as artifact files, not in the database.

## Options

| Option     | Description                                          |
| ---------- | ---------------------------------------------------- |
| `--source` | Path to the directory containing FileStore data.     |
| `--target` | SQLite URI for the target database (e.g. `sqlite:///mlflow.db`). |

The `--source` can point to either:
- A directory containing an `mlruns/` subdirectory
- The `mlruns` directory itself

## Step-by-Step Guide

### 1. Stop the MLflow server

If you're running a tracking server, stop it before migrating.

### 2. Back up your data

```bash
cp -r ./mlruns ./mlruns.backup
```

### 3. Run the migration

```bash
mlflow migrate-filestore --source ./mlruns --target sqlite:///mlflow.db
```

The command prints progress for each phase:

```
Source: ./mlruns
Target: sqlite:///mlflow.db

Initializing database schema...
[1/7] Migrating experiments + tags...
[2/7] Migrating runs + params + tags + metrics...
[3/7] Migrating datasets + inputs...
[4/7] Migrating traces + tags + metadata...
[5/7] Migrating assessments...
[6/7] Migrating logged models...
[7/7] Migrating model registry...

Migration completed successfully!

==================================================
Migration summary:
==================================================
  experiments: 5
  runs: 42
  metrics: 1200
  ...
==================================================
```

Entity types that don't exist in your FileStore (e.g., traces if you never used tracing) are automatically skipped.

### 4. Start the server with the new database

```bash
mlflow server --backend-store-uri sqlite:///mlflow.db
```

### 5. Verify

Open the MLflow UI and confirm your experiments, runs, and models are present.

## Important Notes

- **Target database must be empty.** The migration tool refuses to write to a database that already contains data to prevent conflicts.
- **Original IDs are preserved.** Experiment IDs, run UUIDs, and all timestamps are kept exactly as they were in the file store.
- **Deleted items are included.** Experiments and runs in the `.trash` directory are migrated with their `deleted` lifecycle stage preserved.
- **SQLite only.** The initial version supports SQLite as the target database. FileStore generates large experiment IDs that exceed the 32-bit integer limit enforced by PostgreSQL and MySQL, but SQLite handles them natively.
- **The migration is atomic.** If any error occurs, the entire transaction is rolled back, leaving the target database empty.
