---
sidebar_position: 0
sidebar_label: Overview
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { CardGroup, LogoCard } from "@site/src/components/Card";

# MLflow for Deep Learning

**Deep learning** has revolutionized artificial intelligence, enabling breakthrough capabilities in computer vision, natural language processing, generative AI, and countless other domains. As models grow more sophisticated, managing the complexity of deep learning experiments becomes increasingly challenging.

MLflow provides a comprehensive solution for tracking, managing, and deploying deep learning models across all major frameworks. Whether you're fine-tuning transformers, training computer vision models, or developing custom neural networks, MLflow's powerful toolkit simplifies your workflow from experiment to production.

<details>
  <summary>Why Deep Learning Needs MLflow</summary>

  #### The Challenges of Modern Deep Learning

  - üîÑ **Iterative Development**: Deep learning requires extensive experimentation with architectures, hyperparameters, and training regimes
  - üìä **Complex Metrics**: Models generate numerous metrics across training steps that must be tracked and compared
  - üíæ **Large Artifacts**: Models, checkpoints, and visualizations need systematic storage and versioning
  - üß© **Framework Diversity**: Teams often work across PyTorch, TensorFlow, Keras, and other specialized libraries
  - üî¨ **Reproducibility Crisis**: Without proper tracking, recreating results becomes nearly impossible
  - üë• **Team Collaboration**: Multiple researchers need visibility into experiments and the ability to build on each other's work
  - üöÄ **Deployment Complexities**: Moving from successful experiments to production introduces new challenges

  MLflow addresses these challenges with a framework-agnostic platform that brings structure and clarity to the entire deep learning lifecycle.
</details>

## Key Features for Deep Learning

### üìä Comprehensive Experiment Tracking

MLflow's tracking capabilities are tailor-made for the iterative nature of deep learning:

- **One-Line Autologging** for PyTorch, TensorFlow, and Keras
- **Step-Based Metrics** capture training dynamics across epochs and batches
- **Hyperparameter Tracking** for architecture choices and training configurations
- **Resource Monitoring** tracks GPU utilization, memory consumption, and training time

<details>
  <summary>Advanced Tracking Capabilities</summary>

  #### Beyond Basic Metrics

  MLflow's tracking system supports the specialized needs of deep learning workflows:

  - **Model Architecture Logging**: Automatically capture neural network structures and parameter counts
  - **Dataset Tracking**: Record dataset versions, preprocessing steps, and augmentation parameters
  - **Visual Debugging**: Store sample predictions, attention maps, and other visual artifacts
  - **Distributed Training**: Monitor metrics across multiple nodes in distributed training setups
  - **Custom Artifacts**: Log confusion matrices, embedding projections, and other specialized visualizations
  - **Hardware Profiling**: Track GPU/TPU utilization, memory consumption, and throughput metrics
  - **Early Stopping Points**: Record when early stopping occurred and store the best model states
</details>

<Tabs>
  <TabItem default label="Chart Comparison" value="chart-comparison">
    #### Compare Training Convergence at a Glance

    Visualize multiple deep learning runs to quickly identify which configurations achieve superior performance across training iterations.

    <div style={{ width: "90%" }}>
      ![Training convergence comparison](/images/deep-learning/dl-run-selection.gif)
    </div>
  </TabItem>

  <TabItem label="Chart Customization" value="chart-customization">
    #### Customize Visualizations for Deeper Insights

    Tailor charts to focus on critical metrics and training phases, helping you pinpoint exactly when and why certain models outperform others.

    <div style={{ width: "90%" }}>
      ![Chart customization](/images/deep-learning/dl-run-navigation.gif)
    </div>
  </TabItem>

  <TabItem label="Run Comparison" value="run-comparison">
    #### Analyze Parameter Relationships

    Explore parameter interactions and their effects on model performance through MLflow's comprehensive comparison views.

    <div style={{ width: "90%" }}>
      ![Parameter comparison](/images/deep-learning/dl-run-comparison.gif)
    </div>
  </TabItem>

  <TabItem label="Statistical Evaluation" value="statistical-evaluation">
    #### Statistical Insights into Hyperparameters

    Use boxplot visualizations to quickly determine which hyperparameter values consistently lead to better performance.

    <div style={{ width: "90%" }}>
      ![Statistical evaluation](/images/deep-learning/dl-boxplot.gif)
    </div>
  </TabItem>

  <TabItem label="Realtime Tracking" value="realtime-tracking">
    #### Monitor Training in Real-Time

    Watch your deep learning models train with live-updating metrics, eliminating the need for manual progress checks.

    <div style={{ width: "90%" }}>
      ![Realtime tracking](/images/deep-learning/dl-tracking.gif)
    </div>
  </TabItem>

  <TabItem label="Model Comparison" value="model-comparison">
    #### Model Comparison

    Track your all your DL checkpoints across epochs using the MLflow UI. Compare performance and quickly find the best checkpoints based on any metrics.

    <div  style={{ width: "90%" }}>
      ![](/images/deep-learning/dl-model-comparison.gif)
    </div>
  </TabItem>
</Tabs>

### üèÜ Streamlined Model Management

Deep learning models are valuable assets that require careful management:

- **Versioned Model Registry** provides a central repository for all your models
- **Model Lineage** tracks the complete history from data to deployment
- **Metadata Annotations** store architecture details, training datasets, and performance metrics
- **Stage Transitions** manage models through development, staging, and production phases
- **Team Permissions** control who can view, modify, and deploy models
- **Dependency Management** ensures all required packages are tracked with the model

<details>
  <summary>Model Registry for Teams</summary>

  #### Collaborative Model Development

  The MLflow Model Registry enhances team productivity through:

  - **Transition Requests**: Team members can request model promotion with documented justifications
  - **Approval Workflows**: Implement governance with required approvals for production deployments (managed MLflow only)
  - **Performance Baselines**: Set threshold requirements before models can advance to production
  - **Rollback Capabilities**: Quickly revert to previous versions if issues arise
  - **Activity Feeds**: Track who made changes to models and when (managed MLflow only)
  - **Webhook Integration**: Trigger CI/CD pipelines and notifications based on registry events (managed MLflow only)
  - **Model Documentation**: Store comprehensive documentation alongside model artifacts
</details>

### üöÄ Simplified Deployment

Move from successful experiments to production with ease:

- **Consistent Inference APIs** across all deep learning frameworks
- **GPU-Ready Deployments** for compute-intensive models
- **Batch and Real-Time Serving** options for different application needs
- **Docker Containerization** for portable, isolated environments
- **Serverless Deployments** for scalable, cost-effective serving within your cloud provider infrastructure
- **Edge Deployment** support for mobile and IoT applications

<details>
  <summary>Advanced Deployment Options</summary>

  #### Beyond Basic Serving

  MLflow supports sophisticated deployment scenarios for deep learning:

  - **Model Ensembling**: Deploy multiple models with voting or averaging mechanisms
  - **Custom Preprocessing/Postprocessing**: Attach data transformation pipelines to your model
  - **Optimized Inference**: Support for quantization, pruning, and other optimization techniques
  - **Monitoring Integration**: Connect to observability platforms for production tracking
  - **Hardware Acceleration**: Leverage GPU/TPU resources for high-throughput inference in cloud provider infrastructure
  - **Scalable Architecture**: Handle variable loads with auto-scaling capabilities (managed MLflow only)
  - **Multi-Framework Deployment**: Mix models from different frameworks in the same serving environment
</details>

## Framework Integrations

MLflow provides native support for all major deep learning frameworks, allowing you to use your preferred tools while gaining the benefits of unified experiment tracking and model management.

<CardGroup>
  <LogoCard description="Seamlessly track TensorFlow experiments with one-line autologging. Capture training metrics, model architecture, and TensorBoard visualizations in a centralized repository." link="/ml/deep-learning/tensorflow">
    <span>![TensorFlow Logo](/images/logos/TensorFlow-logo.svg)</span>
  </LogoCard>

  <LogoCard description="Integrate MLflow with PyTorch's flexible deep learning ecosystem. Log metrics from custom training loops, save model checkpoints, and simplify deployment for production." link="/ml/deep-learning/pytorch">
    <span>![PyTorch Logo](/images/logos/pytorch-logo.svg)</span>
  </LogoCard>

  <LogoCard description="Harness Keras 3.0's multi-backend capabilities with comprehensive MLflow tracking. Monitor training across TensorFlow, PyTorch, and JAX backends with consistent experiment management." link="/ml/deep-learning/keras">
    <span>![Keras Logo](/images/logos/keras-logo.svg)</span>
  </LogoCard>

  <LogoCard description="Track and manage spaCy NLP models throughout their lifecycle. Log training metrics, compare model versions, and deploy language processing pipelines to production." link="/ml/model/#spacyspacy">
    <span>![spaCy Logo](/images/logos/spacy-logo.svg)</span>
  </LogoCard>
</CardGroup>

## Getting Started

<details>
  <summary>Quick Setup Guide</summary>

  ### 1. Install MLflow

  ```bash
  pip install mlflow
  ```

  Ensure that you have the appropriate DL integration package installed. For example, for PyTorch with image model support:

  ```bash
  pip install torch torchvision
  ```

  ### 2. Start Tracking Server (Optional)

  ```bash
  # Start a local tracking server
  mlflow server --host 0.0.0.0 --port 5000
  ```

  ### 3. Enable Autologging

  ```python
  import mlflow

  # For TensorFlow/Keras
  mlflow.tensorflow.autolog()

  # For PyTorch Lightning
  mlflow.pytorch.autolog()

  # For all supported frameworks
  mlflow.autolog()
  ```

  ### 4. Train Your Model Normally

  ```python
  # Your existing training code works unchanged!
  model.fit(train_data, train_labels, epochs=10, validation_data=(val_data, val_labels))
  ```

  ### 5. View Results

  Open the MLflow UI to see your tracked experiments:

  ```bash
  mlflow ui
  ```

  Or if using a tracking server:

  ```
  http://localhost:5000
  ```
</details>

## Real-World Applications

Deep learning with MLflow powers a wide range of applications across industries:

- üñºÔ∏è **Computer Vision**: Track performance of object detection, image segmentation, and classification models
- üîä **Speech Recognition**: Monitor acoustic model training and compare word error rates across architectures
- üìù **Natural Language Processing**: Manage fine-tuning of large language models and evaluate performance on downstream tasks
- üéÆ **Reinforcement Learning**: Track agent performance, rewards, and environmental interactions across training runs
- üß¨ **Genomics**: Organize deep learning models analyzing genetic sequences and protein structures
- üìä **Financial Forecasting**: Compare predictive models for time series analysis and risk assessment
- üè≠ **Manufacturing**: Deploy computer vision models for quality control and predictive maintenance
- üè• **Healthcare**: Manage medical imaging models with rigorous versioning and approval workflows

## Advanced Topics

<details>
  <summary>Distributed Training Integration</summary>

  MLflow integrates seamlessly with distributed training frameworks:

  - **Horovod**: Track metrics across distributed TensorFlow and PyTorch training
  - **PyTorch DDP**: Monitor distributed data parallel training
  - **TensorFlow Distribution Strategies**: Log metrics from multi-GPU and multi-node training
  - **Ray**: Integrate with Ray's distributed computing ecosystem

  Example with PyTorch DDP:

  ```python
  import mlflow
  import torch.distributed as dist
  from torch.nn.parallel import DistributedDataParallel

  mlflow.pytorch.autolog()

  # Initialize process group
  dist.init_process_group(backend="nccl")

  # Create model and move to GPU with DDP wrapper
  model = DistributedDataParallel(model.to(rank))

  # MLflow tracking works normally with DDP
  with mlflow.start_run():
      trainer.fit(model)
  ```
</details>

<details>
  <summary>Hyperparameter Optimization</summary>

  MLflow integrates with popular hyperparameter optimization frameworks:

  - **Optuna**: Track trials and visualize optimization results
  - **Ray Tune**: Monitor distributed hyperparameter sweeps
  - **Weights & Biases Sweeps**: Synchronize W&B sweeps with MLflow tracking
  - **HyperOpt**: Organize and compare hyperparameter search results

  Example with Optuna:

  ```python
  import mlflow
  import optuna


  def objective(trial):
      with mlflow.start_run(nested=True):
          # Suggest hyperparameters
          lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
          batch_size = trial.suggest_categorical("batch_size", [16, 32, 64, 128])

          # Log parameters to MLflow
          mlflow.log_params({"lr": lr, "batch_size": batch_size})

          # Train model
          model = create_model(lr)
          result = train_model(model, batch_size)

          # Log results
          mlflow.log_metrics({"accuracy": result["accuracy"]})

          return result["accuracy"]


  # Create study
  with mlflow.start_run():
      study = optuna.create_study(direction="maximize")
      study.optimize(objective, n_trials=100)

      # Log best parameters
      mlflow.log_params({f"best_{k}": v for k, v in study.best_params.items()})
      mlflow.log_metric("best_accuracy", study.best_value)
  ```
</details>

<details>
  <summary>Transfer Learning Workflows</summary>

  MLflow helps organize transfer learning and fine-tuning workflows:

  - **Base Model Registry**: Maintain a catalog of pre-trained models
  - **Fine-Tuning Tracking**: Monitor performance as you adapt models to new tasks
  - **Layer Freezing Analysis**: Compare different layer freezing strategies
  - **Learning Rate Scheduling**: Track the impact of different learning rate strategies for fine-tuning

  Example tracking a fine-tuning run:

  ```python
  import mlflow
  import torch
  from transformers import AutoModelForSequenceClassification

  with mlflow.start_run():
      # Log base model information
      base_model_name = "bert-base-uncased"
      mlflow.log_param("base_model", base_model_name)

      # Create and customize model for fine-tuning
      model = AutoModelForSequenceClassification.from_pretrained(base_model_name)

      # Log which layers are frozen
      frozen_layers = ["embeddings", "encoder.layer.0", "encoder.layer.1"]
      mlflow.log_param("frozen_layers", frozen_layers)

      # Freeze specified layers
      for name, param in model.named_parameters():
          if any(layer in name for layer in frozen_layers):
              param.requires_grad = False

      # Log trainable parameter count
      trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
      total_params = sum(p.numel() for p in model.parameters())
      mlflow.log_params(
          {
              "trainable_params": trainable_params,
              "total_params": total_params,
              "trainable_percentage": trainable_params / total_params,
          }
      )

      # Fine-tune and track results...
  ```
</details>

## Learn More

Dive deeper into MLflow's capabilities for deep learning in our framework-specific guides:

- **[TensorFlow Guide](/ml/deep-learning/tensorflow)**: Master MLflow's integration with TensorFlow and Keras
- **[PyTorch Guide](/ml/deep-learning/pytorch)**: Learn how to track custom PyTorch training loops
- **[Keras Guide](/ml/deep-learning/keras)**: Explore Keras 3.0's multi-backend capabilities with MLflow
- **[Model Registry](/ml/model-registry)**: Manage model versions and transitions through development stages
- **[MLflow Deployments](/ml/deployment)**: Deploy deep learning models to production