---
sidebar_position: 1
sidebar_label: Pytorch
---

import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ImageBox from "@site/src/components/ImageBox";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { Zap, GitBranch, Package, BookOpen } from "lucide-react";

# MLflow PyTorch Integration

## Introduction

**PyTorch** is an open-source deep learning framework developed by Meta's AI Research lab. It provides dynamic computation graphs and a Pythonic API for building neural networks, making it popular for both research and production deep learning applications.

MLflow's PyTorch integration provides experiment tracking, model versioning, and deployment capabilities for deep learning workflows.

<ImageBox src="/images/deep-learning/dl-training-ui.gif" alt="Deep Learning Training" />

## Why MLflow + PyTorch?

<FeatureHighlights
  features={[
    {
      icon: Zap,
      title: "Autologging",
      description: "Enable comprehensive experiment tracking with one line: mlflow.pytorch.autolog() automatically logs metrics, parameters, and models.",
    },
    {
      icon: GitBranch,
      title: "Experiment Tracking",
      description: "Track training metrics, hyperparameters, model architectures, and artifacts across all PyTorch experiments.",
    },
    {
      icon: Package,
      title: "Model Registry",
      description: "Version, stage, and deploy PyTorch models with MLflow's model registry and serving infrastructure.",
    },
    {
      icon: BookOpen,
      title: "Reproducibility",
      description: "Capture model states, random seeds, and environments for reproducible deep learning experiments.",
    },
  ]}
/>

## Getting Started

Enable comprehensive experiment tracking with one line of code:

```python
import mlflow
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Enable autologging
mlflow.pytorch.autolog()

# Create synthetic data
X = torch.randn(1000, 784)
y = torch.randint(0, 10, (1000,))
train_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)

# Your existing PyTorch code works unchanged
model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Training loop - metrics, parameters, and models logged automatically
for epoch in range(10):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
```

Autologging captures training metrics, model parameters, optimizer configuration, and model checkpoints automatically.

## Learn More

<TilesGrid>
  <TileCard
    icon={BookOpen}
    title="PyTorch Quickstart"
    description="Hands-on tutorial covering autologging, manual logging, and model deployment"
    href="/ml/deep-learning/pytorch/quickstart/quickstart-pytorch"
  />
  <TileCard
    icon={BookOpen}
    title="PyTorch Guide"
    description="Comprehensive guide for PyTorch model tracking and deployment"
    href="/ml/deep-learning/pytorch/guide"
  />
  <TileCard
    icon={Package}
    title="Model Registry"
    description="Version and manage PyTorch models"
    href="/ml/model-registry"
  />
  <TileCard
    icon={GitBranch}
    title="MLflow Tracking"
    description="Track experiments, parameters, and metrics"
    href="/ml/tracking"
  />
</TilesGrid>
