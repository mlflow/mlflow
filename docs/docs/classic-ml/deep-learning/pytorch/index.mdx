---
sidebar_position: 1
sidebar_label: Pytorch
---

import FeatureHighlights from "@site/src/components/FeatureHighlights";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { Zap, GitBranch, Package, BookOpen } from "lucide-react";

# MLflow PyTorch Integration

## Introduction

**PyTorch** is an open-source deep learning framework developed by Meta's AI Research lab. It provides dynamic computation graphs and a Pythonic API for building neural networks, making it popular for both research and production deep learning applications.

MLflow's PyTorch integration provides experiment tracking, model versioning, and deployment capabilities for deep learning workflows.

## Why MLflow + PyTorch?

<FeatureHighlights
  features={[
    {
      icon: Zap,
      title: "Autologging",
      description: "Enable comprehensive experiment tracking with one line: mlflow.pytorch.autolog() automatically logs metrics, parameters, and models.",
    },
    {
      icon: GitBranch,
      title: "Experiment Tracking",
      description: "Track training metrics, hyperparameters, model architectures, and artifacts across all PyTorch experiments.",
    },
    {
      icon: Package,
      title: "Model Registry",
      description: "Version, stage, and deploy PyTorch models with MLflow's model registry and serving infrastructure.",
    },
    {
      icon: BookOpen,
      title: "Reproducibility",
      description: "Capture model states, random seeds, and environments for reproducible deep learning experiments.",
    },
  ]}
/>

## Key Features

### Autologging

Enable comprehensive experiment tracking with one line of code:

```python
import mlflow
import torch
import torch.nn as nn

# Enable autologging
mlflow.pytorch.autolog()

# Your existing PyTorch code works unchanged
model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop - metrics, parameters, and models logged automatically
for epoch in range(num_epochs):
    # ... your training code
    pass
```

Autologging captures training metrics, model parameters, optimizer configuration, and model checkpoints automatically.

### Manual Logging

For fine-grained control, use MLflow's manual logging APIs:

```python
import mlflow

with mlflow.start_run():
    # Log hyperparameters
    mlflow.log_params({"learning_rate": 0.001, "batch_size": 32, "epochs": 100})

    # Training loop
    for epoch in range(epochs):
        loss = train_epoch(model, dataloader)
        mlflow.log_metric("loss", loss, step=epoch)

    # Log model
    mlflow.pytorch.log_model(model, name="my_model")
```

### Model Registry Integration

Version and deploy PyTorch models using MLflow's model registry:

```python
# Log model with registry
model_info = mlflow.pytorch.log_model(
    model, name="pytorch_classifier", registered_model_name="ImageClassifier"
)

# Transition to production
from mlflow import MlflowClient

client = MlflowClient()
client.transition_model_version_stage(
    name="ImageClassifier",
    version=model_info.registered_model_version,
    stage="Production",
)
```

## Learn More

<TilesGrid>
  <TileCard
    icon={BookOpen}
    title="PyTorch Quickstart"
    description="Hands-on tutorial covering autologging, manual logging, and model deployment"
    href="/ml/deep-learning/pytorch/quickstart/quickstart-pytorch"
  />
  <TileCard
    icon={BookOpen}
    title="PyTorch Guide"
    description="Comprehensive guide for PyTorch model tracking and deployment"
    href="/ml/deep-learning/pytorch/guide"
  />
  <TileCard
    icon={Package}
    title="Model Registry"
    description="Version and manage PyTorch models"
    href="/ml/model-registry"
  />
  <TileCard
    icon={GitBranch}
    title="MLflow Tracking"
    description="Track experiments, parameters, and metrics"
    href="/ml/tracking"
  />
</TilesGrid>
