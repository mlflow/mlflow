---
sidebar_position: 0
sidebar_label: Overview
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { CardGroup, PageCard, SmallLogoCard } from "@site/src/components/Card";

# MLflow for Traditional Machine Learning

**Traditional machine learning** forms the backbone of data science, powering critical applications across every industry. From fraud detection in banking to demand forecasting in retail, these proven algorithms deliver reliable, interpretable results that businesses depend on every day.

MLflow provides comprehensive support for traditional ML workflows, making it effortless to track experiments, manage models, and deploy solutions at scale. Whether you're building ensemble models, tuning hyperparameters, or deploying batch scoring pipelines, MLflow streamlines your journey from prototype to production.

<details>
  <summary>Why Traditional ML Needs MLflow</summary>

  #### The Challenges of Traditional ML at Scale

  - üîÑ **Extensive Experimentation**: Traditional ML requires systematic testing of algorithms, features, and hyperparameters to find optimal solutions
  - üìä **Model Comparison**: Comparing performance across different algorithms and configurations becomes complex at scale
  - üîß **Pipeline Management**: Managing preprocessing, feature engineering, and model training workflows requires careful orchestration
  - üë• **Team Collaboration**: Data scientists need to share experiments, models, and insights across projects
  - üöÄ **Deployment Complexity**: Moving from notebook experiments to production systems introduces operational challenges
  - üìã **Regulatory Compliance**: Many industries require detailed model documentation and audit trails

  MLflow addresses these challenges with purpose-built tools for traditional ML workflows, providing structure and clarity throughout the entire machine learning lifecycle.
</details>

## Key Features for Traditional ML

### üéØ Intelligent Autologging

MLflow's autologging capabilities are designed specifically for traditional ML libraries:

- **One-Line Integration** for scikit-learn, XGBoost, LightGBM, and more
- **Automatic Parameter Capture** logs all model hyperparameters without manual intervention
- **Built-in Evaluation Metrics** automatically computes and stores relevant performance metrics
- **Model Serialization** handles complex objects like pipelines and custom transformers seamlessly

<details>
  <summary>Advanced Autologging Features</summary>

  #### Beyond Basic Tracking

  MLflow's autologging system provides sophisticated capabilities for traditional ML:

  - **Pipeline Stage Tracking**: Automatically log parameters and transformations for each pipeline component
  - **Hyperparameter Search Integration**: Native support for GridSearchCV, RandomizedSearchCV, and popular optimization libraries
  - **Cross-Validation Results**: Capture detailed CV metrics and fold-by-fold performance
  - **Feature Importance**: Automatically log feature importance scores for supported models
  - **Model Signatures**: Infer and store input/output schemas for deployment validation
  - **Custom Metrics**: Seamlessly integrate domain-specific evaluation functions

</details>

<Tabs>
  <TabItem default label="Experiment Comparison" value="experiment-comparison">
    #### Compare Model Performance Across Algorithms

    Easily compare different traditional ML algorithms and hyperparameter configurations to identify the best performing models for your use case.

  </TabItem>

  <TabItem label="Hyperparameter Tuning" value="hyperparameter-tuning">
    #### Visualize Hyperparameter Search Results

    Track hyperparameter optimization experiments with automatic logging of search spaces, best parameters, and convergence patterns.

  </TabItem>

  <TabItem label="Model Registry" value="model-registry">
    #### Manage Model Versions and Lifecycle

    Organize your traditional ML models through development, staging, and production with comprehensive version control and metadata tracking.

  </TabItem>

  <TabItem label="Pipeline Tracking" value="pipeline-tracking">
    #### Track Complex ML Pipelines

    Monitor end-to-end ML pipelines including preprocessing, feature engineering, model training, and evaluation steps.

  </TabItem>
</Tabs>

### üèóÔ∏è Pipeline Management

Traditional ML workflows often involve complex preprocessing and feature engineering:

- **End-to-End Pipeline Tracking** captures every transformation step
- **Custom Transformer Support** works with sklearn pipelines and custom components
- **Reproducible Workflows** guarantee identical results across different environments
- **Pipeline Versioning** manages evolving feature engineering processes
- **Cross-Validation Integration** tracks performance across different data splits
- **Data Validation** ensures consistent preprocessing across training and inference

<details>
  <summary>Enterprise Pipeline Features</summary>

  #### Production-Ready Pipeline Management

  MLflow provides enterprise-grade capabilities for traditional ML pipelines:

  - **Schema Evolution**: Handle changes in input data schemas gracefully
  - **Batch Processing**: Support for large-scale batch inference workflows
  - **Model Monitoring**: Track data drift and model performance degradation
  - **A/B Testing**: Compare model versions in production environments
  - **Rollback Capabilities**: Quickly revert to previous model versions when issues arise

</details>

### üöÄ Flexible Deployment

Deploy traditional ML models across various environments and use cases:

- **Real-Time Inference** for low-latency prediction services
- **Batch Processing** for large-scale scoring jobs
- **Edge Deployment** for offline and mobile applications
- **Containerized Serving** with Docker and Kubernetes support
- **Cloud Integration** across AWS, Azure, and Google Cloud platforms
- **Custom Serving Logic** for complex preprocessing and postprocessing requirements

<details>
  <summary>Advanced Deployment Options</summary>

  #### Beyond Basic Model Serving

  MLflow supports sophisticated deployment patterns for traditional ML:

  - **Multi-Model Endpoints**: Serve multiple models from a single endpoint with routing logic
  - **Ensemble Serving**: Deploy model ensembles with custom combination strategies
  - **Preprocessing Integration**: Include feature engineering pipelines in served models
  - **Monitoring Integration**: Connect to observability platforms for production tracking
  - **Auto-Scaling**: Handle variable loads with dynamic resource allocation

</details>

## Library Integrations

MLflow provides native support for all major traditional ML libraries, enabling seamless integration with your existing workflows while adding powerful experiment tracking and model management capabilities.

<CardGroup>
  <SmallLogoCard link="/ml/traditional-ml/sklearn">![scikit learn](/images/logos/scikit-learn-logo.svg)</SmallLogoCard>
  <SmallLogoCard link="/ml/traditional-ml/xgboost">![XGBoost Logo](/images/logos/xgboost-logo.svg)</SmallLogoCard>
  <SmallLogoCard link="/ml/traditional-ml/sparkml">![Spark Logo](/images/logos/spark-logo.svg)</SmallLogoCard>
  <SmallLogoCard link="/ml/model#lightgbm-lightgbm">![LightGBM Logo](/images/logos/lightgbm-logo.png)</SmallLogoCard>
  <SmallLogoCard link="/ml/model#catboost-catboost">![CatBoost Logo](/images/logos/catboost-logo.png)</SmallLogoCard>
  <SmallLogoCard link="/ml/model#statsmodels-statsmodels">![Statsmodels Logo](/images/logos/statsmodels-logo.svg)</SmallLogoCard>
  <SmallLogoCard link="/ml/traditional-ml/prophet">![Prophet Logo](/images/logos/prophet-logo.png)</SmallLogoCard>
</CardGroup>

## Getting Started

<details>
  <summary>Quick Setup Guide</summary>

  ### 1. Install MLflow

  ```bash
  pip install mlflow
  ```

  For specific integrations, install the corresponding packages:

  ```bash
  # For scikit-learn
  pip install scikit-learn

  # For XGBoost
  pip install xgboost
  ```

  ### 2. Start Tracking Server (Optional)

  ```bash
  # Start a local tracking server
  mlflow server --host 0.0.0.0 --port 5000
  ```

  ### 3. Enable Autologging

  ```python
  import mlflow

  # For scikit-learn
  mlflow.sklearn.autolog()

  # For XGBoost
  mlflow.xgboost.autolog()

  # For all supported frameworks
  mlflow.autolog()
  ```

  ### 4. Train Your Model Normally

  ```python
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.model_selection import train_test_split

  # Your existing training code works unchanged!
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  with mlflow.start_run():
      model = RandomForestClassifier(n_estimators=100)
      model.fit(X_train, y_train)
  ```

  ### 5. View Results

  Open the MLflow UI to see your tracked experiments:

  ```bash
  mlflow ui
  ```

  Or if using a tracking server:

  ```
  http://localhost:5000
  ```

</details>

## Real-World Applications

Traditional ML with MLflow powers critical applications across industries:

- üí≥ **Financial Services**: Credit scoring, fraud detection, and risk assessment models with comprehensive audit trails
- üè• **Healthcare**: Clinical decision support systems with interpretable models and regulatory compliance
- üõí **Retail & E-commerce**: Demand forecasting, recommendation engines, and customer segmentation analytics
- üè≠ **Manufacturing**: Predictive maintenance, quality control, and supply chain optimization
- üìû **Telecommunications**: Customer churn prediction, network optimization, and service quality monitoring
- üöó **Transportation**: Route optimization, demand prediction, and fleet management systems
- üè¢ **Insurance**: Underwriting models, claims processing, and actuarial analysis
- üéØ **Marketing**: Customer lifetime value, campaign optimization, and market basket analysis

## Advanced Topics

<details>
  <summary>Hyperparameter Optimization</summary>

  MLflow integrates seamlessly with popular hyperparameter optimization frameworks:

  ```python
  import mlflow
  import optuna
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.model_selection import cross_val_score


  def objective(trial):
      with mlflow.start_run(nested=True):
          # Define hyperparameter search space
          n_estimators = trial.suggest_int("n_estimators", 10, 100)
          max_depth = trial.suggest_int("max_depth", 1, 10)

          # Train and evaluate model
          model = RandomForestClassifier(
              n_estimators=n_estimators, max_depth=max_depth, random_state=42
          )

          scores = cross_val_score(model, X_train, y_train, cv=5)
          return scores.mean()


  # Run optimization study
  with mlflow.start_run():
      study = optuna.create_study(direction="maximize")
      study.optimize(objective, n_trials=50)

      # Log best results
      mlflow.log_params(study.best_params)
      mlflow.log_metric("best_accuracy", study.best_value)
  ```

</details>

<details>
  <summary>Model Interpretability Integration</summary>

  MLflow provides built-in SHAP integration for automatic model explanations:

  ```python
  import mlflow

  with mlflow.start_run():
      # Train and log model
      model = RandomForestClassifier(n_estimators=100)
      model.fit(X_train, y_train)
      mlflow.sklearn.log_model(model, name="model")
      model_uri = mlflow.get_artifact_uri("model")

      # Evaluate with automatic SHAP explanations
      result = mlflow.evaluate(
          model_uri,
          eval_data,
          targets="label",
          model_type="classifier",
          evaluator_config={"log_explainer": True},  # Enable SHAP
      )

      # SHAP plots and explainers automatically generated
  ```

</details>

<details>
  <summary>Distributed Training Integration</summary>

  MLflow works with distributed traditional ML frameworks:

  ```python
  import mlflow
  from dask_ml.ensemble import GradientBoostingClassifier
  from dask.distributed import Client

  # Connect to Dask cluster
  client = Client("scheduler-address:8786")

  with mlflow.start_run():
      # Train distributed model
      model = GradientBoostingClassifier(n_estimators=100)
      model.fit(X_train, y_train)

      # MLflow tracking works with distributed training
      accuracy = model.score(X_test, y_test)
      mlflow.log_metric("accuracy", accuracy)

      # Log distributed model
      mlflow.sklearn.log_model(model, name="distributed_model")
  ```

</details>

## Tutorials and Guides

<CardGroup>
  <PageCard headerText="Hyperparameter Tuning with MLflow and Optuna" link="/ml/traditional-ml/tutorials/hyperparameter-tuning" text="Explore the integration of MLflow Tracking with Optuna for hyperparameter optimization. Learn to leverage parent-child run relationships and compare tuning experiments to maximize model performance." />
  <PageCard headerText="Custom PyFunc Models with MLflow" link="/ml/traditional-ml/tutorials/creating-custom-pyfunc" text="Discover the power of MLflow's Custom PyFunc for creating standardized, reproducible workflows. From simple mathematical models to complex machine learning integrations, learn to build flexible model interfaces." />
  <PageCard headerText="Multi-Model Endpoints with PyFunc" link="/ml/traditional-ml/tutorials/serving-multiple-models-with-pyfunc" text="Build sophisticated multi-model inference systems using MLflow's PyFunc framework. Learn to create low-latency endpoints serving multiple models with custom routing logic." />
</CardGroup>

## [MLflow Tracking](/ml/tracking)

Tracking is central to the MLflow ecosystem, facilitating the systematic organization of experiments and runs:

- **Experiments and Runs**: Each experiment encapsulates a specific aspect of your research, and each experiment can house multiple runs. Runs document critical data like metrics, parameters, and the code state.
- **Artifacts**: Store crucial output from runs, be it models, visualizations, datasets, or other metadata. This repository of artifacts ensures traceability and easy access.
- **Metrics and Parameters**: By allowing users to log parameters and metrics, MLflow makes it straightforward to compare different runs, facilitating model optimization.
- **Dependencies and Environment**: The platform automatically captures the computational environment, ensuring that experiments are reproducible across different setups.
- **Input Examples and Model Signatures**: These features allow developers to define the expected format of the model's inputs, making validation and debugging more straightforward.
- **UI Integration**: The integrated UI provides a visual overview of all runs, enabling easy comparison and deeper insights.
- **Search Functionality**: Efficiently sift through your experiments using MLflow's robust search functionality.
- **APIs**: Comprehensive APIs are available, allowing users to interact with the tracking system programmatically, integrating it into existing workflows.

## [MLflow Evaluate](/ml/evaluation)

Ensuring model quality is paramount:

- **Auto-generated Metrics**: MLflow automatically evaluates models, providing key metrics for regression (like RMSE, MAE) and classification (such as F1-score, AUC-ROC).
- **Visualization**: Understand your model better with automatically generated plots. For instance, MLflow can produce confusion matrices, precision-recall curves, and more for classification tasks.
- **Extensibility**: While MLflow provides a rich set of evaluation tools out of the box, it's also designed to accommodate custom metrics and visualizations.

## [Model Registry](/ml/model-registry)

This feature acts as a catalog for models:

- **Versioning**: As models evolve, keeping track of versions becomes crucial. The Model Registry handles versioning, ensuring that users can revert to older versions or compare different iterations.
- **Annotations**: Models in the registry can be annotated with descriptions, use-cases, or other relevant metadata.
- **Lifecycle Stages**: Track the stage of each model version, be it 'staging', 'production', or 'archived'. This ensures clarity in deployment and maintenance processes.

## [Deployment](/ml/deployment)

MLflow simplifies the transition from development to production:

- **Consistency**: By meticulously recording dependencies and the computational environment, MLflow ensures that models behave consistently across different deployment setups.
- **Docker Support**: Facilitate deployment in containerized environments using Docker, encapsulating all dependencies and ensuring a uniform runtime environment.
- **Scalability**: MLflow is designed to accommodate both small-scale deployments and large, distributed setups, ensuring that it scales with your needs.

## Learn More

Dive deeper into MLflow's capabilities for traditional machine learning:

- **[Scikit-learn Guide](/ml/traditional-ml/sklearn)**: Master MLflow's integration with the most popular Python ML library
- **[XGBoost Guide](/ml/traditional-ml/xgboost)**: Learn advanced gradient boosting workflows with automatic experiment tracking
- **[Spark MLlib Guide](/ml/traditional-ml/sparkml)**: Scale traditional ML to big data with distributed computing support
- **[Model Registry](/ml/model-registry)**: Implement enterprise model governance and lifecycle management
- **[MLflow Deployments](/ml/deployment)**: Deploy traditional ML models to production environments