---
title: Plugin Evaluators
sidebar_position: 6
---

# Plugin Evaluators

MLflow's evaluation framework is designed for extensibility, allowing specialized evaluation plugins to seamlessly integrate with the core evaluation workflow. These plugins extend MLflow's capabilities with domain-specific validation, advanced vulnerability scanning, and specialized testing frameworks developed by the broader ML community.

## Available Plugins

MLflow currently supports two powerful evaluation plugins that bring specialized validation capabilities to your model evaluation workflows:

### Giskard Plugin - Advanced Vulnerability Scanning {#giskard_plugin}

The [Giskard](https://docs.giskard.ai/en/latest/integrations/mlflow/index.html) plugin extends MLflow's validation capabilities to help anticipate issues before they reach production. This comprehensive scanning tool detects hidden vulnerabilities that traditional metrics might miss.

#### Key Capabilities

**Vulnerability Detection**: Giskard scans models to identify critical issues including:
- [Performance bias](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/performance_bias/index.html) - Unequal performance across different groups
- [Unrobustness](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/robustness/index.html) - Sensitivity to small input changes
- [Overconfidence](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/overconfidence/index.html) - Excessive confidence in predictions
- [Underconfidence](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/underconfidence/index.html) - Insufficient confidence in accurate predictions
- [Ethical bias](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/ethics/index.html) - Discriminatory behavior patterns
- [Data leakage](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/data_leakage/index.html) - Information bleeding from target to features
- [Stochasticity](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/stochasticity/index.html) - Unpredictable model behavior
- [Spurious correlation](https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/spurious/index.html) - False causal relationships

**Analysis Features**:
- 🔍 **Sample Exploration**: Examine specific data samples that highlight discovered vulnerabilities
- 📊 **Quantified Metrics**: Log vulnerabilities as well-defined, measurable metrics within MLflow
- 🔄 **Model Comparison**: Compare vulnerability metrics across different model versions and architectures

#### Getting Started with Giskard

Explore these example implementations to see Giskard in action:
- [Tabular ML Models](https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-tabular-example.html) - Traditional supervised learning vulnerability assessment
- [Text ML Models (LLMs)](https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-llm-example.html) - Language model specific vulnerability scanning

For comprehensive documentation and setup instructions, visit the [Giskard-MLflow integration docs](https://docs.giskard.ai/en/latest/integrations/mlflow/index.html).

### Trubrics Plugin - Flexible Validation Framework {#trubrics_plugin}

The [Trubrics](https://github.com/trubrics/trubrics-sdk) plugin provides a flexible validation framework that extends MLflow's evaluation capabilities with custom validation logic and comprehensive result reporting.

#### Key Capabilities

**Validation Features**:
- 📋 **Out-of-the-box Validations**: Large library of pre-built validation checks for common ML scenarios
- 🔧 **Custom Python Functions**: Validate runs using any custom Python function or business logic
- 📊 **Comprehensive Reporting**: View all validation results in structured JSON format for easy diagnosis

**Workflow Integration**:
- ⚡ **Flexible Validation Logic**: Define validation criteria that match your specific use case requirements
- 🔍 **Detailed Diagnostics**: Understand exactly why an MLflow run might have failed validation
- 📈 **Result Tracking**: Maintain complete validation history alongside your model experiments

#### Getting Started with Trubrics

See the plugin in action with the [official example notebook](https://github.com/trubrics/trubrics-sdk/blob/main/examples/mlflow/mlflow-trubrics.ipynb), which demonstrates common validation patterns and integration workflows.

For complete documentation and setup instructions, visit the [Trubrics-MLflow integration docs](https://trubrics.github.io/trubrics-sdk/mlflow/).

## Integration Benefits

Plugin evaluators seamlessly integrate with MLflow's existing evaluation framework, providing:

- 🔄 **Unified Workflow**: Use plugins alongside standard MLflow evaluators in the same evaluation run
- 📊 **Consistent Reporting**: Plugin results appear in MLflow's tracking interface with other evaluation metrics
- 🏗️ **Extensible Architecture**: Easy integration path for custom evaluation tools and frameworks
- 📈 **Scalable Validation**: Run plugin evaluations as part of automated model validation pipelines

## Next Steps

Ready to enhance your model evaluation with specialized plugins?

1. **Choose Your Plugin**: Select Giskard for vulnerability scanning or Trubrics for flexible validation
2. **Review Examples**: Explore the provided example notebooks to understand integration patterns
3. **Install and Configure**: Follow the plugin-specific documentation for setup instructions
4. **Integrate with MLflow**: Add plugin evaluators to your existing `mlflow.evaluate()` workflows

These powerful plugins demonstrate the extensibility of MLflow's evaluation framework and provide immediate access to specialized validation capabilities developed by domain experts in the ML community.