import Link from "@docusaurus/Link";
import { CardGroup, PageCard } from "@site/src/components/Card";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { Database, GitBranch, Boxes, Rocket, BookOpen, Package, Cloud } from "lucide-react";

# MLflow Spark MLlib Integration

## Introduction

Apache Spark MLlib provides distributed machine learning algorithms for processing large-scale datasets across clusters. MLflow integrates with Spark MLlib to track distributed ML pipelines, manage models, and enable flexible deployment from cluster training to standalone inference.

## Why MLflow + Spark MLlib?

<FeatureHighlights features={[
  {
    icon: Database,
    title: "Pipeline Tracking",
    description: "Automatically log Spark ML pipelines with all stages, transformers, and estimators. Track parameters from each pipeline component and maintain complete lineage."
  },
  {
    icon: GitBranch,
    title: "Format Flexibility",
    description: "Save models in native Spark format for distributed batch processing or PyFunc format for deployment outside Spark environments with automatic DataFrame conversion."
  },
  {
    icon: Boxes,
    title: "Datasource Autologging",
    description: "Track data sources automatically with paths, formats, and versions. Maintain complete data lineage for distributed ML workflows."
  },
  {
    icon: Rocket,
    title: "Cross-Platform Deployment",
    description: "Deploy Spark models with PyFunc wrappers for REST APIs and edge computing, or convert to ONNX for platform-independent inference."
  }
]} />

## Key Features

### Pipeline and Model Logging

Log Spark ML pipelines with automatic parameter capture:

```python
import mlflow
import mlflow.spark
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import Tokenizer, HashingTF

# Enable autologging
mlflow.spark.autolog()

tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol="words", outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

with mlflow.start_run():
    model = pipeline.fit(training_df)
    mlflow.spark.log_model(model, artifact_path="spark-pipeline")
```

Automatically logs the complete pipeline with all stages and parameters from each component.

### Deployment Formats

**Native Spark**: Preserves full Spark MLlib functionality for distributed batch processing on clusters.

**PyFunc**: Wraps Spark models as Python functions for deployment outside Spark environments. Automatically converts pandas DataFrames to Spark format for inference.

**ONNX**: Convert Spark models to ONNX format using `onnxmltools` for cross-platform deployment (experimental support).

### Datasource Autologging

Track data sources automatically with `mlflow.spark.autolog()`. Requires Spark 3.0+ and MLflow-Spark JAR configuration. Logs paths, formats, and versions for all datasource reads during training.

## Learn More

<TilesGrid>
  <TileCard
    icon={BookOpen}
    iconSize={48}
    title="Spark MLlib Guide"
    description="Complete guide to pipeline tracking, model formats, datasource autologging, and deployment patterns."
    href="/ml/traditional-ml/sparkml/guide"
    linkText="View guide →"
    containerHeight={64}
  />
  <TileCard
    icon={Package}
    iconSize={48}
    title="Model Registry"
    description="Register Spark models, manage versions, and promote models through staging and production."
    href="/ml/model-registry"
    linkText="View registry docs →"
    containerHeight={64}
  />
  <TileCard
    icon={Cloud}
    iconSize={48}
    title="Model Deployment"
    description="Deploy Spark models with MLflow serving, batch inference, and cloud platform integration."
    href="/ml/deployment"
    linkText="Deploy models →"
    containerHeight={64}
  />
</TilesGrid>
