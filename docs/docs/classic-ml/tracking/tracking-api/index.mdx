import Link from "@docusaurus/Link";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { APILink } from "@site/src/components/APILink";
import { Table } from "@site/src/components/Table";

# MLflow Tracking APIs

[MLflow Tracking](/ml/tracking) provides comprehensive APIs across multiple programming languages to capture your machine learning experiments. Whether you prefer automatic instrumentation or granular control, MLflow adapts to your workflow.

## Choose Your Approach

MLflow offers two primary methods for experiment tracking, each optimized for different use cases:

### **ü§ñ Automatic Logging** - Zero Setup, Maximum Coverage

Perfect for getting started quickly or when using supported ML libraries. Just add one line and MLflow captures everything automatically.

```python
import mlflow

mlflow.autolog()  # That's it!

# Your existing training code works unchanged
model.fit(X_train, y_train)
```

**What gets logged automatically:**
- Model parameters and hyperparameters
- Training and validation metrics
- Model artifacts and checkpoints
- Training plots and visualizations
- Framework-specific metadata

**Supported libraries:** Scikit-learn, XGBoost, LightGBM, PyTorch, Keras/TensorFlow, Spark, and more.

[**‚Üí Explore Auto Logging**](/ml/tracking/autolog)

### **üõ†Ô∏è Manual Logging** - Complete Control, Custom Workflows

Ideal for custom training loops, advanced experimentation, or when you need precise control over what gets tracked.

<Tabs>
  <TabItem default label="Python" value="python">
    ```python
    import mlflow

    with mlflow.start_run():
        # Log parameters
        mlflow.log_param("learning_rate", 0.01)
        mlflow.log_param("batch_size", 32)

        # Your training logic here
        for epoch in range(num_epochs):
            train_loss = train_model()
            val_loss = validate_model()

            # Log metrics with step tracking
            mlflow.log_metrics({"train_loss": train_loss, "val_loss": val_loss}, step=epoch)

        # Log final model
        mlflow.sklearn.log_model(model, name="model")
    ```
  </TabItem>
  <TabItem label="Java" value="java">
    ```java
    MlflowClient client = new MlflowClient();
    RunInfo run = client.createRun();

    // Log parameters
    client.logParam(run.getRunId(), "learning_rate", "0.01");
    client.logParam(run.getRunId(), "batch_size", "32");

    // Log metrics with timesteps
    for (int epoch = 0; epoch < numEpochs; epoch++) {
        double trainLoss = trainModel();
        client.logMetric(run.getRunId(), "train_loss", trainLoss,
                        System.currentTimeMillis(), epoch);
    }
    ```
  </TabItem>
  <TabItem label="R" value="r">
    ```r
    library(mlflow)

    with(mlflow_start_run(), {
      # Log parameters
      mlflow_log_param("learning_rate", 0.01)
      mlflow_log_param("batch_size", 32)

      # Training loop
      for (epoch in 1:num_epochs) {
        train_loss <- train_model()
        mlflow_log_metric("train_loss", train_loss, step = epoch)
      }
    })
    ```
  </TabItem>
</Tabs>

## Core Logging Functions

### Setup & Configuration

| Function | Purpose | Example |
|----------|---------|---------|
| <APILink fn="mlflow.set_tracking_uri" /> | Connect to tracking server or database | `mlflow.set_tracking_uri("http://localhost:5000")` |
| <APILink fn="mlflow.get_tracking_uri" /> | Get current tracking URI | `uri = mlflow.get_tracking_uri()` |
| <APILink fn="mlflow.create_experiment" /> | Create new experiment | `exp_id = mlflow.create_experiment("my-experiment")` |
| <APILink fn="mlflow.set_experiment" /> | Set active experiment | `mlflow.set_experiment("fraud-detection")` |

### Run Management

| Function | Purpose | Example |
|----------|---------|---------|
| <APILink fn="mlflow.start_run" /> | Start new run (with context manager) | `with mlflow.start_run(): ...` |
| <APILink fn="mlflow.end_run" /> | End current run | `mlflow.end_run(status="FINISHED")` |
| <APILink fn="mlflow.active_run" /> | Get currently active run | `run = mlflow.active_run()` |
| <APILink fn="mlflow.last_active_run" /> | Get last completed run | `last_run = mlflow.last_active_run()` |

### Data Logging

| Function | Purpose | Example |
|----------|---------|---------|
| <APILink fn="mlflow.log_param" /> / <APILink fn="mlflow.log_params" /> | Log hyperparameters | `mlflow.log_param("lr", 0.01)` |
| <APILink fn="mlflow.log_metric" /> / <APILink fn="mlflow.log_metrics" /> | Log performance metrics | `mlflow.log_metric("accuracy", 0.95, step=10)` |
| <APILink fn="mlflow.log_input" /> | Log dataset information | `mlflow.log_input(dataset)` |
| <APILink fn="mlflow.set_tag" /> / <APILink fn="mlflow.set_tags" /> | Add metadata tags | `mlflow.set_tag("model_type", "CNN")` |

### Artifact Management

| Function | Purpose | Example |
|----------|---------|---------|
| <APILink fn="mlflow.log_artifact" /> | Log single file/directory | `mlflow.log_artifact("model.pkl")` |
| <APILink fn="mlflow.log_artifacts" /> | Log entire directory | `mlflow.log_artifacts("./plots/")` |
| <APILink fn="mlflow.get_artifact_uri" /> | Get artifact storage location | `uri = mlflow.get_artifact_uri()` |

### Language-Specific API Coverage

<Table>
  <thead>
    <tr>
      <th>Capability</th>
      <th>Python</th>
      <th>Java</th>
      <th>R</th>
      <th>REST API</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Basic Logging**</td>
      <td>‚úÖ Full</td>
      <td>‚úÖ Full</td>
      <td>‚úÖ Full</td>
      <td>‚úÖ Full</td>
    </tr>
    <tr>
      <td>**Auto Logging**</td>
      <td>‚úÖ 15+ Libraries</td>
      <td>‚ùå Not Available</td>
      <td>‚úÖ Limited</td>
      <td>‚ùå Not Available</td>
    </tr>
    <tr>
      <td>**Model Logging**</td>
      <td>‚úÖ 20+ Flavors</td>
      <td>‚úÖ Basic Support</td>
      <td>‚úÖ Basic Support</td>
      <td>‚úÖ Via Artifacts</td>
    </tr>
    <tr>
      <td>**Dataset Tracking**</td>
      <td>‚úÖ Full</td>
      <td>‚úÖ Basic</td>
      <td>‚úÖ Basic</td>
      <td>‚úÖ Basic</td>
    </tr>
    <tr>
      <td>**Search & Query**</td>
      <td>‚úÖ Advanced</td>
      <td>‚úÖ Basic</td>
      <td>‚úÖ Basic</td>
      <td>‚úÖ Full</td>
    </tr>
  </tbody>
</Table>

:::note api-parity
The Python API provides the most comprehensive feature set. Java and R APIs offer core functionality with ongoing feature additions in each release.
:::

## Advanced Tracking Patterns

### Precise Metric Tracking

Control exactly when and how metrics are recorded with custom timestamps and steps:

```python
import time
from datetime import datetime

# Log with custom step (training iteration/epoch)
for epoch in range(100):
    loss = train_epoch()
    mlflow.log_metric("train_loss", loss, step=epoch)

# Log with custom timestamp
now = int(time.time() * 1000)  # MLflow expects milliseconds
mlflow.log_metric("inference_latency", latency, timestamp=now)

# Log with both step and timestamp
mlflow.log_metric("gpu_utilization", gpu_usage, step=epoch, timestamp=now)
```

**Step Requirements:**
- Must be a valid 64-bit integer
- Can be negative or out of order
- Supports gaps in sequences (e.g., 1, 5, 75, -20)

### Experiment Organization

Structure your experiments for easy comparison and analysis:

```python
# Method 1: Environment variables
import os

os.environ["MLFLOW_EXPERIMENT_NAME"] = "fraud-detection-v2"

# Method 2: Explicit experiment setting
mlflow.set_experiment("hyperparameter-tuning")

# Method 3: Create with custom configuration
experiment_id = mlflow.create_experiment(
    "production-models",
    artifact_location="s3://my-bucket/experiments/",
    tags={"team": "data-science", "environment": "prod"},
)
```

### Hierarchical Runs with Parent-Child Relationships

Organize complex experiments like hyperparameter sweeps or cross-validation:

```python
# Parent run for the entire experiment
with mlflow.start_run(run_name="hyperparameter_sweep") as parent_run:
    mlflow.log_param("search_strategy", "random")

    best_score = 0
    best_params = {}

    # Child runs for each parameter combination
    for lr in [0.001, 0.01, 0.1]:
        for batch_size in [16, 32, 64]:
            with mlflow.start_run(
                nested=True, run_name=f"lr_{lr}_bs_{batch_size}"
            ) as child_run:
                mlflow.log_params({"learning_rate": lr, "batch_size": batch_size})

                # Train and evaluate
                model = train_model(lr, batch_size)
                score = evaluate_model(model)
                mlflow.log_metric("accuracy", score)

                # Track best configuration in parent
                if score > best_score:
                    best_score = score
                    best_params = {"learning_rate": lr, "batch_size": batch_size}

    # Log best results to parent run
    mlflow.log_params(best_params)
    mlflow.log_metric("best_accuracy", best_score)

# Query child runs
child_runs = mlflow.search_runs(
    filter_string=f"tags.mlflow.parentRunId = '{parent_run.info.run_id}'"
)
print("Child run results:")
print(child_runs[["run_id", "params.learning_rate", "metrics.accuracy"]])
```

### Parallel Execution Strategies

Handle multiple runs efficiently with different parallelization approaches:

<Tabs>
  <TabItem default label="Sequential Runs" value="sequential">
    Perfect for simple hyperparameter sweeps or A/B testing:

    ```python
    configs = [
        {"model": "RandomForest", "n_estimators": 100},
        {"model": "XGBoost", "max_depth": 6},
        {"model": "LogisticRegression", "C": 1.0},
    ]

    for config in configs:
        with mlflow.start_run(run_name=config["model"]):
            mlflow.log_params(config)
            model = train_model(config)
            score = evaluate_model(model)
            mlflow.log_metric("f1_score", score)
    ```
  </TabItem>
  <TabItem label="Multiprocessing" value="multiprocessing">
    Scale training across multiple CPU cores:

    ```python
    import multiprocessing as mp


    def train_with_config(config):
        # Set tracking URI in each process (required for spawn method)
        mlflow.set_tracking_uri("http://localhost:5000")
        mlflow.set_experiment("parallel-training")

        with mlflow.start_run():
            mlflow.log_params(config)
            model = train_model(config)
            score = evaluate_model(model)
            mlflow.log_metric("accuracy", score)
            return score


    if __name__ == "__main__":
        configs = [{"lr": lr, "bs": bs} for lr in [0.01, 0.1] for bs in [16, 32]]

        with mp.Pool(processes=4) as pool:
            results = pool.map(train_with_config, configs)

        print(f"Completed {len(results)} experiments")
    ```
  </TabItem>
  <TabItem label="Multithreading" value="multithreading">
    Use child runs for thread-safe parallel execution:

    ```python
    import threading
    from concurrent.futures import ThreadPoolExecutor


    def train_worker(config):
        with mlflow.start_run(nested=True):
            mlflow.log_params(config)
            model = train_model(config)
            score = evaluate_model(model)
            mlflow.log_metric("accuracy", score)
            return score


    # Start parent run
    with mlflow.start_run(run_name="threaded_experiment"):
        configs = [{"lr": 0.01, "epochs": e} for e in range(10, 101, 10)]

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(train_worker, config) for config in configs]
            results = [future.result() for future in futures]

        # Log summary to parent run
        mlflow.log_metric("avg_accuracy", sum(results) / len(results))
        mlflow.log_metric("max_accuracy", max(results))
    ```
  </TabItem>
</Tabs>

### Smart Tagging for Organization

Use tags strategically to organize and filter experiments:

```python
with mlflow.start_run():
    # Descriptive tags for filtering
    mlflow.set_tags(
        {
            "model_family": "transformer",
            "dataset_version": "v2.1",
            "environment": "production",
            "team": "nlp-research",
            "gpu_type": "V100",
            "experiment_phase": "hyperparameter_tuning",
        }
    )

    # Special notes tag for documentation
    mlflow.set_tag(
        "mlflow.note.content",
        "Baseline transformer model with attention dropout. "
        "Testing different learning rate schedules.",
    )

    # Training code here...
```

**Search experiments by tags:**
```python
# Find all transformer experiments
transformer_runs = mlflow.search_runs(filter_string="tags.model_family = 'transformer'")

# Find production-ready models
prod_models = mlflow.search_runs(
    filter_string="tags.environment = 'production' AND metrics.accuracy > 0.95"
)
```

### System Tags Reference

MLflow automatically sets several system tags to capture execution context:

| Tag | Description | When Set |
|-----|-------------|----------|
| `mlflow.source.name` | Source file or notebook name | Always |
| `mlflow.source.type` | Source type (NOTEBOOK, JOB, LOCAL, etc.) | Always |
| `mlflow.user` | User who created the run | Always |
| `mlflow.source.git.commit` | Git commit hash | When run from git repo |
| `mlflow.source.git.branch` | Git branch name | MLflow Projects only |
| `mlflow.parentRunId` | Parent run ID for nested runs | Child runs only |
| `mlflow.docker.image.name` | Docker image used | Docker environments |
| `mlflow.note.content` | **User-editable** description | Manual only |

:::tip pro-tip
Use `mlflow.note.content` to document experiment insights, hypotheses, or results directly in the MLflow UI. This tag appears in a dedicated Notes section on the run page.
:::

### Integration with Auto Logging

Combine auto logging with manual tracking for the best of both worlds:

```python
import mlflow
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Enable auto logging
mlflow.autolog()

with mlflow.start_run():
    # Auto logging captures model training automatically
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)

    # Add custom metrics and artifacts
    predictions = model.predict(X_test)

    # Log custom evaluation metrics
    report = classification_report(y_test, predictions, output_dict=True)
    mlflow.log_metrics(
        {
            "precision_macro": report["macro avg"]["precision"],
            "recall_macro": report["macro avg"]["recall"],
            "f1_macro": report["macro avg"]["f1-score"],
        }
    )

    # Log custom artifacts
    feature_importance = pd.DataFrame(
        {"feature": feature_names, "importance": model.feature_importances_}
    )
    feature_importance.to_csv("feature_importance.csv")
    mlflow.log_artifact("feature_importance.csv")

    # Access the auto-logged run for additional processing
    current_run = mlflow.active_run()
    print(f"Auto-logged run ID: {current_run.info.run_id}")

# Access the completed run
last_run = mlflow.last_active_run()
print(f"Final run status: {last_run.info.status}")
```

## Language-Specific Guides

- **Python**: [Complete Python API Reference](http://localhost:3000/api_reference/python_api/index.html)
- **Java**: [Java API Documentation](http://localhost:3000/api_reference/java_api/index.html)
- **R**: [R API Documentation](http://localhost:3000/api_reference/R-api.html)
- **REST**: [REST API Reference](http://localhost:3000/api_reference/rest-api.html)

---

**Next Steps:**
- [Set up MLflow Tracking Server](/ml/tracking/server) for team collaboration
- [Explore Auto Logging](/ml/tracking/autolog) for supported frameworks
- [Learn advanced search patterns](/ml/search/search-runs) for experiment analysis
