---
sidebar_position: 2
---

import { CardGroup, PageCard } from "@site/src/components/Card";
import Link from "@docusaurus/Link";
import { NotebookDownloadButton } from "@site/src/components/NotebookDownloadButton";
import { Table } from "@site/src/components/Table";

# MLflow Tracking Quickstart

Welcome to MLflow!

The purpose of this quickstart is to provide a quick guide to the most essential core APIs of MLflow Tracking.
Specifically, those that enable the logging, registering, and loading of a model for inference.

:::note
For a more in-depth and tutorial-based approach (if that is your style), please see the
[Getting Started with MLflow](/ml/getting-started/logging-first-model) tutorial. We recommend that you start here first, though, as this quickstart
uses the most common and frequently-used APIs for MLflow Tracking and serves as a good foundation for the other tutorials in the documentation.
:::

## What you will learn

In just a few minutes of following along with this quickstart, you will learn:

- How to **log** parameters, metrics, and a model
- The basics of the **MLflow fluent API**
- How to **register** a model during logging
- How to navigate to a model in the **MLflow UI**
- How to **load** a logged model for inference

:::note
If you would prefer to view a Jupyter Notebook version of this tutorial, click the following link:

<Link className="button button--primary" to="notebooks/tracking_quickstart" target="_blank">
  <span>View the Notebook</span>
</Link>
:::

## Step 1 - Get MLflow

MLflow is available on PyPI.

### Installing Stable Release

If you don't already have it installed on your system, you can install it with:

```bash
pip install mlflow
```

### Installing a Release Candidate (RC)

If you are eager to test out new features and validate that an upcoming release of MLflow will work well in your infrastructure, installing the latest
release candidate may be of interest to you.

:::note
Release Candidate builds are not recommended for actual use, rather they are intended only for testing validation.
:::

To install the latest version of MLflow's release candidates for a given version, see the example below that uses MLflow 2.14.0 as an example:

```bash
# install the latest release candidate
pip install --pre mlflow

# or install a specific rc version
pip install mlflow==3.1.0rc0
```

## Step 2 - Start a Tracking Server

### Using a Managed MLflow Tracking Server

For details on options for using a managed MLflow Tracking Server, including how to create a Databricks Free Trial account with
managed MLflow, [see the guide for tracking server options](/ml/getting-started/running-notebooks/).

### Run a local Tracking Server

We're going to start a local MLflow Tracking Server, which we will connect to for logging our data for this quickstart.
From a terminal, run:

```bash
mlflow server --host 127.0.0.1 --port 8080
```

:::note
You can choose any port that you would like, provided that it's not already in use.
:::

### Set the Tracking Server URI (if not using a Databricks Managed MLflow Tracking Server)

If you're using a managed MLflow Tracking Server that is not provided by Databricks, or if you're running a local tracking server,
ensure that you set the tracking server's uri using:

```python
import mlflow

mlflow.set_tracking_uri(uri="http://<host>:<port>")
```

If this is not set within your notebook or runtime environment, the runs will be logged to your local file system.

## Step 3 - Train a model and prepare metadata for logging

In this section, we're going to log a model with MLflow. A quick overview of the steps are:

- Load and prepare the Iris dataset for modeling.
- Train a Logistic Regression model and evaluate its performance.
- Prepare the model hyperparameters and calculate metrics for logging.

```python
import mlflow
from mlflow.models import infer_signature

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# Load the Iris dataset
X, y = datasets.load_iris(return_X_y=True)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define the model hyperparameters
params = {
    "solver": "lbfgs",
    "max_iter": 1000,
    "random_state": 8888,
}

# Train the model
lr = LogisticRegression(**params)
lr.fit(X_train, y_train)

# Predict on the test set
y_pred = lr.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
```

## Step 4 - Log the model and its metadata to MLflow

In this next step, we're going to use the model that we trained, the hyperparameters that we specified for the model's fit, and the
loss metrics that were calculated by evaluating the model's performance on the test data to log to MLflow.

The steps that we will take are:

- Initiate an MLflow **run** context to start a new run that we will log the model and metadata to.
- **Log** model **parameters** and performance **metrics**.
- **Tag** the run for easy retrieval.
- **Register** the model in the MLflow Model Registry while **logging** (saving) the model.

:::note
While it can be valid to wrap the entire code within the `start_run` block, this is **not recommended**. If there as in issue with the
training of the model or any other portion of code that is unrelated to MLflow-related actions, an empty or partially-logged run will be
created, which will necessitate manual cleanup of the invalid run. It is best to keep the training execution outside of the run context block
to ensure that the loggable content (parameters, metrics, artifacts, and the model) are fully materialized prior to logging.
:::

```python
# Set our tracking server uri for logging
mlflow.set_tracking_uri(uri="http://127.0.0.1:8080")

# Create a new MLflow Experiment
mlflow.set_experiment("MLflow Quickstart")

# Start an MLflow run
with mlflow.start_run():
    # Log the hyperparameters
    mlflow.log_params(params)

    # Log the loss metric
    mlflow.log_metric("accuracy", accuracy)

    # Infer the model signature
    signature = infer_signature(X_train, lr.predict(X_train))

    # Log the model, which inherits the parameters and metric
    model_info = mlflow.sklearn.log_model(
        sk_model=lr,
        name="iris_model",
        signature=signature,
        input_example=X_train,
        registered_model_name="tracking-quickstart",
    )

    # Set a tag that we can use to remind ourselves what this model was for
    mlflow.set_logged_model_tags(
        model_info.model_id, {"Training Info": "Basic LR model for iris data"}
    )
```

:::note Console Output
When you run the above code, you will see console output that looks something like this:

```text
2025/09/09 17:22:20 ERROR mlflow.webhooks.delivery: Failed to deliver webhook for event registered_model.created: FileStore does not support list_webhooks_by_event
Traceback (most recent call last):
  ...
NotImplementedError: FileStore does not support list_webhooks_by_event
```

You can ignore these errors. They will go away in [MLflow 4](https://github.com/mlflow/mlflow/issues/17562#issuecomment-3259679453) and will not be fixed until then.
:::

## Step 5 - Load the model as a Python Function (pyfunc) and use it for inference

After logging the model, we can perform inference by:

- **Loading** the model using MLflow's `pyfunc` flavor.
- Running **Predict** on new data using the loaded model.

:::note
The iris training data that we used was a numpy array structure. However, we can submit a Pandas DataFrame as well to the `predict` method, as shown
below.
:::

```python
# Load the model back for predictions as a generic Python Function model
loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)

predictions = loaded_model.predict(X_test)

iris_feature_names = datasets.load_iris().feature_names

result = pd.DataFrame(X_test, columns=iris_feature_names)
result["actual_class"] = y_test
result["predicted_class"] = predictions

result[:4]
```

The output of this code will look something like this:

<Table>
  <thead>
    <tr>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>actual_class</th>
      <th>predicted_class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.7</td>
      <td>1.2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>5.7</td>
      <td>3.8</td>
      <td>1.7</td>
      <td>0.3</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>7.7</td>
      <td>2.6</td>
      <td>6.9</td>
      <td>2.3</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>6.0</td>
      <td>2.9</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</Table>

## Step 6 - View the Run and Model in the MLflow UI

In order to see the results of our run, we can navigate to the MLflow UI. Since we have already started the Tracking Server at
_http://localhost:8080_, we can simply navigate to that URL in our browser.

When opening the site, you will see a screen similar to the following:

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Experiment view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-experiment.png)
  <figcaption>The main MLflow Tracking page, showing Experiments that have been created</figcaption>
</figure>

Clicking on the name of the Experiment that we created ("MLflow Quickstart") will give us a list of runs associated with the
Experiment. You should see a random name that has been generated for the run and nothing else show up in the `Table` list view to the right.

Clicking on the name of the run will take you to the Run page, where the details of what we've logged will be shown. The elements have
been highlighted below to show how and where this data is recorded within the UI.

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Run view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-run.png)
  <figcaption>The run view page for our run</figcaption>
</figure>

Switch to the Models tab in the experiments page to view all the logged models under the Experiment, where you can see an entry for the logged model we just created ("tracking-quickstart").

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Experiment view page models tab](/images/tutorials/introductory/quickstart-tracking/quickstart-our-experiment-models-tab.png)
  <figcaption>The models tab of the MLflow Tracking page, showing a list of all models created</figcaption>
</figure>

Clicking on the name of the model will take you to the Logged Model page, with details on the logged model and its metadata.

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Model view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-model.png)
  <figcaption>The model view page for our logged model</figcaption>
</figure>

## Conclusion

Congratulations on working through the MLflow Tracking Quickstart! You should now have a basic understanding of how to use the MLflow Tracking API to log
models.

If you are interested in a more in-depth tutorial, please see the [Getting Started with MLflow](/ml/getting-started/logging-first-model) tutorial as a
good next step in increasing your knowledge about MLflow!
