import Link from "@docusaurus/Link";
import { APILink } from "@site/src/components/APILink";
import { Table } from "@site/src/components/Table";

# Backend Stores

The backend store is a core component in [MLflow Tracking](/ml/tracking) where MLflow stores metadata for
[Runs](/ml/tracking#runs), models and experiments such as:

- Model ID
- Run ID
- Start & end time
- Parameters
- Metrics
- Code version (only if you launch runs from an [MLflow Project](/ml/projects)).
- Source file name (only if you launch runs from an [MLflow Project](/ml/projects)).

Note that large model artifacts such as model weight files are stored in [artifact store](/ml/tracking/artifact-stores).

## Configure Backend Store

By default, MLflow stores metadata in local files in the `./mlruns` directory, but MLflow can store metadata to databases as well.
You can configure the location by passing the desired **tracking URI** to MLflow, via either of the following methods:

- Set the `MLFLOW_TRACKING_URI` environment variable.
- Call <APILink fn="mlflow.set_tracking_uri" /> in your code.
- If you are running a [Tracking Server](/ml/tracking#tracking_server), you can set the `tracking_uri` option when starting the server, like `mlflow server --backend-store-uri sqlite:///mydb.sqlite`

Continue to the next section for the supported format of tracking URLs.
Also visit [this guidance](/ml/tracking#tracking_setup) for how to set up the backend store properly for your workflow.

## Supported Store Types

MLflow supports the following types of tracking URI for backend stores:

- Local file path (specified as `file:/my/local/dir`), where data is just directly stored locally to a system disk where your code is executing.
- A Database, encoded as `<dialect>+<driver>://<username>:<password>@<host>:<port>/<database>`. MLflow supports the dialects `mysql`, `mssql`, `sqlite`, and `postgresql`. For more details, see [SQLAlchemy database uri](https://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls).
- HTTP server (specified as `https://my-server:5000`), which is a server hosting an [MLflow tracking server](/ml/tracking#tracking_server).
- Databricks workspace (specified as `databricks` or as `databricks://<profileName>`, a [Databricks CLI profile](https://github.com/databricks/databricks-cli#installation)).
  Refer to Access the MLflow tracking server from outside Databricks [[AWS]](http://docs.databricks.com/applications/mlflow/access-hosted-tracking-server.html)
  [[Azure]](http://docs.microsoft.com/azure/databricks/applications/mlflow/access-hosted-tracking-server), or [the quickstart](/ml/tracking/quickstart) to
  easily get started with hosted MLflow on the Databricks Free Trial.

:::warning database-requirements
**Database-Backed Store Requirements**

When using database-backed stores, please note:

- **Model Registry Integration**: [Model Registry](/ml/model-registry) functionality requires a database-backed store. See [this FAQ](/ml/tracking#tracking-with-model-registry) for more information.

- **Schema Migrations**: `mlflow server` will fail against a database with an out-of-date schema. Always run `mlflow db upgrade [db_uri]` to upgrade your database schema before starting the server. Schema migrations can result in database downtime and may take longer on larger databases. **Always backup your database before running migrations.**
  :::

:::note parameter-limits
In Sep 2023, we increased the max length for params recorded in a Run from 500 to 8k (but we limit param value max length to 6000 internally).
[mlflow/2d6e25af4d3e_increase_max_param_val_length](https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/versions/2d6e25af4d3e_increase_max_param_val_length.py)
is a non-invertible migration script that increases the cap in existing database to 8k. Please be careful if you want to upgrade and backup your database before upgrading.
:::

## Deletion Behavior

In order to allow MLflow Runs to be restored, Run metadata and artifacts are not automatically removed
from the backend store or artifact store when a Run is deleted. The <APILink fn="mlflow.server.cli" hash="mlflow-gc">mlflow gc</APILink> CLI is provided
for permanently removing Run metadata and artifacts for deleted runs.

## SQLAlchemy Options

You can inject some [SQLAlchemy connection pooling options](https://docs.sqlalchemy.org/en/latest/core/pooling.html) using environment variables.

<Table>
  <thead>
    <tr>
      <th>MLflow Environment Variable</th>
      <th>SQLAlchemy QueuePool Option</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>`MLFLOW_SQLALCHEMYSTORE_POOL_SIZE`</td>
      <td>`pool_size`</td>
    </tr>
    <tr>
      <td>`MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE`</td>
      <td>`pool_recycle`</td>
    </tr>
    <tr>
      <td>`MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW`</td>
      <td>`max_overflow`</td>
    </tr>
  </tbody>
</Table>

## MySQL SSL Options

When connecting to a MySQL database that requires SSL certificates, you can set the following environment variables:

```bash
# Path to SSL CA certificate file
export MLFLOW_MYSQL_SSL_CA=/path/to/ca.pem

# Path to SSL client certificate file (if needed)
export MLFLOW_MYSQL_SSL_CERT=/path/to/client-cert.pem

# Path to SSL client key file (if needed)
export MLFLOW_MYSQL_SSL_KEY=/path/to/client-key.pem
```

Then start the MLflow server with your MySQL URI:

```bash
mlflow server --backend-store-uri="mysql+pymysql://username@hostname:port/database" --default-artifact-root=s3://your-bucket --host=0.0.0.0 --port=5000
```

These environment variables will be used to configure the SSL connection to the MySQL server.

## File Store Performance

MLflow will automatically try to use [LibYAML](https://pyyaml.org/wiki/LibYAML) bindings if they are already installed.
However, if you notice any performance issues when using _file store_ backend, it could mean LibYAML is not installed on your system.
On Linux or Mac you can easily install it using your system package manager:

```bash
# On Ubuntu/Debian
apt-get install libyaml-cpp-dev libyaml-dev

# On macOS using Homebrew
brew install yaml-cpp libyaml
```

After installing LibYAML, you need to reinstall PyYAML:

```bash
# Reinstall PyYAML
pip --no-cache-dir install --force-reinstall -I pyyaml
```

## Troubleshooting Database Migrations

:::info Database Compatibility
**The issues described in this section are specific to PostgreSQL 11 and earlier.**

Users of the following databases are **NOT affected**:

- ✅ PostgreSQL 12 and later
- ✅ MySQL (all versions)
- ✅ SQLite (all versions)
- ✅ MS SQL Server (all versions)

If you're using PostgreSQL 11 or earlier with MLflow 3.3.x, you may encounter migration issues that require manual intervention.
:::

### PostgreSQL 11 Compatibility Issue

:::warning postgresql-11-generated-columns
**Error: "syntax error at or near '(' ... duration_ns BIGINT GENERATED ALWAYS AS"**

**⚠️ This issue ONLY affects PostgreSQL version 11 and earlier. Users of PostgreSQL 12+, MySQL, SQLite, or MS SQL Server are NOT affected.**

If you encounter this error when running MLflow 3.3.x with PostgreSQL 11:

```
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near "("
LINE 12: duration_ns BIGINT GENERATED ALWAYS AS (end_time_unix_nano ...
                                                 ^
```

This occurs because PostgreSQL 11 doesn't support GENERATED columns (introduced in PostgreSQL 12). MLflow 3.4.0+ will automatically detect your PostgreSQL version and use alternative methods for PostgreSQL 11.

**Solution for existing installations:**

If you're upgrading MLflow and encounter this error, you have two options:

**Option 1: Fix the database schema (Recommended)**
Run the following SQL commands directly on your database:

```sql
-- Add the missing column
ALTER TABLE spans ADD COLUMN IF NOT EXISTS duration_ns BIGINT;

-- Create trigger to maintain the column
CREATE OR REPLACE FUNCTION update_span_duration()
RETURNS TRIGGER AS $$
BEGIN
    IF NEW.end_time_unix_nano IS NOT NULL THEN
        NEW.duration_ns = NEW.end_time_unix_nano - NEW.start_time_unix_nano;
    ELSE
        NEW.duration_ns = NULL;
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER span_duration_trigger
BEFORE INSERT OR UPDATE OF end_time_unix_nano, start_time_unix_nano ON spans
FOR EACH ROW
EXECUTE FUNCTION update_span_duration();

-- Update existing rows
UPDATE spans
SET duration_ns = end_time_unix_nano - start_time_unix_nano
WHERE end_time_unix_nano IS NOT NULL AND duration_ns IS NULL;

-- Mark migration as complete
INSERT INTO alembic_version (version_num) VALUES ('a1b2c3d4e5f6')
ON CONFLICT DO NOTHING;
```

**Option 2: Skip the problematic migration (if Option 1 doesn't work)**
If you're completely blocked and need to bypass the spans table creation:

```sql
-- Mark the migration as complete without creating the table
-- WARNING: This will skip spans/tracing functionality
INSERT INTO alembic_version (version_num) VALUES ('a1b2c3d4e5f6')
ON CONFLICT DO NOTHING;
```

**Recommendations:**

- **Upgrade PostgreSQL**: Consider upgrading to PostgreSQL 12 or later for full feature support.
- **Fresh installations**: MLflow automatically handles PostgreSQL 11 compatibility.
- **Report issues**: If you continue experiencing problems, please [report them on GitHub](https://github.com/mlflow/mlflow/issues).

:::

### Downgrading MLflow with Database Backend

:::warning downgrade-with-database
**Error: "No such revision or branch" when downgrading**

**Note:** This is most commonly encountered by PostgreSQL 11 users trying to downgrade from MLflow 3.3.x to 3.2.0 after the spans table migration failed. Other database users (PostgreSQL 12+, MySQL, SQLite, MS SQL) typically don't need to downgrade.

If you're downgrading MLflow (e.g., from 3.3.x to 3.2.0) and encounter:

```
alembic.script.revision.ResolutionError: No such revision or branch '1a0cddfcaa16'
```

This occurs because your database has migrations from the newer version that don't exist in the older version.

**Solution:**

1. **Identify the last migration that exists in your target version:**

   ```sql
   -- Check current migrations in your database
   SELECT * FROM alembic_version;
   ```

2. **Remove migrations that don't exist in the target version:**

   ```sql
   -- For downgrading from 3.3.x to 3.2.0:
   -- Remove migrations that were added in 3.3.0

   DELETE FROM alembic_version WHERE version_num IN (
     '1a0cddfcaa16',  -- webhooks (3.3.0)
     'a1b2c3d4e5f6',  -- spans table (3.3.0 - may have failed on PostgreSQL 11)
     '770bee3ae1dd'   -- assessments table (3.3.0)
   );

   -- Set to the last migration in 3.2.0
   INSERT INTO alembic_version (version_num)
   SELECT 'de4033877273'  -- entity_associations (last in 3.2.0)
   WHERE NOT EXISTS (SELECT 1 FROM alembic_version WHERE version_num = 'de4033877273');
   ```

3. **Clean up tables created by newer migrations (optional but recommended):**

   ```sql
   -- Drop tables that were added in 3.3.0
   DROP TABLE IF EXISTS webhook_events CASCADE;
   DROP TABLE IF EXISTS webhooks CASCADE;
   DROP TABLE IF EXISTS assessments CASCADE;
   DROP TABLE IF EXISTS spans CASCADE;

   -- Note: The spans table may not exist if its creation failed on PostgreSQL 11
   -- The DROP IF EXISTS will handle this gracefully
   ```

4. **Start MLflow with the older version:**
   ```bash
   mlflow server --backend-store-uri <your-database-uri>
   ```

**Important:** Always backup your database before downgrading!
:::

### General Migration Issues

If you encounter other database migration errors:

1. **Always backup your database** before running migrations
2. **Check your database version** compatibility:
   - PostgreSQL: 11+ (12+ recommended)
   - MySQL: 5.7+
   - SQLite: 3.7.0+
3. **Run migrations manually** if automatic migration fails:
   ```bash
   mlflow db upgrade [your-database-uri]
   ```
4. **Check migration status**:
   ```sql
   SELECT * FROM alembic_version;
   ```
