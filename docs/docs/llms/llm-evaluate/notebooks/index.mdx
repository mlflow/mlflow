import { PageCard, CardGroup } from "@site/src/components/Card";

# LLM Evaluation Examples

The notebooks listed below contain step-by-step tutorials on how to use MLflow to evaluate LLMs. 

The first set of notebooks is centered around evaluating an LLM for question-answering with a 
prompt engineering approach. The second set is centered around evaluating a RAG system. 

All the notebooks will demonstrate how to use MLflow's builtin metrics such as token_count and 
toxicity as well as LLM-judged intelligent metrics such as answer_relevance.


## QA Evaluation Tutorial

<CardGroup>
    <PageCard
        link="/llms/llm-evaluate/notebooks/question-answering-evaluation"
        headerText="LLM Question Answering Evaluation with MLflow"
        text="Learn how to evaluate various LLMs and RAG systems with MLflow, leveraging simple metrics such as toxicity, as well as LLM-judged metrics as relevance, and even custom LLM-judged metrics such as professionalism."
    />
    <PageCard
        link="/llms/llm-evaluate/notebooks/huggingface-evaluation"
        headerText="Evaluating a ðŸ¤— Hugging Face LLMs with MLflow"
        text="Learn how to evaluate various Open-Source LLMs available in Hugging Face, leveraging MLflow's built-in LLM metrics and experiment tracking to manage models and evaluation results."
    />
</CardGroup>

## RAG Evaluation Tutorials

<CardGroup>
    <PageCard
        link="/llms/llm-evaluate/notebooks/rag-evaluation"
        headerText="RAG Evaluation with MLflow and GPT-4 as Judge"
        text={["Learn how to evaluate RAG systems with MLflow, leveraging ", <b>OpenAI GPT-4</b>, " model as a judge."]}
    />
    <PageCard
        link="/llms/llm-evaluate/notebooks/rag-evaluation-llama2"
        headerText="RAG Evaluation with MLflow and Llama-2-70B as Judge"
        text={["Learn how to evaluate RAG systems with MLflow, leveraging ", <b>Llama 2 70B model</b>, " hosted on Databricks serving endpoint."]}
    />
</CardGroup>
