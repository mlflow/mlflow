---
sidebar_position: 5
---



# Monitoring in production

## <span style={{color: "red"}}>TODO - bring in more dbx content - this is just one of the pages</span>



This page describes how to use the various features of Lakehouse Monitoring for GenAI. To enable monitoring, follow the steps linked on the [monitoring overview](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring).

::::aws

:::note

If you are not able to use the beta product, a dashboard-based version with limited functionality is available in Public Preview. See [\_](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/evaluating-production-traffic.md) for agents that use [Mosaic AI Agent Evaluation](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/index.md), or [\_](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/evaluating-production-traffic-fm.md) to monitor production traffic sent to a foundation model for which inference tables have been enabled.

:::

::::

## <a id="view"></a>View monitoring results

Before viewing monitoring results, you must have the following:

- Access to a running SQL warehouse. Otherwise, [create one](https://docs.databricks.com/aws/en/compute/sql-warehouse/create.md).
- A monitor you created

After these prerequisites are met, you can view a page summarizing the results generated by a monitor by following these steps:

1. Click **Experiments** in the sidebar under the **Machine Learning** section.
1. Click the MLflow experiment associated with your monitor.

   If you are not sure how to find the name of the relevant experiment, follow the instructions in LINK to retrieve the experiment ID and run `mlflow.get_experiment(experiment_id=$YOUR_EXPERIMENT_ID)` in your notebook to find the experiment name.

   ## <span style={{color: "red"}}>TODO - fix above LINK</span>


1. Click the **Monitoring** tab.
1. Select your SQL warehouse using the **Choose a SQL Warehouse** dropdown.
1. The page updates to show your monitoring results. Results can take a few minutes to load.

## <a id="using"></a> Use the monitoring UI

All data in the monitoring UI, in both the **Charts** and **Logs** tabs, is constrained to a window of time. To change the window, use the **Time Range** dropdown.

### <a id="charts"></a> Charts tab

The Charts tab is composed of four sections: Requests, Metrics, Latency, and Errors.

![Screenshot of page summarizing monitoring results.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/monitoring-results.png)

The **Requests** section shows trace volume over time.

![Screenshot of Requests section.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/ui-requests.png)

The **Metrics** section shows counts of responses that are evaluated by LLM judges. Green indicates responses that pass, while red denotes responses that fail. The metrics listed in this section should correspond to those defined when you [created a monitor](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring#setup) along with an overall pass/fail quality score.

![Screenshot of Metrics section.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/ui-metrics.png)

The **Latency** section shows the trace execution latency over time, taken from the MLflow reported latency.

![Screenshot of Latency section.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/ui-latency.png)

The **Errors** section shows any model errors over time. When no errors have occurred, you will see a “no data” indicator as follows:

![Screenshot of Errors section.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/ui-errors.png)

### <a id="logs"></a> Logs tab

![Screenshot of Logs tab.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/ui-logs-tab.png)

The **Logs** tab lists the requests sent to the selected model, along with the results of LLM evaluations, if any. A maximum of 10,000 requests from the selected time period are shown in the UI. If the request count exceeds this threshold, requests are sampled at a rate different from the sample rate specified in the monitor configuration.

To filter request logs based on text contained in submitted requests, use the search box. You can also use the **Filters** dropdown menu to filter logs by the outcomes of their associated evaluations.

![Screenshot of log filters.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/ui-log-filters.png)

Hover over a request and click on the checkbox to select a request. You can then click **Export traces** to add these requests to an evaluation dataset. The dataset must have been logged using the same MLflow experiment as the monitor.

![Screenshot of export traces dialog.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/export-traces.png)

Click a request to view its details. The modal displays evaluation results, the input, the response, and the document(s) retrieved to answer the request, if any. For more details of the request, including timing information, click **See detailed trace view** at the upper-right of the modal.

![Screenshot of request detail modal.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/request-detail.png)

![Screenshot of detailed trace view of a request.](https://assets.docs.databricks.com/_static/images/generative-ai/agent-evaluation/monitoring/detailed-trace.png)

## <a id="alerts"></a> Add alerts

Use [Databricks SQL alerts](https://docs.databricks.com/aws/en/sql/user/alerts/index.md) to notify users when the evaluated traces table does not match expectations, for example when the fraction of requests marked as harmful exceeds a threshold.

## Update or pause a monitor

To update the configuration of a monitor, call `update_monitor`, which takes the following inputs:

- `endpoint_name: str` - Name of the endpoint being monitored
- `monitoring_config: dict` - Configuration for the monitor. See [\_](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring#setup) for supported parameters.

For example:

```python
from databricks.agents.evals.monitors import update_monitor

monitor = update_monitor(
    endpoint_name = "model-serving-endpoint-name",
    monitoring_config = {
        "sample": 0.1,  # Change sampling rate to 10%
    }
)
```

Similarly, to pause a monitor:

```python
from databricks.agents.evals.monitors import update_monitor

monitor = update_monitor(
    endpoint_name = "model-serving-endpoint-name",
    monitoring_config = {
        "paused": True,
    }
)
```

## <a id="get-metadata"></a>Get monitor metadata

Use the `get_monitor` function to retrieve the current configuration of a monitor for a deployed agent.

```python
from databricks.agents.evals.monitors import get_monitor

get_monitor('model-serving-endpoint-name')
```

The function returns a `Monitor` object including the following attributes:

- `endpoint_name` - Name of the endpoint being monitored.
- `monitoring_config` - Configuration for the monitor. See [\_](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring#setup) for configuration parameters.
- `experiment_id` - The MLflow experiment where the monitoring results are displayed. See LINK.
- ## <span style={{color: "red"}}>TODO - fix above LINK</span>
- `evaluated_traces_table` - :re[UC] table containing monitoring evaluation results.

## Delete a monitor

To remove a monitor from an endpoint, call `delete_monitor`.

```python
from databricks.agents.evals.monitors import delete_monitor

monitor = delete_monitor(
    endpoint_name = "model-serving-endpoint-name",
)
```

The evaluated traces table generated by a monitor will not be deleted by calls to `delete_monitor`.

## Custom metrics in agent monitoring

This approach to enabling custom metrics in monitoring is subject to change in the future. If you have any questions, contact your Databricks representative.

Follow these steps to enable evaluation over custom metrics in your agent monitor.

1. Create a Python (`.py`) file in the workspace and define your custom metrics. This should be a Python file and not a Databricks notebook. See [\_](https://docs.databricks.com/aws/en/files/workspace-basics.md) for instructions on creating a Python file in the Databricks workspace.
2. Define a list in this file called `CUSTOM_METRICS` containing your metric functions. The same custom metrics work online and offline when you use `mlflow.evaluate`. See [\_](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/custom-metrics.md).

   Here is an example file:

   ```python
   # custom_metrics.py

   import mlflow
   import pandas as pd
   from databricks.agents.evals import metric

   @metric
   def response_mentions_llm(response):
     return "LLM" in response

   @metric
   def request_too_long(request):
     return len(request) > 1000


   CUSTOM_METRICS = [response_mentions_llm, request_too_long]
   ```

3. Navigate to the **Workflows** tab.

   - If your agent is hosted on Databricks, find the workflow titled `[<your agent name>] Agent Monitoring Job`.
   - If your agent is hosted outside Databricks, find the workflow titled `monitor_<monitoring table name>`, replacing the . characters in the table name with -.

4. Edit the workflow to add a job parameter. See [\_](https://docs.databricks.com/aws/en/jobs/job-parameters.md).
5. Create a parameter named `CUSTOM_METRICS_NOTEBOOK_PATH`. The value of this parameter should be the full path to your custom metrics python file.

Your custom metrics will be evaluated on subsequent runs of the monitor.

To iterate on your custom metrics before including them in your monitor, run `mlflow.evaluate` on a subset of traces.

If your agent is hosted on Databricks:

```python
import mlflow
import pandas as pd
from databricks.agents.evals import metric

@metric
def response_mentions_llm(response):
  return "LLM" in response

df = spark.table("<your-monitoring-table>")
pdf = df.toPandas()[['request','response','trace']]

import mlflow
result = mlflow.evaluate(
  data = pdf,
  model_type="databricks-agent",
  extra_metrics=[response_mentions_llm]
)
display(result)
```
