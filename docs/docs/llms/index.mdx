---
sidebar_position: 5
---

import { Card, LogoCard, CardGroup, PageCard, SmallLogoCard } from "@site/src/components/Card";
import { APILink } from "@site/src/components/APILink";

# MLflow for GenAI

MLflow is built specifically to help developers build complex, production-grade GenAI applications that create business value by delivering **highâ€‘quality (accurate) answers at the optimal cost and latency**.

## Key challenges

MLflow addresses the 2 inherent challenges in delivering high-quality generative AI:


<CardGroup>
  <PageCard
    headerText="Outputs are plain language ðŸ—£ï¸"
    
    text={[
      "LLMs speak in freeâ€‘form text, so judging accuracy is tough â€” there's rarely one perfect wording â€” so humans often need to look and decide.",
    ]}
    isBoldHeader={true}
  />
  <PageCard
    headerText="LLMs are non-deterministic ðŸŽ²"
    text={[
      "Even if you change nothing in the app, the same prompt can yield different responses â€” making issues hard to debug - so developers must track both accuracy and *consistency*.",
    ]}
    isBoldHeader={true}
  />
</CardGroup>

## How MLflow Helps

:::note 
See MLflow's reccomended [developer workflow](/llms/eval-workflow) to learn how to integrate these capabilities into your workflow.
:::


MLflow unites metrics that reliably measure GenAI quality with operational observability and human feedback, enabling teams to evaluate & improve GenAI quality, cost, and latency so they can deploy GenAI in production with confidence.

<CardGroup singleColumn={true}>
  <PageCard
    headerText="Monitor quality with LLM judges"
    icon="fa-gavel"
    link="/llms/evaluate-monitor/quality"
    text={[
      "Capture and convert expert feedback into metrics (LLM judges) that understand your business requirements and measure the nuances of plain-language GenAI outputs. Evaluate, monitor, and improve quality at scale, without waiting for human review."
    ]}
    isBoldHeader={true}
  />
  <PageCard
    headerText="Debug with end-to-end observability"
    icon="fa-search"
    link="/llms/tracing"
    text={[
      "Trace your app with OpenTelemetry-compatible SDKs that capture every invocation's inputs, outputs, and execution steps alongside cost, latency, and errors. Quickly debug issues, improve business logic, and optimize performance."
    ]}
    isBoldHeader={true}
  />
  <PageCard
    headerText="Improve quality by evaluating on production logs"
    icon="fa-sync"
    link="/llms/evaluate-monitor/eval-datasets"
    text={[
      "Use production logs to understand user behavior, turning low-quality responses into evaluation datasets and high-quality responses into regression tests. Evaluate new variants to deliver high-quality responses as user behavior evolves."
    ]}
    isBoldHeader={true}
  />
  <PageCard
    headerText="Collect human feedback"
    icon="fa-users"
    link="/llms/human-feedback"
    text={[
      "Capture domain expert feedback via web-based UIs and end-user ratings from your app via APIs. Use this feedback to enrich your understanding of how your app should behave and improve your custom LLM-judge metrics."
    ]}
    isBoldHeader={true}
  />
  <PageCard
    headerText="Enterprise governance"
    icon="fa-shield-alt"
    link="/llms/prompts"
    text={[
      "MLflow integrates with Unity Catalog to track the lifecycle and lineage of your app's assets - models, prompts, datasets, and metrics - and apply access controls for enterprise-ready governance."
    ]}
    isBoldHeader={true}
  />
</CardGroup>

