---
sidebar_position: 5
pagination_next: tracing/index
---

import { Card, LogoCard, CardGroup, PageCard, SmallLogoCard } from "@site/src/components/Card";
import { APILink } from "@site/src/components/APILink";

# MLflow for GenAI

**MLflow enables AI developers to build production-grade GenAI applications that deliver business value. To deliver business value, GenAI applications must consistently provide high quality responses while optimizing for cost and latency.**

However, building GenAI applications presents unique challenges compared to both traditional software development and ML development.  Just as traditional software engineering relies on test suites and traditional ML relies on validation sets, GenAI applications need robust evaluation frameworks to ensure they deliver high-quality, performant, and cost-effective responses.  However, evaluating these aspects in GenAI is more complex:
- Unlike traditional software or ML, GenAI most often produces plain language outputs.  These outputs are either "right" or "wrong" but there are many right answers and some "right" answers are more correct than others.  Human feedback is often required to confidently assess these outputs.
- The non-deterministic nature of LLMs adds an extra layer of complexity - teams must evaluate not just for correctness, but for consistency and stability.

MLflow provides the infrastructure and tooling to enable AI developers to address these challenges by using an **evaluation-centric development workflow.**  An evaluation-centric workflow allows teams to:

- **Quickly experiment to improve quality/performance:** Iteratively refine prompts and model configurations, systematically comparing each iteration for quality, cost, and latency
- **Monitor production quality/performance in real-time:** Identify and quickly address failure modes that surface during production
- **Collaborate with business stakeholders:**  Define and measure quality standards based on input from key business stakeholders and domain experts
- **Define metrics to evaluate plain language outputs:** TALK ABOUT LLM JUDGES

## What is an evaluation-centric development workflow?

![Evaluation Driven Workflow](/images/genai/eval-driven-dev.png)

The most effective GenAI development workflows treat production as an extension of development. Teams deploy early versions to pre-production environments where beta testers can interact with the application. This traffic becomes valuable evaluation data, helping identify edge cases, failure modes, and performance bottlenecks that might not emerge in controlled testing. 

This evaluation-driven workflow continues in the production deployment. Teams monitor live traffic, identify new edge cases, failure modes, and performance issues, and use this real-world data to continuously update their evaluation data and improve their models and prompts. This ensures that GenAI applications maintain high quality, stay within cost constraints, and meet latency requirements as they evolve and encounter new scenarios.

Using this approach, teams can:
- Collect and analyze production traces to understand real-world usage patterns
- Identify problematic interactions and create targeted evaluation datasets
- Run offline evaluations on these datasets to test potential improvements
- Deploy refined versions and monitor their impact on key metrics

Without an evaluation-driven development workflow, teams risk deploying GenAI solutions that work well in demos but fail to deliver consistent value in production. By making evaluation a cornerstone of the development process and treating production as an extension of the development environment, organizations can more effectively bridge the gap between proof-of-concept and production-ready GenAI applications.