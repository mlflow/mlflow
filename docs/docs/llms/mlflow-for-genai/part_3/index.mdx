---
title: "Part 3: Custom MLflow Model"
---

![MLflow for GenAI Guide Overview](/images/llms/mlflow-for-genai/steps_3.png)

<!-- TODOs:
- Visuals
- Links
- Standardize language on "model" or "application" or "system" or whatever.

 -->

This is part three of a four-part guide for using MLflow to experiment with and deploy your generative AI projects.

In part one, we started with the problem of generating social media posts following the style of a set of example posts. We used MLautologging and tracing to keep track of our early informal experiments as we developed a simple prototype. In part two, we used MLflow's evaluation tools to set up and run a more structured experiment comparing the performance of a couple of different models and prompts.

Now we are ready to prepare our GenAI application for production. In this part of the guide, we will:

- Wrap our model logic in a custom MLflow `PyFunc` model, allowing us to keep track of model versions and configurations, specify how we would like tracing handled in different environments, and validate the model prior to serving.
- Create a staging and production model in the model registry and register our custom model as a model version in the registry.

## Prerequisites and Setup

This guide assumes you have read and worked through parts one and two, and will frequently reference code and concepts introduced there. It also assumes you have followed the setup instructions detailed there.

## Part 1: Encapsulating our Model Logic

At this point, we have prototyped our GenAI application and refined that prototype through the evaluation process, settling on a model and prompt that generate posts that are accurate and that effectively copy the style of the example posts.

<!--TODO: How much do I need to introduce the idea of an MLflow model here? Mostly want to link to resources but still worth giving a couple of sentences. -->

Now that we have a working application, we need to prepare it for production use. We need to ensure that the application's interface is standardized and well defined so any downstream parts of the application, such as a frontend GUI, can effectively use the model. We also need to make sure the model environment and dependencies are recorded and packaged with the model in order to make sure the model can be deployed across different environments without dependency conflicts. Furthermore, encapsulating our application logic in a custom PyFunc model will allow us to keep track of model versions and configurations, specify how we would like tracing handled in different environments, and validate the model prior to serving.

### ChatModel or PyFunc?

MLflow offers the [ChatModel](https://mlflow.org/docs/latest/llms/chat-model-intro/index.html) class for setting up custom GenAI models with a chat interface. While this may seem like the obvious choice for our application, we will actually be using a custom [PyFunc](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html) model instead. Though our application *uses* a chat model, it is not, itself, a chat model: the inputs and outputs are not lists of messages.

The ChatModel interface is a great choice for developing and deploying conversational models with standardized input/output schemas following the OpenAI spec. PyFunc models, on the other hand, offer full control over the model interface.

### Defining the Custom Model

There are a few key considerations we will make when defining the custom model.

1. **Tracing:** We want to make sure tracing is easily configurable. There may, for example, be circumstances where tracing is not permissible in a production environment because of user data privacy concerns. To address cases like this, we will make tracing configurable via an environment variable.
2. **Configuration:** We will make some of the application's configuration, such as the system prompt and model provider, available to the model via a configuration dictionary. This is a good idea for parameters we expect we may want to change in the future. It will allow us to log updated versions of the model without needing to change the custom PyFunc model code.
<!-- TODO: Add a link to the MLflow documentation on how to set up a custom PyFunc model. -->
3. **Models from Code:** We will save our model logic in a separate file called `model.py` and include the line `set_model(SocialPoster())` at the end of the file. When we log the model, we will set the `code_path` argument to `model.py`. This is the "models from code" approach to logging models, which is a good fit for GenAI applications using LLM APIs, where there are no model weights that need to be serialized.
<!--TODO: Links to docs on the new model signatures feature -->
4. **Input and Output Schema:** MLflow 2.20 introduced the ability to define model signatures for custom PyFunc models using model signatures in the `predict` method. We will use this approach to define the input and output schema for our model.

#### Defining the Custom PythonModel

```python
# model.py
import mlflow
from mlflow.pyfunc import PythonModel
from mlflow.models import set_model
from markdownify import markdownify
import requests
import os
from openai import OpenAI
from pydantic import BaseModel
from typing import List

class SocialPostInput(BaseModel):
    example_posts: List[str]
    context_url: str
    additional_instructions: str

class SocialPostOutput(BaseModel):
    post: str

class SocialPoster(PythonModel):
    def __init__(self):
        self.tracing_enabled = False

    @mlflow.trace(span_type="FUNCTION")
    def _webpage_to_markdown(self, url):
        # Get webpage content
        response = requests.get(url)
        html_content = response.text

        # Convert to markdown
        markdown_content = markdownify(html_content)

        return markdown_content

    @mlflow.trace(span_type="FUNCTION")
    def _generate_prompt(
        self, example_posts, context, additional_instructions
    ):
        """Generate a prompt for the LLM based on the example posts, context, and additional instructions."""
        example_posts = "\n".join(
            [f"Example {i+1}:\n{post}" for i, post in enumerate(example_posts)]
        )
        prompt = self.prompt_template.format(
            example_posts=example_posts,
            context=context,
            additional_instructions=additional_instructions,
        )

        formatted_prompt = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt},
        ]

        return formatted_prompt

    @mlflow.trace(span_type="LLM")
    def _generate_post(self, messages):
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=1000,
        )
        return response.choices[0].message.content

    def load_context(self, context):
        self.system_prompt = context.model_config["system_prompt"]
        self.prompt_template = context.model_config["prompt_template"]
        self.model_provider = context.model_config["model_provider"]
        self.model_name = context.model_config["model_name"]
        self.tracing_enabled = os.getenv("MLFLOW_TRACING_ENABLED", "false").lower() == "true"

        if self.model_provider == "openai":
            self.client = OpenAI()
        elif self.model_provider == "google":
            self.client = OpenAI(
                base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
                api_key=os.getenv("GEMINI_API_KEY")
            )
        else:
            raise ValueError(f"Unsupported model provider: {self.model_provider}")

    
    def predict(self, context, model_input: list[SocialPostInput]) -> list[SocialPostOutput]:
        # only one input for illustration purposes
        model_input = model_input[0].model_dump()
        # check whether tracing is enabled and set it according to self.tracing_enabled
        if not mlflow.tracing.provider.is_tracing_enabled() == self.tracing_enabled:
            mlflow.tracing.enable() if self.tracing_enabled else mlflow.tracing.disable()

        with mlflow.start_span(name="predict", span_type="CHAIN") as parent_span:
            parent_span.set_inputs(model_input)
            example_posts = model_input.get("example_posts")
            context_url = model_input.get("context_url")
            markdown_context = self._webpage_to_markdown(context_url)
            additional_instructions = model_input.get("additional_instructions")

            prompt = self._generate_prompt(example_posts, markdown_context, additional_instructions)
            post = self._generate_post(prompt)
            parent_span.set_outputs({"post": post})
        return [{"post": post}]


set_model(SocialPoster())
```

Let's walk through the code in detail.

- The `SocialPoster` class inherits from `PythonModel`. This is the base class for all custom PyFunc models. It requires a `predict` method that takes a `context` and a list of `model_input` and returns a list of `model_output`.
- The core application logic is implemented in the `predict` method. This method:
    1. Checks whether tracing is enabled and sets it according to the `tracing_enabled` attribute using the `mlflow.tracing.enable()` and `mlflow.tracing.disable()` methods.
    2. Starts a new tracing span with the name "predict" and the type "CHAIN". This span will be the parent span for the rest of the model's execution.
    3. Assembles the prompt from the example posts, context, and additional instructions using the `_webpage_to_markdown` and `_generate_prompt` helper functions.
    4. Generates a post using the using the `_generate_post` helper function.
    5. Returns the generated post.
- The `predict` method calls the `_webpage_to_markdown`, `_generate_prompt`, and `_generate_post` helper functions that define specific steps in the application logic. We defined these functions in part 1 of this guide. Here, we add the `@mlflow.trace` decorator to the functions to make sure they are traced, giving us visibility into the application's execution flow and enabling us to diagnose any issues that may arise.
- The `load_context` method is called once when the model is loaded and is responsible for initializing model-specific attributes and resources. In our case, it loads configuration values from the model context, sets up tracing based on an environment variable, and initializes the appropriate client based on the configured model provider. This initialization approach ensures that our model has all necessary resources and configurations ready before any predictions are made, while keeping sensitive information like API keys separate from the model artifacts.

#### Setting up the Model Configuration

Now that we have defined the custom model, we need to set up the model configuration. The `load_context` method we defined expects a `model_config` dictionary with the following keys:

- `system_prompt`: The system prompt to use for the model.
- `prompt_template`: The prompt template to use for the model.
- `model_provider`: The model provider to use for the model.
- `model_name`: The name of the model to use for the model.

Consistent with the evaluation-based results from part 2, we will configure the model to use the `gemini-2.0-flash-exp` model from Google along with a detailed system prompt. We will save all of these expected elements in a `config` dictionary, which we will save along with the model during model logging.

```python
system_prompt = """You are a social media content specialist with expertise in matching writing styles and voice across platforms. Your task is to:

1. Analyze the provided example post(s) by examining:
   - Writing style, tone, and voice
   - Sentence structure and length
   - Use of hashtags, emojis, and formatting
   - Engagement techniques and calls-to-action

2. Generate a new LinkedIn post about the given topic that matches:
   - The identified writing style and tone
   - Similar structure and formatting choices
   - Equivalent use of platform features and hashtags
   - Comparable engagement elements

3. Return only the generated post, formatted exactly as it would appear on LinkedIn, without any additional commentary or explanations."""

prompt_template = """
example posts:
{example_posts}

context:
{context}


additional instructions:
{additional_instructions}
"""

config = {
    "system_prompt": system_prompt,
    "prompt_template": prompt_template,
    "model_provider": "google",
    "model_name": "gemini-2.0-flash-exp",
}
```

Using a configuration that is separate from the application logic gives us an easy way to see and update aspects of the model's behavior without needing to change the model code.

#### Logging the model

Now that we have defined the `SocialPoster` class (saved in `model.py`) and the `config` dictionary, we have everything we need to log the model. We will use the `mlflow.pyfunc.log_model` function to log the model.

<!-- TODO: Check in about best pattern for validation at logging time. -->
```python
code_path = "model.py"

with mlflow.start_run():
    model_info = mlflow.pyfunc.log_model(
        "social_poster",
        python_model=code_path,
        model_config=config,
    )
```

This code logs the model and saves it to the MLflow tracking server. It logs the model code defined in `model.py` and saves the model configuration in the `config` dictionary. You can inspect all of this information in the MLflow UI.

<!-- TODO: Add screenshot of logged model in MLflow UI -->

Though we have successfully logged the model, we still aren't certain that it will work as expected in production. Our final step in this section is to validate the model.

### Validating the model prior to deployment

We need to be sure that our model will work as expected once we deploy it to a serving endpoint. To this end, we will construct a simple input example in the format we intend to use in production and validate it with the `validate_serving_input` method.

```python
import os
from mlflow.models import convert_input_example_to_serving_input


sample_input = {
    "example_posts": ["Example 1: This is an example post.", "Example 2: This is another example post."],
    "context_url": "https://www.example.com",
    "additional_instructions": "The post should be concise and to the point..."
}

serving_payload = convert_input_example_to_serving_input(input_example=sample_input)

model_uri = model_info.model_uri

# Validate the serving payload works on the model
validate_serving_input(model_uri, serving_payload)
```

Which returns:

```python
[{'post': 'Example Domain. This domain is for illustrative examples. You can use it freely. More info: [https://www.iana.org/domains/example](https://www.iana.org/domains/example)\n'}]
```

We can also check whether our approach to enabling/disabling tracing works as expected. Tracing is disabled by default (see the `__init__` method of the `SocialPoster` class). We can enable tracing by setting the `MLFLOW_TRACING_ENABLED` environment variable to `true`. Let's try our model validation again with tracing enabled.

```python
os.environ["MLFLOW_TRACING_ENABLED"] = "true"
validate_serving_input(model_uri, serving_payload)
```

Inspecting the traces in the MLflow UI, we can see that the traces are now enabled and that the traces have been logged.

<!-- TODO: Add screenshot of traces in MLflow UI -->

Our model is ready for deployment!

### Registering the Model

Now that we have created and validated our custom PyFunc model, let's register it to the MLflow model registry. The model registry is a great way to manage your models and their versions. It will allow us to track the history of our model versions, define separate staging and production models, assign model aliases, and more.

First, let's register empty models for our staging and production environments:

```python
from mlflow import MlflowClient
client = MlflowClient()

client.create_registered_model("mlflow-social-ai-staging")
client.create_registered_model("mlflow-social-ai-production")
```

Now we can register our logged model version to the staging model:

```python
mv = client.create_model_version(
    name="mlflow-social-ai-staging",
    source=model_info.model_uri,
)
```

In this process, we've:

1. Created separate registered models for staging and production environments.
2. Registered our custom PyFunc model to the staging environment.

### Conclusion

In this part of the guide, we have encapsulated our model logic in a custom PyFunc model, set up tracing, validated the model, and registered it with MLflow. Specifically, we have seen how to:

- Define a custom PyFunc model and set up tracing.
- Define the PyFunc model predict method's signature with a Pydantic class as a type hint.
- Set up tracing to be configurable via an environment variable.
- Set up an external configuration to manage configurable aspects of the model we expect we might want to change in the future.
- Validate the model prior to deployment.
- Register the model with MLflow's model registry.

In the next and final part of this guide, we will deploy the model to a staging environment, promote it to production, introduce a new challenger model to evaluate against the production model, and then promote the challenger model to production.
