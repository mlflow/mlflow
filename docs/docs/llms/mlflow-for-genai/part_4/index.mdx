---
title: "Part 4: Model Deployment"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

![MLflow for GenAI Guide Overview](/images/llms/mlflow-for-genai/steps_4.png)

In the final part of this tutorial on using MLflow for GenAI, we will focus on model deployment with MLflow.

So far, we have:

1. Run informal experiments to validate our initial idea of generating social media posts following the style of example posts.
2. Refined our prototype through structured evaluation, arriving at an effective model and prompt.
3. Encapsulated our model logic in a custom PyFunc model with a standardized interface, configurable tracing, and various configuration options.
4. Registered our model with MLflow's model registry, creating separate staging and production models.

Building on this foundation, in this part of the guide we will:

1. Deploy the model to a staging environment and test it.
2. Promote the model to production.
3. Introduce a new challenger model and evaluate it against the production model using data collected from traces.
4. Promote the challenger model to production.

## Simplifying Assumptions

Like the previous parts of the guide, this part makes some simplifying assumptions to make it possible to follow the guide on your laptop and to keep the focus on the MLflow features and capabilities we are exploring.

In practice, you will likely want to [deploy your model to another environment](/deployment/) such as a cloud provider. MLflow supports a number of different cloud providers, including AWS, GCP, and Azure. Furthermore, you will likely want to set up a CI/CD pipeline to automate the deployment process. You might also want to set up separate staging and production MLflow environments with different permissions and configurations, controlling who has access to the models and endpoints. To keep this guide focused, we will simply use a local MLflow server and deploy our staging and production models to different ports on localhost.

## Environment Setup and Deployment Scripts

Before we start deploying and testing out models, we need to set up our )environments and some scripts to help us deploy and test our models in different environments. We also need to come up with a good system for managing our API keys. We will:

- store our API keys in [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) and create a helper function to retrieve the secrets
- a yaml file with configuration options for our deployment environments
- a CLI script to deploy a model to a given environment

### Secrets Management

One approach to managing our AI API keys is to use a secrets manager, which provides a centralized and secure place to store, rotate, and invalidate credentials.

Configuring the secrets manager is not strictly necessary for this guide‚Äîwe will provide a fallback option that fetches API keys from the local environment‚Äîbut careful management of API keys is important when deploying a GenAI application.

You can follow [this guide](https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html) to adding secrets to AWS Secrets Manager.

We will create a secret called `social-ai` that contains keys `OPENAI_API_KEY` and `GEMINI_API_KEY`, with the respective API keys as values.

Our deployment script, defined below, includes a function for retrieving these keys. Again, if you don't want to use a secrets manager for the purpose of this guide, that's okay‚Äîyou can work through the remainder of the guide with your API keys saved as local environment variables.

### Environment Configuration

We will create a yaml file defining three different deployment environments: staging, staging_challenger, and production. For each environment, this configuration defines the tracking URI, experiment name, environment (whether tracing is enabled), AWS secret name, and model serving argument, particularly the port on localhost which will simulate different environments in this guide.

- `staging`: an environment for testing and evaluating the model, with tracing enabled
- `staging_challenger`: an environment for testing and evaluating a *challenger* model that we may want to promote to production if it is better than our current best *champion* model.
- `production`: an environment representing the version of the model we would use in the version of the application deployed for real-world usage, which might mean it is used by the public, our customers, or internal users at our company.

The specifics of both the environment configuration and the deployment approach will depend heavily on your use case and deployment destinations. As such, we won't dwell too long on this specific configuration.

```yaml title="deployment.yaml"
staging:
  mlflow_tracking_uri: "file:/Users/daniel.liden/git/mlflow-social-ai/mlruns"
  mlflow_experiment: "genai-social-staging"
  environment_variables:
    MLFLOW_TRACING_ENABLED: "true"
  aws_secret_name: "social-ai"
  serve_args:
    port: 8501
    host: "0.0.0.0"
    workers: 2

staging_challenger:
  mlflow_tracking_uri: "file:/Users/daniel.liden/git/mlflow-social-ai/mlruns"
  mlflow_experiment: "genai-social-staging"
  environment_variables:
    MLFLOW_TRACING_ENABLED: "true"
  aws_secret_name: "social-ai"
  serve_args:
    port: 8502
    host: "0.0.0.0"
    workers: 2

production:
  mlflow_tracking_uri: "file:/Users/daniel.liden/git/mlflow-social-ai/mlruns"
  mlflow_experiment: "genai-social-production"
  environment_variables:
    MLFLOW_TRACING_ENABLED: "false"
  aws_secret_name: "social-ai"
  serve_args:
    port: 8500
    host: "0.0.0.0"
    workers: 4
```

### Deployment Script

Next, we will write a CLI script that serves a registered MLflow model version to the target environment, as configured in the yaml file we created above. This script:

1. Takes an environment name, model name, and model version as arguments
2. Loads the configuration for the specified environment from the yaml file defined above
3. Sets the MLflow tracking uri and experiment based on the environment configuration
4. Gets the API keys from the secrets manager or, failing that, from the local environment
5. Runs `mlflow models serve` with environment variables and other deployment options configured according to the yaml configuration

```python title="deploy.py"
import click
import yaml
import os
import subprocess
import boto3
import json
import mlflow

class ModelDeployer:
    def __init__(self, config_path="deployment.yaml"):
        self.config_path = config_path
    
    def _get_aws_secrets(self, secret_name, region_name="us-west-2"):
        """Fetch secrets from AWS Secrets Manager, falling back to environment variables."""
        try:
            client = boto3.session.Session().client('secretsmanager', region_name=region_name)
            response = client.get_secret_value(SecretId=secret_name)
            return json.loads(response['SecretString'])
        except Exception as e:
            # Fall back to environment variables if AWS access fails
            return {
                "GEMINI_API_KEY": os.environ.get("GEMINI_API_KEY"),
                    "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY")
                }
    
    def deploy(self, environment, model_name="genai_social_poster", model_version="latest"):
        """Deploy the model to the specified environment."""
        # Load config
        with open(self.config_path, 'r') as f:
            config = yaml.safe_load(f)

        env_config = config[environment]

        # Set tracking uri and experiment
        mlflow.set_tracking_uri(env_config["mlflow_tracking_uri"])
        mlflow.set_experiment(env_config["mlflow_experiment"])
        
        # Prepare environment variables
        env = os.environ.copy()
        env.update(env_config['environment_variables'])
        
        # Add secrets
        secrets = self._get_aws_secrets(env_config['aws_secret_name'])
        env.update(secrets)
        
        # Build and execute mlflow serve command
        cmd = [
            "mlflow", "models", "serve",
            "-m", f"models:/{model_name}/{model_version}",
            "--port", str(env_config['serve_args']['port']),
            "--host", env_config['serve_args']['host'],
            "--no-conda",
            "--workers", str(env_config['serve_args']['workers'])
        ]
        
        subprocess.run(cmd, env=env)

@click.command()
@click.option('--environment', type=click.Choice(['staging', 'staging_challenger', 'production']), required=True)
@click.option('--model-name', default="mlflow-social-ai-staging")
@click.option('--model-version', default="latest")
def cli_deploy(environment, model_name, model_version):
    deployer = ModelDeployer()
    deployer.deploy(environment, model_name, model_version)

if __name__ == '__main__':
    cli_deploy()
```

Now that we have set up a rudimentary system for managing secrets, configuring our environments, and deploying our models to those environments, it's time to serve our first model!

## Deploying the Staging Model

In the [previous part of this guide](/llms/mlflow-for-genai/part_3), we registered our model in the MLflow model registry under the name `mlflow-social-ai-staging`. Now we're ready to deploy it to our staging environment. Recall that we've already created separate registered models for staging and production, and registered our custom PyFunc model to the staging environment.

We can serve the model using the CLI script with:

```bash
python deploy.py --environment=staging --model-name=mlflow-social-ai-staging
```

or, if you are using `uv` for environment managent, you can use:

```bash
uv run deploy.py --environment=staging --model-name=mlflow-social-ai-staging
```

### Testing the staging model

Let's test the model! First, we need to make sure we still have access to the example posts we set up in part 2:

<Tabs>
  <TabItem label="Post Example 1" value="post1">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">
```python
post_example_1 = """MLflow's GenAI evaluation metrics now work as callable functions as of MLflow 2.17, making them easier to use and integrate.

Now you can use metrics like answer_relevance, answer_correctness, faithfulness, and toxicity directly as functions‚Äîno need to go through mlflow.evaluate() anymore if you're just prototyping with individual metrics or integrating metric calls into systems where mlflow.evalaute is not necessary.

This means:

üîç Easier debugging during prototyping
üîå More flexible integration options
üéØ Works with or without other MLflow features

Check it out in action ‚¨áÔ∏è

Learn more:
üìö Docs: https://lnkd.in/gyBzcrDr
üìù Release notes: https://lnkd.in/gBrNQfFC

#MachineLearning #AI #LLMs #LLMOps #Evals"""
```
        </div>
      </div>
    </div>
  </TabItem>
  <TabItem label="Post Example 2" value="post2">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">
```python
post_example_2 = """If you're already building with Python ML libraries, adding mlflow.autolog() to your code instantly gives you production-grade experiment tracking, model management, and observability‚Äîno extra infrastructure or code changes needed.

The automatic logging works across a remarkable breadth of libraries‚Äîfrom GenAI frameworks like LangChain, OpenAI, and LlamaIndex to traditional ML and deep learning libraries like PyTorch, scikit-learn, and Fastai.

MLflow's autolog feature changes this equation. With a single line‚Äîmlflow.autolog()‚Äîyou get automatic logging of:

üìä Training metrics and parameters for scikit-learn, PyTorch, many other ML frameworks
üîÑ LLM traces, prompts, responses, and tool calls for OpenAI and LangChain
üì¶ Model signatures and artifacts
üíæ Dataset information and example inputs

The best part is that it works out of the box with the most popular libraries in the Python ML ecosystem: no need to modify your existing training code or add manual logging statements.

Read more: https://lnkd.in/e_aTp6HH

#machinelearning #mlops #ai #llmops"""
```
        </div>
      </div>
    </div>
  </TabItem>
  <TabItem label="Post Example 3" value="post3">
    <div class="flex-column">
      <div class="flex-row">
        <div class="flex-item">
```python
post_example_3 = """New tutorial: Step-by-step guide to building a tool-calling LLM application using MLflow's ChatModel wrapper and tracing system.

This tutorial shows you how to:

üîß Create a tool-calling model using mlflow.pyfunc.ChatModel
üîÑ Implement OpenAI function calling with automatic input/output handling
üîç Add comprehensive tracing to debug multi-step LLM interactions
üöÄ Deploy your model with full MLflow lifecycle management

The guide includes a practical example building a weather information agent, showing how ChatModel simplifies complex LLM patterns while providing enterprise-grade observability.

Check out the complete tutorial here: https://lnkd.in/gdTw8N2S

#MLOps #AIEngineering #LLMOps #AI"""
```
        </div>
      </div>
    </div>
  </TabItem>
</Tabs>

As before, we'll save these examples as a list of strings.

```python
example_posts = [post_example_1, post_example_2, post_example_3]
```


And now we can query the endpoint. We will use the `requests` library:

```python
import requests

input = {"inputs":
         {
             "example_posts": example_posts,
             "context_url": "https://mlflow.org/docs/latest/system-metrics/index.html",
             "additional_instructions": "The post should talk about how straightforward it is to track system metrics in MLflow."
         }}

response = requests.post("http://0.0.0.0:8501/invocations", json=input)
print(response.json()['predictions']['post'])
```

which returns:

```text
Want effortless observability for your ML experiments? MLflow makes it incredibly easy to track system metrics like
CPU, GPU, memory, and disk usage. 

With just a simple `mlflow.start_run(log_system_metrics=True)`, you get detailed insights into your resource 
utilization‚Äîno need for complex setup.

MLflow automatically logs:

üìä CPU & GPU usage
üíæ Memory & disk consumption
üö¶ Network traffic

These metrics are viewable directly in the MLflow UI, helping you optimize performance and troubleshoot issues.

Learn more about system metrics logging: https://mlflow.org/docs/latest/system-metrics.html

#MachineLearning #MLOps #AI #SystemMonitoring
```


## Promoting the Model to Production

Now that we have deployed and tested our model in the staging environment, we can promote it to production and deploy it to the production environment. We will start by copying the staging model version over to the production model we created in the registry.

```python
from mlflow import MlflowClient

client = MlflowClient()
client.copy_model_version(
    src_model_uri="models:/mlflow-social-ai-staging/latest",
    dst_name="mlflow-social-ai-production",
)
```

In this code snippet, we are copying the staging model version we just deployed and tested to the empty production model we created earlier. We can inspect the model in the MLflow UI to see that it has been registered and to discover the expected schema (which will be identical to the staging model).

![Production Model](/images/llms/mlflow-for-genai/14_prod_model.png)


Now we can use the CLI script to deploy this production model with

```bash
python deploy.py --environment=production --model-name=mlflow-social-ai-production --model-version=latest
```

This will start the model server on port 8500. Our model is now being served on `http://0.0.0.0:8500`‚Äîour "production" environment. Once again, we can test this and verify that it is working as intended.

```python
response = requests.post("http://0.0.0.0:8500/invocations", json=input)
print(response.json()['predictions']['post'])
```

which returns:

```text
Tracking system metrics with MLflow is incredibly simple! üìä Just a few lines of code or an environment variable 
and you can monitor CPU, GPU, memory, network, and disk usage during your ML runs. 

No more guessing about resource consumption‚ÄîMLflow gives you the observability you need, whether you're using local
or remote tracking. 

Here's how:

*   Enable it globally with an env variable or an API call
*   Control it for individual runs
*   View the results directly in the MLflow UI
*   Customize the logging frequency

Ready to level up your ML observability? Check out the docs: https://lnkd.in/g4gHjVvH

#MachineLearning #MLOps #AI #SystemMetrics
```

We have successfully deployed our model to our production environment! But let's take a closer look at that response. The generated post includes a (shortened) link, but the link itself is broken. It appears our model inserted a link because there are links in the example, but it's not a valid link. This brings up an important point about the model deployment process: deploying a model to production doesn't mean we're done. We need to monitor the model's performance and be prepared to update it if necessary.

## Improving our Production Model

Right now, our staging and production models are identical. In this section, we will evaluate our model's performance based on traces collected from the staging model, use what we learn to update the model and introduce a "challenger" model, and then evaluate the challenger model.

There are many different approaches we might follow to collect feedback and iterate on the model. We could, for instance:

- Conduct A/B tests with real users and collect human feedback on the generated posts
- Mirror all of the production traffic to a challenger model and compare the outputs
- Deploy both models to staging and rely on internal testers or synthetic data to evaluate the models

Since we are not collecting traces from the production model in this scenario, we will instead collect traces from the staging model (reflecting internal usage/testing) and use them to inform development of a new challenger model.

### Evaluating the Staging Model

Suppose have a lot of internal users using the staging model, where we are collecting traces. We can use those traces to evaluate the model's performance. First, let's simulate some traffic to the staging model, generating a collection of traces.

First, we'll write a helper function to query the model.

```python
def query_model(context_url, additional_instructions, example_posts=example_posts, endpoint="http://0.0.0.0:8501"):
    input_data = {
        "inputs": {
            "example_posts": example_posts,
            "context_url": context_url,
            "additional_instructions": additional_instructions
        }
    }
    response = requests.post(f"{endpoint}/invocations", json=input_data)
    return response.json()['predictions']['post']
```

This function lets us simply provide the context URL, additional instructions, and example posts, without needing to format the input data or write the request logic each time.

Now, let's put together some pairs of URLs and additional instructions to query the model, simulating the kinds of requests we might expect our internal users to make against the staging model.

```python
instruction_context_pairs = {
    "url": ["https://mlflow.org/blog/custom-pyfunc",
            "https://mlflow.org/blog/custom-pyfunc",
            "https://mlflow.org/blog/custom-pyfunc",
            "https://mlflow.org/releases/2.19.0",
            "https://mlflow.org/releases/2.19.0",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/deployment/index.html",
            "https://mlflow.org/docs/latest/deployment/index.html",
            "https://mlflow.org/docs/latest/llms/tracing/index.html",
            "https://mlflow.org/docs/latest/llms/tracing/index.html",
            "https://mlflow.org/docs/latest/llms/tracing/index.html",
            "https://mlflow.org/docs/latest/deep-learning/index.html",
            "https://mlflow.org/docs/latest/deep-learning/index.html",
            "https://mlflow.org/docs/latest/auth/index.html",
            "https://mlflow.org/docs/latest/auth/index.html",
            "https://mlflow.org/docs/latest/llms/langchain/index.html",
            "https://mlflow.org/docs/latest/llms/langchain/index.html"
            ],
    "instructions": [
        "Write an engaging post about MLflow's custom PyFunc models. Focus on how they enable working with ANY ML framework.",
        "Explain why the load_context method is crucial for custom PyFunc models. Emphasize efficiency and proper artifact handling.",
        "Share how to parameterize MLflow PyFunc models. Highlight both __init__ and inference-time parameters.",
        "Write a post outlining the major new features introduced in MLflow 2.19.0.",
        "Write a post summarizing the updates changes to MLflow tracing in 2.19.0.",
        "The purpose of this post is to share this guide about fine tuning with the transformers library and logging metrics with mlflow.",
        "Write a post focusing on the role of model signatures for model logging in mlflow, especially in the context of fine-tuning transformer models.",
        "Write a post sharing this guide, with an emphasis on the key takeaways.",
        "Share about the role of MLflow in fine-tuning deep learning models",
        "Write a post on the role of MLflow in deploying models to a production environment.",
        "Write a very short post about the options available for deploying mlflow models.",
        "Focus on highlighting the variety of libraries compatible with autologging for tracing.",
        "Explain the difference between the fluent API an the client API for tracing.",
        "Talk about the benefits of MLflow tracing and highlight the compatibility of tracing with outside otel collectors.",
        "Make the case, in an informative and direct way, that MLflow is a great tool for deep learning MLOps.",
        "Talk about the visualizations available in the mlflow UI for deep learning training runs.",
        "Write a post for people who might not be aware of the permissions/auth features of mlflow describing the basic features.",
        "Explain how multi-user support works in mlflow.",
        "Share this page from the MLflow docs about the features of the LangChain native model flavor.",
        "Create a social media post about the use of MLflow for agent development, with an emphasis on LangChain and LangGraph."
    ]
}
```

Now we can run through each of these examples with our staging model.

```python
import time

for context_url, additional_instructions in zip(instruction_context_pairs["url"], instruction_context_pairs["instructions"]):
    query_model(context_url, additional_instructions)
    # making sure to avoid rate limits
    time.sleep(5)
```

This will generate a collection of traces, which we can use to evaluate the model's performance.

We can extract the outputs from the traces for use in our evaluation as follows. This highlights a key point: traces aren't just for looking at in the UI! They can provide a valuable source of data for evaluating model performance.

```python
traces_df = mlflow.search_traces(
    experiment_ids=[mlflow.get_experiment_by_name("genai-social-staging").experiment_id],
    # get the last 20 traces, corresponding to the 20 requests we made
    max_results=20,
    # Extract the post output from the predict span
    extract_fields=["predict.outputs.post"]
)

generated_posts = traces_df["predict.outputs.post"].tolist()
```

Now we have a list of generated posts. We want to check these posts for links, giving us a baseline for how often the model is including potentially-broken links in its outputs. We can create a custom MLflow metric for this. Unlike the metric we developed in part 1, this will be a heuristic metric rather than an LLM-as-judge metric. We will use a simple regex to check for links in the generated posts.

```python
import re
from mlflow.metrics import MetricValue, make_metric, EvaluationMetric
from mlflow.metrics.base import standard_aggregations

def _url_eval_fn(predictions, targets=None, metrics=None):
    """Check for URLs in text predictions."""
    url_pattern = r'https?://[^\s]+'
    scores = [1 if re.search(url_pattern, pred) else 0 for pred in predictions]
    
    return MetricValue(
        scores=scores,
        aggregate_results=standard_aggregations(scores)
    )

def url_presence_metric() -> EvaluationMetric:
    """
    Creates a metric for detecting URLs in text content.
    Returns 1 if URLs are present, 0 if not.
    """
    return make_metric(
        eval_fn=_url_eval_fn,
        greater_is_better=False,
        name="url_presence",
        version="v1"
    )

url_metric = url_presence_metric()
```

Now we have an evaluation dataset and a metric. Let's figure out the extent of the problem and decide whether we need to update the model to address it. In this case, we will just use the evaluation metric as a callable, but we could also use the `mlflow.evaluate()` approach detailed in part 2.

```python
results_old_model = url_metric(predictions=generated_posts)
results_old_model
```

Which returns:

```python
MetricValue(scores=[0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], justifications=None, aggregate_results={'mean': np.float64(0.75), 'variance': np.float64(0.1875), 'p90': np.float64(1.0)})
```

This tells us that the model is including links in 75% of its outputs! This is a real problem‚Äîif any users directly post the outputs without checking them, they will likely be sharing broken links. We should update the model to address this issue.

### Developing a Challenger Model

We can address this issue by updating the system prompt with the instruction to include placeholders where links should be inserted, rather than making up links. We don't need to update our model code to do this‚Äîrecall, we set up the model such that the system prompt can be changed via the external configuration.

```python
challenger_system_prompt = """You are a social media content specialist with expertise in matching writing styles and voice across platforms. Your task is to:

1. Analyze the provided example post(s) by examining:
   - Writing style, tone, and voice
   - Sentence structure and length
   - Use of hashtags, emojis, and formatting
   - Engagement techniques and calls-to-action

2. Generate a new LinkedIn post about the given topic that matches:
   - The identified writing style and tone
   - Similar structure and formatting choices
   - Equivalent use of platform features and hashtags
   - Comparable engagement elements

3. Use <INSERT_LINK> in the generated post to indicate where the link should go. Do not include any other links in the generated post unless explicitly instructed to do so.

4. Return only the generated post, formatted exactly as it would appear on LinkedIn, without any additional commentary or explanations."""

challenger_config = {
    "system_prompt": challenger_system_prompt,
    "prompt_template": prompt_template,
    "model_provider": "google",
    "model_name": "gemini-2.0-flash-exp",
}
```

#### Model Aliases

Now, we want to register this model to staging in MLflow. But we need a way to distinguish it from our existing staging model! We could create a completely separate model in the registry. But this isn't necessary‚ÄîMLflow provides a powerful model alias system that allows us to create a new model version with a different name, but still point to the same model in the registry.

First, let's assign our existing staging model to the alias "champion." This is the current, deployed version of the model, the same one we have also deployed to production.

```python
client.set_registered_model_alias(
    name="mlflow-social-ai-staging",
    alias="champion",
    version="1"
)
```

Now we can log the new challenger model and register it to the staging model with the alias "challenger."

```python
model_info = mlflow.pyfunc.log_model(
    "mlflow-social-ai-challenger",
    python_model=code_path,
    model_config=challenger_config,
    signature=signature,
)

mv = client.create_model_version(
    name="mlflow-social-ai-staging",
    source=model_info.model_uri,
)

client.set_registered_model_alias(
    name="mlflow-social-ai-staging",
    alias="challenger",
    version=2
)
```

Let's deploy this new challenger model to staging and see if it addresses the issue. You can deploy it with the CLI with:

```bash
python deploy.py --environment=staging_challenger --model-name=mlflow-social-ai-staging --model-version=challenger
```

Let's try a sample query and see how it works.

```python
print(query_model(context_url, additional_instructions, endpoint="http://0.0.0.0:8502"))
```

Which returns:

```text
Building complex AI agents? MLflow's LangChain and LangGraph integrations provide the tools you need for robust 
development, tracking, and deployment.

With MLflow you can:

‚úÖ Easily track experiments with various agents and tools
üì¶ Automatically manage code and environment dependencies
üìä Evaluate agent performance with MLflow's evaluation capabilities
üîç Gain observability into agent behavior with MLflow Tracing

From simple chains to complex multi-agent systems, MLflow has you covered.

Learn more about using MLflow with LangChain and LangGraph: <INSERT_LINK>

#MLOps #AI #LLMs #LangChain #LangGraph #AIAgents
```

This is just what we wanted. This output now includes the `<INSERT_LINK>` placeholder instead of a broken link. Of course, one example doesn't verify that we have addressed the broader issue‚Äîrecall that 75% of our test cases resulted in outputs with links. Let's run through the same evaluation process again with the challenger model.

```python
for context_url, additional_instructions in zip(instruction_context_pairs["url"], instruction_context_pairs["instructions"]):
    query_model(context_url, additional_instructions, endpoint="http://0.0.0.0:8502")
    time.sleep(5)

traces_df = mlflow.search_traces(
    experiment_ids=[mlflow.get_experiment_by_name("genai-social-staging").experiment_id],
    max_results=20,
    # Extract the post output from the predict span
    extract_fields=["predict.outputs.post"]
)

generated_posts = traces_df["predict.outputs.post"].tolist()

results_challenger_model = url_metric(predictions=generated_posts)
results_challenger_model
```

Which returns:

```python
MetricValue(scores=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], justifications=None, aggregate_results={'mean': np.float64(0.0), 'variance': np.float64(0.0), 'p90': np.float64(0.0)})
```

No links! This is exactly what we wanted and addresses a significant issue we found in the first version of the model. Now we want to get this new challenger model into production so our users don't have to worry about broken links.

### Promoting the Challenger Model

We can use the same process by which we prompted the staging model to production to get this new challenger model into production.

```python
client.copy_model_version(
    src_model_uri="models:/mlflow-social-ai-staging@challenger",
    dst_name="mlflow-social-ai-production",
)
```

Notice that we idenfitied the challenger model by appending the `@challenger` suffix to the model name. This code snippet copies the challenger model version to production.

## Conclusion

In this guide, we have walked through the complete lifecycle of a GenAI project, using MLflow for tracing, evaluation, model management, and deployment. Let's briefly review what we covered.

1. **Part 1: Experimentation and Prototyping**
    - Used MLflow tracing to capture and learn from informal experiments
    - Compared models from different providers and tagged their respective traces
2. **Part 2: Evaluation and Comparison**
    - Formulated questions and hypotheses about model performance
    - Developed a rigorous evaluation framework to test hypotheses
    - Used custom and built-in MLflow metrics to compare models quantitatively
3. **Part 3: Custom Model Setup**
    - Set up a custom PyFunc model with external configuration
    - Integrated tracing into the custom model, with the ability to activate and deactivate tracing via environment variables
4. **Part 4: Model Deployment**
    - Deployed the model to staging and production
    - Used model aliases to manage multiple versions of the same model
    - Used traces to generate evaluation data
    - Evaluated a challenger model against the champion model, and promoted the challenger model to production

Throughout this process, we've seen how MLflow provides the tools needed to manage the complexity of GenAI applications:

- Tracing gives us visibility into model behavior
- Evaluation metrics help us make data-driven decisions
- Custom models let us standardize our application logic
- The model registry helps us manage different versions and environments

While this guide used a simplified deployment setup for illustration, the patterns and practices demonstrated here can be adapted to more complex production environments with multiple models, automated deployment pipelines, and sophisticated monitoring systems.

## Further Reading

For more information on using MLflow for GenAI, see the following resources:

- [MLflow tracing for LLM Observability](https://mlflow.org/docs/latest/llms/tracing/index.html)
- [MLflow LLM Evaluation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)
- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [Deploy and Organize models with aliases](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags)
- [Serving an MLflow model from Model Registry](https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry)
