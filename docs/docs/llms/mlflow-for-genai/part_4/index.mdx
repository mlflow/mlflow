---
title: "MLflow for Generative AI: from Experimentation to Production, Part Four"
---

## Part 4: Model Deployment

In this final part of the guide, we will focus on model deployment with MLflow.

So far, we have:

1. Run informal experiments to validate our initial idea of generating social media posts following the style of example posts.
2. Refined our prototype through structured evaluation, arriving at an effective model and prompt.
3. Encapsulated our model logic in a custom PyFunc model with a standardized interface, configurable tracing, and various configuration options.
4. Registered our model with MLflow's model registry, creating separate staging and production models.

Building on this foundation, in this part of the guide we will:

1. Deploy the model to a staging environment and test it.
2. Promote the model to production.
3. Introduce a new challenger model and evaluate it against the production model using data collected from traces.
4. Promote the challenger model to production.

### Simplifying Assumptions

Like the previous parts of the guide, this part makes some simplifying assumptions to make it possible to follow the guide on your laptop and to keep the focus on the MLflow features and capabilities we are exploring.

<!-- TODO: links to deployment docs -->
In practice, you will likely want to deploy your model to a cloud environment. MLflow supports a number of different cloud providers, including AWS, GCP, and Azure. Furthermore, you will likely want to set up a CI/CD pipeline to automate the deployment process. You might also want to set up separate staging and production MLflow environments with different permissions and configurations, controlling who has access to the models and endpoints. To keep this guide focused, we will simply use a local MLflow server and deploy our staging and production models to different ports on localhost.

### Deploying the Staging Model

In the previous part of this guide, we registered our model in the MLflow model registry. Now we're ready to deploy it to our staging environment. Recall that we've already created separate registered models for staging and production, and registered our custom PyFunc model to the staging environment.

With our model registered, we can now proceed to deploy it to our staging environment for testing.

### Promoting the Model to Production

Now that we have deployed and tested our model in the staging environment, we can promote it to production and deploy it to the production environment.

```python
from mlflow import MlflowClient

client = MlflowClient()
client.copy_model_version(
    src_model_uri="models:/mlflow-social-ai-staging/latest",
    dst_name="mlflow-social-ai-production",
)
```

In this code snippet, we are copying the staging model version we just deployed and tested to the empty production model we created earlier. Now we can use the CLI script to deploy this production model with `uv run deploy.py --environment=production --model-name=mlflow-social-ai-production --model-version=latest`.

This will start the model server on port 8500. Our model is now being served on `http://0.0.0.0:8500`—our "production" environment. Once again, we can test this and verify that it is working as intended.

```python
response = requests.post("http://0.0.0.0:8500/invocations", json=input)
print(response.json()['predictions']['post'])
```

which returns:

```text
Tracking system metrics with MLflow is incredibly simple! 📊 Just a few lines of code or an environment variable 
and you can monitor CPU, GPU, memory, network, and disk usage during your ML runs. 

No more guessing about resource consumption—MLflow gives you the observability you need, whether you're using local
or remote tracking. 

Here's how:

*   Enable it globally with an env variable or an API call
*   Control it for individual runs
*   View the results directly in the MLflow UI
*   Customize the logging frequency

Ready to level up your ML observability? Check out the docs: https://lnkd.in/g4gHjVvH

#MachineLearning #MLOps #AI #SystemMetrics
```

We have successfully deployed our model to our production environment! But let's take a closer look at that response. The generated post includes a (shortened) link, but the link itself is broken. It appears our model inserted a link because there are links in the example, but it's not a valid link. This brings up an important point about the model deployment process: deploying a model to production doesn't mean we're done. We need to monitor the model's performance and be prepared to update it if necessary.

### Introduce a Challenger

Right now, our staging and production models are identical. In this section, we will evaluate our model's performance based on traces collected from the staging model, use what we learn to update the model and introduce a "challenger" model, and then evaluate the challenger model.

There are many different approaches we might follow to collect feedback and iterate on the model. We could, for instance:

- Conduct A/B tests with real users and collect human feedback on the generated posts
- Mirror all of the production traffic to a challenger model and compare the outputs
- Deploy both models to staging and rely on internal testers or synthetic data to evaluate the models

Since we are not collecting traces from the production model in this scenario, we will instead collect traces from the staging model and use them to inform development of a new challenger model.

#### Evaluating the Staging Model

Suppose have a lot of internal users using the staging model, where we are collecting traces. We can use those traces to evaluate the model's performance. First, let's simulate some traffic to the staging model, generating a collection of traces.

First, we'll write a helper function to query the model.

```python
def query_model(context_url, additional_instructions, example_posts=example_posts, endpoint="http://0.0.0.0:8501"):
    input_data = {
        "inputs": {
            "example_posts": example_posts,
            "context_url": context_url,
            "additional_instructions": additional_instructions
        }
    }
    response = requests.post(f"{endpoint}/invocations", json=input_data)
    return response.json()['predictions']['post']
```

This function lets us simply provide the context URL, additional instructions, and example posts, without needing to format the input data or write the request logic each time.

Now, let's put together some pairs of URLs and additional instructions to query the model, simulating the kinds of requests we might expect our internal users to make against the staging model.

```python
instruction_context_pairs = {
    "url": ["https://mlflow.org/blog/custom-pyfunc",
            "https://mlflow.org/blog/custom-pyfunc",
            "https://mlflow.org/blog/custom-pyfunc",
            "https://mlflow.org/releases/2.19.0",
            "https://mlflow.org/releases/2.19.0",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.html",
            "https://mlflow.org/docs/latest/deployment/index.html",
            "https://mlflow.org/docs/latest/deployment/index.html",
            "https://mlflow.org/docs/latest/llms/tracing/index.html",
            "https://mlflow.org/docs/latest/llms/tracing/index.html",
            "https://mlflow.org/docs/latest/llms/tracing/index.html",
            "https://mlflow.org/docs/latest/deep-learning/index.html",
            "https://mlflow.org/docs/latest/deep-learning/index.html",
            "https://mlflow.org/docs/latest/auth/index.html",
            "https://mlflow.org/docs/latest/auth/index.html",
            "https://mlflow.org/docs/latest/llms/langchain/index.html",
            "https://mlflow.org/docs/latest/llms/langchain/index.html"
            ],
    "instructions": [
        "Write an engaging post about MLflow's custom PyFunc models. Focus on how they enable working with ANY ML framework.",
        "Explain why the load_context method is crucial for custom PyFunc models. Emphasize efficiency and proper artifact handling.",
        "Share how to parameterize MLflow PyFunc models. Highlight both __init__ and inference-time parameters.",
        "Write a post outlining the major new features introduced in MLflow 2.19.0.",
        "Write a post summarizing the updates changes to MLflow tracing in 2.19.0.",
        "The purpose of this post is to share this guide about fine tuning with the transformers library and logging metrics with mlflow.",
        "Write a post focusing on the role of model signatures for model logging in mlflow, especially in the context of fine-tuning transformer models.",
        "Write a post sharing this guide, with an emphasis on the key takeaways.",
        "Share about the role of MLflow in fine-tuning deep learning models",
        "Write a post on the role of MLflow in deploying models to a production environment.",
        "Write a very short post about the options available for deploying mlflow models.",
        "Focus on highlighting the variety of libraries compatible with autologging for tracing.",
        "Explain the difference between the fluent API an the client API for tracing.",
        "Talk about the benefits of MLflow tracing and highlight the compatibility of tracing with outside otel collectors.",
        "Make the case, in an informative and direct way, that MLflow is a great tool for deep learning MLOps.",
        "Talk about the visualizations available in the mlflow UI for deep learning training runs.",
        "Write a post for people who might not be aware of the permissions/auth features of mlflow describing the basic features.",
        "Explain how multi-user support works in mlflow.",
        "Share this page from the MLflow docs about the features of the LangChain native model flavor.",
        "Create a social media post about the use of MLflow for agent development, with an emphasis on LangChain and LangGraph."
    ]
}
```

Now we can run through each of these examples with our staging model.

```python
import time

for context_url, additional_instructions in zip(instruction_context_pairs["url"], instruction_context_pairs["instructions"]):
    query_model(context_url, additional_instructions)
    # making sure to avoid rate limits
    time.sleep(5)
```

This will generate a collection of traces, which we can use to evaluate the model's performance.

We can extract the outputs from the traces for use in our evaluation as follows. This highlights a key point: traces aren't just for looking at in the UI! They can provide a valuable source of data for evaluating model performance.

```python
traces_df = mlflow.search_traces(
    experiment_ids=[mlflow.get_experiment_by_name("genai-social-staging").experiment_id],
    # get the last 20 traces, corresponding to the 20 requests we made
    max_results=20,
    # Extract the post output from the predict span
    extract_fields=["predict.outputs.post"]
)

generated_posts = traces_df["predict.outputs.post"].tolist()
```

Now we have a list of generated posts. We want to check these posts for links, giving us a baseline for how often the model is including potentially-broken links in its outputs. We can create a custom MLflow metric for this. Unlike the metric we developed in part 1, this will be a heuristic metric rather than an LLM-as-judge metric. We will use a simple regex to check for links in the generated posts.

```python
import re
from mlflow.metrics import MetricValue, make_metric, EvaluationMetric
from mlflow.metrics.base import standard_aggregations

def _url_eval_fn(predictions, targets=None, metrics=None):
    """Check for URLs in text predictions."""
    url_pattern = r'https?://[^\s]+'
    scores = [1 if re.search(url_pattern, pred) else 0 for pred in predictions]
    
    return MetricValue(
        scores=scores,
        aggregate_results=standard_aggregations(scores)
    )

def url_presence_metric() -> EvaluationMetric:
    """
    Creates a metric for detecting URLs in text content.
    Returns 1 if URLs are present, 0 if not.
    """
    return make_metric(
        eval_fn=_url_eval_fn,
        greater_is_better=False,
        name="url_presence",
        version="v1"
    )

url_metric = url_presence_metric()
```

Now we have an evaluation dataset and a metric. Let's figure out the extent of the problem and decide whether we need to update the model to address it.

```python
results_old_model = url_metric(predictions=generated_posts)
results_old_model
```

Which returns:

```python
MetricValue(scores=[0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], justifications=None, aggregate_results={'mean': np.float64(0.75), 'variance': np.float64(0.1875), 'p90': np.float64(1.0)})
```

This tells us that the model is including links in 75% of its outputs! This is a real problem—if any users directly post the outputs without checking them, they will likely be passing in broken links. We should update the model to address this issue.

#### Developing a Challenger Model

We can address this issue by updating the system prompt with the instruction to include placeholders where links should be inserted, rather than making up links. We don't need to update our model code to do this—recall, we set up the model such that the system prompt can be changed via the external configuration.

```python
challenger_system_prompt = """You are a social media content specialist with expertise in matching writing styles and voice across platforms. Your task is to:

1. Analyze the provided example post(s) by examining:
   - Writing style, tone, and voice
   - Sentence structure and length
   - Use of hashtags, emojis, and formatting
   - Engagement techniques and calls-to-action

2. Generate a new LinkedIn post about the given topic that matches:
   - The identified writing style and tone
   - Similar structure and formatting choices
   - Equivalent use of platform features and hashtags
   - Comparable engagement elements

3. Use <INSERT_LINK> in the generated post to indicate where the link should go. Do not include any other links in the generated post unless explicitly instructed to do so.

4. Return only the generated post, formatted exactly as it would appear on LinkedIn, without any additional commentary or explanations."""

challenger_config = {
    "system_prompt": challenger_system_prompt,
    "prompt_template": prompt_template,
    "model_provider": "google",
    "model_name": "gemini-2.0-flash-exp",
}
```

Now, we want to register this model to staging in MLflow. But we need a way to distinguish it from our existing staging model! We could create a completely separate model in the registry. But this isn't necessary—MLflow provides a powerful model alias system that allows us to create a new model version with a different name, but still point to the same model in the registry.

First, let's assign our existing staging model to the alias "champion." This is the current, deployed version of the model, the same one we have also deployed to production.

```python
client.set_registered_model_alias(
    name="mlflow-social-ai-staging",
    alias="champion",
    version="1"
)
```

Now we can log the new challenger model and register it to the staging model with the alias "challenger."

```python
model_info = mlflow.pyfunc.log_model(
    "mlflow-social-ai-challenger",
    python_model=code_path,
    model_config=challenger_config,
    signature=signature,
)

mv = client.create_model_version(
    name="mlflow-social-ai-staging",
    source=model_info.model_uri,
)

client.set_registered_model_alias(
    name="mlflow-social-ai-staging",
    alias="challenger",
    version=2
)
```

Let's deploy this new challenger model to staging and see if it addresses the issue. You can deploy it with the CLI with: `uv run deploy.py --environment=staging_challenger --model-name=mlflow-social-ai-staging --model-version=challenger`.

Let's try a sample query and see how it works.

```python
print(query_model(context_url, additional_instructions, endpoint="http://0.0.0.0:8502"))
```

Which returns:

```text
Building complex AI agents? MLflow's LangChain and LangGraph integrations provide the tools you need for robust 
development, tracking, and deployment.

With MLflow you can:

✅ Easily track experiments with various agents and tools
📦 Automatically manage code and environment dependencies
📊 Evaluate agent performance with MLflow's evaluation capabilities
🔍 Gain observability into agent behavior with MLflow Tracing

From simple chains to complex multi-agent systems, MLflow has you covered.

Learn more about using MLflow with LangChain and LangGraph: <INSERT_LINK>

#MLOps #AI #LLMs #LangChain #LangGraph #AIAgents
```

This is just what we wanted. This output now includes the `<INSERT_LINK>` placeholder instead of a broken link. Of course, one example doesn't verify that we have addressed the broader issue—recall that 75% of our test cases resulted in outputs with links. Let's run through the same evaluation process again with the challenger model.

```python
for context_url, additional_instructions in zip(instruction_context_pairs["url"], instruction_context_pairs["instructions"]):
    query_model(context_url, additional_instructions, endpoint="http://0.0.0.0:8502")
    time.sleep(5)

traces_df = mlflow.search_traces(
    experiment_ids=[mlflow.get_experiment_by_name("genai-social-staging").experiment_id],
    max_results=20,
    # Extract the post output from the predict span
    extract_fields=["predict.outputs.post"]
)

generated_posts = traces_df["predict.outputs.post"].tolist()

results_challenger_model = url_metric(predictions=generated_posts)
results_challenger_model
```

Which returns:

```python
MetricValue(scores=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], justifications=None, aggregate_results={'mean': np.float64(0.0), 'variance': np.float64(0.0), 'p90': np.float64(0.0)})
```

No links! This is exactly what we wanted and addresses a significant issue we found in the first version of the model. Now we want to get this new challenger model into production so our users don't have to worry about broken links.

We can use the same process by which we prompted the staging model to production to get this new challenger model into production.

```python
client.copy_model_version(
    src_model_uri="models:/mlflow-social-ai-staging@challenger",
    dst_name="mlflow-social-ai-production",
)
```

Notice that we idenfitied the challenger model by appending the `@challenger` suffix to the model name. This code snippet copies the challenger model version to production.

## Conclusion

In this guide, we have walked through the complete lifecycle of a GenAI project, using MLflow for tracing, evaluation, model management, and deployment. Let's briefly review what we covered.

1. **Part 1: Experimentation and Prototyping**
    - Used MLflow tracing to capture and learn from informal experiments
    - Compared models from different providers and tagged their respective traces
2. **Part 2: Evaluation and Comparison**
    - Formulated questions and hypotheses about model performance
    - Developed a rigorous evaluation framework to test hypotheses
    - Used custom and built-in MLflow metrics to compare models quantitatively
3. **Part 3: Custom Model Setup**
    - Set up a custom PyFunc model with external configuration
    - Integrated tracing into the custom model, with the ability to activate and deactivate tracing via environment variables
4. **Part 4: Model Deployment**
    - Deployed the model to staging and production
    - Used model aliases to manage multiple versions of the same model
    - Used traces to generate evaluation data
    - Evaluated a challenger model against the champion model, and promoted the challenger model to production

Throughout this process, we've seen how MLflow provides the tools needed to manage the complexity of GenAI applications:

- Tracing gives us visibility into model behavior
- Evaluation metrics help us make data-driven decisions
- Custom models let us standardize our application logic
- The model registry helps us manage different versions and environments

While this guide used a simplified deployment setup for illustration, the patterns and practices demonstrated here can be adapted to more complex production environments with multiple models, automated deployment pipelines, and sophisticated monitoring systems.

## Further Reading

For more information on using MLflow for GenAI, see the following resources:

- [MLflow tracing for LLM Observability](https://mlflow.org/docs/latest/llms/tracing/index.html)
- [MLflow LLM Evaluation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)
- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [Deploy and Organize models with aliases](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags)
- [Serving an MLflow model from Model Registry](https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry)
