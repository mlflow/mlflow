---
description: MLflow Tracing is a feature that enables LLM observability in your apps. MLflow automatically logs traces for LangChain, LlamaIndex, and more.
---

import { APILink } from "@site/src/components/APILink";
import TOCInline from "@theme/TOCInline";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Introduction to MLflow Tracing

{/* WIP */}
{/* Missing styes in logos at the top of the document */}

:::note
MLflow Tracing is currently in **Experimental Status** and is subject to change without deprecation warning or notification.
:::

<section>
  <div class="logo-grid">
    <a href="../langchain/autologging.html">
      <div class="logo-card">
        <img src="/images/logos/langchain-logo.png" alt="LangChain Logo" />
      </div>
    </a>
    <a href="../langchain/autologging.html">
      <div class="logo-card">
        <img src="/images/logos/langgraph-logo.png" alt="LangGraph Logo" />
      </div>
    </a>
    <a href="../llama-index/index.html##enable-tracing">
      <div class="logo-card">
        <img src="/images/logos/llamaindex-logo.svg" alt="LlamaIndex Logo" />
      </div>
    </a>
    <a href="#automatic-tracing">
      <div class="logo-card">
        <img src="/images/logos/dspy-logo.png" alt="DSPy Logo" />
      </div>
    </a>
    <a href="../openai/autologging.html">
      <div class="logo-card">
        <img src="/images/logos/openai-logo.png" alt="OpenAI Logo" />
      </div>
    </a>
    <a href="../openai/autologging.html#auto-tracing-for-openai-swarm">
      <div class="logo-card">
        <img
          src="/images/logos/openai-swarm-logo.png"
          alt="OpenAI Swarm Logo"
        />
      </div>
    </a>
    <a href="#automatic-tracing">
      <div class="logo-card">
        <img src="/images/logos/autogen-logo.svg" alt="AutoGen Logo" />
      </div>
    </a>
  </div>
</section>
<br />

MLflow Tracing is a feature that enhances LLM observability in your Generative
AI (GenAI) applications by capturing detailed information about the execution of
your application's services. Tracing provides a way to record the inputs,
outputs, and metadata associated with each intermediate step of a request,
enabling you to easily pinpoint the source of bugs and unexpected behaviors.

MLflow offers a number of different options to enable tracing of your GenAI applications.

- **Automated tracing**: MLflow provides fully automated integrations with various GenAI libraries such as LangChain, OpenAI, LlamaIndex, DSPy, AutoGen, and more that can be activated by simply enabling `mlflow.<library>.autolog()`.
- **Manual trace instrumentation with high-level fluent APIs**: Decorators, function wrappers and context managers via the fluent API allow you to add tracing functionality with minor code modifications.
- **Low-level client APIs for tracing**: The MLflow client API provides a thread-safe way to handle trace implementations, even in aysnchronous modes of operation.

To learn more about what tracing is, see our [Tracing Concepts Overview](./overview/) guide.

To explore the structure and schema of MLflow Tracing, please see the [Tracing Schema](./tracing-schema/) guide.

:::note
MLflow Tracing support is available with the **MLflow 2.14.0** release. Versions of MLflow prior to this release
do not contain the full set of features that are required for trace logging support.
:::

<TOCInline toc={toc} maxHeadingLevel={2} minHeadingLevel={2} />

## Automatic Tracing

:::info
Is your favorite library missing from the list? Consider [contributing to MLflow Tracing](/contribute) or [submitting a feature request](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=enhancement&projects=&template=feature_request_template.yaml&title=%5BFR%5D) to our Github repository.
:::

The easiest way to get started with MLflow Tracing is to leverage the built-in capabilities with MLflow's integrated libraries. MLflow provides automatic tracing capabilities for some of the integrated libraries such as
LangChain, OpenAI, LlamaIndex, and AutoGen. For these libraries, you can instrument your code with
just a single command `mlflow.<library>.autolog()` and MLflow will automatically log traces
for model/API invocations to the active MLflow Experiment.

<Tabs>
  <TabItem value="langchain" label="LangChain / LangGraph" default>
    ### LangChain Automatic Tracing

    As part of the LangChain autologging integration, traces are logged to the active MLflow Experiment when calling invocation APIs on chains. You can enable tracing
    for LangChain by calling the [`mlflow.langchain.autolog()`](/python_api/mlflow.langchain/#mlflow.langchain.autolog) function.
    ```python
    import mlflow

    mlflow.langchain.autolog()
    ```

    In the full example below, the model and its associated metadata will be logged as a run, while the traces are logged separately to the active experiment. To learn more, please visit [LangChain Autologging documentation](../langchain/autologging/).

    :::note
    This example has been confirmed working with the following requirement versions:
    ```shell
    pip install openai==1.30.5 langchain==0.2.1 langchain-openai==0.1.8 langchain-community==0.2.1 mlflow==2.14.0 tiktoken==0.7.0
    ```
    :::

    ```python
    import os

    from langchain.prompts import PromptTemplate
    from langchain_openai import OpenAI

    import mlflow

    assert (
        "OPENAI_API_KEY" in os.environ
    ), "Please set your OPENAI_API_KEY environment variable."

    # Using a local MLflow tracking server
    mlflow.set_tracking_uri("http://localhost:5000")

    # Create a new experiment that the model and the traces will be logged to
    mlflow.set_experiment("LangChain Tracing")

    # Enable LangChain autologging
    # Note that models and examples are not required to be logged in order to log traces.
    # Simply enabling autolog for LangChain via mlflow.langchain.autolog() will enable trace logging.
    mlflow.langchain.autolog(log_models=True, log_input_examples=True)

    llm = OpenAI(temperature=0.7, max_tokens=1000)

    prompt_template = (
        "Imagine that you are {person}, and you are embodying their manner of answering questions posed to them. "
        "While answering, attempt to mirror their conversational style, their wit, and the habits of their speech "
        "and prose. You will emulate them as best that you can, attempting to distill their quirks, personality, "
        "and habits of engagement to the best of your ability. Feel free to fully embrace their personality, whether "
        "aspects of it are not guaranteed to be productive or entirely constructive or inoffensive."
        "The question you are asked, to which you will reply as that person, is: {question}"
    )

    chain = prompt_template | llm

    # Test the chain
    chain.invoke(
        {
            "person": "Richard Feynman",
            "question": "Why should we colonize Mars instead of Venus?",
        }
    )

    # Let's test another call
    chain.invoke(
        {
            "person": "Linus Torvalds",
            "question": "Can I just set everyone's access to sudo to make things easier?",
        }
    )
    ```

    If we navigate to the MLflow UI, we can see not only the model that has been auto-logged, but the traces as well, as shown in the below video:

    ![LangChain Tracing via autolog](/images/llms/tracing/langchain-tracing.gif)

    :::note
    The example above is purposely simple (a simple chat completions demonstration) for purposes of brevity. In real-world scenarios involving complex
    RAG chains, the trace that is recorded by MLflow will be significantly more complex and verbose.
    :::

  </TabItem>
  <TabItem value="openai" label="OpenAI">
    ### OpenAI Automatic Tracing

    The MLflow OpenAI flavor's autologging feature has a direct integration with MLflow tracing. When OpenAI autologging is enabled with [`mlflow.openai.autolog()`](/python_api/openai/#mlflow.openai.autolog),
    usage of the OpenAI SDK will automatically record generated traces during interactive development.

    ```python
    import mlflow

    mlflow.openai.autolog()
    ```

    For example, the code below will log traces to the currently active experiment (in this case, the activated experiment `"OpenAI"`, set through the use of the [`mlflow.set_experiment()`](/python_api/mlflow/#mlflow.set_experiment) API).
    To learn more about OpenAI autologging, you can [view the documentation here](../openai/autologging/).

    ```python
    import os
    import openai
    import mlflow

    # Calling the autolog API will enable trace logging by default.
    mlflow.openai.autolog()

    mlflow.set_experiment("OpenAI")

    openai_client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    messages = [
        {
            "role": "user",
            "content": "How can I improve my resting metabolic rate most effectively?",
        }
    ]

    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.99,
    )

    print(response)
    ```

    The logged trace, associated with the `OpenAI` experiment, can be seen in the MLflow UI, as shown below:

    ![OpenAI Tracing](/images/llms/tracing/openai-tracing.png)

  </TabItem>
  <TabItem value="swarm" label="Swarm">
    ### OpenAI Swarm Automatic Tracing

    The MLflow OpenAI flavor supports automatic tracing for [Swarm](https://github.com/openai/swarm), a multi-agent orchestration
    framework from OpenAI. To enable tracing for **Swarm**, just call [`mlflow.openai.autolog()`](/python_api/openai/#mlflow.openai.autolog)
    before running your multi-agent interactions. MLflow will trace all LLM interactions, tool calls, and agent operations automatically.

    ```python
    import mlflow

    mlflow.openai.autolog()
    ```

    For example, the code below will run the simplest example of multi-agent interaction using OpenAI Swarm.

    ```python
    import mlflow
    from swarm import Swarm, Agent

    # Calling the autolog API will enable trace logging by default.
    mlflow.openai.autolog()

    mlflow.set_experiment("OpenAI Swarm")

    client = Swarm()


    def transfer_to_agent_b():
        return agent_b


    agent_a = Agent(
        name="Agent A",
        instructions="You are a helpful agent.",
        functions=[transfer_to_agent_b],
    )

    agent_b = Agent(
        name="Agent B",
        instructions="Only speak in Haikus.",
    )

    response = client.run(
        agent=agent_a,
        messages=[{"role": "user", "content": "I want to talk to agent B."}],
    )
    print(response)
    ```

    The logged trace, associated with the `OpenAI Swarm` experiment, can be seen in the MLflow UI, as shown below:

    ![OpenAI Swarm Tracing](/images/llms/tracing/openai-swarm-tracing.png)

  </TabItem>
  <TabItem value="llamaindex" label="LlamaIndex">
    ### LlamaIndex Automatic Tracing

    The MLflow LlamaIndex flavor's autologging feature has a direct integration with MLflow tracing. When LlamaIndex autologging is enabled with [`mlflow.llama_index.autolog()`](/python_api/mlflow.llama_index/#mlflow.llama_index.autolog), invocation of components
    such as LLMs, agents, and query/chat engines will automatically record generated traces during interactive development.

    ```python
    import mlflow

    mlflow.llama_index.autolog()
    ```

    To see the full example of tracing LlamaIndex, please visit [LLamaIndex Tracing documentation](../llama-index/##enable-tracing).

    ![LlamaIndex Tracing](/images/llms/llama-index/llama-index-trace.png)

  </TabItem>
  <TabItem value="autogen" label="AutoGen">
    ### AutoGen Automatic Tracing

    MLflow Tracing ensures observability for your AutoGen application that involves complex multi-agent interactions. You can enable auto-tracing by calling [`mlflow.autogen.autolog()`](/python_api/mlflow.autogen/#mlflow.autogen.autolog),
    then the internal steps of the agents chat session will be logged to the active MLflow Experiment.

    ```python
    import mlflow

    mlflow.autogen.autolog()
    ```

    To see the full example of tracing AutoGen, please refer to the [AutoGen Tracing example](https://github.com/mlflow/mlflow/tree/master/examples/autogen/tracing.py).

    ![AutoGen Tracing](/images/llms/autogen/autogen-trace.png)

  </TabItem>
</Tabs>

## Tracing Fluent APIs

MLflow's [`fluent APIs`](/python_api/mlflow#mlflow.start_span) provide a straightforward way to add tracing to your functions and code blocks.
By using decorators, function wrappers, and context managers, you can easily capture detailed trace data with minimal code changes.

As a comparison between the fluent and the client APIs for tracing, the figure below illustrates the differences in complexity between the two APIs,
with the fluent API being more concise and the recommended approach if your tracing use case can support using the higher-level APIs.

<img
  src="/images/llms/tracing/fluent-vs-client-tracing.png"
  alt="Fluent vs Client APIs"
  style={{ display: "block", height: "auto", margin: "auto", width: "60%" }}
/>

This section will cover how to initiate traces using these fluent APIs.

### Initiating a Trace

In this section, we will explore different methods to initiate a trace using MLflow's fluent APIs. These methods allow you to add tracing
functionality to your code with minimal modifications, enabling you to capture detailed information about the execution of your functions and workflows.

#### Trace Decorator

The trace decorator allows you to automatically capture the inputs and outputs of a function by simply adding the [`@mlflow.trace`](/python_api/mlflow#mlflow.trace) decorator
to its definition. This approach is ideal for quickly adding tracing to individual functions without significant changes to your existing code.

```python
import mlflow

# Create a new experiment to log the trace to
mlflow.set_experiment("Tracing Demo")


# Mark any function with the trace decorator to automatically capture input(s) and output(s)
@mlflow.trace
def some_function(x, y, z=2):
    return x + (y - z)


# Invoking the function will generate a trace that is logged to the active experiment
some_function(2, 4)
```

You can add additional metadata to the tracing decorator as follows:

```python
@mlflow.trace(name="My Span", span_type="func", attributes={"a": 1, "b": 2})
def my_func(x, y):
    return x + y
```

When adding additional metadata to the trace decorator constructor, these additional components will be logged along with the span entry within
the trace that is stored within the active MLflow experiment.

Since MLflow 2.16.0, the trace decorator also supports async functions:

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()


@mlflow.trace
async def async_func(message: str):
    return await client.chat.completion.create(
        model="gpt-4o", messages=[{"role": "user", "content": message}]
    )


await async_func("What is MLflow Tracing?")
```

#### What is captured?

If we navigate to the MLflow UI, we can see that the trace decorator automatically captured the following information, in addition to the basic
metadata associated with any span (start time, end time, status, etc):

- **Inputs**: In the case of our decorated function, this includes the state of all input arguments (including the default `z` value that is applied).
- **Response**: The output of the function is also captured, in this case the result of the addition and subtraction operations.
- **Trace Name**: The name of the decorated function.

![Trace UI - simple use case](/images/llms/tracing/trace-demo-1.png)

#### Error Handling with Traces

If an `Exception` is raised during processing of a trace-instrumented operation, an indication will be shown within the UI that the invocation was not
successful and a partial capture of data will be available to aid in debugging. Additionally, details about the Exception that was raised will be included
within the `events` attribute of the partially completed span, further aiding the identification of where issues are occuring within your code.

An example of a trace that has been recorded from code that raised an Exception is shown below:

```python
# This will raise an AttributeError exception
do_math(3, 2, "multiply")
```

![Trace Error](/images/llms/tracing/trace-error.png)

#### How to handle parent-child relationships

When using the trace decorator, each decorated function will be treated as a separate span within the trace. The relationship between dependent function calls
is handled directly through the native call excecution order within Python. For example, the following code will introduce two "child" spans to the main
parent span, all using decorators.

```python
import mlflow


@mlflow.trace(span_type="func", attributes={"key": "value"})
def add_1(x):
    return x + 1


@mlflow.trace(span_type="func", attributes={"key1": "value1"})
def minus_1(x):
    return x - 1


@mlflow.trace(name="Trace Test")
def trace_test(x):
    step1 = add_1(x)
    return minus_1(step1)


trace_test(4)
```

If we look at this trace from within the MLflow UI, we can see the relationship of the call order shown in the structure of the trace.

![Trace Decorator](/images/llms/tracing/trace-decorator.gif)

#### Span Type

Span types are a way to categorize spans within a trace. By default, the span type is set to `"UNKNOWN"` when using the trace decorator. MLflow provides a set of predefined span types for common use cases, while also allowing you to setting custom span types.

The following span types are available:

| **Span Type**  | **Description**                                                                        |
| -------------- | -------------------------------------------------------------------------------------- |
| `"LLM"`        | Represents a call to an LLM endpoint or a local model.                                 |
| `"CHAT_MODEL"` | Represents a query to a chat model. This is a special case of an LLM interaction.      |
| `"CHAIN"`      | Represents a chain of operations.                                                      |
| `"AGENT"`      | Represents an autonomous agent operation.                                              |
| `"TOOL"`       | Represents a tool execution (typically by an agent), such as querying a search engine. |
| `"EMBEDDING"`  | Represents a text embedding operation.                                                 |
| `"RETRIEVER"`  | Represents a context retrieval operation, such as querying a vector database.          |
| `"PARSER"`     | Represents a parsing operation, transforming text into a structured format.            |
| `"RERANKER"`   | Represents a re-ranking operation, ordering the retrieved contexts based on relevance. |
| `"UNKNOWN"`    | A default span type that is used when no other span type is specified.                 |

To set a span type, you can pass the `span_type` parameter to the [`@mlflow.trace`](/python_api/mlflow#mlflow.trace) decorator or [`mlflow.start_span`](python_api/mlflow/#mlflow.start_span) context manager. When you are using [automatic tracing](#automatic-tracing), the span type is automatically set by MLflow.

```python
import mlflow
from mlflow.entities import SpanType


# Using a built-in span type
@mlflow.trace(span_type=SpanType.RETRIEVER)
def retrieve_documents(query: str):
    ...


# Setting a custom span type
with mlflow.start_span(name="add", span_type="MATH") as span:
    span.set_inputs({"x": z, "y": y})
    z = x + y
    span.set_outputs({"z": z})

    print(span.span_type)
    # Output: MATH
```

#### Context Handler

The context handler provides a way to create nested traces or spans, which can be useful for capturing complex interactions within your code.
By using the [`mlflow.start_span()`](/python_api/mlflow#mlflow.start_span) context manager, you can group multiple traced functions under a single parent span, making it easier to understand
the relationships between different parts of your code.

The context handler is recommended when you need to refine the scope of data capture for a given span. If your code is logically constructed such that
individual calls to services or models are contained within functions or methods, on the other hand, using the decorator approach is more straight-forward
and less complex.

```python
import mlflow


@mlflow.trace
def first_func(x, y=2):
    return x + y


@mlflow.trace
def second_func(a, b=3):
    return a * b


def do_math(a, x, operation="add"):
    # Use the fluent API context handler to create a new span
    with mlflow.start_span(name="Math") as span:
        # Specify the inputs and attributes that will be associated with the span
        span.set_inputs({"a": a, "x": x})
        span.set_attributes({"mode": operation})

        # Both of these functions are decorated for tracing and will be associated
        # as 'children' of the parent 'span' defined with the context handler
        first = first_func(x)
        second = second_func(a)

        result = None

        if operation == "add":
            result = first + second
        elif operation == "subtract":
            result = first - second
        else:
            raise ValueError(f"Unsupported Operation Mode: {operation}")

        # Specify the output result to the span
        span.set_outputs({"result": result})

        return result
```

When calling the `do_math` function, a trace will be generated that has the root span (parent) defined as the
context handler `with mlflow.start_span():` call. The `first_func` and `second_func` calls will be associated as child spans
to this parent span due to the fact that they are both decorated functions (having `@mlflow.trace` decorated on the function definition).

Running the following code will generate a trace.

```python
do_math(8, 3, "add")
```

This trace can be seen within the MLflow UI:

![Trace within the MLflow UI](/images/llms/tracing/trace-view.png)

#### Function wrapping

Function wrapping provides a flexible way to add tracing to existing functions without modifying their definitions. This is particularly useful when
you want to add tracing to third-party functions or functions defined outside of your control. By wrapping an external function with [`mlflow.trace()`](/python_api/mlflow/#mlflow.trace), you can
capture its inputs, outputs, and execution context.

```python
import math

import mlflow

mlflow.set_experiment("External Function Tracing")


def invocation(x, y=4, exp=2):
    # Initiate a context handler for parent logging
    with mlflow.start_span(name="Parent") as span:
        span.set_attributes({"level": "parent", "override": y == 4})
        span.set_inputs({"x": x, "y": y, "exp": exp})

        # Wrap an external function instead of modifying
        traced_pow = mlflow.trace(math.pow)

        # Call the wrapped function as you would call it directly
        raised = traced_pow(x, exp)

        # Wrap another external function
        traced_factorial = mlflow.trace(math.factorial)

        factorial = traced_factorial(int(raised))

        # Wrap another and call it directly
        response = mlflow.trace(math.sqrt)(factorial)

        # Set the outputs to the parent span prior to returning
        span.set_outputs({"result": response})

        return response


for i in range(8):
    invocation(i)
```

The video below shows our external function wrapping runs within the MLflow UI. Note that

![External Function tracing](/images/llms/tracing/external-trace.gif)

## Tracing Client APIs

The MLflow client API provides a comprehensive set of thread-safe methods for manually managing traces. These APIs allow for fine-grained
control over tracing, enabling you to create, manipulate, and retrieve traces programmatically. This section will cover how to use these APIs
to manually trace a model, providing step-by-step instructions and examples.

### Starting a Trace

Unlike with the fluent API, the MLflow Trace Client API requires that you explicitly start a trace before adding child spans. This initial API call
starts the root span for the trace, providing a context request_id that is used for associating subsequent spans to the root span.

To start a new trace, use the <APILink fn="mlflow.client.MlflowClient.start_trace" /> method. This method creates a new trace and returns the root span object.
