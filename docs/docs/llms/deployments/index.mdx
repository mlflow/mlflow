import Link from "@docusaurus/Link";

# MLflow AI Gateway (Experimental)

:::warning
MLflow AI Gateway does not support Windows.
:::

The MLflow AI Gateway is a powerful tool designed to streamline the usage and management of
various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization.
It offers a high-level interface that simplifies the interaction with these services by providing
a unified endpoint to handle specific LLM related requests.

A major advantage of using the MLflow AI Gateway is its centralized management of API keys.
By storing these keys in one secure location, organizations can significantly enhance their
security posture by minimizing the exposure of sensitive API keys throughout the system. It also
helps to prevent exposing these keys within code or requiring end-users to manage keys safely.

The gateway server is designed to be flexible and adaptable, capable of easily defining and managing endpoints by updating the
configuration file. This enables the easy incorporation
of new LLM providers or provider LLM types into the system without necessitating changes to
applications that interface with the gateway server. This level of adaptability makes the MLflow AI Gateway
Service an invaluable tool in environments that require agility and quick response to changes.

This simplification and centralization of language model interactions, coupled with the added
layer of security for API key management, make the MLflow AI Gateway an ideal choice for
organizations that use LLMs on a regular basis.

## Tutorials and Guides

If you're interested in diving right in to a step by step guide that will get you up and running with the MLflow AI Gateway
as fast as possible, the guides below will be your best first stop.

<Link className="button button--primary" to="/llms/deployments/guides">
  <span>View the gateway server Getting Started Guide</span>
</Link>

## Quickstart \{#deployments-quickstart}

The following guide will assist you in getting up and running, using a 3-endpoint configuration to
OpenAI services for chat, completions, and embeddings.

### Step 1: Install the MLflow AI Gateway

First, you need to install the MLflow AI Gateway on your machine. You can do this using pip from PyPI or from the MLflow repository.

#### Installing from PyPI

```sh
pip install 'mlflow[genai]'
```

### Step 2: Set the OpenAI API Key(s) for each provider

The gateway server needs to communicate with the OpenAI API. To do this, it requires an API key.
You can create an API key from the OpenAI dashboard.

For this example, we're only connecting with OpenAI. If there are additional providers within the
configuration, these keys will need to be set as well.

Once you have the key, you can set it as an environment variable in your terminal:

```sh
export OPENAI_API_KEY=your_api_key_here
```

This sets a temporary session-based environment variable. For production use cases, it is advisable
to store this key in the `.bashrc` or `.zshrc` files so that the key doesn't have to be re-entered upon
system restart.

### Step 3: Create a gateway server Configuration File

Next, you need to create a gateway server configuration file. This is a YAML file where you specify the
endpoints that the MLflow AI Gateway should expose. Let's create a file with three endpoints using OpenAI as a provider: completions, chat, and embeddings.

For details about the configuration file's parameters (including parameters for other providers besides OpenAI), see the :ref:`deployments_configuration_details` section below.

```yaml
endpoints:
  - name: completions
    endpoint_type: llm/v1/completions
    model:
      provider: openai
      name: gpt-4o-mini
      config:
        openai_api_key: $OPENAI_API_KEY
    limit:
      renewal_period: minute
      calls: 10

  - name: chat
    endpoint_type: llm/v1/chat
    model:
      provider: openai
      name: gpt-4o-mini
      config:
        openai_api_key: $OPENAI_API_KEY

  - name: embeddings
    endpoint_type: llm/v1/embeddings
    model:
      provider: openai
      name: text-embedding-ada-002
      config:
        openai_api_key: $OPENAI_API_KEY
```

Save this file to a location on the system that is going to be running the MLflow AI Gateway.

### Step 4: Start the gateway server

You're now ready to start the gateway server!

Use the MLflow AI Gateway `start-server` command and specify the path to your configuration file:

```sh
mlflow gateway start --config-path config.yaml --port {port} --host {host} --workers {worker count}
```

The configuration file can also be set using the `MLFLOW_DEPLOYMENTS_CONFIG` environment variable:

```bash
export MLFLOW_DEPLOYMENTS_CONFIG=/path/to/config.yaml
```

If you do not specify the host, a localhost address will be used.

If you do not specify the port, port 5000 will be used.

The worker count for gunicorn defaults to 2 workers.

### Step 5: Access the Interactive API Documentation

The MLflow AI Gateway provides an interactive API documentation endpoint that you can use to explore
and test the exposed endpoints. Navigate to `http://{host}:{port}/` (or `http://{host}:{port}/docs`) in your browser to access it.

The docs endpoint allow for direct interaction with the endpoints and permits submitting actual requests to the
provider services by click on the "try it now" option within the endpoint definition entry.

### Step 6: Send Requests Using the Client API

See the [Client API](#deployments-client-api) section for further information.

### Step 7: Send Requests to Endpoints via REST API

You can now send requests to the exposed endpoints.
See the [REST examples](#deployments-rest-api) for guidance on request formatting.

### Step 8: Compare Provider Models

Here's an example of adding a new model from a provider to determine which model instance is better for a given use case.

Firstly, update the [MLflow AI Gateway config](#deployments-configuration) YAML file with the additional endpoint definition to test:

```yaml
endpoints:
  - name: completions
    endpoint_type: llm/v1/completions
    model:
      provider: openai
      name: gpt-4o-mini
      config:
        openai_api_key: $OPENAI_API_KEY
  - name: completions-gpt4
    endpoint_type: llm/v1/completions
    model:
      provider: openai
      name: gpt-4
      config:
        openai_api_key: $OPENAI_API_KEY
```

This updated configuration adds a new completions endpoint `completions-gpt4` while still preserving the original `completions`
endpoint that was configured with the `gpt-4o-mini` model.

Once the configuration file is updated, simply save your changes. The gateway server will automatically create the new endpoint with zero downtime.

If you no longer need an endpoint, you can delete it from the configuration YAML and save your changes. The gateway server will automatically remove the endpoint.

### Step 9: Use gateway server endpoints for model development

Now that you have created several gateway server endpoints, you can create MLflow Models that query these
endpoints to build application-specific logic using techniques like prompt engineering. For more
information, see [gateway server and MLflow Models](#deployments-mlflow-models)`.

## Concepts \{#deployments-concepts}

There are several concepts that are referred to within the MLflow AI Gateway APIs, the configuration definitions, examples, and documentation.
Becoming familiar with these terms will help to simplify both configuring new endpoints and using the MLflow AI Gateway APIs.

### Providers \{#deployments-providers}

The MLflow AI Gateway is designed to support a variety of model providers.
A provider represents the source of the machine learning models, such as OpenAI, Anthropic, and so on.
Each provider has its specific characteristics and configurations that are encapsulated within the model part of an endpoint in the MLflow AI Gateway.

#### Supported Providers

The table below presents supported corresponding endpoint type for each LLM provider within the MLflow AI Gateway.
Note that ✅ mark does not mean all models from the provider are compatible with the endpoint types. For example, OpenAI provider supports all three endpoint types, but the model `gpt-4` is only compatible with the `llm/v1/chat` endpoint types.

<table>
  <thead>
    <tr>
      <th>Provider</th>
      <th>llm/v1/completions</th>
      <th>llm/v1/chat</th>
      <th>llm/v1/embeddings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OpenAI §</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>Azure OpenAI</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>MosaicML</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>Anthropic</td>
      <td>✅</td>
      <td>✅</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>Cohere</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>PaLM</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>MLflow</td>
      <td>✅\*</td>
      <td>✅\*</td>
      <td>✅\*\*</td>
    </tr>
    <tr>
      <td>HuggingFace TGI</td>
      <td>❌</td>
      <td>✅</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>AI21 Labs</td>
      <td>✅</td>
      <td>❌</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>Amazon Bedrock</td>
      <td>✅</td>
      <td>✅</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>Mistral</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>TogetherAI</td>
      <td>✅</td>
      <td>✅</td>
      <td>✅</td>
    </tr>
  </tbody>
</table>

§ For full compatibility references for `OpenAI`, see the [OpenAI Model Compatibility Matrix](https://platform.openai.com/docs/models/model-endpoint-compatibility).

Within each model block in the configuration file, the provider field is used to specify the name
of the provider for that model. This is a string value that needs to correspond to a provider the MLflow AI Gateway supports.

:::note
_\*_ MLflow Model Serving will only work for chat or completions if the output return is in an endpoint-compatible format. The
response must conform to either an output of `{"predictions": str}` or `{"predictions": {"candidates": str}}`. Any complex return type from a model that
does not conform to these structures will raise an exception at query time.

_\*\*_ Embeddings support is only available for models whose response signatures conform to the structured format of `{"predictions": List[float]}`
or `{"predictions": List[List[float]]}`. Any other return type will raise an exception at query time. `FeatureExtractionPipeline` in `transformers` and
models using the `sentence_transformers` flavor will return the correct data structures for the embeddings endpoint.
:::

Here's an example of a provider configuration within an endpoint:

```yaml
endpoints:
  - name: chat
    endpoint_type: llm/v1/chat
    model:
      provider: openai
      name: gpt-4
      config:
        openai_api_key: $OPENAI_API_KEY
    limit:
      renewal_period: minute
      calls: 10
```

In the above configuration, `openai` is the _provider_ for the model.

As of now, the MLflow AI Gateway supports the following providers:

- **mosaicml**: This is used for models offered by [MosaicML](https://docs.mosaicml.com/en/latest/).
- **openai**: This is used for models offered by [OpenAI](https://platform.openai.com/) and the [Azure](https://learn.microsoft.com/en-gb/azure/cognitive-services/openai/) integrations for Azure OpenAI and Azure OpenAI with AAD.
- **anthropic**: This is used for models offered by [Anthropic](https://docs.anthropic.com/claude/docs).
- **cohere**: This is used for models offered by [Cohere](https://docs.cohere.com/docs).
- **palm**: This is used for models offered by [PaLM](https://developers.generativeai.google/api/rest/generativelanguage/models/).
- **huggingface text generation inference**: This is used for models deployed using [Huggingface Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index).
- **ai21labs**: This is used for models offered by [AI21 Labs](https://studio.ai21.com/foundation-models).
- **bedrock**: This is used for models offered by [Amazon Bedrock](https://aws.amazon.com/bedrock/).
- **mistral**: This is used for models offered by [Mistral](https://docs.mistral.ai/).
- **togetherai**: This is used for models offered by [TogetherAI](https://docs.together.ai/docs/).

More providers are being added continually. Check the latest version of the MLflow AI Gateway Docs for the
most up-to-date list of supported providers.

If you would like to use a LLM model that is not offered by the above providers, or if you
would like to integrate a private LLM model, you can create a [provider plugin](#deployments-plugin)
to integrate with the MLflow AI Gateway.

## Endpoints \{#deployments-endpoints}

_Endpoints_ are central to how the MLflow AI Gateway functions. Each endpoint acts as a proxy endpoint for the
user, forwarding requests to the underlying [Models](#deployments-models) and [providers](#deployments-providers) specified in the configuration file.

an endpoint in the MLflow AI Gateway consists of the following fields:

- **name**: This is the unique identifier for the endpoint. This will be part of the URL when making API calls via the MLflow AI Gateway.
- **type**: The type of the endpoint corresponds to the type of language model interaction you desire. For instance, `llm/v1/completions` for text completion operations, `llm/v1/embeddings` for text embeddings, and `llm/v1/chat` for chat operations.
- **model**: Defines the model to which this endpoint will forward requests. The model contains the following details:
  - **provider**: Specifies the name of the [provider](#deployments-providers) for this model. For example, `openai` for OpenAI's `GPT-4o` models.
  - **name**: The name of the model to use. For example, `gpt-4o-mini` for OpenAI's `GPT-4o-Mini` model.
  - **config**: Contains any additional configuration details required for the model. This includes specifying the API base URL and the API key.
- **limit**: Specify the rate limit setting this endpoint will follow. The limit field contains the following fields:
  - **renewal_period**: The time unit of the rate limit, one of [second|minute|hour|day|month|year].
  - **calls**: The number of calls this endpoint will accept within the specified time unit.

Here's an example of an endpoint configuration:

```yaml
endpoints:
  - name: completions
    endpoint_type: llm/v1/chat
    model:
      provider: openai
      name: gpt-4o-mini
      config:
        openai_api_key: $OPENAI_API_KEY
    limit:
      renewal_period: minute
      calls: 10
```

In the example above, a request sent to the completions endpoint would be forwarded to the
`gpt-4o-mini` model provided by `openai`.

The endpoints in the configuration file can be updated at any time, and the MLflow AI Gateway will
automatically update its available endpoints without requiring a restart. This feature provides you
with the flexibility to add, remove, or modify endpoints as your needs change. It enables 'hot-swapping'
of endpoints, providing a seamless experience for any applications or services that interact with the MLflow AI Gateway.

When defining endpoints in the configuration file, ensure that each name is unique to prevent conflicts.
Duplicate endpoint names will raise an `MlflowException`.

## Models \{#deployments-models}

The `model` section within an `endpoint` specifies which model to use for generating responses.
This configuration block needs to contain a `name` field which is used to specify the exact model instance to be used.
Additionally, a [provider](#deployments-providers) needs to be specified, one that you have an authenticated access api key for.

Different endpoint types are often associated with specific models.
For instance, the `llm/v1/chat` and `llm/v1/completions` endpoints are generally associated with
conversational models, while `llm/v1/embeddings` endpoints would typically be associated with
embedding or transformer models. The model you choose should be appropriate for the type of endpoint specified.

Here's an example of a model name configuration within an endpoint:

```yaml
endpoints:
  - name: embeddings
    endpoint_type: llm/v1/embeddings
    model:
      provider: openai
      name: text-embedding-ada-002
      config:
        openai_api_key: $OPENAI_API_KEY
```

In the above configuration, `text-embedding-ada-002` is the model used for the embeddings endpoint.

When specifying a model, it is critical that the provider supports the model you are requesting.
For instance, `openai` as a provider supports models like `text-embedding-ada-002`, but other providers
may not. If the model is not supported by the provider, the MLflow AI Gateway will return an HTTP 4xx error
when trying to route requests to that model.

:::warning important
Always check the latest documentation of the specified provider to ensure that the model you want
to use is supported for the type of endpoint you're configuring.
:::

Remember, the model you choose directly affects the results of the responses you'll get from the
API calls. Therefore, choose a model that fits your use-case requirements. For instance,
for generating conversational responses, you would typically choose a chat model.
Conversely, for generating embeddings of text, you would choose an embedding model.

## Configuring the gateway server \{#deployments-configuration}

The MLflow AI Gateway relies on a user-provided configuration file, written in YAML,
that defines the endpoints and providers available to the server. The configuration file dictates
how the gateway server interacts with various language model providers and determines the end-points that
users can access.

### AI Gateway server Configuration

The configuration file includes a series of sections, each representing a unique endpoint.
Each endpoint section has a name, a type, and a model specification, which includes the model
provider, name, and configuration details. The configuration section typically contains the base
URL for the API and an environment variable for the API key.

Here is an example of a single-endpoint configuration:

```yaml
endpoints:
  - name: chat
    endpoint_type: llm/v1/chat
    model:
      provider: openai
      name: gpt-4o-mini
      config:
        openai_api_key: $OPENAI_API_KEY
    limit:
      renewal_period: minute
      calls: 10
```

In this example, we define an endpoint named `chat` that corresponds to the `llm/v1/chat` type, which
will use the `gpt-4o-mini` model from OpenAI to return query responses from the OpenAI service, and accept up to 10 requests per minute.

The MLflow AI Gateway configuration is very easy to update.
Simply edit the configuration file and save your changes, and the MLflow AI Gateway will automatically
update the endpoints with zero disruption or down time. This allows you to try out new providers or model types while keeping your applications steady and reliable.

In order to define an API key for a given provider, there are three primary options:

1. Directly include it in the YAML configuration file.
2. Use an environment variable to store the API key and reference it in the YAML configuration file.
3. Define your API key in a file and reference the location of that key-bearing file within the YAML configuration file.

If you choose to include the API key directly, replace `$OPENAI_API_KEY` in the YAML file with your
actual API key.

:::warning
The MLflow AI Gateway provides direct access to billed external LLM services. It is strongly recommended to restrict access to this server. See the section on [security](#deployments-security) for guidance.
:::

If you prefer to use an environment variable (recommended), you can define it in your shell
environment. For example:

```bash
export OPENAI_API_KEY="your_openai_api_key"
```

**Note:** Replace "your_openai_api_key" with your actual OpenAI API key.

#### AI Gateway server Configuration Details \{#deployments-configuration-details}

The MLflow AI Gateway relies on a user-provided configuration file. It defines how the gateway server interacts with various language model providers and dictates the endpoints that users can access.

The configuration file is written in YAML and includes a series of sections, each representing a unique endpoint. Each endpoint section has a name, a type, and a model specification,
which includes the provider, model name, and provider-specific configuration details.

Here are the details of each configuration parameter:

##### General Configuration Parameters

- **endpoints**: This is a list of endpoint configurations. Each endpoint represents a unique endpoint that maps to a particular language model service.

Each endpoint has the following configuration parameters:

- **name**: This is the name of the endpoint. It needs to be a unique name without spaces or any non-alphanumeric characters other than hyphen and underscore.
- **endpoint_type**: This specifies the type of service offered by this endpoint. This determines the interface for inputs to an endpoint and the returned outputs. Current supported endpoint types are:
  - "llm/v1/completions"
  - "llm/v1/chat"
  - "llm/v1/embeddings"
- **model**: This defines the provider-specific details of the language model. It contains the following fields:
  - **provider**: This indicates the provider of the AI model. It accepts the following values:
    - "openai"
    - "mosaicml"
    - "anthropic"
    - "cohere"
    - "palm"
    - "azure" / "azuread"
    - "mlflow-model-serving"
    - "huggingface-text-generation-inference"
    - "ai21labs"
    - "bedrock"
    - "mistral"
    - "togetherai"
  - **name**: This is an optional field to specify the name of the model.
  - **config**: This contains provider-specific configuration details.

##### Provider-Specific Configuration Parameters

###### OpenAI

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**openai_api_key**</td>
      <td>Yes</td>
      <td></td>
      <td>This is the API key for the OpenAI service.</td>
    </tr>
    <tr>
      <td>**openai_api_type**</td>
      <td>No</td>
      <td></td>
      <td>This is an optional field to specify the type of OpenAI API to use.</td>
    </tr>
    <tr>
      <td>**openai_api_base**</td>
      <td>No</td>
      <td>_https://api.openai.com/v1_</td>
      <td>This is the base URL for the OpenAI API.</td>
    </tr>
    <tr>
      <td>**openai_api_version**</td>
      <td>No</td>
      <td></td>
      <td>This is an optional field to specify the OpenAI API version.</td>
    </tr>
    <tr>
      <td>**openai_organization**</td>
      <td>No</td>
      <td></td>
      <td>This is an optional field to specify the organization in OpenAI.</td>
    </tr>
  </tbody>
</table>

###### MosaicML

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**mosaicml_api_key**</td>
      <td>Yes</td>
      <td>N/A</td>
      <td>This is the API key for the MosaicML service.</td>
    </tr>
  </tbody>
</table>

###### Cohere

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**cohere_api_key**</td>
      <td>Yes</td>
      <td>N/A</td>
      <td>This is the API key for the Cohere service.</td>
    </tr>
  </tbody>
</table>

###### HuggingFace Text Generation Inference

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**hf_server_url**</td>
      <td>Yes</td>
      <td>N/A</td>
      <td>This is the url of the Huggingface TGI Server.</td>
    </tr>
  </tbody>
</table>

###### PaLM

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**palm_api_key**</td>
      <td>Yes</td>
      <td>N/A</td>
      <td>This is the API key for the PaLM service.</td>
    </tr>
  </tbody>
</table>

###### AI21 Labs

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**ai21labs_api_key**</td>
      <td>Yes</td>
      <td>N/A</td>
      <td>This is the API key for the AI21 Labs service.</td>
    </tr>
  </tbody>
</table>

###### Anthropic

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**anthropic_api_key**</td>
      <td>Yes</td>
      <td>N/A</td>
      <td>This is the API key for the Anthropic service.</td>
    </tr>
  </tbody>
</table>

###### Amazon Bedrock

Top-level model configuration for Amazon Bedrock endpoints must be one of the following two supported authentication modes: _key-based_ or _role-based_.

<table>
  <thead>
    <tr>
      <th>Configuration Parameter</th>
      <th>Required</th>
      <th>Default</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**aws_config**</td>
      <td>No</td>
      <td></td>
      <td>An object with either the key-based or role-based schema below.</td>
    </tr>
  </tbody>
</table>

To use key-based authentication, define an Amazon Bedrock endpoint with the required fields below.

:::note
If using a configured endpoint purely for development or testing, utilizing an IAM User role or a temporary short-lived standard IAM role are recommended;
while for production deployments, a standard long-expiry IAM role is recommended to ensure that the endpoint is capable of handling authentication for a long period.
If the authentication expires and a new set of keys need to be supplied, the endpoint must be recreated in order to persist the new keys.
:::
