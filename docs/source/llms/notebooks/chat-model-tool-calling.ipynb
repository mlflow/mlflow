{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tool-calling model with `mlflow.pyfunc.ChatModel`\n",
    "\n",
    "<a href=\"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/notebooks/chat-model-tool-calling.ipynb\" class=\"notebook-download-btn\"><i class=\"fas fa-download\"></i>Download this Notebook</a>\n",
    "\n",
    "Welcome to the notebook tutorial on building a simple tool calling model using the [`mlflow.pyfunc.ChatModel`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel) wrapper. ChatModel is a subclass of MLflow's highly customizable [PythonModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel), which was specifically designed to make GenAI workflows easier.\n",
    "\n",
    "Briefly, here are some of the benefits of using ChatModel:\n",
    "1. No need to define a complex signature! Chat models often accept complex inputs with many levels of nesting, and this can be cumbersome to define yourself.\n",
    "2. Support for JSON / dict inputs (no need to wrap inputs or convert to Pandas DataFrame)\n",
    "2. Dataclasses for expected inputs / outputs for a better development experience\n",
    "\n",
    "For a more in-depth exploration of ChatModel, please check out the [detailed guide](https://mlflow.org/docs/latest/llms/chat-model-guide/index.html).\n",
    "\n",
    "In this tutorial, we'll be building a simple OpenAI wrapper that makes use of the tool calling support (released in MLflow 2.17.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "First, let's set up the environment. We'll need the OpenAI Python SDK, as well as MLflow >= 2.17.0. We'll also need to set our OpenAI API key in order to use the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'mlflow>=2.17.0' openai -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creating the tool definition\n",
    "\n",
    "Since our model will be a tool calling model, we'll be defining a `get_weather` tool that we can pass to the OpenAI model to use. We do this by using [`mlflow.types.llm.FunctionToolDefinition`](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.FunctionToolDefinition) to describe the parameters that our tool accepts. The format of this dataclass is aligned with the OpenAI spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.types.llm import (\n",
    "    FunctionToolDefinition,\n",
    "    ParamProperty,\n",
    "    ToolParamsSchema,\n",
    ")\n",
    "\n",
    "# a sample tool definition. we use the `FunctionToolDefinition`\n",
    "# class to describe the name and expected params for the tool.\n",
    "# for this example, we're defining a simple tool that returns\n",
    "# the weather for a given city.\n",
    "weather_tool = FunctionToolDefinition(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get weather information\",\n",
    "    parameters=ToolParamsSchema(\n",
    "        {\n",
    "            \"city\": ParamProperty(\n",
    "                type=\"string\",\n",
    "                description=\"City name to get weather information for\",\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "    # make sure to call `to_tool_definition()` to convert the `FunctionToolDefinition`\n",
    "    # to a `ToolDefinition` object. this step is necessary to normalize the data format,\n",
    "    # as multiple types of tools (besides just functions) might be available in the future.\n",
    ").to_tool_definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implementing the tool\n",
    "\n",
    "Now that we have a definition for the tool, we need to actually implement it. For the purposes of this tutorial, we're just going to mock a response, but the implementation can be arbitraryâ€”you might make an API call to an actual weather service, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    return f\"It's sunny in {city}, with a temperature of 20C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Defining our model\n",
    "\n",
    "Now we're ready to create our model! We'll be subclassing `mlflow.pyfunc.ChatModel`, and this involves defining a `predict()` function that accepts the following arguments:\n",
    "\n",
    "1. `context`: [PythonModelContext](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModelContext) (not used in this tutorial)\n",
    "2. `messages`: List\\[[ChatMessage](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatMessage)\\]. This is the chat input that the model uses for generation.\n",
    "3. `params`: [ChatParams](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatParams). These are commonly used params used to configure the chat model, e.g. `temperature`, `max_tokens`, etc. This is where the tool specifications can be found.\n",
    "\n",
    "For the implementation, we'll simply forward the user's input to OpenAI, and provide the `get_weather` tool as an option for the LLM to use if it chooses to do so. If we receive a tool call request, we'll call the `get_weather()` function and return the response back to OpenAI. We'll need to use what we've defined in the previous two steps in order to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import mlflow\n",
    "from mlflow.types.llm import (\n",
    "    ChatMessage,\n",
    "    ChatParams,\n",
    "    ChatResponse,\n",
    "    FunctionToolDefinition,\n",
    "    ParamProperty,\n",
    "    ToolParamsSchema,\n",
    ")\n",
    "\n",
    "# replace this with your own MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "\n",
    "class WeatherModel(mlflow.pyfunc.ChatModel):\n",
    "    def __init__(self):\n",
    "        # from step 1 above, our tool definition\n",
    "        weather_tool = FunctionToolDefinition(\n",
    "            name=\"get_weather\",\n",
    "            description=\"Get weather information\",\n",
    "            parameters=ToolParamsSchema(\n",
    "                {\n",
    "                    \"city\": ParamProperty(\n",
    "                        type=\"string\",\n",
    "                        description=\"City name to get weather information for\",\n",
    "                    ),\n",
    "                }\n",
    "            ),\n",
    "        ).to_tool_definition()\n",
    "\n",
    "        # OpenAI expects tools to be provided as a list of dictionaries\n",
    "        self.tools = [weather_tool.to_dict()]\n",
    "\n",
    "    # from step 2 above, the implementation of the tool\n",
    "    def get_weather(self, city: str) -> str:\n",
    "        return \"It's sunny in {}, with a temperature of 20C\".format(city)\n",
    "\n",
    "    # the core method that needs to be implemented. this function\n",
    "    # will be called every time a user sends messages to our model\n",
    "    def predict(self, context, messages: List[ChatMessage], params: ChatParams):\n",
    "        # instantiate the OpenAI client\n",
    "        client = OpenAI()\n",
    "\n",
    "        # convert the messages to a format that the OpenAI API expects\n",
    "        messages = [m.to_dict() for m in messages]\n",
    "\n",
    "        # call the OpenAI API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            # pass the tools in the request\n",
    "            tools=self.tools,\n",
    "        )\n",
    "\n",
    "        # if OpenAI returns a tool_calling response, then we call\n",
    "        # our tool. otherwise, we just return the response as is\n",
    "        tool_calls = response.choices[0].message.tool_calls\n",
    "        if tool_calls:\n",
    "            print(\"Received a tool call, calling the weather tool...\")\n",
    "\n",
    "            # for this example, we only provide the model with one tool,\n",
    "            # so we can assume the tool call is for the weather tool. if\n",
    "            # we had more, we'd need to check the name of the tool that\n",
    "            # was called\n",
    "            city = json.loads(tool_calls[0].function.arguments)[\"city\"]\n",
    "            tool_call_id = tool_calls[0].id\n",
    "\n",
    "            # call the tool and construct a new chat message\n",
    "            tool_response = ChatMessage(\n",
    "                role=\"tool\", content=self.get_weather(city), tool_call_id=tool_call_id\n",
    "            ).to_dict()\n",
    "\n",
    "            # send another request to the API, making sure to append\n",
    "            # the assistant's tool call along with the tool response.\n",
    "            messages.append(response.choices[0].message)\n",
    "            messages.append(tool_response)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                tools=self.tools,\n",
    "            )\n",
    "\n",
    "        # return the result as a ChatResponse, as this\n",
    "        # is the expected output of the predict method\n",
    "        return ChatResponse.from_dict(response.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Logging the model\n",
    "\n",
    "Next, we need to log the model. This saves the model as an artifact in MLflow Tracking, and allows us to load and serve it later on.\n",
    "\n",
    "(Note: this is a fundamental pattern in MLflow. To learn more, check out the [Quickstart guide](https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html)!)\n",
    "\n",
    "In order to do this, we need to do a few things:\n",
    "\n",
    "1. Define an input example to inform users about the input we expect\n",
    "2. Instantiate the model\n",
    "3. Call `mlflow.pyfunc.log_model()` with the above as arguments\n",
    "\n",
    "Of course, since this is a tool calling tutorial, we'll have to include an example of how to define a tool call. This is handled by instantiating a [FunctionToolDefinition](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.FunctionToolDefinition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/25 11:39:22 INFO mlflow.pyfunc: Predicting on input example to validate output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received a tool call, calling the weather tool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.lok/miniconda3/envs/test/lib/python3.9/site-packages/mlflow/models/utils.py:523: FutureWarning: Since MLflow 2.16.0, we no longer convert dictionary input example to pandas Dataframe, and directly save it as a json object. If the model expects a pandas DataFrame input instead, please pass the pandas DataFrame as input example directly.\n",
      "  example = _Example(input_example)\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 216.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received a tool call, calling the weather tool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/25 11:39:29 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run gaudy-rook-875 at: http://localhost:5000/#/experiments/0/runs/567216afacbe4cd9bf2fcbe019e71b5b.\n",
      "2024/10/25 11:39:29 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "# messages to use as input examples\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please use the provided tools to answer user queries.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in Singapore?\"},\n",
    "]\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": messages,\n",
    "}\n",
    "\n",
    "# instantiate the model\n",
    "model = WeatherModel()\n",
    "\n",
    "# log the model\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"weather-model\",\n",
    "        python_model=model,\n",
    "        input_example=input_example,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for generations\n",
    "\n",
    "Now that the model is logged, our work is more or less done! In order to use the model for predictions, let's load it back using `mlflow.pyfunc.load_model()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 160.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received a tool call, calling the weather tool...\n",
      "The weather in Singapore is sunny, with a temperature of 20Â°C.\n",
      "Received a tool call, calling the weather tool...\n",
      "The weather in San Francisco is sunny, with a temperature of 20Â°C.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the previously logged ChatModel\n",
    "tool_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Please use the provided tools to answer user queries.\",\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in Singapore?\"},\n",
    "]\n",
    "\n",
    "# Call the model's predict method\n",
    "response = tool_model.predict({\"messages\": messages})\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "messages = [\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"},\n",
    "]\n",
    "\n",
    "# Generating another response\n",
    "response = tool_model.predict({\"messages\": messages})\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving the model\n",
    "\n",
    "MLflow also allows you to serve models, using the `mlflow models serve` CLI tool. In another terminal shell, run the following (you might have to set the `MLFLOW_TRACKING_URI` environment variable to `http://localhost:5000`, or to wherever your tracking server is located).\n",
    "\n",
    "```sh\n",
    "$ export OPENAI_API_KEY=<YOUR OPENAI API KEY>\n",
    "$ mlflow models serve -m <MODEL_URI> -p 8000\n",
    "```\n",
    "\n",
    "This will start serving the model on `http://localhost:8000`, and the model can be queried via POST request to the `/invocations` route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'The weather in Tokyo is sunny, with a temperature of 20Â°C.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 100, 'completion_tokens': 16, 'total_tokens': 116},\n",
       " 'id': 'chatcmpl-AM5lXVHQqsuAzI1F0OoZx8dYZsogq',\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1729828743}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "messages = [\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"},\n",
    "]\n",
    "\n",
    "requests.post(\"http://localhost:8000/invocations\", json={\"messages\": messages}).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this tutorial, we covered how to use MLflow's `ChatModel` class to create a convenient OpenAI wrapper that supports tool calling. Though the use-case was simple, the concepts covered here can be easily extended to support more complex functionality.\n",
    "\n",
    "If you're looking to dive deeper into building quality GenAI apps, you might be also be interested in checking out [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html), an observability tool you can use to trace the execution of arbitrary functions (such as your tool calls, for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
