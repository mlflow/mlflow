

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/langchain -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.langchain</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/langchain.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.1.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/tabs.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.langchain</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/langchain" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.langchain</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The ``mlflow.langchain`` module provides an API for logging and loading LangChain models.</span>
<span class="sd">This module exports multivariate LangChain models in the langchain flavor and univariate</span>
<span class="sd">LangChain models in the pyfunc flavor:</span>

<span class="sd">LangChain (native) format</span>
<span class="sd">    This is the main flavor that can be accessed with LangChain APIs.</span>
<span class="sd">:py:mod:`mlflow.pyfunc`</span>
<span class="sd">    Produced for use by generic pyfunc-based deployment tools and for batch inference.</span>

<span class="sd">.. _LangChain:</span>
<span class="sd">    https://python.langchain.com/en/latest/index.html</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">importlib</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">cloudpickle</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">Version</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow</span> <span class="kn">import</span> <span class="n">pyfunc</span>
<span class="kn">from</span> <span class="nn">mlflow.exceptions</span> <span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span> <span class="nn">mlflow.langchain.databricks_dependencies</span> <span class="kn">import</span> <span class="n">_detect_databricks_dependencies</span>
<span class="kn">from</span> <span class="nn">mlflow.langchain.runnables</span> <span class="kn">import</span> <span class="n">_load_runnables</span><span class="p">,</span> <span class="n">_save_runnables</span>
<span class="kn">from</span> <span class="nn">mlflow.langchain.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_BASE_LOAD_KEY</span><span class="p">,</span>
    <span class="n">_MODEL_LOAD_KEY</span><span class="p">,</span>
    <span class="n">_RUNNABLE_LOAD_KEY</span><span class="p">,</span>
    <span class="n">_load_base_lcs</span><span class="p">,</span>
    <span class="n">_save_base_lcs</span><span class="p">,</span>
    <span class="n">_validate_and_prepare_lc_model_or_path</span><span class="p">,</span>
    <span class="n">lc_runnables_types</span><span class="p">,</span>
    <span class="n">patch_langchain_type_to_cls_dict</span><span class="p">,</span>
    <span class="n">register_pydantic_v1_serializer_cm</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">ModelInputExample</span><span class="p">,</span> <span class="n">ModelSignature</span><span class="p">,</span> <span class="n">get_model_info</span>
<span class="kn">from</span> <span class="nn">mlflow.models.dependencies_schemas</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_clear_dependencies_schemas</span><span class="p">,</span>
    <span class="n">_get_dependencies_schemas</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.models.model</span> <span class="kn">import</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">,</span> <span class="n">MODEL_CODE_PATH</span><span class="p">,</span> <span class="n">MODEL_CONFIG</span>
<span class="kn">from</span> <span class="nn">mlflow.models.resources</span> <span class="kn">import</span> <span class="n">DatabricksFunction</span><span class="p">,</span> <span class="n">_ResourceBuilder</span>
<span class="kn">from</span> <span class="nn">mlflow.models.signature</span> <span class="kn">import</span> <span class="n">_infer_signature_from_input_example</span>
<span class="kn">from</span> <span class="nn">mlflow.models.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_convert_llm_input_data</span><span class="p">,</span>
    <span class="n">_load_model_code_path</span><span class="p">,</span>
    <span class="n">_save_example</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.pyfunc</span> <span class="kn">import</span> <span class="n">FLAVOR_NAME</span> <span class="k">as</span> <span class="n">PYFUNC_FLAVOR_NAME</span>
<span class="kn">from</span> <span class="nn">mlflow.pyfunc.context</span> <span class="kn">import</span> <span class="n">get_prediction_context</span>
<span class="kn">from</span> <span class="nn">mlflow.tracing.provider</span> <span class="kn">import</span> <span class="n">trace_disabled</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking._model_registry</span> <span class="kn">import</span> <span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking.artifact_utils</span> <span class="kn">import</span> <span class="n">_download_artifact_from_uri</span>
<span class="kn">from</span> <span class="nn">mlflow.types.schema</span> <span class="kn">import</span> <span class="n">ColSpec</span><span class="p">,</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">Schema</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.annotations</span> <span class="kn">import</span> <span class="n">experimental</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.autologging_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">autologging_integration</span><span class="p">,</span>
    <span class="n">autologging_is_disabled</span><span class="p">,</span>
    <span class="n">safe_patch</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.databricks_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_in_databricks_model_serving_environment</span><span class="p">,</span>
    <span class="n">is_in_databricks_serverless</span><span class="p">,</span>
    <span class="n">is_mlflow_tracing_enabled_in_model_serving</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.docstring_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LOG_MODEL_PARAM_DOCS</span><span class="p">,</span>
    <span class="n">docstring_version_compatibility_warning</span><span class="p">,</span>
    <span class="n">format_docstring</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.environment</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_mlflow_conda_env</span><span class="p">,</span>
    <span class="n">_process_conda_env</span><span class="p">,</span>
    <span class="n">_process_pip_requirements</span><span class="p">,</span>
    <span class="n">_PythonEnv</span><span class="p">,</span>
    <span class="n">_validate_env_arguments</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.file_utils</span> <span class="kn">import</span> <span class="n">get_total_file_size</span><span class="p">,</span> <span class="n">write_to</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.model_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">,</span>
    <span class="n">_get_flavor_configuration</span><span class="p">,</span>
    <span class="n">_validate_and_copy_code_paths</span><span class="p">,</span>
    <span class="n">_validate_and_copy_file_to_directory</span><span class="p">,</span>
    <span class="n">_validate_and_get_model_config_from_file</span><span class="p">,</span>
    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.requirements_utils</span> <span class="kn">import</span> <span class="n">_get_pinned_requirement</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">FLAVOR_NAME</span> <span class="o">=</span> <span class="s2">&quot;langchain&quot;</span>
<span class="n">_MODEL_TYPE_KEY</span> <span class="o">=</span> <span class="s2">&quot;model_type&quot;</span>


<div class="viewcode-block" id="get_default_pip_requirements"><a class="viewcode-back" href="../../python_api/mlflow.langchain.html#mlflow.langchain.get_default_pip_requirements">[docs]</a><span class="k">def</span> <span class="nf">get_default_pip_requirements</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        A list of default pip requirements for MLflow Models produced by this flavor.</span>
<span class="sd">        Calls to :func:`save_model()` and :func:`log_model()` produce a pip environment</span>
<span class="sd">        that, at a minimum, contains these requirements.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pin pydantic and cloudpickle version as they are used in langchain</span>
    <span class="c1"># model saving and loading</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">_get_pinned_requirement</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;langchain&quot;</span><span class="p">,</span> <span class="s2">&quot;pydantic&quot;</span><span class="p">,</span> <span class="s2">&quot;cloudpickle&quot;</span><span class="p">]))</span></div>


<div class="viewcode-block" id="get_default_conda_env"><a class="viewcode-back" href="../../python_api/mlflow.langchain.html#mlflow.langchain.get_default_conda_env">[docs]</a><span class="k">def</span> <span class="nf">get_default_conda_env</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        The default Conda environment for MLflow Models produced by calls to</span>
<span class="sd">        :func:`save_model()` and :func:`log_model()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_mlflow_conda_env</span><span class="p">(</span><span class="n">additional_pip_deps</span><span class="o">=</span><span class="n">get_default_pip_requirements</span><span class="p">())</span></div>


<span class="k">def</span> <span class="nf">_get_databricks_serverless_env_vars</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the environment variables required to to initialize WorkspaceClient in a subprocess</span>
<span class="sd">    with serverless compute.</span>

<span class="sd">    Note:</span>
<span class="sd">        Databricks authentication related environment variables are set in the</span>
<span class="sd">        _capture_imported_modules function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">envs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="s2">&quot;SPARK_REMOTE&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">envs</span><span class="p">[</span><span class="s2">&quot;SPARK_LOCAL_REMOTE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SPARK_REMOTE&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Missing required environment variable `SPARK_LOCAL_REMOTE` or `SPARK_REMOTE`.&quot;</span>
            <span class="s2">&quot;These are necessary to initialize the WorkspaceClient with serverless compute in &quot;</span>
            <span class="s2">&quot;a subprocess in Databricks for UC function execution. Setting the value to &#39;true&#39;.&quot;</span>
        <span class="p">)</span>
        <span class="n">envs</span><span class="p">[</span><span class="s2">&quot;SPARK_LOCAL_REMOTE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
    <span class="k">return</span> <span class="n">envs</span>


<div class="viewcode-block" id="save_model"><a class="viewcode-back" href="../../python_api/mlflow.langchain.html#mlflow.langchain.save_model">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@format_docstring</span><span class="p">(</span><span class="n">LOG_MODEL_PARAM_DOCS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">package_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">))</span>
<span class="nd">@docstring_version_compatibility_warning</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="nd">@trace_disabled</span>  <span class="c1"># Suppress traces for internal predict calls while saving model</span>
<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span>
    <span class="n">lc_model</span><span class="p">,</span>
    <span class="n">path</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlflow_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">ModelSignature</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="p">:</span> <span class="n">ModelInputExample</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">loader_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">persist_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">example_no_conversion</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">streamable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save a LangChain model to a path on the local file system.</span>

<span class="sd">    Args:</span>
<span class="sd">        lc_model: A LangChain model, which could be a</span>
<span class="sd">            `Chain &lt;https://python.langchain.com/docs/modules/chains/&gt;`_,</span>
<span class="sd">            `Agent &lt;https://python.langchain.com/docs/modules/agents/&gt;`_,</span>
<span class="sd">            `retriever &lt;https://python.langchain.com/docs/modules/data_connection/retrievers/&gt;`_,</span>
<span class="sd">            or `RunnableSequence &lt;https://python.langchain.com/docs/modules/chains/foundational/sequential_chains#using-lcel&gt;`_,</span>
<span class="sd">            or a path containing the `LangChain model code &lt;https://github.com/mlflow/mlflow/blob/master/examples/langchain/chain_as_code_driver.py&gt;`</span>
<span class="sd">            for the above types. When using model as path, make sure to set the model</span>
<span class="sd">            by using :func:`mlflow.models.set_model()`.</span>

<span class="sd">            .. Note:: Experimental: Using model as path may change or be removed in a future</span>
<span class="sd">                        release without warning.</span>
<span class="sd">        path: Local path where the serialized model (as YAML) is to be saved.</span>
<span class="sd">        conda_env: {{ conda_env }}</span>
<span class="sd">        code_paths: {{ code_paths }}</span>
<span class="sd">        mlflow_model: :py:mod:`mlflow.models.Model` this flavor is being added to.</span>
<span class="sd">        signature: :py:class:`ModelSignature &lt;mlflow.models.ModelSignature&gt;`</span>
<span class="sd">            describes model input and output :py:class:`Schema &lt;mlflow.types.Schema&gt;`.</span>
<span class="sd">            If not specified, the model signature would be set according to</span>
<span class="sd">            `lc_model.input_keys` and `lc_model.output_keys` as columns names, and</span>
<span class="sd">            `DataType.string` as the column type.</span>
<span class="sd">            Alternatively, you can explicitly specify the model signature.</span>
<span class="sd">            The model signature can be :py:func:`inferred &lt;mlflow.models.infer_signature&gt;`</span>
<span class="sd">            from datasets with valid model input (e.g. the training dataset with target</span>
<span class="sd">            column omitted) and valid model output (e.g. model predictions generated on</span>
<span class="sd">            the training dataset), for example:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from mlflow.models import infer_signature</span>

<span class="sd">                chain = LLMChain(llm=llm, prompt=prompt)</span>
<span class="sd">                prediction = chain.run(input_str)</span>
<span class="sd">                input_columns = [</span>
<span class="sd">                    {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: input_key} for input_key in chain.input_keys</span>
<span class="sd">                ]</span>
<span class="sd">                signature = infer_signature(input_columns, predictions)</span>

<span class="sd">        input_example: {{ input_example }}</span>
<span class="sd">        pip_requirements: {{ pip_requirements }}</span>
<span class="sd">        extra_pip_requirements: {{ extra_pip_requirements }}</span>
<span class="sd">        metadata: {{ metadata }}</span>
<span class="sd">        loader_fn: A function that&#39;s required for models containing objects that aren&#39;t natively</span>
<span class="sd">            serialized by LangChain.</span>
<span class="sd">            This function takes a string `persist_dir` as an argument and returns the</span>
<span class="sd">            specific object that the model needs. Depending on the model,</span>
<span class="sd">            this could be a retriever, vectorstore, requests_wrapper, embeddings, or</span>
<span class="sd">            database. For RetrievalQA Chain and retriever models, the object is a</span>
<span class="sd">            (`retriever &lt;https://python.langchain.com/docs/modules/data_connection/retrievers/&gt;`_).</span>
<span class="sd">            For APIChain models, it&#39;s a</span>
<span class="sd">            (`requests_wrapper &lt;https://python.langchain.com/docs/modules/agents/tools/integrations/requests&gt;`_).</span>
<span class="sd">            For HypotheticalDocumentEmbedder models, it&#39;s an</span>
<span class="sd">            (`embeddings &lt;https://python.langchain.com/docs/modules/data_connection/text_embedding/&gt;`_).</span>
<span class="sd">            For SQLDatabaseChain models, it&#39;s a</span>
<span class="sd">            (`database &lt;https://python.langchain.com/docs/modules/agents/toolkits/sql_database&gt;`_).</span>
<span class="sd">        persist_dir: The directory where the object is stored. The `loader_fn`</span>
<span class="sd">            takes this string as the argument to load the object.</span>
<span class="sd">            This is optional for models containing objects that aren&#39;t natively</span>
<span class="sd">            serialized by LangChain. MLflow logs the content in this directory as</span>
<span class="sd">            artifacts in the subdirectory named `persist_dir_data`.</span>

<span class="sd">            Here is the code snippet for logging a RetrievalQA chain with `loader_fn`</span>
<span class="sd">            and `persist_dir`:</span>

<span class="sd">            .. Note:: In langchain_community &gt;= 0.0.27, loading pickled data requires providing the</span>
<span class="sd">                ``allow_dangerous_deserialization`` argument.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                qa = RetrievalQA.from_llm(llm=OpenAI(), retriever=db.as_retriever())</span>


<span class="sd">                def load_retriever(persist_directory):</span>
<span class="sd">                    embeddings = OpenAIEmbeddings()</span>
<span class="sd">                    vectorstore = FAISS.load_local(</span>
<span class="sd">                        persist_directory,</span>
<span class="sd">                        embeddings,</span>
<span class="sd">                        # you may need to add the line below</span>
<span class="sd">                        # for langchain_community &gt;= 0.0.27</span>
<span class="sd">                        allow_dangerous_deserialization=True,</span>
<span class="sd">                    )</span>
<span class="sd">                    return vectorstore.as_retriever()</span>


<span class="sd">                with mlflow.start_run() as run:</span>
<span class="sd">                    logged_model = mlflow.langchain.log_model(</span>
<span class="sd">                        qa,</span>
<span class="sd">                        artifact_path=&quot;retrieval_qa&quot;,</span>
<span class="sd">                        loader_fn=load_retriever,</span>
<span class="sd">                        persist_dir=persist_dir,</span>
<span class="sd">                    )</span>

<span class="sd">            See a complete example in examples/langchain/retrieval_qa_chain.py.</span>
<span class="sd">        example_no_conversion: This parameter is deprecated and will be removed in a future</span>
<span class="sd">                release. It&#39;s no longer used and can be safely removed. Input examples are</span>
<span class="sd">                not converted anymore.</span>
<span class="sd">        model_config: The model configuration to apply to the model if saving model from code. This</span>
<span class="sd">            configuration is available during model loading.</span>

<span class="sd">            .. Note:: Experimental: This parameter may change or be removed in a future</span>
<span class="sd">                                    release without warning.</span>
<span class="sd">        streamable: A boolean value indicating if the model supports streaming prediction. If</span>
<span class="sd">            True, the model must implement `stream` method. If None, streamable is</span>
<span class="sd">            set to True if the model implements `stream` method. Default to `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">langchain</span>
        <span class="kn">from</span> <span class="nn">langchain.schema</span> <span class="kn">import</span> <span class="n">BaseRetriever</span>

        <span class="n">lc_model_or_path</span> <span class="o">=</span> <span class="n">_validate_and_prepare_lc_model_or_path</span><span class="p">(</span><span class="n">lc_model</span><span class="p">,</span> <span class="n">loader_fn</span><span class="p">,</span> <span class="n">temp_dir</span><span class="p">)</span>

        <span class="n">_validate_env_arguments</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span><span class="p">)</span>

        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">_validate_and_prepare_target_save_path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_config</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">model_config</span> <span class="o">=</span> <span class="n">_validate_and_get_model_config_from_file</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span>

        <span class="n">model_code_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lc_model_or_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># The LangChain model is defined as Python code located in the file at the path</span>
            <span class="c1"># specified by `lc_model`. Verify that the path exists and, if so, copy it to the</span>
            <span class="c1"># model directory along with any other specified code modules</span>
            <span class="n">model_code_path</span> <span class="o">=</span> <span class="n">lc_model_or_path</span>

            <span class="n">lc_model</span> <span class="o">=</span> <span class="n">_load_model_code_path</span><span class="p">(</span><span class="n">model_code_path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
            <span class="n">_validate_and_copy_file_to_directory</span><span class="p">(</span><span class="n">model_code_path</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="s2">&quot;code&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lc_model</span> <span class="o">=</span> <span class="n">lc_model_or_path</span>

    <span class="n">code_dir_subpath</span> <span class="o">=</span> <span class="n">_validate_and_copy_code_paths</span><span class="p">(</span><span class="n">code_paths</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mlflow_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
    <span class="n">saved_example</span> <span class="o">=</span> <span class="n">_save_example</span><span class="p">(</span><span class="n">mlflow_model</span><span class="p">,</span> <span class="n">input_example</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">example_no_conversion</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">signature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">saved_example</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wrapped_model</span> <span class="o">=</span> <span class="n">_LangChainModelWrapper</span><span class="p">(</span><span class="n">lc_model</span><span class="p">)</span>
            <span class="n">signature</span> <span class="o">=</span> <span class="n">_infer_signature_from_input_example</span><span class="p">(</span><span class="n">saved_example</span><span class="p">,</span> <span class="n">wrapped_model</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">lc_model</span><span class="p">,</span> <span class="s2">&quot;input_keys&quot;</span><span class="p">):</span>
                <span class="n">input_columns</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="n">DataType</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">input_key</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">input_key</span> <span class="ow">in</span> <span class="n">lc_model</span><span class="o">.</span><span class="n">input_keys</span>
                <span class="p">]</span>
                <span class="n">input_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">(</span><span class="n">input_columns</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_schema</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="n">lc_model</span><span class="p">,</span> <span class="s2">&quot;output_keys&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">lc_model</span><span class="o">.</span><span class="n">output_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lc_model</span><span class="p">,</span> <span class="n">BaseRetriever</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">output_columns</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="n">DataType</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">output_key</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">output_key</span> <span class="ow">in</span> <span class="n">lc_model</span><span class="o">.</span><span class="n">output_keys</span>
                <span class="p">]</span>
                <span class="n">output_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">(</span><span class="n">output_columns</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># TODO: empty output schema if multiple output_keys or is a retriever. fix later!</span>
                <span class="c1"># https://databricks.atlassian.net/browse/ML-34706</span>
                <span class="n">output_schema</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">signature</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">ModelSignature</span><span class="p">(</span><span class="n">input_schema</span><span class="p">,</span> <span class="n">output_schema</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">input_schema</span> <span class="ow">or</span> <span class="n">output_schema</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="n">metadata</span>

    <span class="k">with</span> <span class="n">_get_dependencies_schemas</span><span class="p">()</span> <span class="k">as</span> <span class="n">dependencies_schemas</span><span class="p">:</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">dependencies_schemas</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">streamable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">streamable</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">lc_model</span><span class="p">,</span> <span class="s2">&quot;stream&quot;</span><span class="p">)</span>

    <span class="n">model_data_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">flavor_conf</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_code_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">model_data_kwargs</span> <span class="o">=</span> <span class="n">_save_model</span><span class="p">(</span><span class="n">lc_model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">loader_fn</span><span class="p">,</span> <span class="n">persist_dir</span><span class="p">)</span>
        <span class="n">flavor_conf</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">_MODEL_TYPE_KEY</span><span class="p">:</span> <span class="n">lc_model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_data_kwargs</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="n">pyfunc</span><span class="o">.</span><span class="n">add_to_model</span><span class="p">(</span>
        <span class="n">mlflow_model</span><span class="p">,</span>
        <span class="n">loader_module</span><span class="o">=</span><span class="s2">&quot;mlflow.langchain&quot;</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">python_env</span><span class="o">=</span><span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
        <span class="n">predict_stream_fn</span><span class="o">=</span><span class="s2">&quot;predict_stream&quot;</span><span class="p">,</span>
        <span class="n">streamable</span><span class="o">=</span><span class="n">streamable</span><span class="p">,</span>
        <span class="n">model_code_path</span><span class="o">=</span><span class="n">model_code_path</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_data_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">needs_databricks_auth</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">langchain</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.0.311&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">resources</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">databricks_resources</span> <span class="o">:=</span> <span class="n">_detect_databricks_dependencies</span><span class="p">(</span><span class="n">lc_model</span><span class="p">):</span>
            <span class="n">serialized_databricks_resources</span> <span class="o">=</span> <span class="n">_ResourceBuilder</span><span class="o">.</span><span class="n">from_resources</span><span class="p">(</span><span class="n">databricks_resources</span><span class="p">)</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">resources</span> <span class="o">=</span> <span class="n">serialized_databricks_resources</span>
            <span class="n">needs_databricks_auth</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">DatabricksFunction</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">databricks_resources</span>
            <span class="p">)</span>

    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">add_flavor</span><span class="p">(</span>
        <span class="n">FLAVOR_NAME</span><span class="p">,</span>
        <span class="n">langchain_version</span><span class="o">=</span><span class="n">langchain</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
        <span class="n">streamable</span><span class="o">=</span><span class="n">streamable</span><span class="p">,</span>
        <span class="o">**</span><span class="n">flavor_conf</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="o">:=</span> <span class="n">get_total_file_size</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">model_size_bytes</span> <span class="o">=</span> <span class="n">size</span>
    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">conda_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pip_requirements</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="n">get_default_pip_requirements</span><span class="p">()</span>
            <span class="n">extra_env_vars</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_get_databricks_serverless_env_vars</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">needs_databricks_auth</span> <span class="ow">and</span> <span class="n">is_in_databricks_serverless</span><span class="p">()</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="n">inferred_reqs</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_pip_requirements</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="n">default_reqs</span><span class="p">,</span> <span class="n">extra_env_vars</span><span class="o">=</span><span class="n">extra_env_vars</span>
            <span class="p">)</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">inferred_reqs</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">default_reqs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_pip_requirements</span><span class="p">(</span>
            <span class="n">default_reqs</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_conda_env</span><span class="p">(</span><span class="n">conda_env</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">safe_dump</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pip_constraints</span><span class="p">:</span>
        <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_constraints</span><span class="p">))</span>

    <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_requirements</span><span class="p">))</span>

    <span class="n">_PythonEnv</span><span class="o">.</span><span class="n">current</span><span class="p">()</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">))</span></div>


<div class="viewcode-block" id="log_model"><a class="viewcode-back" href="../../python_api/mlflow.langchain.html#mlflow.langchain.log_model">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@format_docstring</span><span class="p">(</span><span class="n">LOG_MODEL_PARAM_DOCS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">package_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">))</span>
<span class="nd">@docstring_version_compatibility_warning</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="nd">@trace_disabled</span>  <span class="c1"># Suppress traces for internal predict calls while logging model</span>
<span class="k">def</span> <span class="nf">log_model</span><span class="p">(</span>
    <span class="n">lc_model</span><span class="p">,</span>
    <span class="n">artifact_path</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">ModelSignature</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="p">:</span> <span class="n">ModelInputExample</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">await_registration_for</span><span class="o">=</span><span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">loader_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">persist_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">example_no_conversion</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">run_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">streamable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">resources</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log a LangChain model as an MLflow artifact for the current run.</span>

<span class="sd">    Args:</span>
<span class="sd">        lc_model: A LangChain model, which could be a</span>
<span class="sd">            `Chain &lt;https://python.langchain.com/docs/modules/chains/&gt;`_,</span>
<span class="sd">            `Agent &lt;https://python.langchain.com/docs/modules/agents/&gt;`_, or</span>
<span class="sd">            `retriever &lt;https://python.langchain.com/docs/modules/data_connection/retrievers/&gt;`_</span>
<span class="sd">            or a path containing the `LangChain model code &lt;https://github.com/mlflow/mlflow/blob/master/examples/langchain/chain_as_code_driver.py&gt;`</span>
<span class="sd">            for the above types. When using model as path, make sure to set the model</span>
<span class="sd">            by using :func:`mlflow.models.set_model()`.</span>

<span class="sd">            .. Note:: Experimental: Using model as path may change or be removed in a future</span>
<span class="sd">                                    release without warning.</span>
<span class="sd">        artifact_path: Run-relative artifact path.</span>
<span class="sd">        conda_env: {{ conda_env }}</span>
<span class="sd">        code_paths: {{ code_paths }}</span>
<span class="sd">        registered_model_name: This argument may change or be removed in a</span>
<span class="sd">            future release without warning. If given, create a model</span>
<span class="sd">            version under ``registered_model_name``, also creating a</span>
<span class="sd">            registered model if one with the given name does not exist.</span>
<span class="sd">        signature: :py:class:`ModelSignature &lt;mlflow.models.ModelSignature&gt;`</span>
<span class="sd">            describes model input and output</span>
<span class="sd">            :py:class:`Schema &lt;mlflow.types.Schema&gt;`.</span>
<span class="sd">            If not specified, the model signature would be set according to</span>
<span class="sd">            `lc_model.input_keys` and `lc_model.output_keys` as columns names, and</span>
<span class="sd">            `DataType.string` as the column type.</span>
<span class="sd">            Alternatively, you can explicitly specify the model signature.</span>
<span class="sd">            The model signature can be :py:func:`inferred</span>
<span class="sd">            &lt;mlflow.models.infer_signature&gt;` from datasets with valid model input</span>
<span class="sd">            (e.g. the training dataset with target column omitted) and valid model</span>
<span class="sd">            output (e.g. model predictions generated on the training dataset),</span>
<span class="sd">            for example:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from mlflow.models import infer_signature</span>

<span class="sd">                chain = LLMChain(llm=llm, prompt=prompt)</span>
<span class="sd">                prediction = chain.run(input_str)</span>
<span class="sd">                input_columns = [</span>
<span class="sd">                    {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: input_key} for input_key in chain.input_keys</span>
<span class="sd">                ]</span>
<span class="sd">                signature = infer_signature(input_columns, predictions)</span>

<span class="sd">        input_example: {{ input_example }}</span>
<span class="sd">        await_registration_for: Number of seconds to wait for the model version</span>
<span class="sd">            to finish being created and is in ``READY`` status.</span>
<span class="sd">            By default, the function waits for five minutes.</span>
<span class="sd">            Specify 0 or None to skip waiting.</span>
<span class="sd">        pip_requirements: {{ pip_requirements }}</span>
<span class="sd">        extra_pip_requirements: {{ extra_pip_requirements }}</span>
<span class="sd">        metadata: {{ metadata }}</span>
<span class="sd">        loader_fn: A function that&#39;s required for models containing objects that aren&#39;t natively</span>
<span class="sd">            serialized by LangChain.</span>
<span class="sd">            This function takes a string `persist_dir` as an argument and returns the</span>
<span class="sd">            specific object that the model needs. Depending on the model,</span>
<span class="sd">            this could be a retriever, vectorstore, requests_wrapper, embeddings, or</span>
<span class="sd">            database. For RetrievalQA Chain and retriever models, the object is a</span>
<span class="sd">            (`retriever &lt;https://python.langchain.com/docs/modules/data_connection/retrievers/&gt;`_).</span>
<span class="sd">            For APIChain models, it&#39;s a</span>
<span class="sd">            (`requests_wrapper &lt;https://python.langchain.com/docs/modules/agents/tools/integrations/requests&gt;`_).</span>
<span class="sd">            For HypotheticalDocumentEmbedder models, it&#39;s an</span>
<span class="sd">            (`embeddings &lt;https://python.langchain.com/docs/modules/data_connection/text_embedding/&gt;`_).</span>
<span class="sd">            For SQLDatabaseChain models, it&#39;s a</span>
<span class="sd">            (`database &lt;https://python.langchain.com/docs/modules/agents/toolkits/sql_database&gt;`_).</span>
<span class="sd">        persist_dir: The directory where the object is stored. The `loader_fn`</span>
<span class="sd">            takes this string as the argument to load the object.</span>
<span class="sd">            This is optional for models containing objects that aren&#39;t natively</span>
<span class="sd">            serialized by LangChain. MLflow logs the content in this directory as</span>
<span class="sd">            artifacts in the subdirectory named `persist_dir_data`.</span>

<span class="sd">            Here is the code snippet for logging a RetrievalQA chain with `loader_fn`</span>
<span class="sd">            and `persist_dir`:</span>

<span class="sd">            .. Note:: In langchain_community &gt;= 0.0.27, loading pickled data requires providing the</span>
<span class="sd">                ``allow_dangerous_deserialization`` argument.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                qa = RetrievalQA.from_llm(llm=OpenAI(), retriever=db.as_retriever())</span>


<span class="sd">                def load_retriever(persist_directory):</span>
<span class="sd">                    embeddings = OpenAIEmbeddings()</span>
<span class="sd">                    vectorstore = FAISS.load_local(</span>
<span class="sd">                        persist_directory,</span>
<span class="sd">                        embeddings,</span>
<span class="sd">                        # you may need to add the line below</span>
<span class="sd">                        # for langchain_community &gt;= 0.0.27</span>
<span class="sd">                        allow_dangerous_deserialization=True,</span>
<span class="sd">                    )</span>
<span class="sd">                    return vectorstore.as_retriever()</span>


<span class="sd">                with mlflow.start_run() as run:</span>
<span class="sd">                    logged_model = mlflow.langchain.log_model(</span>
<span class="sd">                        qa,</span>
<span class="sd">                        artifact_path=&quot;retrieval_qa&quot;,</span>
<span class="sd">                        loader_fn=load_retriever,</span>
<span class="sd">                        persist_dir=persist_dir,</span>
<span class="sd">                    )</span>

<span class="sd">            See a complete example in examples/langchain/retrieval_qa_chain.py.</span>
<span class="sd">        example_no_conversion: This parameter is deprecated and will be removed in a future</span>
<span class="sd">                release. It&#39;s no longer used and can be safely removed. Input examples are</span>
<span class="sd">                not converted anymore.</span>
<span class="sd">        run_id: run_id to associate with this model version. If specified, we resume the</span>
<span class="sd">                run and log the model to that run. Otherwise, a new run is created.</span>
<span class="sd">                Default to None.</span>
<span class="sd">        model_config: The model configuration to apply to the model if saving model from code. This</span>
<span class="sd">            configuration is available during model loading.</span>

<span class="sd">            .. Note:: Experimental: This parameter may change or be removed in a future</span>
<span class="sd">                                    release without warning.</span>
<span class="sd">        streamable: A boolean value indicating if the model supports streaming prediction. If</span>
<span class="sd">            True, the model must implement `stream` method. If None, If None, streamable is</span>
<span class="sd">            set to True if the model implements `stream` method. Default to `None`.</span>
<span class="sd">        resources: A list of model resources or a resources.yaml file containing a list of</span>
<span class="sd">                    resources required to serve the model.</span>

<span class="sd">            .. Note:: Experimental: This parameter may change or be removed in a future</span>
<span class="sd">                                    release without warning.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A :py:class:`ModelInfo &lt;mlflow.models.model.ModelInfo&gt;` instance that contains the</span>
<span class="sd">        metadata of the logged model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="n">artifact_path</span><span class="p">,</span>
        <span class="n">flavor</span><span class="o">=</span><span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="p">,</span>
        <span class="n">registered_model_name</span><span class="o">=</span><span class="n">registered_model_name</span><span class="p">,</span>
        <span class="n">lc_model</span><span class="o">=</span><span class="n">lc_model</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">conda_env</span><span class="p">,</span>
        <span class="n">code_paths</span><span class="o">=</span><span class="n">code_paths</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
        <span class="n">await_registration_for</span><span class="o">=</span><span class="n">await_registration_for</span><span class="p">,</span>
        <span class="n">pip_requirements</span><span class="o">=</span><span class="n">pip_requirements</span><span class="p">,</span>
        <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="n">extra_pip_requirements</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">loader_fn</span><span class="o">=</span><span class="n">loader_fn</span><span class="p">,</span>
        <span class="n">persist_dir</span><span class="o">=</span><span class="n">persist_dir</span><span class="p">,</span>
        <span class="n">example_no_conversion</span><span class="o">=</span><span class="n">example_no_conversion</span><span class="p">,</span>
        <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
        <span class="n">streamable</span><span class="o">=</span><span class="n">streamable</span><span class="p">,</span>
        <span class="n">resources</span><span class="o">=</span><span class="n">resources</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="c1"># patch_langchain_type_to_cls_dict here as we attempt to load model</span>
<span class="c1"># if it&#39;s saved by `dict` method</span>
<span class="nd">@patch_langchain_type_to_cls_dict</span>
<span class="k">def</span> <span class="nf">_save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">loader_fn</span><span class="p">,</span> <span class="n">persist_dir</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">cloudpickle</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;2.1.0&quot;</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;If you are constructing a custom LangChain model, &quot;</span>
            <span class="s2">&quot;please upgrade cloudpickle to version 2.1.0 or later &quot;</span>
            <span class="s2">&quot;using `pip install cloudpickle&gt;=2.1.0` &quot;</span>
            <span class="s2">&quot;to ensure the model can be loaded correctly.&quot;</span>
        <span class="p">)</span>

    <span class="k">with</span> <span class="n">register_pydantic_v1_serializer_cm</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lc_runnables_types</span><span class="p">()):</span>
            <span class="k">return</span> <span class="n">_save_runnables</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">loader_fn</span><span class="o">=</span><span class="n">loader_fn</span><span class="p">,</span> <span class="n">persist_dir</span><span class="o">=</span><span class="n">persist_dir</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_save_base_lcs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">loader_fn</span><span class="p">,</span> <span class="n">persist_dir</span><span class="p">)</span>


<span class="nd">@patch_langchain_type_to_cls_dict</span>
<span class="k">def</span> <span class="nf">_load_model</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">mlflow.langchain._langchain_autolog</span> <span class="kn">import</span> <span class="n">_update_langchain_model_config</span>

    <span class="c1"># model_type is not accurate as the class can be subclass</span>
    <span class="c1"># of supported types, we define _MODEL_LOAD_KEY to ensure</span>
    <span class="c1"># which load function to use</span>
    <span class="n">model_load_fn</span> <span class="o">=</span> <span class="n">flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_MODEL_LOAD_KEY</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">register_pydantic_v1_serializer_cm</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">model_load_fn</span> <span class="o">==</span> <span class="n">_RUNNABLE_LOAD_KEY</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">_load_runnables</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model_load_fn</span> <span class="o">==</span> <span class="n">_BASE_LOAD_KEY</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">_load_base_lcs</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Failed to load LangChain model. Unknown model type: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_MODEL_TYPE_KEY</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="c1"># To avoid double logging, we set _mlflow_model_logged to True</span>
    <span class="c1"># when the model is loaded</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">autologging_is_disabled</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">_update_langchain_model_config</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">_mlflow_model_logged</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">model</span><span class="o">.</span><span class="n">run_id</span> <span class="o">=</span> <span class="n">get_model_info</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">)</span><span class="o">.</span><span class="n">run_id</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">class</span> <span class="nc">_LangChainModelWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lc_model</span><span class="p">,</span> <span class="n">model_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lc_model</span> <span class="o">=</span> <span class="n">lc_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span> <span class="o">=</span> <span class="n">model_path</span>

    <span class="k">def</span> <span class="nf">get_raw_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the underlying model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lc_model</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            data: Model input data.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: We don&#39;t automatically turn tracing on in OSS model serving, because we haven&#39;t</span>
        <span class="c1"># implemented storage option for traces in OSS model serving (counterpart to the</span>
        <span class="c1"># Inference Table in Databricks model serving).</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">is_in_databricks_model_serving_environment</span><span class="p">()</span>
            <span class="c1"># TODO: This env var was once used for controlling whether or not to inject the</span>
            <span class="c1">#   tracer in Databricks model serving. However, now we have the new env var</span>
            <span class="c1">#   `ENABLE_MLFLOW_TRACING` to control that. We don&#39;t remove this condition</span>
            <span class="c1">#   right now in the interest of caution, but we should remove this condition</span>
            <span class="c1">#   after making sure that the functionality is stable.</span>
            <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;MLFLOW_ENABLE_TRACE_IN_SERVING&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span>
            <span class="c1"># if this is False, tracing is disabled and we shouldn&#39;t inject the tracer</span>
            <span class="ow">and</span> <span class="n">is_mlflow_tracing_enabled_in_model_serving</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="kn">from</span> <span class="nn">mlflow.langchain.langchain_tracer</span> <span class="kn">import</span> <span class="n">MlflowLangchainTracer</span>

            <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">MlflowLangchainTracer</span><span class="p">()]</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">context</span> <span class="o">:=</span> <span class="n">get_prediction_context</span><span class="p">())</span> <span class="ow">and</span> <span class="n">context</span><span class="o">.</span><span class="n">is_evaluate</span><span class="p">:</span>
            <span class="c1"># NB: We enable traces automatically for the model evaluation. Note that we have to</span>
            <span class="c1">#   manually pass the context instance to callback, because LangChain callback may be</span>
            <span class="c1">#   invoked asynchronously and it doesn&#39;t correctly propagate the thread-local context.</span>
            <span class="kn">from</span> <span class="nn">mlflow.langchain.langchain_tracer</span> <span class="kn">import</span> <span class="n">MlflowLangchainTracer</span>

            <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">MlflowLangchainTracer</span><span class="p">(</span><span class="n">prediction_context</span><span class="o">=</span><span class="n">context</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">callbacks</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_with_callbacks</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">callback_handlers</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_dependencies_schemas_in_prediction_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback_handlers</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">mlflow.langchain.langchain_tracer</span> <span class="kn">import</span> <span class="n">MlflowLangchainTracer</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">callback_handlers</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="n">tracer</span> <span class="o">:=</span> <span class="nb">next</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callback_handlers</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">MlflowLangchainTracer</span><span class="p">)),</span> <span class="kc">None</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span>
        <span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">_prediction_context</span>
            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">metadata</span> <span class="ow">and</span> <span class="n">context</span><span class="p">:</span>
                <span class="n">dependencies_schemas</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dependencies_schemas&quot;</span><span class="p">,</span> <span class="p">{})</span>
                <span class="n">context</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="n">dependencies_schemas</span><span class="o">=</span><span class="p">{</span>
                        <span class="n">dependency</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">dependency</span><span class="p">,</span> <span class="n">schema</span> <span class="ow">in</span> <span class="n">dependencies_schemas</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="p">)</span>

    <span class="nd">@experimental</span>
    <span class="k">def</span> <span class="nf">_predict_with_callbacks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callback_handlers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">convert_chat_responses</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            data: Model input data.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>
<span class="sd">            callback_handlers: Callback handlers to pass to LangChain.</span>
<span class="sd">            convert_chat_responses: If true, forcibly convert response to chat model</span>
<span class="sd">                response format.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mlflow.langchain.api_request_parallel_processor</span> <span class="kn">import</span> <span class="n">process_api_requests</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dependencies_schemas_in_prediction_context</span><span class="p">(</span><span class="n">callback_handlers</span><span class="p">)</span>
        <span class="n">messages</span><span class="p">,</span> <span class="n">return_first_element</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_predict_messages</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">process_api_requests</span><span class="p">(</span>
            <span class="n">lc_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lc_model</span><span class="p">,</span>
            <span class="n">requests</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">callback_handlers</span><span class="o">=</span><span class="n">callback_handlers</span><span class="p">,</span>
            <span class="n">convert_chat_responses</span><span class="o">=</span><span class="n">convert_chat_responses</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span> <span class="ow">or</span> <span class="p">{},</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">return_first_element</span> <span class="k">else</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">_prepare_predict_messages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a tuple of (preprocessed_data, return_first_element)</span>
<span class="sd">        `preprocessed_data` is always a list,</span>
<span class="sd">        and `return_first_element` means if True, we should return the first element</span>
<span class="sd">        of inference result, otherwise we should return the whole inference result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">_convert_llm_input_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># if the input data is not a list (i.e. single input),</span>
            <span class="c1"># we still need to convert it to a one-element list `[data]`</span>
            <span class="c1"># because `process_api_requests` only accepts list as valid input.</span>
            <span class="c1"># and in this case,</span>
            <span class="c1"># we should return the first element of the inference result</span>
            <span class="c1"># because we change input `data` to `[data]`</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">data</span><span class="p">],</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="kc">False</span>
        <span class="k">raise</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
            <span class="s2">&quot;Input must be a pandas DataFrame or a list &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;for model </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">lc_model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_predict_stream_messages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">_convert_llm_input_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># `predict_stream` only accepts single input.</span>
            <span class="c1"># but `enforce_schema` might convert single input into a list like `[single_input]`</span>
            <span class="c1"># so extract the first element in the list.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;predict_stream&#39; requires single input, but it got input data </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">predict_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            data: Model input data, only single input is allowed.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An iterator of model prediction chunks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mlflow.langchain.api_request_parallel_processor</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">process_stream_request</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_predict_stream_messages</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">process_stream_request</span><span class="p">(</span>
            <span class="n">lc_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lc_model</span><span class="p">,</span>
            <span class="n">request_json</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span> <span class="ow">or</span> <span class="p">{},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_predict_stream_with_callbacks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callback_handlers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">convert_chat_responses</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            data: Model input data, only single input is allowed.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>
<span class="sd">            callback_handlers: Callback handlers to pass to LangChain.</span>
<span class="sd">            convert_chat_responses: If true, forcibly convert response to chat model</span>
<span class="sd">                response format.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An iterator of model prediction chunks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mlflow.langchain.api_request_parallel_processor</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">process_stream_request</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dependencies_schemas_in_prediction_context</span><span class="p">(</span><span class="n">callback_handlers</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_predict_stream_messages</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">process_stream_request</span><span class="p">(</span>
            <span class="n">lc_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lc_model</span><span class="p">,</span>
            <span class="n">request_json</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
            <span class="n">callback_handlers</span><span class="o">=</span><span class="n">callback_handlers</span><span class="p">,</span>
            <span class="n">convert_chat_responses</span><span class="o">=</span><span class="n">convert_chat_responses</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span> <span class="ow">or</span> <span class="p">{},</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_load_pyfunc</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: D417</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load PyFunc implementation for LangChain. Called by ``pyfunc.load_model``.</span>

<span class="sd">    Args:</span>
<span class="sd">        path: Local filesystem path to the MLflow Model with the ``langchain`` flavor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_LangChainModelWrapper</span><span class="p">(</span><span class="n">_load_model_from_local_fs</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">),</span> <span class="n">path</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_load_model_from_local_fs</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">model_config_overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">flavor_conf</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
    <span class="n">pyfunc_flavor_conf</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span>
        <span class="n">model_path</span><span class="o">=</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">PYFUNC_FLAVOR_NAME</span>
    <span class="p">)</span>
    <span class="c1"># Add code from the langchain flavor to the system path</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">)</span>
    <span class="c1"># The model_code_path and the model_config were previously saved langchain flavor but now we</span>
    <span class="c1"># also save them inside the pyfunc flavor. For backwards compatibility of previous models,</span>
    <span class="c1"># we need to check both places.</span>
    <span class="k">if</span> <span class="n">MODEL_CODE_PATH</span> <span class="ow">in</span> <span class="n">pyfunc_flavor_conf</span> <span class="ow">or</span> <span class="n">MODEL_CODE_PATH</span> <span class="ow">in</span> <span class="n">flavor_conf</span><span class="p">:</span>
        <span class="n">model_config</span> <span class="o">=</span> <span class="n">pyfunc_flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">MODEL_CONFIG</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">MODEL_CONFIG</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_config</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">local_model_path</span><span class="p">,</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">model_config</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">model_config</span> <span class="o">=</span> <span class="n">_validate_and_get_model_config_from_file</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>

        <span class="n">flavor_code_path</span> <span class="o">=</span> <span class="n">pyfunc_flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">MODEL_CODE_PATH</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">MODEL_CODE_PATH</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">model_code_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">local_model_path</span><span class="p">,</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">flavor_code_path</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">_load_model_code_path</span><span class="p">(</span>
                <span class="n">model_code_path</span><span class="p">,</span> <span class="p">{</span><span class="o">**</span><span class="p">(</span><span class="n">model_config</span> <span class="ow">or</span> <span class="p">{}),</span> <span class="o">**</span><span class="p">(</span><span class="n">model_config_overrides</span> <span class="ow">or</span> <span class="p">{})}</span>
            <span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># We would like to clean up the dependencies schema which is set to global</span>
            <span class="c1"># after loading the mode to avoid the schema being used in the next model loading</span>
            <span class="n">_clear_dependencies_schemas</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_load_model</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">)</span>


<div class="viewcode-block" id="load_model"><a class="viewcode-back" href="../../python_api/mlflow.langchain.html#mlflow.langchain.load_model">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@docstring_version_compatibility_warning</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="nd">@trace_disabled</span>  <span class="c1"># Suppress traces while loading model</span>
<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">dst_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load a LangChain model from a local file or a run.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_uri: The location, in URI format, of the MLflow model. For example:</span>

<span class="sd">            - ``/Users/me/path/to/local/model``</span>
<span class="sd">            - ``relative/path/to/local/model``</span>
<span class="sd">            - ``s3://my_bucket/path/to/model``</span>
<span class="sd">            - ``runs:/&lt;mlflow_run_id&gt;/run-relative/path/to/model``</span>

<span class="sd">            For more information about supported URI schemes, see</span>
<span class="sd">            `Referencing Artifacts &lt;https://www.mlflow.org/docs/latest/tracking.html#</span>
<span class="sd">            artifact-locations&gt;`_.</span>
<span class="sd">        dst_path: The local filesystem path to which to download the model artifact.</span>
<span class="sd">            This directory must already exist. If unspecified, a local output</span>
<span class="sd">            path will be created.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A LangChain model instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">local_model_path</span> <span class="o">=</span> <span class="n">_download_artifact_from_uri</span><span class="p">(</span><span class="n">artifact_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">dst_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_load_model_from_local_fs</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_patch_runnable_cls</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For classes that are subclasses of Runnable, we patch the `invoke`, `batch`, `stream` and</span>
<span class="sd">    `ainvoke`, `abatch`, `astream` methods for autologging.</span>

<span class="sd">    Args:</span>
<span class="sd">        cls: The class to patch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlflow.langchain._langchain_autolog</span> <span class="kn">import</span> <span class="n">patched_inference</span>

    <span class="n">patch_functions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;invoke&quot;</span><span class="p">,</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="s2">&quot;ainvoke&quot;</span><span class="p">,</span> <span class="s2">&quot;abatch&quot;</span><span class="p">,</span> <span class="s2">&quot;astream&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">patch_functions</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func_name</span><span class="p">):</span>
            <span class="n">safe_patch</span><span class="p">(</span>
                <span class="n">FLAVOR_NAME</span><span class="p">,</span>
                <span class="bp">cls</span><span class="p">,</span>
                <span class="n">func_name</span><span class="p">,</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">patched_inference</span><span class="p">,</span> <span class="n">func_name</span><span class="p">),</span>
            <span class="p">)</span>


<span class="k">def</span> <span class="nf">_inspect_module_and_patch_cls</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">inspected_modules</span><span class="p">,</span> <span class="n">patched_classes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Internal method to inspect the module and patch classes that are</span>
<span class="sd">    subclasses of Runnable for autologging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">langchain.schema.runnable</span> <span class="kn">import</span> <span class="n">Runnable</span>

    <span class="k">if</span> <span class="n">module_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inspected_modules</span><span class="p">:</span>
        <span class="n">inspected_modules</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module_name</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmembers</span><span class="p">(</span><span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">module_name</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">inspect</span><span class="o">.</span><span class="n">ismodule</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                    <span class="n">obj</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;langchain&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">obj</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;langgraph&quot;</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">_inspect_module_and_patch_cls</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">inspected_modules</span><span class="p">,</span> <span class="n">patched_classes</span><span class="p">)</span>
                <span class="k">elif</span> <span class="p">(</span>
                    <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">patched_classes</span>
                    <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Runnable</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">_patch_runnable_cls</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
                    <span class="n">patched_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>


<div class="viewcode-block" id="autolog"><a class="viewcode-back" href="../../python_api/mlflow.langchain.html#mlflow.langchain.autolog">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@autologging_integration</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">autolog</span><span class="p">(</span>
    <span class="n">log_input_examples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_model_signatures</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_datasets</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_inputs_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">disable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">disable_for_unsupported_versions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_tags</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_model_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">log_traces</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enables (or disables) and configures autologging from Langchain to MLflow.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_input_examples: If ``True``, input examples from inference data are collected and</span>
<span class="sd">            logged along with Langchain model artifacts during inference. If</span>
<span class="sd">            ``False``, input examples are not logged.</span>
<span class="sd">            Note: Input examples are MLflow model attributes</span>
<span class="sd">            and are only collected if ``log_models`` is also ``True``.</span>
<span class="sd">        log_model_signatures: If ``True``,</span>
<span class="sd">            :py:class:`ModelSignatures &lt;mlflow.models.ModelSignature&gt;`</span>
<span class="sd">            describing model inputs and outputs are collected and logged along</span>
<span class="sd">            with Langchain model artifacts during inference. If ``False``,</span>
<span class="sd">            signatures are not logged.</span>
<span class="sd">            Note: Model signatures are MLflow model attributes</span>
<span class="sd">            and are only collected if ``log_models`` is also ``True``.</span>
<span class="sd">        log_models: If ``True``, langchain models are logged as MLflow model artifacts.</span>
<span class="sd">            If ``False``, langchain models are not logged.</span>
<span class="sd">            Input examples and model signatures, which are attributes of MLflow models,</span>
<span class="sd">            are also omitted when ``log_models`` is ``False``.</span>
<span class="sd">        log_datasets: If ``True``, dataset information is logged to MLflow Tracking</span>
<span class="sd">            if applicable. If ``False``, dataset information is not logged.</span>
<span class="sd">        log_inputs_outputs: **Deprecated** The legacy parameter used for logging inference</span>
<span class="sd">            inputs and outputs. This argument will be removed in a future version of MLflow.</span>
<span class="sd">            The alternative is to use ``log_traces`` which logs traces for Langchain models,</span>
<span class="sd">            including inputs and outputs for each stage.</span>
<span class="sd">            If ``True``, inference data and results are combined into a single</span>
<span class="sd">            pandas DataFrame and logged to MLflow Tracking as an artifact.</span>
<span class="sd">            If ``False``, inference data and results are not logged.</span>
<span class="sd">            Default to ``False``.</span>
<span class="sd">        disable: If ``True``, disables the Langchain autologging integration. If ``False``,</span>
<span class="sd">            enables the Langchain autologging integration.</span>
<span class="sd">        exclusive: If ``True``, autologged content is not logged to user-created fluent runs.</span>
<span class="sd">            If ``False``, autologged content is logged to the active fluent run,</span>
<span class="sd">            which may be user-created.</span>
<span class="sd">        disable_for_unsupported_versions: If ``True``, disable autologging for versions of</span>
<span class="sd">            langchain that have not been tested against this version of the MLflow</span>
<span class="sd">            client or are incompatible.</span>
<span class="sd">        silent: If ``True``, suppress all event logs and warnings from MLflow during Langchain</span>
<span class="sd">            autologging. If ``False``, show all events and warnings during Langchain</span>
<span class="sd">            autologging.</span>
<span class="sd">        registered_model_name: If given, each time a model is trained, it is registered as a</span>
<span class="sd">            new model version of the registered model with this name.</span>
<span class="sd">            The registered model is created if it does not already exist.</span>
<span class="sd">        extra_tags: A dictionary of extra tags to set on each managed run created by autologging.</span>
<span class="sd">        extra_model_classes: A list of langchain classes to log in addition to the default classes.</span>
<span class="sd">            We do not guarantee classes specified in this list can be logged as a model, but tracing</span>
<span class="sd">            will be supported. Note that all classes within the list must be subclasses of Runnable,</span>
<span class="sd">            and we only patch `invoke`, `batch`, and `stream` methods for tracing.</span>
<span class="sd">        log_traces: If ``True``, traces are logged for Langchain models by using</span>
<span class="sd">            MlflowLangchainTracer as a callback during inference. If ``False``, no traces are</span>
<span class="sd">            collected during inference. Default to ``True``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">mlflow.langchain._langchain_autolog</span> <span class="kn">import</span> <span class="n">patched_inference</span>

        <span class="c1"># avoid duplicate patching</span>
        <span class="n">patched_classes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="c1"># avoid infinite recursion</span>
        <span class="n">inspected_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="c1"># Get all installed LangChain packages</span>
        <span class="k">for</span> <span class="n">pkg</span> <span class="ow">in</span> <span class="n">importlib</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">distributions</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">pkg</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;langchain&quot;</span><span class="p">):</span>
                <span class="n">module_name</span> <span class="o">=</span> <span class="n">pkg</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
                <span class="n">_inspect_module_and_patch_cls</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">inspected_modules</span><span class="p">,</span> <span class="n">patched_classes</span><span class="p">)</span>

            <span class="c1"># If LangGraph is installed, patch the classes. LangGraph does not define members</span>
            <span class="c1"># under the top level module, so we need to hardcode the submodules to patch.</span>
            <span class="k">if</span> <span class="n">pkg</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;langgraph&quot;</span><span class="p">:</span>
                <span class="n">langgraph_submodules</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="s2">&quot;langgraph.graph&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;langgraph.prebuilt&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;langgraph.pregel&quot;</span><span class="p">,</span>
                <span class="p">]</span>
                <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">langgraph_submodules</span><span class="p">:</span>
                    <span class="n">_inspect_module_and_patch_cls</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">inspected_modules</span><span class="p">,</span> <span class="n">patched_classes</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">extra_model_classes</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">Runnable</span>

            <span class="n">unsupported_classes</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="n">extra_model_classes</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">patched_classes</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">Runnable</span><span class="p">):</span>
                    <span class="n">_patch_runnable_cls</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
                    <span class="n">patched_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">unsupported_classes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">unsupported_classes</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unsupported classes found in extra_model_classes: </span><span class="si">{</span><span class="n">unsupported_classes</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Only subclasses of Runnable are supported.&quot;</span>
                <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">langchain.agents.agent</span> <span class="kn">import</span> <span class="n">AgentExecutor</span>
            <span class="kn">from</span> <span class="nn">langchain.chains.base</span> <span class="kn">import</span> <span class="n">Chain</span>

            <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="p">[</span><span class="n">AgentExecutor</span><span class="p">,</span> <span class="n">Chain</span><span class="p">]:</span>
                <span class="n">safe_patch</span><span class="p">(</span>
                    <span class="n">FLAVOR_NAME</span><span class="p">,</span>
                    <span class="bp">cls</span><span class="p">,</span>
                    <span class="s2">&quot;__call__&quot;</span><span class="p">,</span>
                    <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">patched_inference</span><span class="p">,</span> <span class="s2">&quot;__call__&quot;</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">langchain_core.retrievers</span> <span class="kn">import</span> <span class="n">BaseRetriever</span>

            <span class="n">safe_patch</span><span class="p">(</span>
                <span class="n">FLAVOR_NAME</span><span class="p">,</span>
                <span class="n">BaseRetriever</span><span class="p">,</span>
                <span class="s2">&quot;get_relevant_documents&quot;</span><span class="p">,</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">patched_inference</span><span class="p">,</span> <span class="s2">&quot;get_relevant_documents&quot;</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span></div>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>