

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/onnx -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.onnx</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/onnx.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.1.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/tabs.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.onnx</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/onnx" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.onnx</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The ``mlflow.onnx`` module provides APIs for logging and loading ONNX models in the MLflow Model</span>
<span class="sd">format. This module exports MLflow Models with the following flavors:</span>

<span class="sd">ONNX (native) format</span>
<span class="sd">    This is the main flavor that can be loaded back as an ONNX model object.</span>
<span class="sd">:py:mod:`mlflow.pyfunc`</span>
<span class="sd">    Produced for use by generic pyfunc-based deployment tools and batch inference.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">Version</span>

<span class="kn">import</span> <span class="nn">mlflow.tracking</span>
<span class="kn">from</span> <span class="nn">mlflow</span> <span class="kn">import</span> <span class="n">pyfunc</span>
<span class="kn">from</span> <span class="nn">mlflow.exceptions</span> <span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">ModelInputExample</span><span class="p">,</span> <span class="n">ModelSignature</span>
<span class="kn">from</span> <span class="nn">mlflow.models.model</span> <span class="kn">import</span> <span class="n">MLMODEL_FILE_NAME</span>
<span class="kn">from</span> <span class="nn">mlflow.models.utils</span> <span class="kn">import</span> <span class="n">_save_example</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking._model_registry</span> <span class="kn">import</span> <span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking.artifact_utils</span> <span class="kn">import</span> <span class="n">_download_artifact_from_uri</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.docstring_utils</span> <span class="kn">import</span> <span class="n">LOG_MODEL_PARAM_DOCS</span><span class="p">,</span> <span class="n">format_docstring</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.environment</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_mlflow_conda_env</span><span class="p">,</span>
    <span class="n">_process_conda_env</span><span class="p">,</span>
    <span class="n">_process_pip_requirements</span><span class="p">,</span>
    <span class="n">_PythonEnv</span><span class="p">,</span>
    <span class="n">_validate_env_arguments</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.file_utils</span> <span class="kn">import</span> <span class="n">get_total_file_size</span><span class="p">,</span> <span class="n">write_to</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.model_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">,</span>
    <span class="n">_get_flavor_configuration</span><span class="p">,</span>
    <span class="n">_validate_and_copy_code_paths</span><span class="p">,</span>
    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">,</span>
    <span class="n">_validate_onnx_session_options</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.requirements_utils</span> <span class="kn">import</span> <span class="n">_get_pinned_requirement</span>

<span class="n">FLAVOR_NAME</span> <span class="o">=</span> <span class="s2">&quot;onnx&quot;</span>
<span class="n">ONNX_EXECUTION_PROVIDERS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CUDAExecutionProvider&quot;</span><span class="p">,</span> <span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">]</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="n">_MODEL_DATA_SUBPATH</span> <span class="o">=</span> <span class="s2">&quot;model.onnx&quot;</span>


<div class="viewcode-block" id="get_default_pip_requirements"><a class="viewcode-back" href="../../python_api/mlflow.onnx.html#mlflow.onnx.get_default_pip_requirements">[docs]</a><span class="k">def</span> <span class="nf">get_default_pip_requirements</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        A list of default pip requirements for MLflow Models produced by this flavor.</span>
<span class="sd">        Calls to :func:`save_model()` and :func:`log_model()` produce a pip environment</span>
<span class="sd">        that, at minimum, contains these requirements.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span>
        <span class="nb">map</span><span class="p">(</span>
            <span class="n">_get_pinned_requirement</span><span class="p">,</span>
            <span class="p">[</span>
                <span class="s2">&quot;onnx&quot;</span><span class="p">,</span>
                <span class="c1"># The ONNX pyfunc representation requires the OnnxRuntime</span>
                <span class="c1"># inference engine. Therefore, the conda environment must</span>
                <span class="c1"># include OnnxRuntime</span>
                <span class="s2">&quot;onnxruntime&quot;</span><span class="p">,</span>
            <span class="p">],</span>
        <span class="p">)</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="get_default_conda_env"><a class="viewcode-back" href="../../python_api/mlflow.onnx.html#mlflow.onnx.get_default_conda_env">[docs]</a><span class="k">def</span> <span class="nf">get_default_conda_env</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        The default Conda environment for MLflow Models produced by calls to</span>
<span class="sd">        :func:`save_model()` and :func:`log_model()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_mlflow_conda_env</span><span class="p">(</span><span class="n">additional_pip_deps</span><span class="o">=</span><span class="n">get_default_pip_requirements</span><span class="p">())</span></div>


<div class="viewcode-block" id="save_model"><a class="viewcode-back" href="../../python_api/mlflow.onnx.html#mlflow.onnx.save_model">[docs]</a><span class="nd">@format_docstring</span><span class="p">(</span><span class="n">LOG_MODEL_PARAM_DOCS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">package_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span>
    <span class="n">onnx_model</span><span class="p">,</span>
    <span class="n">path</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlflow_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">ModelSignature</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="p">:</span> <span class="n">ModelInputExample</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">onnx_execution_providers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">onnx_session_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">save_as_external_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save an ONNX model to a path on the local file system.</span>

<span class="sd">    Args:</span>
<span class="sd">        onnx_model: ONNX model to be saved.</span>
<span class="sd">        path: Local path where the model is to be saved.</span>
<span class="sd">        conda_env: {{ conda_env }}</span>
<span class="sd">        code_paths: {{ code_paths }}</span>
<span class="sd">        mlflow_model: :py:mod:`mlflow.models.Model` this flavor is being added to.</span>
<span class="sd">        signature: :py:class:`ModelSignature &lt;mlflow.models.ModelSignature&gt;`</span>
<span class="sd">            describes model input and output :py:class:`Schema &lt;mlflow.types.Schema&gt;`.</span>
<span class="sd">            The model signature can be :py:func:`inferred &lt;mlflow.models.infer_signature&gt;`</span>
<span class="sd">            from datasets with valid model input (e.g. the training dataset with target</span>
<span class="sd">            column omitted) and valid model output (e.g. model predictions generated on</span>
<span class="sd">            the training dataset), for example:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from mlflow.models import infer_signature</span>

<span class="sd">                train = df.drop_column(&quot;target_label&quot;)</span>
<span class="sd">                predictions = ...  # compute model predictions</span>
<span class="sd">                signature = infer_signature(train, predictions)</span>

<span class="sd">        input_example: {{ input_example }}</span>
<span class="sd">        pip_requirements: {{ pip_requirements }}</span>
<span class="sd">        extra_pip_requirements: {{ extra_pip_requirements }}</span>
<span class="sd">        onnx_execution_providers: List of strings defining onnxruntime execution providers.</span>
<span class="sd">            Defaults to example:</span>
<span class="sd">            ``[&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]``</span>
<span class="sd">            This uses GPU preferentially over CPU.</span>
<span class="sd">            See onnxruntime API for further descriptions:</span>
<span class="sd">            https://onnxruntime.ai/docs/execution-providers/</span>
<span class="sd">        onnx_session_options: Dictionary of options to be passed to onnxruntime.InferenceSession.</span>
<span class="sd">            For example:</span>
<span class="sd">            ``{</span>
<span class="sd">            &#39;graph_optimization_level&#39;: 99,</span>
<span class="sd">            &#39;intra_op_num_threads&#39;: 1,</span>
<span class="sd">            &#39;inter_op_num_threads&#39;: 1,</span>
<span class="sd">            &#39;execution_mode&#39;: &#39;sequential&#39;</span>
<span class="sd">            }``</span>
<span class="sd">            &#39;execution_mode&#39; can be set to &#39;sequential&#39; or &#39;parallel&#39;.</span>
<span class="sd">            See onnxruntime API for further descriptions:</span>
<span class="sd">            https://onnxruntime.ai/docs/api/python/api_summary.html#sessionoptions</span>
<span class="sd">        metadata: {{ metadata }}</span>
<span class="sd">        save_as_external_data: Save tensors to external file(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">onnx</span>

    <span class="k">if</span> <span class="n">onnx_execution_providers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">onnx_execution_providers</span> <span class="o">=</span> <span class="n">ONNX_EXECUTION_PROVIDERS</span>

    <span class="n">_validate_env_arguments</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span><span class="p">)</span>

    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">code_dir_subpath</span> <span class="o">=</span> <span class="n">_validate_and_copy_code_paths</span><span class="p">(</span><span class="n">code_paths</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mlflow_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>
    <span class="k">if</span> <span class="n">input_example</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_save_example</span><span class="p">(</span><span class="n">mlflow_model</span><span class="p">,</span> <span class="n">input_example</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="n">metadata</span>
    <span class="n">model_data_subpath</span> <span class="o">=</span> <span class="n">_MODEL_DATA_SUBPATH</span>
    <span class="n">model_data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">model_data_subpath</span><span class="p">)</span>

    <span class="c1"># Save onnx-model</span>
    <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">onnx</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.9.0&quot;</span><span class="p">):</span>
        <span class="n">onnx</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">model_data_path</span><span class="p">,</span> <span class="n">save_as_external_data</span><span class="o">=</span><span class="n">save_as_external_data</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">onnx</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">model_data_path</span><span class="p">)</span>

    <span class="n">pyfunc</span><span class="o">.</span><span class="n">add_to_model</span><span class="p">(</span>
        <span class="n">mlflow_model</span><span class="p">,</span>
        <span class="n">loader_module</span><span class="o">=</span><span class="s2">&quot;mlflow.onnx&quot;</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">model_data_subpath</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">python_env</span><span class="o">=</span><span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">_validate_onnx_session_options</span><span class="p">(</span><span class="n">onnx_session_options</span><span class="p">)</span>

    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">add_flavor</span><span class="p">(</span>
        <span class="n">FLAVOR_NAME</span><span class="p">,</span>
        <span class="n">onnx_version</span><span class="o">=</span><span class="n">onnx</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">model_data_subpath</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="n">onnx_execution_providers</span><span class="p">,</span>
        <span class="n">onnx_session_options</span><span class="o">=</span><span class="n">onnx_session_options</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="o">:=</span> <span class="n">get_total_file_size</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">model_size_bytes</span> <span class="o">=</span> <span class="n">size</span>
    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">conda_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pip_requirements</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="n">get_default_pip_requirements</span><span class="p">()</span>
            <span class="c1"># To ensure `_load_pyfunc` can successfully load the model during the dependency</span>
            <span class="c1"># inference, `mlflow_model.save` must be called beforehand to save an MLmodel file.</span>
            <span class="n">inferred_reqs</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_pip_requirements</span><span class="p">(</span>
                <span class="n">path</span><span class="p">,</span>
                <span class="n">FLAVOR_NAME</span><span class="p">,</span>
                <span class="n">fallback</span><span class="o">=</span><span class="n">default_reqs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">inferred_reqs</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">default_reqs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_pip_requirements</span><span class="p">(</span>
            <span class="n">default_reqs</span><span class="p">,</span>
            <span class="n">pip_requirements</span><span class="p">,</span>
            <span class="n">extra_pip_requirements</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_conda_env</span><span class="p">(</span><span class="n">conda_env</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">safe_dump</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Save `constraints.txt` if necessary</span>
    <span class="k">if</span> <span class="n">pip_constraints</span><span class="p">:</span>
        <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_constraints</span><span class="p">))</span>

    <span class="c1"># Save `requirements.txt`</span>
    <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_requirements</span><span class="p">))</span>

    <span class="n">_PythonEnv</span><span class="o">.</span><span class="n">current</span><span class="p">()</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">_load_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">onnx</span>

    <span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_OnnxModelWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">onnxruntime</span>

        <span class="c1"># Get the model meta data from the MLModel yaml file which may contain the providers</span>
        <span class="c1"># specification.</span>
        <span class="n">local_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span><span class="p">)</span>
        <span class="n">model_meta</span> <span class="o">=</span> <span class="n">Model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">))</span>

        <span class="c1"># Check if the MLModel config has the providers meta data</span>
        <span class="k">if</span> <span class="s2">&quot;providers&quot;</span> <span class="ow">in</span> <span class="n">model_meta</span><span class="o">.</span><span class="n">flavors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">providers</span> <span class="o">=</span> <span class="n">model_meta</span><span class="o">.</span><span class="n">flavors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)[</span><span class="s2">&quot;providers&quot;</span><span class="p">]</span>
        <span class="c1"># If not, then default to the predefined list.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">providers</span> <span class="o">=</span> <span class="n">ONNX_EXECUTION_PROVIDERS</span>

        <span class="n">sess_options</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>
        <span class="n">options</span> <span class="o">=</span> <span class="n">model_meta</span><span class="o">.</span><span class="n">flavors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;onnx_session_options&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">options</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">inter_op_num_threads</span> <span class="o">:=</span> <span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inter_op_num_threads&quot;</span><span class="p">):</span>
                <span class="n">sess_options</span><span class="o">.</span><span class="n">inter_op_num_threads</span> <span class="o">=</span> <span class="n">inter_op_num_threads</span>
            <span class="k">if</span> <span class="n">intra_op_num_threads</span> <span class="o">:=</span> <span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;intra_op_num_threads&quot;</span><span class="p">):</span>
                <span class="n">sess_options</span><span class="o">.</span><span class="n">intra_op_num_threads</span> <span class="o">=</span> <span class="n">intra_op_num_threads</span>
            <span class="k">if</span> <span class="n">execution_mode</span> <span class="o">:=</span> <span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;execution_mode&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">execution_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;SEQUENTIAL&quot;</span><span class="p">:</span>
                    <span class="n">sess_options</span><span class="o">.</span><span class="n">execution_mode</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">ExecutionMode</span><span class="o">.</span><span class="n">ORT_SEQUENTIAL</span>
                <span class="k">elif</span> <span class="n">execution_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;PARALLEL&quot;</span><span class="p">:</span>
                    <span class="n">sess_options</span><span class="o">.</span><span class="n">execution_mode</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">ExecutionMode</span><span class="o">.</span><span class="n">ORT_PARALLEL</span>
            <span class="k">if</span> <span class="n">graph_optimization_level</span> <span class="o">:=</span> <span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;graph_optimization_level&quot;</span><span class="p">):</span>
                <span class="n">sess_options</span><span class="o">.</span><span class="n">graph_optimization_level</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">GraphOptimizationLevel</span><span class="p">(</span>
                    <span class="n">graph_optimization_level</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">extra_session_config</span> <span class="o">:=</span> <span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;extra_session_config&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">extra_session_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">sess_options</span><span class="o">.</span><span class="n">add_session_config_entry</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="c1"># NOTE: Some distributions of onnxruntime require the specification of the providers</span>
        <span class="c1"># argument on calling. E.g. onnxruntime-gpu. The package import call does not differentiate</span>
        <span class="c1">#  which architecture specific version has been installed, as all are imported with</span>
        <span class="c1"># onnxruntime. onnxruntime documentation says that from v1.9.0 some distributions require</span>
        <span class="c1">#  the providers list to be provided on calling an InferenceSession. Therefore the try</span>
        <span class="c1">#  catch structure below attempts to create an inference session with just the model path</span>
        <span class="c1">#  as pre v1.9.0. If that fails, it will use the providers list call.</span>
        <span class="c1"># At the moment this is just CUDA and CPU, and probably should be expanded.</span>
        <span class="c1"># A method of user customization has been provided by adding a variable in the save_model()</span>
        <span class="c1"># function, which allows the ability to pass the list of execution providers via a</span>
        <span class="c1"># optional argument e.g.</span>
        <span class="c1">#</span>
        <span class="c1"># mlflow.onnx.save_model(..., providers=[&#39;CUDAExecutionProvider&#39;...])</span>
        <span class="c1">#</span>
        <span class="c1"># For details of the execution providers construct of onnxruntime, see:</span>
        <span class="c1"># https://onnxruntime.ai/docs/execution-providers/</span>
        <span class="c1">#</span>
        <span class="c1"># For a information on how execution providers are used with onnxruntime InferenceSession,</span>
        <span class="c1"># see the API page below:</span>
        <span class="c1"># https://onnxruntime.ai/docs/api/python/api_summary.html#id8</span>
        <span class="c1">#</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rt</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">sess_options</span><span class="o">=</span><span class="n">sess_options</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rt</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
                <span class="n">path</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="n">providers</span><span class="p">,</span> <span class="n">sess_options</span><span class="o">=</span><span class="n">sess_options</span>
            <span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rt</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">())</span> <span class="o">&gt;=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">inp</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">type</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">rt</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">outp</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">outp</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">rt</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()]</span>

    <span class="k">def</span> <span class="nf">get_raw_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the underlying model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rt</span>

    <span class="k">def</span> <span class="nf">_cast_float64_to_float32</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feeds</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">input_name</span><span class="p">,</span> <span class="n">input_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_type</span> <span class="o">==</span> <span class="s2">&quot;tensor(float)&quot;</span><span class="p">:</span>
                <span class="n">feed</span> <span class="o">=</span> <span class="n">feeds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">feed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">feed</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
                    <span class="n">feeds</span><span class="p">[</span><span class="n">input_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">feed</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">feeds</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            data: Either a pandas DataFrame, numpy.ndarray or a dictionary.</span>
<span class="sd">                Dictionary input is expected to be a valid ONNX model feed dictionary.</span>

<span class="sd">                Numpy array input is supported iff the model has a single tensor input and is</span>
<span class="sd">                converted into an ONNX feed dictionary with the appropriate key.</span>

<span class="sd">                Pandas DataFrame is converted to ONNX inputs as follows:</span>
<span class="sd">                    - If the underlying ONNX model only defines a *single* input tensor, the</span>
<span class="sd">                      DataFrame&#39;s values are converted to a NumPy array representation using the</span>
<span class="sd">                      `DataFrame.values()</span>
<span class="sd">                      &lt;https://pandas.pydata.org/pandas-docs/stable/reference/api/</span>
<span class="sd">                      pandas.DataFrame.values.html#pandas.DataFrame.values&gt;`_ method.</span>
<span class="sd">                    - If the underlying ONNX model defines *multiple* input tensors, each column</span>
<span class="sd">                      of the DataFrame is converted to a NumPy array representation.</span>

<span class="sd">                For more information about the ONNX Runtime, see</span>
<span class="sd">                `&lt;https://github.com/microsoft/onnxruntime&gt;`_.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model predictions. If the input is a pandas.DataFrame, the predictions are returned</span>
<span class="sd">            in a pandas.DataFrame. If the input is a numpy array or a dictionary the</span>
<span class="sd">            predictions are returned in a dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">feed_dict</span> <span class="o">=</span> <span class="n">data</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="c1"># NB: We do allow scoring with a single tensor (ndarray) in order to be compatible with</span>
            <span class="c1"># supported pyfunc inputs iff the model has a single input. The passed tensor is</span>
            <span class="c1"># assumed to be the first input.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">]</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to map numpy array input to the expected model &quot;</span>
                    <span class="s2">&quot;input. &quot;</span>
                    <span class="s2">&quot;Numpy arrays can only be used as input for MLflow ONNX &quot;</span>
                    <span class="s2">&quot;models that have a single input. This model requires &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2"> inputs. Please pass in data as either a &quot;</span>
                    <span class="s2">&quot;dictionary or a DataFrame with the following tensors&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;: </span><span class="si">{</span><span class="n">inputs</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span> <span class="n">data</span><span class="p">}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span> <span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">}</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Input should be a dictionary or a numpy array or a pandas.DataFrame, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;got &#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
            <span class="p">)</span>

        <span class="c1"># ONNXRuntime throws the following exception for some operators when the input</span>
        <span class="c1"># contains float64 values. Unfortunately, even if the original user-supplied input</span>
        <span class="c1"># did not contain float64 values, the serialization/deserialization between the</span>
        <span class="c1"># client and the scoring server can introduce 64-bit floats. This is being tracked in</span>
        <span class="c1"># https://github.com/mlflow/mlflow/issues/1286. Meanwhile, we explicitly cast the input to</span>
        <span class="c1"># 32-bit floats when needed. TODO: Remove explicit casting when issue #1286 is fixed.</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cast_float64_to_float32</span><span class="p">(</span><span class="n">feed_dict</span><span class="p">)</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rt</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_names</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>

            <span class="k">def</span> <span class="nf">format_output</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
                <span class="c1"># Output can be list and it should be converted to a numpy array</span>
                <span class="c1"># https://github.com/mlflow/mlflow/issues/2499</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">format_output</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_names</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)}</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_names</span><span class="p">,</span> <span class="n">predicted</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_load_pyfunc</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load PyFunc implementation. Called by ``pyfunc.load_model``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_OnnxModelWrapper</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>


<div class="viewcode-block" id="load_model"><a class="viewcode-back" href="../../python_api/mlflow.onnx.html#mlflow.onnx.load_model">[docs]</a><span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">dst_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load an ONNX model from a local file or a run.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_uri: The location, in URI format, of the MLflow model, for example:</span>

<span class="sd">            - ``/Users/me/path/to/local/model``</span>
<span class="sd">            - ``relative/path/to/local/model``</span>
<span class="sd">            - ``s3://my_bucket/path/to/model``</span>
<span class="sd">            - ``runs:/&lt;mlflow_run_id&gt;/run-relative/path/to/model``</span>
<span class="sd">            - ``models:/&lt;model_name&gt;/&lt;model_version&gt;``</span>
<span class="sd">            - ``models:/&lt;model_name&gt;/&lt;stage&gt;``</span>

<span class="sd">            For more information about supported URI schemes, see the</span>
<span class="sd">            `Artifacts Documentation &lt;https://www.mlflow.org/docs/latest/</span>
<span class="sd">            tracking.html#artifact-stores&gt;`_.</span>
<span class="sd">        dst_path: The local filesystem path to which to download the model artifact.</span>
<span class="sd">            This directory must already exist. If unspecified, a local output</span>
<span class="sd">            path will be created.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An ONNX model instance.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">local_model_path</span> <span class="o">=</span> <span class="n">_download_artifact_from_uri</span><span class="p">(</span><span class="n">artifact_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">dst_path</span><span class="p">)</span>
    <span class="n">flavor_conf</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">)</span>
    <span class="n">onnx_model_artifacts_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">_load_model</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="n">onnx_model_artifacts_path</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_model"><a class="viewcode-back" href="../../python_api/mlflow.onnx.html#mlflow.onnx.log_model">[docs]</a><span class="nd">@format_docstring</span><span class="p">(</span><span class="n">LOG_MODEL_PARAM_DOCS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">package_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">log_model</span><span class="p">(</span>
    <span class="n">onnx_model</span><span class="p">,</span>
    <span class="n">artifact_path</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">ModelSignature</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="p">:</span> <span class="n">ModelInputExample</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">await_registration_for</span><span class="o">=</span><span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">onnx_execution_providers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">onnx_session_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">save_as_external_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log an ONNX model as an MLflow artifact for the current run.</span>

<span class="sd">    Args:</span>
<span class="sd">        onnx_model: ONNX model to be saved.</span>
<span class="sd">        artifact_path: Run-relative artifact path.</span>
<span class="sd">        conda_env: {{ conda_env }}</span>
<span class="sd">        code_paths: {{ code_paths }}</span>
<span class="sd">        registered_model_name: If given, create a model version under</span>
<span class="sd">            ``registered_model_name``, also creating a registered model if one</span>
<span class="sd">            with the given name does not exist.</span>
<span class="sd">        signature: :py:class:`ModelSignature &lt;mlflow.models.ModelSignature&gt;`</span>
<span class="sd">            describes model input and output :py:class:`Schema &lt;mlflow.types.Schema&gt;`.</span>
<span class="sd">            The model signature can be :py:func:`inferred &lt;mlflow.models.infer_signature&gt;`</span>
<span class="sd">            from datasets with valid model input (e.g. the training dataset with target</span>
<span class="sd">            column omitted) and valid model output (e.g. model predictions generated on</span>
<span class="sd">            the training dataset), for example:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from mlflow.models import infer_signature</span>

<span class="sd">                train = df.drop_column(&quot;target_label&quot;)</span>
<span class="sd">                predictions = ...  # compute model predictions</span>
<span class="sd">                signature = infer_signature(train, predictions)</span>

<span class="sd">        input_example: {{ input_example }}</span>
<span class="sd">        await_registration_for: Number of seconds to wait for the model version to finish</span>
<span class="sd">            being created and is in ``READY`` status. By default, the function</span>
<span class="sd">            waits for five minutes. Specify 0 or None to skip waiting.</span>
<span class="sd">        pip_requirements: {{ pip_requirements }}</span>
<span class="sd">        extra_pip_requirements: {{ extra_pip_requirements }}</span>
<span class="sd">        onnx_execution_providers: List of strings defining onnxruntime execution providers.</span>
<span class="sd">            Defaults to example:</span>
<span class="sd">            [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]</span>
<span class="sd">            This uses GPU preferentially over CPU.</span>
<span class="sd">            See onnxruntime API for further descriptions:</span>
<span class="sd">            https://onnxruntime.ai/docs/execution-providers/</span>
<span class="sd">        onnx_session_options: Dictionary of options to be passed to onnxruntime.InferenceSession.</span>
<span class="sd">            For example:</span>
<span class="sd">            ``{</span>
<span class="sd">            &#39;graph_optimization_level&#39;: 99,</span>
<span class="sd">            &#39;intra_op_num_threads&#39;: 1,</span>
<span class="sd">            &#39;inter_op_num_threads&#39;: 1,</span>
<span class="sd">            &#39;execution_mode&#39;: &#39;sequential&#39;</span>
<span class="sd">            }``</span>
<span class="sd">            &#39;execution_mode&#39; can be set to &#39;sequential&#39; or &#39;parallel&#39;.</span>
<span class="sd">            See onnxruntime API for further descriptions:</span>
<span class="sd">            https://onnxruntime.ai/docs/api/python/api_summary.html#sessionoptions</span>
<span class="sd">        metadata: {{ metadata }}</span>
<span class="sd">        save_as_external_data: Save tensors to external file(s).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A :py:class:`ModelInfo &lt;mlflow.models.model.ModelInfo&gt;` instance that contains the</span>
<span class="sd">        metadata of the logged model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="n">artifact_path</span><span class="p">,</span>
        <span class="n">flavor</span><span class="o">=</span><span class="n">mlflow</span><span class="o">.</span><span class="n">onnx</span><span class="p">,</span>
        <span class="n">onnx_model</span><span class="o">=</span><span class="n">onnx_model</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">conda_env</span><span class="p">,</span>
        <span class="n">code_paths</span><span class="o">=</span><span class="n">code_paths</span><span class="p">,</span>
        <span class="n">registered_model_name</span><span class="o">=</span><span class="n">registered_model_name</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
        <span class="n">await_registration_for</span><span class="o">=</span><span class="n">await_registration_for</span><span class="p">,</span>
        <span class="n">pip_requirements</span><span class="o">=</span><span class="n">pip_requirements</span><span class="p">,</span>
        <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="n">extra_pip_requirements</span><span class="p">,</span>
        <span class="n">onnx_execution_providers</span><span class="o">=</span><span class="n">onnx_execution_providers</span><span class="p">,</span>
        <span class="n">onnx_session_options</span><span class="o">=</span><span class="n">onnx_session_options</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">save_as_external_data</span><span class="o">=</span><span class="n">save_as_external_data</span><span class="p">,</span>
    <span class="p">)</span></div>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>