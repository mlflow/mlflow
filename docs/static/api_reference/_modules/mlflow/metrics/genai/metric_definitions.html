

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/metrics/genai/metric_definitions -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.metrics.genai.metric_definitions</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/metrics/genai/metric_definitions.html">
  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../../search.html"/>
    <link rel="top" title="MLflow 2.17.1.dev0 documentation" href="../../../../index.html"/>
        <link rel="up" title="mlflow.metrics" href="../../metrics.html"/> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../../_static/tabs.js"></script>
<script type="text/javascript" src="../../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="../../../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../../index.html" class="main-navigation-home"><img src="../../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rest-api.html">REST API</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../metrics.html">mlflow.metrics</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.metrics.genai.metric_definitions</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/metrics/genai/metric_definitions" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.metrics.genai.metric_definitions</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">from</span> <span class="nn">mlflow.exceptions</span> <span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span> <span class="nn">mlflow.metrics.genai.base</span> <span class="kn">import</span> <span class="n">EvaluationExample</span>
<span class="kn">from</span> <span class="nn">mlflow.metrics.genai.genai_metric</span> <span class="kn">import</span> <span class="n">make_genai_metric</span>
<span class="kn">from</span> <span class="nn">mlflow.metrics.genai.utils</span> <span class="kn">import</span> <span class="n">_get_latest_metric_version</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">EvaluationMetric</span>
<span class="kn">from</span> <span class="nn">mlflow.protos.databricks_pb2</span> <span class="kn">import</span> <span class="n">INTERNAL_ERROR</span><span class="p">,</span> <span class="n">INVALID_PARAMETER_VALUE</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.annotations</span> <span class="kn">import</span> <span class="n">experimental</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.class_utils</span> <span class="kn">import</span> <span class="n">_get_class_from_string</span>


<div class="viewcode-block" id="answer_similarity"><a class="viewcode-back" href="../../../../python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">answer_similarity</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EvaluationExample</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvaluationMetric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function will create a genai metric used to evaluate the answer similarity of an LLM</span>
<span class="sd">    using the model provided. Answer similarity will be assessed by the semantic similarity of the</span>
<span class="sd">    output to the ``ground_truth``, which should be specified in the ``targets`` column.</span>

<span class="sd">    The ``targets`` eval_arg must be provided as part of the input dataset or output</span>
<span class="sd">    predictions. This can be mapped to a column of a different name using ``col_mapping``</span>
<span class="sd">    in the ``evaluator_config`` parameter, or using the ``targets`` parameter in mlflow.evaluate().</span>

<span class="sd">    An MlflowException will be raised if the specified version for this metric does not exist.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: (Optional) Model uri of an openai or gateway judge model in the format of</span>
<span class="sd">            &quot;openai:/gpt-4&quot; or &quot;gateway:/my-route&quot;. Defaults to</span>
<span class="sd">            &quot;openai:/gpt-4&quot;. Your use of a third party LLM service (e.g., OpenAI) for</span>
<span class="sd">            evaluation may be subject to and governed by the LLM service&#39;s terms of use.</span>
<span class="sd">        metric_version: (Optional) The version of the answer similarity metric to use.</span>
<span class="sd">            Defaults to the latest version.</span>
<span class="sd">        examples: (Optional) Provide a list of examples to help the judge model evaluate the</span>
<span class="sd">            answer similarity. It is highly recommended to add examples to be used as a reference to</span>
<span class="sd">            evaluate the new results.</span>
<span class="sd">        metric_metadata: (Optional) Dictionary of metadata to be attached to the</span>
<span class="sd">            EvaluationMetric object. Useful for model evaluators that require additional</span>
<span class="sd">            information to determine how to evaluate this metric.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A metric object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">metric_version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">metric_version</span> <span class="o">=</span> <span class="n">_get_latest_metric_version</span><span class="p">()</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mlflow.metrics.genai.prompts.</span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.AnswerSimilarityMetric&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">answer_similarity_class_module</span> <span class="o">=</span> <span class="n">_get_class_from_string</span><span class="p">(</span><span class="n">class_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to find answer similarity metric for version </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; Please check the version&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to construct answer similarity metric </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INTERNAL_ERROR</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">examples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">answer_similarity_class_module</span><span class="o">.</span><span class="n">default_examples</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">answer_similarity_class_module</span><span class="o">.</span><span class="n">default_model</span>

    <span class="k">return</span> <span class="n">make_genai_metric</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;answer_similarity&quot;</span><span class="p">,</span>
        <span class="n">definition</span><span class="o">=</span><span class="n">answer_similarity_class_module</span><span class="o">.</span><span class="n">definition</span><span class="p">,</span>
        <span class="n">grading_prompt</span><span class="o">=</span><span class="n">answer_similarity_class_module</span><span class="o">.</span><span class="n">grading_prompt</span><span class="p">,</span>
        <span class="n">include_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">metric_version</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">grading_context_columns</span><span class="o">=</span><span class="n">answer_similarity_class_module</span><span class="o">.</span><span class="n">grading_context_columns</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">answer_similarity_class_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;p90&quot;</span><span class="p">],</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="answer_correctness"><a class="viewcode-back" href="../../../../python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">answer_correctness</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EvaluationExample</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvaluationMetric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function will create a genai metric used to evaluate the answer correctness of an LLM</span>
<span class="sd">    using the model provided. Answer correctness will be assessed by the accuracy of the provided</span>
<span class="sd">    output based on the ``ground_truth``, which should be specified in the ``targets`` column.</span>

<span class="sd">    The ``targets`` eval_arg must be provided as part of the input dataset or output</span>
<span class="sd">    predictions. This can be mapped to a column of a different name using ``col_mapping``</span>
<span class="sd">    in the ``evaluator_config`` parameter, or using the ``targets`` parameter in mlflow.evaluate().</span>

<span class="sd">    An MlflowException will be raised if the specified version for this metric does not exist.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model uri of an openai or gateway judge model in the format of</span>
<span class="sd">            &quot;openai:/gpt-4&quot; or &quot;gateway:/my-route&quot;. Defaults to</span>
<span class="sd">            &quot;openai:/gpt-4&quot;. Your use of a third party LLM service (e.g., OpenAI) for</span>
<span class="sd">            evaluation may be subject to and governed by the LLM service&#39;s terms of use.</span>
<span class="sd">        metric_version: The version of the answer correctness metric to use.</span>
<span class="sd">            Defaults to the latest version.</span>
<span class="sd">        examples: Provide a list of examples to help the judge model evaluate the</span>
<span class="sd">            answer correctness. It is highly recommended to add examples to be used as a reference</span>
<span class="sd">            to evaluate the new results.</span>
<span class="sd">        metric_metadata: (Optional) Dictionary of metadata to be attached to the</span>
<span class="sd">            EvaluationMetric object. Useful for model evaluators that require additional</span>
<span class="sd">            information to determine how to evaluate this metric.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A metric object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">metric_version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">metric_version</span> <span class="o">=</span> <span class="n">_get_latest_metric_version</span><span class="p">()</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mlflow.metrics.genai.prompts.</span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.AnswerCorrectnessMetric&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">answer_correctness_class_module</span> <span class="o">=</span> <span class="n">_get_class_from_string</span><span class="p">(</span><span class="n">class_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to find answer correctness metric for version </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Please check the version&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to construct answer correctness metric </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INTERNAL_ERROR</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">examples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">answer_correctness_class_module</span><span class="o">.</span><span class="n">default_examples</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">answer_correctness_class_module</span><span class="o">.</span><span class="n">default_model</span>

    <span class="k">return</span> <span class="n">make_genai_metric</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;answer_correctness&quot;</span><span class="p">,</span>
        <span class="n">definition</span><span class="o">=</span><span class="n">answer_correctness_class_module</span><span class="o">.</span><span class="n">definition</span><span class="p">,</span>
        <span class="n">grading_prompt</span><span class="o">=</span><span class="n">answer_correctness_class_module</span><span class="o">.</span><span class="n">grading_prompt</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">metric_version</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">grading_context_columns</span><span class="o">=</span><span class="n">answer_correctness_class_module</span><span class="o">.</span><span class="n">grading_context_columns</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">answer_correctness_class_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;p90&quot;</span><span class="p">],</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="faithfulness"><a class="viewcode-back" href="../../../../python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">faithfulness</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_latest_metric_version</span><span class="p">(),</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EvaluationExample</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvaluationMetric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function will create a genai metric used to evaluate the faithfullness of an LLM using the</span>
<span class="sd">    model provided. Faithfulness will be assessed based on how factually consistent the output</span>
<span class="sd">    is to the ``context``.</span>

<span class="sd">    The ``context`` eval_arg must be provided as part of the input dataset or output</span>
<span class="sd">    predictions. This can be mapped to a column of a different name using ``col_mapping``</span>
<span class="sd">    in the ``evaluator_config`` parameter.</span>

<span class="sd">    An MlflowException will be raised if the specified version for this metric does not exist.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model uri of an openai or gateway judge model in the format of</span>
<span class="sd">            &quot;openai:/gpt-4&quot; or &quot;gateway:/my-route&quot;. Defaults to</span>
<span class="sd">            &quot;openai:/gpt-4&quot;. Your use of a third party LLM service (e.g., OpenAI) for</span>
<span class="sd">            evaluation may be subject to and governed by the LLM service&#39;s terms of use.</span>
<span class="sd">        metric_version: The version of the faithfulness metric to use.</span>
<span class="sd">            Defaults to the latest version.</span>
<span class="sd">        examples: Provide a list of examples to help the judge model evaluate the</span>
<span class="sd">            faithfulness. It is highly recommended to add examples to be used as a reference to</span>
<span class="sd">            evaluate the new results.</span>
<span class="sd">        metric_metadata: (Optional) Dictionary of metadata to be attached to the</span>
<span class="sd">            EvaluationMetric object. Useful for model evaluators that require additional</span>
<span class="sd">            information to determine how to evaluate this metric.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A metric object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mlflow.metrics.genai.prompts.</span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.FaithfulnessMetric&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">faithfulness_class_module</span> <span class="o">=</span> <span class="n">_get_class_from_string</span><span class="p">(</span><span class="n">class_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to find faithfulness metric for version </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; Please check the version&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to construct faithfulness metric </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INTERNAL_ERROR</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">examples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">faithfulness_class_module</span><span class="o">.</span><span class="n">default_examples</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">faithfulness_class_module</span><span class="o">.</span><span class="n">default_model</span>

    <span class="k">return</span> <span class="n">make_genai_metric</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">,</span>
        <span class="n">definition</span><span class="o">=</span><span class="n">faithfulness_class_module</span><span class="o">.</span><span class="n">definition</span><span class="p">,</span>
        <span class="n">grading_prompt</span><span class="o">=</span><span class="n">faithfulness_class_module</span><span class="o">.</span><span class="n">grading_prompt</span><span class="p">,</span>
        <span class="n">include_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">metric_version</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">grading_context_columns</span><span class="o">=</span><span class="n">faithfulness_class_module</span><span class="o">.</span><span class="n">grading_context_columns</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">faithfulness_class_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;p90&quot;</span><span class="p">],</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="answer_relevance"><a class="viewcode-back" href="../../../../python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_relevance">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">answer_relevance</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_latest_metric_version</span><span class="p">(),</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EvaluationExample</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvaluationMetric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function will create a genai metric used to evaluate the answer relevance of an LLM</span>
<span class="sd">    using the model provided. Answer relevance will be assessed based on the appropriateness and</span>
<span class="sd">    applicability of the output with respect to the input.</span>

<span class="sd">    An MlflowException will be raised if the specified version for this metric does not exist.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model uri of an openai or gateway judge model in the format of</span>
<span class="sd">            &quot;openai:/gpt-4&quot; or &quot;gateway:/my-route&quot;. Defaults to</span>
<span class="sd">            &quot;openai:/gpt-4&quot;. Your use of a third party LLM service (e.g., OpenAI) for</span>
<span class="sd">            evaluation may be subject to and governed by the LLM service&#39;s terms of use.</span>
<span class="sd">        metric_version: The version of the answer relevance metric to use.</span>
<span class="sd">            Defaults to the latest version.</span>
<span class="sd">        examples: Provide a list of examples to help the judge model evaluate the</span>
<span class="sd">            answer relevance. It is highly recommended to add examples to be used as a reference to</span>
<span class="sd">            evaluate the new results.</span>
<span class="sd">        metric_metadata: (Optional) Dictionary of metadata to be attached to the</span>
<span class="sd">            EvaluationMetric object. Useful for model evaluators that require additional</span>
<span class="sd">            information to determine how to evaluate this metric.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A metric object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mlflow.metrics.genai.prompts.</span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.AnswerRelevanceMetric&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">answer_relevance_class_module</span> <span class="o">=</span> <span class="n">_get_class_from_string</span><span class="p">(</span><span class="n">class_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to find answer relevance metric for version </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; Please check the version&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to construct answer relevance metric </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INTERNAL_ERROR</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">examples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">answer_relevance_class_module</span><span class="o">.</span><span class="n">default_examples</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">answer_relevance_class_module</span><span class="o">.</span><span class="n">default_model</span>

    <span class="k">return</span> <span class="n">make_genai_metric</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;answer_relevance&quot;</span><span class="p">,</span>
        <span class="n">definition</span><span class="o">=</span><span class="n">answer_relevance_class_module</span><span class="o">.</span><span class="n">definition</span><span class="p">,</span>
        <span class="n">grading_prompt</span><span class="o">=</span><span class="n">answer_relevance_class_module</span><span class="o">.</span><span class="n">grading_prompt</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">metric_version</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">answer_relevance_class_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;p90&quot;</span><span class="p">],</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="relevance"><a class="viewcode-back" href="../../../../python_api/mlflow.metrics.html#mlflow.metrics.genai.relevance">[docs]</a><span class="k">def</span> <span class="nf">relevance</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EvaluationExample</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvaluationMetric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function will create a genai metric used to evaluate the evaluate the relevance of an</span>
<span class="sd">    LLM using the model provided. Relevance will be assessed by the appropriateness, significance,</span>
<span class="sd">    and applicability of the output with respect to the input and ``context``.</span>

<span class="sd">    The ``context`` eval_arg must be provided as part of the input dataset or output</span>
<span class="sd">    predictions. This can be mapped to a column of a different name using ``col_mapping``</span>
<span class="sd">    in the ``evaluator_config`` parameter.</span>

<span class="sd">    An MlflowException will be raised if the specified version for this metric does not exist.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: (Optional) Model uri of an openai or gateway judge model in the format of</span>
<span class="sd">            &quot;openai:/gpt-4&quot; or &quot;gateway:/my-route&quot;. Defaults to</span>
<span class="sd">            &quot;openai:/gpt-4&quot;. Your use of a third party LLM service (e.g., OpenAI) for</span>
<span class="sd">            evaluation may be subject to and governed by the LLM service&#39;s terms of use.</span>
<span class="sd">        metric_version: (Optional) The version of the relevance metric to use.</span>
<span class="sd">            Defaults to the latest version.</span>
<span class="sd">        examples: (Optional) Provide a list of examples to help the judge model evaluate the</span>
<span class="sd">            relevance. It is highly recommended to add examples to be used as a reference to</span>
<span class="sd">            evaluate the new results.</span>
<span class="sd">        metric_metadata: (Optional) Dictionary of metadata to be attached to the</span>
<span class="sd">            EvaluationMetric object. Useful for model evaluators that require additional</span>
<span class="sd">            information to determine how to evaluate this metric.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A metric object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">metric_version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">metric_version</span> <span class="o">=</span> <span class="n">_get_latest_metric_version</span><span class="p">()</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mlflow.metrics.genai.prompts.</span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.RelevanceMetric&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">relevance_class_module</span> <span class="o">=</span> <span class="n">_get_class_from_string</span><span class="p">(</span><span class="n">class_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to find relevance metric for version </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Please check the version&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to construct relevance metric </span><span class="si">{</span><span class="n">metric_version</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INTERNAL_ERROR</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">examples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">relevance_class_module</span><span class="o">.</span><span class="n">default_examples</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">relevance_class_module</span><span class="o">.</span><span class="n">default_model</span>

    <span class="k">return</span> <span class="n">make_genai_metric</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relevance&quot;</span><span class="p">,</span>
        <span class="n">definition</span><span class="o">=</span><span class="n">relevance_class_module</span><span class="o">.</span><span class="n">definition</span><span class="p">,</span>
        <span class="n">grading_prompt</span><span class="o">=</span><span class="n">relevance_class_module</span><span class="o">.</span><span class="n">grading_prompt</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">metric_version</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">grading_context_columns</span><span class="o">=</span><span class="n">relevance_class_module</span><span class="o">.</span><span class="n">grading_context_columns</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">relevance_class_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;p90&quot;</span><span class="p">],</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
    <span class="p">)</span></div>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../../',
      VERSION:'2.17.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>