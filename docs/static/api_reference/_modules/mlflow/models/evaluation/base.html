

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/models/evaluation/base -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.models.evaluation.base</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/models/evaluation/base.html">
  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../../search.html"/>
    <link rel="top" title="MLflow 2.17.1.dev0 documentation" href="../../../../index.html"/>
        <link rel="up" title="Module code" href="../../../index.html"/> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../../_static/tabs.js"></script>
<script type="text/javascript" src="../../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="../../../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../../index.html" class="main-navigation-home"><img src="../../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rest-api.html">REST API</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.models.evaluation.base</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/models/evaluation/base" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.models.evaluation.base</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">keyword</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">urllib</span>
<span class="kn">import</span> <span class="nn">urllib.parse</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">inspect</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Signature</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">FunctionType</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.data.dataset</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">mlflow.data.evaluation_dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">EvaluationDataset</span><span class="p">,</span>
    <span class="n">convert_data_to_mlflow_dataset</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.entities.dataset_input</span> <span class="kn">import</span> <span class="n">DatasetInput</span>
<span class="kn">from</span> <span class="nn">mlflow.entities.input_tag</span> <span class="kn">import</span> <span class="n">InputTag</span>
<span class="kn">from</span> <span class="nn">mlflow.exceptions</span> <span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span> <span class="nn">mlflow.protos.databricks_pb2</span> <span class="kn">import</span> <span class="n">INVALID_PARAMETER_VALUE</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking.artifact_utils</span> <span class="kn">import</span> <span class="n">_download_artifact_from_uri</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking.client</span> <span class="kn">import</span> <span class="n">MlflowClient</span>
<span class="kn">from</span> <span class="nn">mlflow.utils</span> <span class="kn">import</span> <span class="n">_get_fully_qualified_class_name</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.annotations</span> <span class="kn">import</span> <span class="n">developer_stable</span><span class="p">,</span> <span class="n">experimental</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.class_utils</span> <span class="kn">import</span> <span class="n">_get_class_from_string</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.file_utils</span> <span class="kn">import</span> <span class="n">TempDir</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.mlflow_tags</span> <span class="kn">import</span> <span class="n">MLFLOW_DATASET_CONTEXT</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.proto_json_utils</span> <span class="kn">import</span> <span class="n">NumpyEncoder</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># `pandas` is not required for `mlflow-skinny`.</span>
    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ModelType</span><span class="p">:</span>
    <span class="n">REGRESSOR</span> <span class="o">=</span> <span class="s2">&quot;regressor&quot;</span>
    <span class="n">CLASSIFIER</span> <span class="o">=</span> <span class="s2">&quot;classifier&quot;</span>
    <span class="n">QUESTION_ANSWERING</span> <span class="o">=</span> <span class="s2">&quot;question-answering&quot;</span>
    <span class="n">TEXT_SUMMARIZATION</span> <span class="o">=</span> <span class="s2">&quot;text-summarization&quot;</span>
    <span class="n">TEXT</span> <span class="o">=</span> <span class="s2">&quot;text&quot;</span>
    <span class="n">RETRIEVER</span> <span class="o">=</span> <span class="s2">&quot;retriever&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;This class is not meant to be instantiated.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">REGRESSOR</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">CLASSIFIER</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">QUESTION_ANSWERING</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">TEXT_SUMMARIZATION</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">RETRIEVER</span><span class="p">,</span>
        <span class="p">)</span>


<div class="viewcode-block" id="EvaluationMetric"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.EvaluationMetric">[docs]</a><span class="k">class</span> <span class="nc">EvaluationMetric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    An evaluation metric.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_fn: A function that computes the metric with the following signature:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def eval_fn(</span>
<span class="sd">                    predictions: pandas.Series,</span>
<span class="sd">                    targets: pandas.Series,</span>
<span class="sd">                    metrics: Dict[str, MetricValue],</span>
<span class="sd">                    **kwargs,</span>
<span class="sd">                ) -&gt; Union[float, MetricValue]:</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    Args:</span>
<span class="sd">                        predictions: A pandas Series containing the predictions made by the model.</span>
<span class="sd">                        targets: (Optional) A pandas Series containing the corresponding labels</span>
<span class="sd">                            for the predictions made on that input.</span>
<span class="sd">                        metrics: (Optional) A dictionary containing the metrics calculated by the</span>
<span class="sd">                            default evaluator.  The keys are the names of the metrics and the values</span>
<span class="sd">                            are the metric values.  To access the MetricValue for the metrics</span>
<span class="sd">                            calculated by the system, make sure to specify the type hint for this</span>
<span class="sd">                            parameter as Dict[str, MetricValue].  Refer to the DefaultEvaluator</span>
<span class="sd">                            behavior section for what metrics will be returned based on the type of</span>
<span class="sd">                            model (i.e. classifier or regressor).</span>
<span class="sd">                        kwargs: Includes a list of args that are used to compute the metric. These</span>
<span class="sd">                            args could be information coming from input data, model outputs,</span>
<span class="sd">                            other metrics, or parameters specified in the `evaluator_config`</span>
<span class="sd">                            argument of the `mlflow.evaluate` API.</span>

<span class="sd">                    Returns: MetricValue with per-row scores, per-row justifications, and aggregate</span>
<span class="sd">                        results.</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    ...</span>

<span class="sd">        name: The name of the metric.</span>
<span class="sd">        greater_is_better: Whether a higher value of the metric is better.</span>
<span class="sd">        long_name: (Optional) The long name of the metric. For example,</span>
<span class="sd">            ``&quot;root_mean_squared_error&quot;`` for ``&quot;mse&quot;``.</span>
<span class="sd">        version: (Optional) The metric version. For example ``v1``.</span>
<span class="sd">        metric_details: (Optional) A description of the metric and how it is calculated.</span>
<span class="sd">        metric_metadata: (Optional) A dictionary containing metadata for the metric.</span>
<span class="sd">        genai_metric_args: (Optional) A dictionary containing arguments specified by users</span>
<span class="sd">            when calling make_genai_metric or make_genai_metric_from_prompt. Those args</span>
<span class="sd">            are persisted so that we can deserialize the same metric object later.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">eval_fn</span><span class="p">,</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">greater_is_better</span><span class="p">,</span>
        <span class="n">long_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metric_details</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">genai_metric_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_fn</span> <span class="o">=</span> <span class="n">eval_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greater_is_better</span> <span class="o">=</span> <span class="n">greater_is_better</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">long_name</span> <span class="o">=</span> <span class="n">long_name</span> <span class="ow">or</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="n">version</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_details</span> <span class="o">=</span> <span class="n">metric_details</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_metadata</span> <span class="o">=</span> <span class="n">metric_metadata</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">genai_metric_args</span> <span class="o">=</span> <span class="n">genai_metric_args</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;name=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, greater_is_better=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">greater_is_better</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">long_name</span><span class="p">:</span>
            <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;long_name=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">long_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">version</span><span class="p">:</span>
            <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;version=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_details</span><span class="p">:</span>
            <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;metric_details=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_details</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_metadata</span><span class="p">:</span>
            <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;metric_metadata=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_metadata</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="s2">&quot;EvaluationMetric(&quot;</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span></div>


<span class="c1"># NB: we need this function because we cannot modify the signature of</span>
<span class="c1"># a class&#39;s __call__ method after the class has been defined.</span>
<span class="c1"># This is also useful to distinguish between the metric signatures with different eval_fn signatures</span>
<span class="k">def</span> <span class="nf">_generate_eval_metric_class</span><span class="p">(</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">require_strict_signature</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dynamically generate a GenAIEvaluationMetric class that can be used to evaluate the metric</span>
<span class="sd">    on the given input data. The generated class is callable with a __call__ method that</span>
<span class="sd">    takes the arguments specified in the signature of the eval_fn function.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_fn: the evaluation function of the EvaluationMetric.</span>
<span class="sd">        require_strict_signature: (Optional) Whether the eval_fn needs to follow a strict signature.</span>
<span class="sd">            If True, then the eval_fn must follow below signature:</span>

<span class="sd">                .. code-block:: python</span>

<span class="sd">                    def eval_fn(</span>
<span class="sd">                        predictions: &quot;pd.Series&quot;,</span>
<span class="sd">                        metrics: Dict[str, MetricValue],</span>
<span class="sd">                        inputs: &quot;pd.Series&quot;,</span>
<span class="sd">                        *args,</span>
<span class="sd">                    ) -&gt; MetricValue:</span>
<span class="sd">                        pass</span>

<span class="sd">            When generating a metric from `make_genai_metric`, this should be set to True.</span>
<span class="sd">            Default to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dynamically generated callable CallableEvaluationMetric class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlflow.metrics.base</span> <span class="kn">import</span> <span class="n">MetricValue</span>

    <span class="k">if</span> <span class="n">require_strict_signature</span><span class="p">:</span>
        <span class="n">allowed_kwargs_names</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">param_name</span>
            <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">eval_fn</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">param_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span> <span class="s2">&quot;metrics&quot;</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="k">def</span> <span class="nf">genai_call_method</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="o">*</span><span class="p">,</span>
            <span class="n">predictions</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
            <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">MetricValue</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MetricValue</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">missed_kwargs</span> <span class="o">:=</span> <span class="nb">set</span><span class="p">(</span><span class="n">allowed_kwargs_names</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Missing required arguments: </span><span class="si">{</span><span class="n">missed_kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">extra_kwargs</span> <span class="o">:=</span> <span class="nb">set</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">allowed_kwargs_names</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected arguments: </span><span class="si">{</span><span class="n">extra_kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_fn</span><span class="p">(</span>
                <span class="n">_convert_val_to_pd_Series</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="s2">&quot;predictions&quot;</span><span class="p">),</span>
                <span class="n">metrics</span> <span class="ow">or</span> <span class="p">{},</span>
                <span class="n">_convert_val_to_pd_Series</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">),</span>
                <span class="c1"># Note: based on https://github.com/mlflow/mlflow/blob/4fef77afdbe4d76302cb0b1aad2bd72b5cde64e9/mlflow/metrics/genai/genai_metric.py#L49-L53</span>
                <span class="c1"># the extra params passed https://github.com/mlflow/mlflow/blob/4fef77afdbe4d76302cb0b1aad2bd72b5cde64e9/mlflow/metrics/genai/genai_metric.py#L513</span>
                <span class="c1"># should always be pandas Series</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="n">_convert_val_to_pd_Series</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="n">arg_name</span><span class="p">],</span> <span class="n">arg_name</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">allowed_kwargs_names</span>
                <span class="p">],</span>
            <span class="p">)</span>

        <span class="n">genai_call_method</span><span class="o">.</span><span class="n">__signature__</span> <span class="o">=</span> <span class="n">Signature</span><span class="p">(</span>
            <span class="n">parameters</span><span class="o">=</span><span class="p">[</span>
                <span class="n">Parameter</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="n">Parameter</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span><span class="p">),</span>
                <span class="n">Parameter</span><span class="p">(</span>
                    <span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
                    <span class="n">Parameter</span><span class="o">.</span><span class="n">KEYWORD_ONLY</span><span class="p">,</span>
                    <span class="n">annotation</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
                <span class="p">),</span>
                <span class="n">Parameter</span><span class="p">(</span>
                    <span class="s2">&quot;inputs&quot;</span><span class="p">,</span>
                    <span class="n">Parameter</span><span class="o">.</span><span class="n">KEYWORD_ONLY</span><span class="p">,</span>
                    <span class="n">annotation</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
                <span class="p">),</span>
                <span class="n">Parameter</span><span class="p">(</span>
                    <span class="s2">&quot;metrics&quot;</span><span class="p">,</span>
                    <span class="n">Parameter</span><span class="o">.</span><span class="n">KEYWORD_ONLY</span><span class="p">,</span>
                    <span class="n">annotation</span><span class="o">=</span><span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">MetricValue</span><span class="p">]],</span>
                    <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="n">Parameter</span><span class="p">(</span>
                        <span class="n">name</span><span class="p">,</span> <span class="n">Parameter</span><span class="o">.</span><span class="n">KEYWORD_ONLY</span><span class="p">,</span> <span class="n">annotation</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">allowed_kwargs_names</span>
                <span class="p">],</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">genai_call_method</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Evaluate the metric on the given inputs and predictions.</span>
<span class="s2">            Note: only keyword arguments are supported.</span>

<span class="s2">            Args:</span>
<span class="s2">                predictions: predictions made by the model.</span>
<span class="s2">                inputs: inputs used to make the predictions.</span>
<span class="s2">                metrics: metrics calculated by the default evaluator.</span>
<span class="s2">                kwargs: additional arguments used to compute the metric.</span>
<span class="s2">                    Required arguments: </span><span class="si">{</span><span class="n">allowed_kwargs_names</span><span class="si">}</span>

<span class="s2">            Returns:</span>
<span class="s2">                evaluation result as MetricValue object.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="n">call_method</span> <span class="o">=</span> <span class="n">genai_call_method</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">_call_method</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MetricValue</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_fn</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">allowed_kwargs_params</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">eval_fn</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">_call_method</span><span class="o">.</span><span class="n">__signature__</span> <span class="o">=</span> <span class="n">Signature</span><span class="p">(</span>
            <span class="n">parameters</span><span class="o">=</span><span class="p">[</span>
                <span class="n">Parameter</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="n">Parameter</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span><span class="p">),</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="n">Parameter</span><span class="p">(</span>
                        <span class="n">name</span><span class="p">,</span>
                        <span class="n">Parameter</span><span class="o">.</span><span class="n">KEYWORD_ONLY</span><span class="p">,</span>
                        <span class="n">annotation</span><span class="o">=</span><span class="n">allowed_kwargs_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">annotation</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">allowed_kwargs_params</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">],</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">_call_method</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Evaluate the metric on the given inputs and predictions.</span>
<span class="s2">            Note: only keyword arguments are supported.</span>

<span class="s2">            Args:</span>
<span class="s2">                kwargs: additional arguments used to compute the metric.</span>
<span class="s2">                    Required arguments: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">allowed_kwargs_params</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span>

<span class="s2">            Returns:</span>
<span class="s2">                evaluation result as MetricValue object.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="n">call_method</span> <span class="o">=</span> <span class="n">_call_method</span>

    <span class="k">return</span> <span class="nb">type</span><span class="p">(</span>
        <span class="s2">&quot;CallableEvaluationMetric&quot;</span><span class="p">,</span>
        <span class="p">(</span><span class="n">EvaluationMetric</span><span class="p">,),</span>
        <span class="p">{</span><span class="s2">&quot;__call__&quot;</span><span class="p">:</span> <span class="n">call_method</span><span class="p">},</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_convert_val_to_pd_Series</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">val</span><span class="p">])</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> to be a string, list, or Pandas Series, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">val</span>


<div class="viewcode-block" id="make_metric"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.make_metric">[docs]</a><span class="k">def</span> <span class="nf">make_metric</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">eval_fn</span><span class="p">,</span>
    <span class="n">greater_is_better</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">long_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_details</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">genai_metric_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A factory function to create an :py:class:`EvaluationMetric` object.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_fn: A function that computes the metric with the following signature:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def eval_fn(</span>
<span class="sd">                    predictions: pandas.Series,</span>
<span class="sd">                    targets: pandas.Series,</span>
<span class="sd">                    metrics: Dict[str, MetricValue],</span>
<span class="sd">                    **kwargs,</span>
<span class="sd">                ) -&gt; Union[float, MetricValue]:</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    Args:</span>
<span class="sd">                        predictions: A pandas Series containing the predictions made by the model.</span>
<span class="sd">                        targets: (Optional) A pandas Series containing the corresponding labels</span>
<span class="sd">                            for the predictions made on that input.</span>
<span class="sd">                        metrics: (Optional) A dictionary containing the metrics calculated by the</span>
<span class="sd">                            default evaluator.  The keys are the names of the metrics and the values</span>
<span class="sd">                            are the metric values.  To access the MetricValue for the metrics</span>
<span class="sd">                            calculated by the system, make sure to specify the type hint for this</span>
<span class="sd">                            parameter as Dict[str, MetricValue].  Refer to the DefaultEvaluator</span>
<span class="sd">                            behavior section for what metrics will be returned based on the type of</span>
<span class="sd">                            model (i.e. classifier or regressor).  kwargs: Includes a list of args</span>
<span class="sd">                            that are used to compute the metric. These args could information coming</span>
<span class="sd">                            from input data, model outputs or parameters specified in the</span>
<span class="sd">                            `evaluator_config` argument of the `mlflow.evaluate` API.</span>
<span class="sd">                        kwargs: Includes a list of args that are used to compute the metric. These</span>
<span class="sd">                            args could be information coming from input data, model outputs,</span>
<span class="sd">                            other metrics, or parameters specified in the `evaluator_config`</span>
<span class="sd">                            argument of the `mlflow.evaluate` API.</span>

<span class="sd">                    Returns: MetricValue with per-row scores, per-row justifications, and aggregate</span>
<span class="sd">                        results.</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    ...</span>

<span class="sd">        greater_is_better: Whether a higher value of the metric is better.</span>
<span class="sd">        name: The name of the metric. This argument must be specified if ``eval_fn`` is a lambda</span>
<span class="sd">                    function or the ``eval_fn.__name__`` attribute is not available.</span>
<span class="sd">        long_name: (Optional) The long name of the metric. For example, ``&quot;mean_squared_error&quot;``</span>
<span class="sd">            for ``&quot;mse&quot;``.</span>
<span class="sd">        version: (Optional) The metric version. For example ``v1``.</span>
<span class="sd">        metric_details: (Optional) A description of the metric and how it is calculated.</span>
<span class="sd">        metric_metadata: (Optional) A dictionary containing metadata for the metric.</span>
<span class="sd">        genai_metric_args: (Optional) A dictionary containing arguments specified by users</span>
<span class="sd">            when calling make_genai_metric or make_genai_metric_from_prompt. Those args</span>
<span class="sd">            are persisted so that we can deserialize the same metric object later.</span>

<span class="sd">    .. seealso::</span>

<span class="sd">        - :py:class:`mlflow.models.EvaluationMetric`</span>
<span class="sd">        - :py:func:`mlflow.evaluate`</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">_make_metric</span><span class="p">(</span>
        <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="n">greater_is_better</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">long_name</span><span class="o">=</span><span class="n">long_name</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
        <span class="n">metric_details</span><span class="o">=</span><span class="n">metric_details</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
        <span class="n">genai_metric_args</span><span class="o">=</span><span class="n">genai_metric_args</span><span class="p">,</span>
        <span class="n">require_strict_signature</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_make_metric</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">eval_fn</span><span class="p">,</span>
    <span class="n">greater_is_better</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">long_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_details</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">genai_metric_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">require_strict_signature</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A factory function to create an :py:class:`EvaluationMetric` object.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_fn: A function that computes the metric with the following signature:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def eval_fn(</span>
<span class="sd">                    predictions: pandas.Series,</span>
<span class="sd">                    targets: pandas.Series,</span>
<span class="sd">                    metrics: Dict[str, MetricValue],</span>
<span class="sd">                    **kwargs,</span>
<span class="sd">                ) -&gt; Union[float, MetricValue]:</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    Args:</span>
<span class="sd">                        predictions: A pandas Series containing the predictions made by the model.</span>
<span class="sd">                        targets: (Optional) A pandas Series containing the corresponding labels</span>
<span class="sd">                            for the predictions made on that input.</span>
<span class="sd">                        metrics: (Optional) A dictionary containing the metrics calculated by the</span>
<span class="sd">                            default evaluator.  The keys are the names of the metrics and the values</span>
<span class="sd">                            are the metric values.  To access the MetricValue for the metrics</span>
<span class="sd">                            calculated by the system, make sure to specify the type hint for this</span>
<span class="sd">                            parameter as Dict[str, MetricValue].  Refer to the DefaultEvaluator</span>
<span class="sd">                            behavior section for what metrics will be returned based on the type of</span>
<span class="sd">                            model (i.e. classifier or regressor).  kwargs: Includes a list of args</span>
<span class="sd">                            that are used to compute the metric. These args could information coming</span>
<span class="sd">                            from input data, model outputs or parameters specified in the</span>
<span class="sd">                            `evaluator_config` argument of the `mlflow.evaluate` API.</span>
<span class="sd">                        kwargs: Includes a list of args that are used to compute the metric. These</span>
<span class="sd">                            args could be information coming from input data, model outputs,</span>
<span class="sd">                            other metrics, or parameters specified in the `evaluator_config`</span>
<span class="sd">                            argument of the `mlflow.evaluate` API.</span>

<span class="sd">                    Returns: MetricValue with per-row scores, per-row justifications, and aggregate</span>
<span class="sd">                        results.</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    ...</span>

<span class="sd">        greater_is_better: Whether a higher value of the metric is better.</span>
<span class="sd">        name: The name of the metric. This argument must be specified if ``eval_fn`` is a lambda</span>
<span class="sd">                    function or the ``eval_fn.__name__`` attribute is not available.</span>
<span class="sd">        long_name: (Optional) The long name of the metric. For example, ``&quot;mean_squared_error&quot;``</span>
<span class="sd">            for ``&quot;mse&quot;``.</span>
<span class="sd">        version: (Optional) The metric version. For example ``v1``.</span>
<span class="sd">        metric_details: (Optional) A description of the metric and how it is calculated.</span>
<span class="sd">        metric_metadata: (Optional) A dictionary containing metadata for the metric.</span>
<span class="sd">        genai_metric_args: (Optional) A dictionary containing arguments specified by users</span>
<span class="sd">            when calling make_genai_metric or make_genai_metric_from_prompt. Those args</span>
<span class="sd">            are persisted so that we can deserialize the same metric object later.</span>
<span class="sd">        require_strict_signature: (Optional) Whether the eval_fn needs to follow a strict signature.</span>
<span class="sd">            If True, then the eval_fn must follow below signature:</span>

<span class="sd">                .. code-block:: python</span>

<span class="sd">                    def eval_fn(</span>
<span class="sd">                        predictions: &quot;pd.Series&quot;,</span>
<span class="sd">                        metrics: Dict[str, MetricValue],</span>
<span class="sd">                        inputs: &quot;pd.Series&quot;,</span>
<span class="sd">                        *args,</span>
<span class="sd">                    ) -&gt; MetricValue:</span>
<span class="sd">                        pass</span>

<span class="sd">            When generating a metric from `make_genai_metric`, this should be set to True.</span>
<span class="sd">            Default to False.</span>

<span class="sd">    .. seealso::</span>

<span class="sd">        - :py:class:`mlflow.models.EvaluationMetric`</span>
<span class="sd">        - :py:func:`mlflow.evaluate`</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">FunctionType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">eval_fn</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;&lt;lambda&gt;&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;`name` must be specified if `eval_fn` is a lambda function.&quot;</span><span class="p">,</span>
                <span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">eval_fn</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;`name` must be specified if `eval_fn` does not have a `__name__` attribute.&quot;</span><span class="p">,</span>
                <span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">eval_fn</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">if</span> <span class="s2">&quot;/&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid metric name &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;. Metric names cannot include forward slashes (&#39;/&#39;).&quot;</span><span class="p">,</span>
            <span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">isidentifier</span><span class="p">():</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The metric name &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; provided is not a valid Python identifier, which will &quot;</span>
            <span class="s2">&quot;prevent its use as a base metric for derived metrics. Please use a valid identifier &quot;</span>
            <span class="s2">&quot;to enable creation of derived metrics that use the given metric.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">keyword</span><span class="o">.</span><span class="n">iskeyword</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The metric name &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; is a reserved Python keyword, which will &quot;</span>
            <span class="s2">&quot;prevent its use as a base metric for derived metrics. Please use a valid identifier &quot;</span>
            <span class="s2">&quot;to enable creation of derived metrics that use the given metric.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span> <span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="s2">&quot;metrics&quot;</span><span class="p">]:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The metric name &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; is used as a special parameter in MLflow metrics, which &quot;</span>
            <span class="s2">&quot;will prevent its use as a base metric for derived metrics. Please use a different &quot;</span>
            <span class="s2">&quot;name to enable creation of derived metrics that use the given metric.&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">_generate_eval_metric_class</span><span class="p">(</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">require_strict_signature</span><span class="o">=</span><span class="n">require_strict_signature</span><span class="p">)(</span>
        <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="n">greater_is_better</span><span class="p">,</span>
        <span class="n">long_name</span><span class="o">=</span><span class="n">long_name</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
        <span class="n">metric_details</span><span class="o">=</span><span class="n">metric_details</span><span class="p">,</span>
        <span class="n">metric_metadata</span><span class="o">=</span><span class="n">metric_metadata</span><span class="p">,</span>
        <span class="n">genai_metric_args</span><span class="o">=</span><span class="n">genai_metric_args</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="EvaluationArtifact"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.EvaluationArtifact">[docs]</a><span class="nd">@developer_stable</span>
<span class="k">class</span> <span class="nc">EvaluationArtifact</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A model evaluation artifact containing an artifact uri and content.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">uri</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_uri</span> <span class="o">=</span> <span class="n">uri</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_content</span> <span class="o">=</span> <span class="n">content</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_load_content_from_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">local_artifact_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Abstract interface to load the content from local artifact file path,</span>
<span class="sd">        and return the loaded content.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">local_artifact_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If ``local_artifact_path`` is ``None``, download artifact from the artifact uri.</span>
<span class="sd">        Otherwise, load artifact content from the specified path. Assign the loaded content to</span>
<span class="sd">        ``self._content``, and return the loaded content.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">local_artifact_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_content</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_content_from_file</span><span class="p">(</span><span class="n">local_artifact_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">TempDir</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
                <span class="n">temp_dir_path</span> <span class="o">=</span> <span class="n">temp_dir</span><span class="o">.</span><span class="n">path</span><span class="p">()</span>
                <span class="n">_download_artifact_from_uri</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_uri</span><span class="p">,</span> <span class="n">temp_dir_path</span><span class="p">)</span>
                <span class="n">local_artifact_file</span> <span class="o">=</span> <span class="n">temp_dir</span><span class="o">.</span><span class="n">path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">temp_dir_path</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_content</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_content_from_file</span><span class="p">(</span><span class="n">local_artifact_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_content</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_artifact_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save artifact content into specified path.&quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">content</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The content of the artifact (representation varies)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_content</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_load</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_content</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">uri</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The URI of the artifact</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uri</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(uri=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">uri</span><span class="si">}</span><span class="s2">&#39;)&quot;</span></div>


<div class="viewcode-block" id="EvaluationResult"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.EvaluationResult">[docs]</a><span class="k">class</span> <span class="nc">EvaluationResult</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents the model evaluation outputs of a `mlflow.evaluate()` API call, containing</span>
<span class="sd">    both scalar metrics and output artifacts such as performance plots.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">artifacts</span><span class="p">,</span> <span class="n">run_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_artifacts</span> <span class="o">=</span> <span class="n">artifacts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_run_id</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">run_id</span>
            <span class="k">if</span> <span class="n">run_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">active_run</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span> <span class="k">if</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">active_run</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="EvaluationResult.load"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.EvaluationResult.load">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the evaluation results from the specified local filesystem path&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;metrics.json&quot;</span><span class="p">))</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;artifacts_metadata.json&quot;</span><span class="p">))</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">artifacts_metadata</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>

        <span class="n">artifacts</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">artifacts_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;artifacts&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">artifact_name</span><span class="p">,</span> <span class="n">meta</span> <span class="ow">in</span> <span class="n">artifacts_metadata</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">uri</span> <span class="o">=</span> <span class="n">meta</span><span class="p">[</span><span class="s2">&quot;uri&quot;</span><span class="p">]</span>
            <span class="n">ArtifactCls</span> <span class="o">=</span> <span class="n">_get_class_from_string</span><span class="p">(</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;class_name&quot;</span><span class="p">])</span>
            <span class="n">artifact</span> <span class="o">=</span> <span class="n">ArtifactCls</span><span class="p">(</span><span class="n">uri</span><span class="o">=</span><span class="n">uri</span><span class="p">)</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
            <span class="n">artifact</span><span class="o">.</span><span class="n">_load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">artifacts_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
            <span class="n">artifacts</span><span class="p">[</span><span class="n">artifact_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">artifact</span>

        <span class="k">return</span> <span class="n">EvaluationResult</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">artifacts</span><span class="o">=</span><span class="n">artifacts</span><span class="p">)</span></div>

<div class="viewcode-block" id="EvaluationResult.save"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.EvaluationResult.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Write the evaluation results to the specified local filesystem path&quot;&quot;&quot;</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;metrics.json&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="bp">cls</span><span class="o">=</span><span class="n">NumpyEncoder</span><span class="p">)</span>

        <span class="n">artifacts_metadata</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">artifact_name</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;uri&quot;</span><span class="p">:</span> <span class="n">artifact</span><span class="o">.</span><span class="n">uri</span><span class="p">,</span>
                <span class="s2">&quot;class_name&quot;</span><span class="p">:</span> <span class="n">_get_fully_qualified_class_name</span><span class="p">(</span><span class="n">artifact</span><span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">artifact_name</span><span class="p">,</span> <span class="n">artifact</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;artifacts_metadata.json&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">artifacts_metadata</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>

        <span class="n">artifacts_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;artifacts&quot;</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">artifacts_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">artifact</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">artifact</span><span class="o">.</span><span class="n">uri</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
            <span class="n">artifact</span><span class="o">.</span><span class="n">_save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">artifacts_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A dictionary mapping scalar metric names to scalar metric values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">artifacts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;mlflow.models.EvaluationArtifact&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A dictionary mapping standardized artifact names (e.g. &quot;roc_data&quot;) to</span>
<span class="sd">        artifact content and location information</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_artifacts</span>

    <span class="nd">@experimental</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tables</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pd.DataFrame&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A dictionary mapping standardized artifact names (e.g. &quot;eval_results_table&quot;) to</span>
<span class="sd">        corresponding table content as pandas DataFrame.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_tables</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Cannot load eval_results_table because run_id is not specified.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">eval_tables</span>

        <span class="k">for</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">table_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_artifacts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">table_path</span><span class="o">.</span><span class="n">uri</span><span class="p">)</span><span class="o">.</span><span class="n">path</span>
            <span class="n">table_fileName</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">eval_tables</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">load_table</span><span class="p">(</span><span class="n">table_fileName</span><span class="p">,</span> <span class="n">run_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_run_id</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>  <span class="c1"># Swallow the exception since we assume its not a table.</span>

        <span class="k">return</span> <span class="n">eval_tables</span></div>


<span class="nd">@developer_stable</span>
<span class="k">class</span> <span class="nc">ModelEvaluator</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">can_evaluate</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            model_type: A string describing the model type (e.g., &quot;regressor&quot;, &quot;classifier&quot;, …).</span>
<span class="sd">            evaluator_config: A dictionary of additional configurations for</span>
<span class="sd">                the evaluator.</span>
<span class="sd">            kwargs: For forwards compatibility, a placeholder for additional arguments</span>
<span class="sd">                that may be added to the evaluation interface in the future.</span>

<span class="sd">        Returns:</span>
<span class="sd">            True if the evaluator can evaluate the specified model on the</span>
<span class="sd">            specified dataset. False otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">model_type</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">,</span>
        <span class="n">run_id</span><span class="p">,</span>
        <span class="n">evaluator_config</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_artifacts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">predictions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The abstract API to log metrics and artifacts, and return evaluation results.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_type: A string describing the model type</span>
<span class="sd">                (e.g., ``&quot;regressor&quot;``, ``&quot;classifier&quot;``, …).</span>
<span class="sd">            dataset: An instance of `mlflow.models.evaluation.base._EvaluationDataset`</span>
<span class="sd">                containing features and labels (optional) for model evaluation.</span>
<span class="sd">            run_id: The ID of the MLflow Run to which to log results.</span>
<span class="sd">            evaluator_config: A dictionary of additional configurations for</span>
<span class="sd">                the evaluator.</span>
<span class="sd">            model: A pyfunc model instance. If None, the model output is supposed to be found in</span>
<span class="sd">                ``dataset.predictions_data``.</span>
<span class="sd">            custom_metrics: Deprecated. Use ``extra_metrics`` instead.</span>
<span class="sd">            extra_metrics: A list of :py:class:`EvaluationMetric` objects.</span>
<span class="sd">            custom_artifacts: A list of callable custom artifact functions.</span>
<span class="sd">            predictions: The column name of the model output column that is used for evaluation.</span>
<span class="sd">                This is only used when a model returns a pandas dataframe that contains</span>
<span class="sd">                multiple columns.</span>
<span class="sd">            kwargs: For forwards compatibility, a placeholder for additional arguments that</span>
<span class="sd">                may be added to the evaluation interface in the future.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A :py:class:`mlflow.models.EvaluationResult` instance containing</span>
<span class="sd">            evaluation metrics and artifacts for the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<div class="viewcode-block" id="list_evaluators"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.list_evaluators">[docs]</a><span class="k">def</span> <span class="nf">list_evaluators</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a name list for all available Evaluators.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># import _model_evaluation_registry inside function to avoid circuit importing</span>
    <span class="kn">from</span> <span class="nn">mlflow.models.evaluation.evaluator_registry</span> <span class="kn">import</span> <span class="n">_model_evaluation_registry</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">_model_evaluation_registry</span><span class="o">.</span><span class="n">_registry</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></div>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">_start_run_or_reuse_active_run</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A manager context return:</span>
<span class="sd">     - If there&#39;s an active run, return the active run id.</span>
<span class="sd">     - otherwise start a mflow run with the specified run_id,</span>
<span class="sd">       if specified run_id is None, start a new run.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">active_run</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">active_run</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">active_run</span><span class="p">:</span>
        <span class="c1"># Note `mlflow.start_run` throws if `run_id` is not found.</span>
        <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">active_run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>


<span class="k">def</span> <span class="nf">_resolve_default_evaluator</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine which built-in evaluators should be used for the given model type by default.</span>

<span class="sd">    Previously, MLflow evaluate API only had a single &quot;default&quot; evaluator used for all models like</span>
<span class="sd">    classifier, regressor, etc. We split it into multiple built-in evaluators for different model</span>
<span class="sd">    types for maintainability, but in order to maintain backward compatibility, we need to map</span>
<span class="sd">    the &quot;default&quot; provided by users to the correct built-in evaluators.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_type: A string describing the model type (e.g., &quot;regressor&quot;, &quot;classifier&quot;, …).</span>
<span class="sd">        evaluator_config: A dictionary of additional configurations for the evaluator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlflow.models.evaluation.evaluator_registry</span> <span class="kn">import</span> <span class="n">_model_evaluation_registry</span>

    <span class="n">builtin_evaluators</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">evaluator</span> <span class="ow">in</span> <span class="n">_model_evaluation_registry</span><span class="o">.</span><span class="n">_registry</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span>
            <span class="ow">and</span> <span class="n">_model_evaluation_registry</span><span class="o">.</span><span class="n">is_builtin</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">can_evaluate</span><span class="p">(</span>
                <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="o">=</span><span class="n">evaluator_config</span> <span class="ow">or</span> <span class="p">{}</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">builtin_evaluators</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># We should use DefaultEvaluator only if there is no other built-in evaluator applicable.</span>
    <span class="k">return</span> <span class="n">builtin_evaluators</span> <span class="ow">or</span> <span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">]</span>


<span class="c1"># NB: We often pass around evaluator name, config, and its instance together. Ideally, the</span>
<span class="c1"># evaluator class should have name and config as class attributes, however, it was not</span>
<span class="c1"># designed that way. Adding them while keeping backward compatibility is not trivial.</span>
<span class="c1"># So, we use a dataclass to bundle them together.</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">EvaluatorBundle</span><span class="p">:</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">evaluator</span><span class="p">:</span> <span class="n">ModelEvaluator</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">resolve_evaluators_and_configs</span><span class="p">(</span>
    <span class="n">evaluators</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">evaluator_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">model_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvaluatorBundle</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The `evaluators` and `evaluator_config` arguments of the `evaluate` API can be specified</span>
<span class="sd">    in multiple ways. This function normalizes the arguments into a single format for easier</span>
<span class="sd">    downstream processing.</span>

<span class="sd">    Args:</span>
<span class="sd">        evaluators: A string or a list of strings specifying the evaluators to use for model</span>
<span class="sd">            evaluation. If None, all available evaluators will be used.</span>
<span class="sd">        evaluator_config: A dictionary containing configuration items for the evaluators.</span>
<span class="sd">        model_type: A string describing the model type (e.g., &quot;regressor&quot;, &quot;classifier&quot;, …).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of EvaluatorBundle that contains name, evaluator, config for each evaluator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlflow.models.evaluation.evaluator_registry</span> <span class="kn">import</span> <span class="n">_model_evaluation_registry</span> <span class="k">as</span> <span class="n">rg</span>

    <span class="k">def</span> <span class="nf">check_nesting_config_dict</span><span class="p">(</span><span class="n">_evaluator_name_list</span><span class="p">,</span> <span class="n">_evaluator_name_to_conf_map</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_evaluator_name_to_conf_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">k</span> <span class="ow">in</span> <span class="n">_evaluator_name_list</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">_evaluator_name_to_conf_map</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">evaluators</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># If no evaluators are specified, use all available evaluators.</span>
        <span class="n">evaluators</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">rg</span><span class="o">.</span><span class="n">_registry</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="n">evaluator_config</span> <span class="o">=</span> <span class="n">evaluator_config</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">evaluator_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">name</span> <span class="ow">in</span> <span class="n">evaluator_config</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">evaluators</span>
        <span class="p">):</span>
            <span class="c1"># If evaluator config is passed but any of available evaluator key is not</span>
            <span class="c1"># in the evaluator config, we assume the evaluator config to be a flat dict,</span>
            <span class="c1"># which is globally applied to all evaluators.</span>
            <span class="n">evaluator_config</span> <span class="o">=</span> <span class="p">{</span><span class="n">ev</span><span class="p">:</span> <span class="n">evaluator_config</span> <span class="k">for</span> <span class="n">ev</span> <span class="ow">in</span> <span class="n">evaluators</span><span class="p">}</span>

        <span class="c1"># Filter out evaluators that cannot evaluate the model type.</span>
        <span class="n">resolved</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">evaluators</span><span class="p">:</span>
            <span class="n">evaluator</span> <span class="o">=</span> <span class="n">rg</span><span class="o">.</span><span class="n">get_evaluator</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">evaluator_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">{})</span>
            <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">can_evaluate</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="o">=</span><span class="n">config</span><span class="p">):</span>
                <span class="n">resolved</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EvaluatorBundle</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">evaluator</span><span class="o">=</span><span class="n">evaluator</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">))</span>

        <span class="c1"># If any of built-in evaluator can apply, skip &quot;default&quot; evaluator.</span>
        <span class="n">default</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">ev</span> <span class="k">for</span> <span class="n">ev</span> <span class="ow">in</span> <span class="n">resolved</span> <span class="k">if</span> <span class="n">ev</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">non_default_builtins</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">ev</span> <span class="k">for</span> <span class="n">ev</span> <span class="ow">in</span> <span class="n">resolved</span> <span class="k">if</span> <span class="n">ev</span><span class="o">.</span><span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span> <span class="ow">and</span> <span class="n">rg</span><span class="o">.</span><span class="n">is_builtin</span><span class="p">(</span><span class="n">ev</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">default</span> <span class="ow">and</span> <span class="n">non_default_builtins</span><span class="p">:</span>
            <span class="n">resolved</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>
            <span class="c1"># Apply default config (passed like `evaluator_config={&quot;default&quot;: config}`) to</span>
            <span class="c1"># non-default built-in evaluators (e.g., ClassifierEvaluator) if they don&#39;t have</span>
            <span class="c1"># explicitly specified configs. This is for backward compatibility where we only</span>
            <span class="c1"># had a single &quot;default&quot; evaluator used for all models.</span>
            <span class="c1"># For example, if the user passes this for a classifier model:</span>
            <span class="c1">#     evaluator_config = {&quot;default&quot;: my_config}</span>
            <span class="c1"># it should be equivalent to</span>
            <span class="c1">#    evaluator_config = {&quot;classifier&quot;: my_config, &quot;shap&quot;: my_config}</span>
            <span class="k">for</span> <span class="n">ev</span> <span class="ow">in</span> <span class="n">non_default_builtins</span><span class="p">:</span>
                <span class="n">ev</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">config</span> <span class="ow">or</span> <span class="n">default</span><span class="o">.</span><span class="n">config</span>

        <span class="k">return</span> <span class="n">resolved</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">evaluators</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="c1"># Single evaluator name specified</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">evaluator_config</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">evaluator_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="s2">&quot;If `evaluators` argument is the name of an evaluator, evaluator_config&quot;</span>
                <span class="s2">&quot; must be None or a dict containing config items for the evaluator.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">evaluators</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="c1"># Previously we only had a single &quot;default&quot; evaluator used for all models.</span>
            <span class="c1"># We need to map &quot;default&quot; to the new dedicated builtin evaluators.</span>
            <span class="n">builtin_evaluators</span> <span class="o">=</span> <span class="n">_resolve_default_evaluator</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">EvaluatorBundle</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">rg</span><span class="o">.</span><span class="n">get_evaluator</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">evaluator_config</span> <span class="ow">or</span> <span class="p">{})</span>
                <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">builtin_evaluators</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="n">rg</span><span class="o">.</span><span class="n">is_registered</span><span class="p">(</span><span class="n">evaluators</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">EvaluatorBundle</span><span class="p">(</span><span class="n">evaluators</span><span class="p">,</span> <span class="n">rg</span><span class="o">.</span><span class="n">get_evaluator</span><span class="p">(</span><span class="n">evaluators</span><span class="p">),</span> <span class="n">evaluator_config</span> <span class="ow">or</span> <span class="p">{})</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">evaluators</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">evaluator_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_nesting_config_dict</span><span class="p">(</span>
            <span class="n">evaluators</span><span class="p">,</span> <span class="n">evaluator_config</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="s2">&quot;If `evaluators` argument is an evaluator name list, evaluator_config &quot;</span>
                <span class="s2">&quot;must be a dict containing mapping from evaluator name to individual &quot;</span>
                <span class="s2">&quot;evaluator config dict.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">evaluator_config</span> <span class="o">=</span> <span class="n">evaluator_config</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">EvaluatorBundle</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">rg</span><span class="o">.</span><span class="n">get_evaluator</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">evaluator_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">evaluators</span>
            <span class="k">if</span> <span class="n">rg</span><span class="o">.</span><span class="n">is_registered</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Invalid `evaluators` and `evaluator_config` arguments. &quot;</span>
            <span class="s2">&quot;Please refer to the documentation for correct usage.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_model_validation_contains_model_comparison</span><span class="p">(</span><span class="n">validation_thresholds</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for determining if validation_thresholds contains</span>
<span class="sd">    thresholds for model comparsion: either min_relative_change or min_absolute_change</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">validation_thresholds</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="n">thresholds</span> <span class="o">=</span> <span class="n">validation_thresholds</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">threshold</span><span class="o">.</span><span class="n">min_relative_change</span> <span class="ow">or</span> <span class="n">threshold</span><span class="o">.</span><span class="n">min_absolute_change</span> <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span>
    <span class="p">)</span>


<span class="n">_last_failed_evaluator</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_get_last_failed_evaluator</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the evaluator name of the last failed evaluator when calling `evaluate`.</span>
<span class="sd">    This can be used to check which evaluator fail when `evaluate` API fail.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_last_failed_evaluator</span>


<span class="k">def</span> <span class="nf">_evaluate</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">model_type</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">run_id</span><span class="p">,</span>
    <span class="n">evaluators</span><span class="p">,</span>
    <span class="n">custom_metrics</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="p">,</span>
    <span class="n">custom_artifacts</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The public API &quot;evaluate&quot; will verify argument first, and then pass normalized arguments</span>
<span class="sd">    to the _evaluate method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># import _model_evaluation_registry and PyFuncModel inside function to avoid circuit importing</span>

    <span class="k">global</span> <span class="n">_last_failed_evaluator</span>
    <span class="n">_last_failed_evaluator</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">MlflowClient</span><span class="p">()</span>

    <span class="n">model_uuid</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;metadata&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_uuid</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model_uuid</span> <span class="o">=</span> <span class="n">model_uuid</span><span class="o">.</span><span class="n">model_uuid</span>
        <span class="n">dataset</span><span class="o">.</span><span class="n">_log_dataset_tag</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">model_uuid</span><span class="p">)</span>

    <span class="n">eval_results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">eval_</span> <span class="ow">in</span> <span class="n">evaluators</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating the model with the </span><span class="si">{</span><span class="n">eval_</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> evaluator.&quot;</span><span class="p">)</span>
        <span class="n">_last_failed_evaluator</span> <span class="o">=</span> <span class="n">eval_</span><span class="o">.</span><span class="n">name</span>
        <span class="k">if</span> <span class="n">eval_</span><span class="o">.</span><span class="n">evaluator</span><span class="o">.</span><span class="n">can_evaluate</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="o">=</span><span class="n">eval_</span><span class="o">.</span><span class="n">config</span><span class="p">):</span>
            <span class="n">eval_result</span> <span class="o">=</span> <span class="n">eval_</span><span class="o">.</span><span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span>
                <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
                <span class="n">evaluator_config</span><span class="o">=</span><span class="n">eval_</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
                <span class="n">extra_metrics</span><span class="o">=</span><span class="n">extra_metrics</span><span class="p">,</span>
                <span class="n">custom_artifacts</span><span class="o">=</span><span class="n">custom_artifacts</span><span class="p">,</span>
                <span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">eval_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">eval_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_result</span><span class="p">)</span>

    <span class="n">_last_failed_evaluator</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The model could not be evaluated by any of the registered evaluators, please &quot;</span>
            <span class="s2">&quot;verify that the model type and other configs are set correctly.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">merged_eval_result</span> <span class="o">=</span> <span class="n">EvaluationResult</span><span class="p">({},</span> <span class="p">{},</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">eval_result</span> <span class="ow">in</span> <span class="n">eval_results</span><span class="p">:</span>
        <span class="n">merged_eval_result</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">eval_result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        <span class="n">merged_eval_result</span><span class="o">.</span><span class="n">artifacts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">eval_result</span><span class="o">.</span><span class="n">artifacts</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">merged_eval_result</span>


<span class="k">def</span> <span class="nf">_get_model_from_function</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">mlflow.pyfunc.model</span> <span class="kn">import</span> <span class="n">_PythonModelPyfuncWrapper</span>

    <span class="k">class</span> <span class="nc">ModelFromFunction</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">PythonModel</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>

    <span class="n">python_model</span> <span class="o">=</span> <span class="n">ModelFromFunction</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">_PythonModelPyfuncWrapper</span><span class="p">(</span><span class="n">python_model</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_model_deployment_endpoint_uri</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="kn">from</span> <span class="nn">mlflow.metrics.genai.model_utils</span> <span class="kn">import</span> <span class="n">_parse_model_uri</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">schema</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">_parse_model_uri</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">schema</span> <span class="o">==</span> <span class="s2">&quot;endpoints&quot;</span>
    <span class="k">except</span> <span class="n">MlflowException</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_get_model_from_deployment_endpoint_uri</span><span class="p">(</span>
    <span class="n">endpoint_uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">):</span>
    <span class="kn">from</span> <span class="nn">mlflow.metrics.genai.model_utils</span> <span class="kn">import</span> <span class="n">_parse_model_uri</span>
    <span class="kn">from</span> <span class="nn">mlflow.pyfunc.model</span> <span class="kn">import</span> <span class="n">ModelFromDeploymentEndpoint</span><span class="p">,</span> <span class="n">_PythonModelPyfuncWrapper</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">endpoint</span> <span class="o">=</span> <span class="n">_parse_model_uri</span><span class="p">(</span><span class="n">endpoint_uri</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="ow">or</span> <span class="p">{}</span>

    <span class="n">python_model</span> <span class="o">=</span> <span class="n">ModelFromDeploymentEndpoint</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_PythonModelPyfuncWrapper</span><span class="p">(</span><span class="n">python_model</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<div class="viewcode-block" id="evaluate"><a class="viewcode-back" href="../../../../python_api/mlflow.models.html#mlflow.evaluate">[docs]</a><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>  <span class="c1"># noqa: D417</span>
    <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dataset_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">evaluators</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">evaluator_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_artifacts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">validation_thresholds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">baseline_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">env_manager</span><span class="o">=</span><span class="s2">&quot;local&quot;</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">baseline_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">inference_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Evaluate the model performance on given data and selected metrics.</span>

<span class="sd">    This function evaluates a PyFunc model or custom callable on the specified dataset using</span>
<span class="sd">    specified ``evaluators``, and logs resulting metrics &amp; artifacts to MLflow tracking server.</span>
<span class="sd">    Users can also skip setting ``model`` and put the model outputs in ``data`` directly for</span>
<span class="sd">    evaluation. For detailed information, please read</span>
<span class="sd">    :ref:`the Model Evaluation documentation &lt;model-evaluation&gt;`.</span>

<span class="sd">    Default Evaluator behavior:</span>
<span class="sd">     - The default evaluator, which can be invoked with ``evaluators=&quot;default&quot;`` or</span>
<span class="sd">       ``evaluators=None``, supports model types listed below. For each pre-defined model type, the</span>
<span class="sd">       default evaluator evaluates your model on a selected set of metrics and generate artifacts</span>
<span class="sd">       like plots. Please find more details below.</span>

<span class="sd">     - For both the ``&quot;regressor&quot;`` and ``&quot;classifier&quot;`` model types, the default evaluator</span>
<span class="sd">       generates model summary plots and feature importance plots using</span>
<span class="sd">       `SHAP &lt;https://shap.readthedocs.io/en/latest/index.html&gt;`_.</span>

<span class="sd">     - For regressor models, the default evaluator additionally logs:</span>
<span class="sd">        - **metrics**: example_count, mean_absolute_error, mean_squared_error,</span>
<span class="sd">          root_mean_squared_error, sum_on_target, mean_on_target, r2_score, max_error,</span>
<span class="sd">          mean_absolute_percentage_error.</span>

<span class="sd">     - For binary classifiers, the default evaluator additionally logs:</span>
<span class="sd">        - **metrics**: true_negatives, false_positives, false_negatives, true_positives, recall,</span>
<span class="sd">          precision, f1_score, accuracy_score, example_count, log_loss, roc_auc,</span>
<span class="sd">          precision_recall_auc.</span>
<span class="sd">        - **artifacts**: lift curve plot, precision-recall plot, ROC plot.</span>

<span class="sd">     - For multiclass classifiers, the default evaluator additionally logs:</span>
<span class="sd">        - **metrics**: accuracy_score, example_count, f1_score_micro, f1_score_macro, log_loss</span>
<span class="sd">        - **artifacts**: A CSV file for &quot;per_class_metrics&quot; (per-class metrics includes</span>
<span class="sd">          true_negatives/false_positives/false_negatives/true_positives/recall/precision/roc_auc,</span>
<span class="sd">          precision_recall_auc), precision-recall merged curves plot, ROC merged curves plot.</span>

<span class="sd">     - For question-answering models, the default evaluator logs:</span>
<span class="sd">        - **metrics**: ``exact_match``, ``token_count``, `toxicity`_ (requires `evaluate`_,</span>
<span class="sd">          `torch`_, `flesch_kincaid_grade_level`_ (requires `textstat`_) and `ari_grade_level`_.</span>
<span class="sd">        - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``</span>
<span class="sd">          argument is supplied), and per-row metrics of the model in tabular format.</span>

<span class="sd">        .. _toxicity:</span>
<span class="sd">            https://huggingface.co/spaces/evaluate-measurement/toxicity</span>

<span class="sd">        .. _torch:</span>
<span class="sd">            https://pytorch.org/get-started/locally/</span>

<span class="sd">        .. _transformers:</span>
<span class="sd">            https://huggingface.co/docs/transformers/installation</span>

<span class="sd">        .. _ari_grade_level:</span>
<span class="sd">            https://en.wikipedia.org/wiki/Automated_readability_index</span>

<span class="sd">        .. _flesch_kincaid_grade_level:</span>
<span class="sd">            https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level</span>

<span class="sd">        .. _evaluate:</span>
<span class="sd">            https://pypi.org/project/evaluate</span>

<span class="sd">        .. _textstat:</span>
<span class="sd">            https://pypi.org/project/textstat</span>

<span class="sd">     - For text-summarization models, the default evaluator logs:</span>
<span class="sd">        - **metrics**: ``token_count``, `ROUGE`_ (requires `evaluate`_, `nltk`_, and</span>
<span class="sd">          `rouge_score`_ to be installed), `toxicity`_ (requires `evaluate`_, `torch`_,</span>
<span class="sd">          `transformers`_), `ari_grade_level`_ (requires `textstat`_),</span>
<span class="sd">          `flesch_kincaid_grade_level`_ (requires `textstat`_).</span>
<span class="sd">        - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``</span>
<span class="sd">          argument is supplied), and per-row metrics of the model in the tabular format.</span>

<span class="sd">        .. _ROUGE:</span>
<span class="sd">            https://huggingface.co/spaces/evaluate-metric/rouge</span>

<span class="sd">        .. _toxicity:</span>
<span class="sd">            https://huggingface.co/spaces/evaluate-measurement/toxicity</span>

<span class="sd">        .. _torch:</span>
<span class="sd">            https://pytorch.org/get-started/locally/</span>

<span class="sd">        .. _transformers:</span>
<span class="sd">            https://huggingface.co/docs/transformers/installation</span>

<span class="sd">        .. _ari_grade_level:</span>
<span class="sd">            https://en.wikipedia.org/wiki/Automated_readability_index</span>

<span class="sd">        .. _flesch_kincaid_grade_level:</span>
<span class="sd">            https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level</span>

<span class="sd">        .. _evaluate:</span>
<span class="sd">            https://pypi.org/project/evaluate</span>

<span class="sd">        .. _nltk:</span>
<span class="sd">            https://pypi.org/project/nltk</span>

<span class="sd">        .. _rouge_score:</span>
<span class="sd">            https://pypi.org/project/rouge-score</span>

<span class="sd">        .. _textstat:</span>
<span class="sd">            https://pypi.org/project/textstat</span>

<span class="sd">     - For text models, the default evaluator logs:</span>
<span class="sd">        - **metrics**: ``token_count``, `toxicity`_ (requires `evaluate`_, `torch`_,</span>
<span class="sd">          `transformers`_), `ari_grade_level`_ (requires `textstat`_),</span>
<span class="sd">          `flesch_kincaid_grade_level`_ (requires `textstat`_).</span>
<span class="sd">        - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``</span>
<span class="sd">          argument is supplied), and per-row metrics of the model in tabular format.</span>

<span class="sd">        .. _evaluate:</span>
<span class="sd">            https://pypi.org/project/evaluate</span>

<span class="sd">        .. _toxicity:</span>
<span class="sd">            https://huggingface.co/spaces/evaluate-measurement/toxicity</span>

<span class="sd">        .. _torch:</span>
<span class="sd">            https://pytorch.org/get-started/locally/</span>

<span class="sd">        .. _transformers:</span>
<span class="sd">            https://huggingface.co/docs/transformers/installation</span>

<span class="sd">        .. _ari_grade_level:</span>
<span class="sd">            https://en.wikipedia.org/wiki/Automated_readability_index</span>

<span class="sd">        .. _flesch_kincaid_grade_level:</span>
<span class="sd">            https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level</span>

<span class="sd">        .. _textstat:</span>
<span class="sd">            https://pypi.org/project/textstat</span>

<span class="sd">     - For retriever models, the default evaluator logs:</span>
<span class="sd">        - **metrics**: :mod:`precision_at_k(k) &lt;mlflow.metrics.precision_at_k&gt;`,</span>
<span class="sd">          :mod:`recall_at_k(k) &lt;mlflow.metrics.recall_at_k&gt;` and</span>
<span class="sd">          :mod:`ndcg_at_k(k) &lt;mlflow.metrics.ndcg_at_k&gt;` - all have a default value of</span>
<span class="sd">          ``retriever_k`` = 3.</span>
<span class="sd">        - **artifacts**: A JSON file containing the inputs, outputs, targets, and per-row metrics</span>
<span class="sd">          of the model in tabular format.</span>

<span class="sd">     - For sklearn models, the default evaluator additionally logs the model&#39;s evaluation criterion</span>
<span class="sd">       (e.g. mean accuracy for a classifier) computed by `model.score` method.</span>

<span class="sd">     - The metrics/artifacts listed above are logged to the active MLflow run.</span>
<span class="sd">       If no active run exists, a new MLflow run is created for logging these metrics and</span>
<span class="sd">       artifacts.</span>

<span class="sd">     - Additionally, information about the specified dataset - hash, name (if specified), path</span>
<span class="sd">       (if specified), and the UUID of the model that evaluated it - is logged to the</span>
<span class="sd">       ``mlflow.datasets`` tag.</span>

<span class="sd">     - The available ``evaluator_config`` options for the default evaluator include:</span>
<span class="sd">        - **log_model_explainability**: A boolean value specifying whether or not to log model</span>
<span class="sd">          explainability insights, default value is True.</span>
<span class="sd">        - **explainability_algorithm**: A string to specify the SHAP Explainer algorithm for model</span>
<span class="sd">          explainability. Supported algorithm includes: &#39;exact&#39;, &#39;permutation&#39;, &#39;partition&#39;,</span>
<span class="sd">          &#39;kernel&#39;.</span>
<span class="sd">          If not set, ``shap.Explainer`` is used with the &quot;auto&quot; algorithm, which chooses the best</span>
<span class="sd">          Explainer based on the model.</span>
<span class="sd">        - **explainability_nsamples**: The number of sample rows to use for computing model</span>
<span class="sd">          explainability insights. Default value is 2000.</span>
<span class="sd">        - **explainability_kernel_link**: The kernel link function used by shap kernal explainer.</span>
<span class="sd">          Available values are &quot;identity&quot; and &quot;logit&quot;. Default value is &quot;identity&quot;.</span>
<span class="sd">        - **max_classes_for_multiclass_roc_pr**:</span>
<span class="sd">          For multiclass classification tasks, the maximum number of classes for which to log</span>
<span class="sd">          the per-class ROC curve and Precision-Recall curve. If the number of classes is</span>
<span class="sd">          larger than the configured maximum, these curves are not logged.</span>
<span class="sd">        - **metric_prefix**: An optional prefix to prepend to the name of each metric and artifact</span>
<span class="sd">          produced during evaluation.</span>
<span class="sd">        - **log_metrics_with_dataset_info**: A boolean value specifying whether or not to include</span>
<span class="sd">          information about the evaluation dataset in the name of each metric logged to MLflow</span>
<span class="sd">          Tracking during evaluation, default value is True.</span>
<span class="sd">        - **pos_label**: If specified, the positive label to use when computing classification</span>
<span class="sd">          metrics such as precision, recall, f1, etc. for binary classification models. For</span>
<span class="sd">          multiclass classification and regression models, this parameter will be ignored.</span>
<span class="sd">        - **average**: The averaging method to use when computing classification metrics such as</span>
<span class="sd">          precision, recall, f1, etc. for multiclass classification models</span>
<span class="sd">          (default: ``&#39;weighted&#39;``). For binary classification and regression models, this</span>
<span class="sd">          parameter will be ignored.</span>
<span class="sd">        - **sample_weights**: Weights for each sample to apply when computing model performance</span>
<span class="sd">          metrics.</span>
<span class="sd">        - **col_mapping**: A dictionary mapping column names in the input dataset or output</span>
<span class="sd">          predictions to column names used when invoking the evaluation functions.</span>
<span class="sd">        - **retriever_k**: A parameter used when ``model_type=&quot;retriever&quot;`` as the number of</span>
<span class="sd">          top-ranked retrieved documents to use when computing the built-in metric</span>
<span class="sd">          :mod:`precision_at_k(k) &lt;mlflow.metrics.precision_at_k&gt;`,</span>
<span class="sd">          :mod:`recall_at_k(k) &lt;mlflow.metrics.recall_at_k&gt;` and</span>
<span class="sd">          :mod:`ndcg_at_k(k) &lt;mlflow.metrics.ndcg_at_k&gt;`. Default value is 3. For all other</span>
<span class="sd">          model types, this parameter will be ignored.</span>

<span class="sd">     - Limitations of evaluation dataset:</span>
<span class="sd">        - For classification tasks, dataset labels are used to infer the total number of classes.</span>
<span class="sd">        - For binary classification tasks, the negative label value must be 0 or -1 or False, and</span>
<span class="sd">          the positive label value must be 1 or True.</span>

<span class="sd">     - Limitations of metrics/artifacts computation:</span>
<span class="sd">        - For classification tasks, some metric and artifact computations require the model to</span>
<span class="sd">          output class probabilities. Currently, for scikit-learn models, the default evaluator</span>
<span class="sd">          calls the ``predict_proba`` method on the underlying model to obtain probabilities. For</span>
<span class="sd">          other model types, the default evaluator does not compute metrics/artifacts that require</span>
<span class="sd">          probability outputs.</span>

<span class="sd">     - Limitations of default evaluator logging model explainability insights:</span>
<span class="sd">        - The ``shap.Explainer`` ``auto`` algorithm uses the ``Linear`` explainer for linear models</span>
<span class="sd">          and the ``Tree`` explainer for tree models. Because SHAP&#39;s ``Linear`` and ``Tree``</span>
<span class="sd">          explainers do not support multi-class classification, the default evaluator falls back to</span>
<span class="sd">          using the ``Exact`` or ``Permutation`` explainers for multi-class classification tasks.</span>
<span class="sd">        - Logging model explainability insights is not currently supported for PySpark models.</span>
<span class="sd">        - The evaluation dataset label values must be numeric or boolean, all feature values</span>
<span class="sd">          must be numeric, and each feature column must only contain scalar values.</span>

<span class="sd">     - Limitations when environment restoration is enabled:</span>
<span class="sd">        - When environment restoration is enabled for the evaluated model (i.e. a non-local</span>
<span class="sd">          ``env_manager`` is specified), the model is loaded as a client that invokes a MLflow</span>
<span class="sd">          Model Scoring Server process in an independent Python environment with the model&#39;s</span>
<span class="sd">          training time dependencies installed. As such, methods like ``predict_proba`` (for</span>
<span class="sd">          probability outputs) or ``score`` (computes the evaluation criterian for sklearn models)</span>
<span class="sd">          of the model become inaccessible and the default evaluator does not compute metrics or</span>
<span class="sd">          artifacts that require those methods.</span>
<span class="sd">        - Because the model is an MLflow Model Server process, SHAP explanations are slower to</span>
<span class="sd">          compute. As such, model explainaibility is disabled when a non-local ``env_manager``</span>
<span class="sd">          specified, unless the ``evaluator_config`` option **log_model_explainability** is</span>
<span class="sd">          explicitly set to ``True``.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Optional. If specified, it should be one of the following:</span>

<span class="sd">            - A pyfunc model instance</span>
<span class="sd">            - A URI referring to a pyfunc model</span>
<span class="sd">            - A URI referring to an MLflow Deployments endpoint e.g. ``&quot;endpoints:/my-chat&quot;``</span>
<span class="sd">            - A callable function: This function should be able to take in model input and</span>
<span class="sd">              return predictions. It should follow the signature of the</span>
<span class="sd">              :py:func:`predict &lt;mlflow.pyfunc.PyFuncModel.predict&gt;` method. Here&#39;s an example</span>
<span class="sd">              of a valid function:</span>

<span class="sd">              .. code-block:: python</span>

<span class="sd">                  model = mlflow.pyfunc.load_model(model_uri)</span>


<span class="sd">                  def fn(model_input):</span>
<span class="sd">                      return model.predict(model_input)</span>

<span class="sd">            If omitted, it indicates a static dataset will be used for evaluation instead of a</span>
<span class="sd">            model.  In this case, the ``data`` argument must be a Pandas DataFrame or an mlflow</span>
<span class="sd">            PandasDataset that contains model outputs, and the ``predictions`` argument must be the</span>
<span class="sd">            name of the column in ``data`` that contains model outputs.</span>

<span class="sd">        data: One of the</span>
<span class="sd">            following:</span>

<span class="sd">            - A numpy array or list of evaluation features, excluding labels.</span>
<span class="sd">            - A Pandas DataFrame containing evaluation features, labels, and optionally model</span>
<span class="sd">                outputs. Model outputs are required to be provided when model is unspecified.</span>
<span class="sd">                If ``feature_names`` argument not specified, all columns except for the label</span>
<span class="sd">                column and model_output column are regarded as feature columns. Otherwise,</span>
<span class="sd">                only column names present in ``feature_names`` are regarded as feature columns.</span>
<span class="sd">            -  A Spark DataFrame containing evaluation features and labels. If</span>
<span class="sd">                ``feature_names`` argument not specified, all columns except for the label</span>
<span class="sd">                column are regarded as feature columns. Otherwise, only column names present in</span>
<span class="sd">                ``feature_names`` are regarded as feature columns. Only the first 10000 rows in</span>
<span class="sd">                the Spark DataFrame will be used as evaluation data.</span>
<span class="sd">            - A :py:class:`mlflow.data.dataset.Dataset` instance containing evaluation</span>
<span class="sd">                features, labels, and optionally model outputs. Model outputs are only supported</span>
<span class="sd">                with a PandasDataset. Model outputs are required when model is unspecified, and</span>
<span class="sd">                should be specified via the ``predictions`` prerty of the PandasDataset.</span>

<span class="sd">        model_type: (Optional) A string describing the model type. The default evaluator</span>
<span class="sd">            supports the following model types:</span>

<span class="sd">            - ``&#39;classifier&#39;``</span>
<span class="sd">            - ``&#39;regressor&#39;``</span>
<span class="sd">            - ``&#39;question-answering&#39;``</span>
<span class="sd">            - ``&#39;text-summarization&#39;``</span>
<span class="sd">            - ``&#39;text&#39;``</span>
<span class="sd">            - ``&#39;retriever&#39;``</span>

<span class="sd">            If no ``model_type`` is specified, then you must provide a a list of</span>
<span class="sd">            metrics to compute via the ``extra_metrics`` param.</span>

<span class="sd">            .. note::</span>
<span class="sd">                ``&#39;question-answering&#39;``, ``&#39;text-summarization&#39;``, ``&#39;text&#39;``, and</span>
<span class="sd">                ``&#39;retriever&#39;`` are experimental and may be changed or removed in a</span>
<span class="sd">                future release.</span>

<span class="sd">        targets: If ``data`` is a numpy array or list, a numpy array or list of evaluation</span>
<span class="sd">            labels. If ``data`` is a DataFrame, the string name of a column from ``data``</span>
<span class="sd">            that contains evaluation labels. Required for classifier and regressor models,</span>
<span class="sd">            but optional for question-answering, text-summarization, and text models. If</span>
<span class="sd">            ``data`` is a :py:class:`mlflow.data.dataset.Dataset` that defines targets,</span>
<span class="sd">            then ``targets`` is optional.</span>

<span class="sd">        predictions: Optional. The name of the column that contains model outputs.</span>

<span class="sd">            - When ``model`` is specified and outputs multiple columns, ``predictions`` can be used</span>
<span class="sd">              to specify the name of the column that will be used to store model outputs for</span>
<span class="sd">              evaluation.</span>
<span class="sd">            - When ``model`` is not specified and ``data`` is a pandas dataframe,</span>
<span class="sd">              ``predictions`` can be used to specify the name of the column in ``data`` that</span>
<span class="sd">              contains model outputs.</span>

<span class="sd">            .. code-block:: python</span>
<span class="sd">                :caption: Example usage of predictions</span>

<span class="sd">                # Evaluate a model that outputs multiple columns</span>
<span class="sd">                data = pd.DataFrame({&quot;question&quot;: [&quot;foo&quot;]})</span>


<span class="sd">                def model(inputs):</span>
<span class="sd">                    return pd.DataFrame({&quot;answer&quot;: [&quot;bar&quot;], &quot;source&quot;: [&quot;baz&quot;]})</span>


<span class="sd">                results = evaluate(</span>
<span class="sd">                    model=model,</span>
<span class="sd">                    data=data,</span>
<span class="sd">                    predictions=&quot;answer&quot;,</span>
<span class="sd">                    # other arguments if needed</span>
<span class="sd">                )</span>

<span class="sd">                # Evaluate a static dataset</span>
<span class="sd">                data = pd.DataFrame({&quot;question&quot;: [&quot;foo&quot;], &quot;answer&quot;: [&quot;bar&quot;], &quot;source&quot;: [&quot;baz&quot;]})</span>
<span class="sd">                results = evaluate(</span>
<span class="sd">                    data=data,</span>
<span class="sd">                    predictions=&quot;answer&quot;,</span>
<span class="sd">                    # other arguments if needed</span>
<span class="sd">                )</span>
<span class="sd">        dataset_path: (Optional) The path where the data is stored. Must not contain double</span>
<span class="sd">            quotes (``“``). If specified, the path is logged to the ``mlflow.datasets``</span>
<span class="sd">            tag for lineage tracking purposes.</span>

<span class="sd">        feature_names: (Optional) A list. If the ``data`` argument is a numpy array or list,</span>
<span class="sd">            ``feature_names`` is a list of the feature names for each feature. If</span>
<span class="sd">            ``feature_names=None``, then the ``feature_names`` are generated using the</span>
<span class="sd">            format ``feature_{feature_index}``. If the ``data`` argument is a Pandas</span>
<span class="sd">            DataFrame or a Spark DataFrame, ``feature_names`` is a list of the names</span>
<span class="sd">            of the feature columns in the DataFrame. If ``feature_names=None``, then</span>
<span class="sd">            all columns except the label column and the predictions column are</span>
<span class="sd">            regarded as feature columns.</span>

<span class="sd">        evaluators: The name of the evaluator to use for model evaluation, or a list of</span>
<span class="sd">            evaluator names. If unspecified, all evaluators capable of evaluating the</span>
<span class="sd">            specified model on the specified dataset are used. The default evaluator</span>
<span class="sd">            can be referred to by the name ``&quot;default&quot;``. To see all available</span>
<span class="sd">            evaluators, call :py:func:`mlflow.models.list_evaluators`.</span>

<span class="sd">        evaluator_config: A dictionary of additional configurations to supply to the evaluator.</span>
<span class="sd">            If multiple evaluators are specified, each configuration should be</span>
<span class="sd">            supplied as a nested dictionary whose key is the evaluator name.</span>

<span class="sd">        custom_metrics: Deprecated. Use ``extra_metrics`` instead.</span>
<span class="sd">        extra_metrics:</span>
<span class="sd">            (Optional) A list of :py:class:`EvaluationMetric &lt;mlflow.models.EvaluationMetric&gt;`</span>
<span class="sd">            objects.  These metrics are computed in addition to the default metrics associated with</span>
<span class="sd">            pre-defined `model_type`, and setting `model_type=None` will only compute the metrics</span>
<span class="sd">            specified in `extra_metrics`. See the `mlflow.metrics` module for more information about</span>
<span class="sd">            the builtin metrics and how to define extra metrics.</span>

<span class="sd">            .. code-block:: python</span>
<span class="sd">                :caption: Example usage of extra metrics</span>

<span class="sd">                import mlflow</span>
<span class="sd">                import numpy as np</span>


<span class="sd">                def root_mean_squared_error(eval_df, _builtin_metrics):</span>
<span class="sd">                    return np.sqrt((np.abs(eval_df[&quot;prediction&quot;] - eval_df[&quot;target&quot;]) ** 2).mean)</span>


<span class="sd">                rmse_metric = mlflow.models.make_metric(</span>
<span class="sd">                    eval_fn=root_mean_squared_error,</span>
<span class="sd">                    greater_is_better=False,</span>
<span class="sd">                )</span>
<span class="sd">                mlflow.evaluate(..., extra_metrics=[rmse_metric])</span>

<span class="sd">        custom_artifacts:</span>
<span class="sd">            (Optional) A list of custom artifact functions with the following signature:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def custom_artifact(</span>
<span class="sd">                    eval_df: Union[pandas.Dataframe, pyspark.sql.DataFrame],</span>
<span class="sd">                    builtin_metrics: Dict[str, float],</span>
<span class="sd">                    artifacts_dir: str,</span>
<span class="sd">                ) -&gt; Dict[str, Any]:</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    Args:</span>
<span class="sd">                        eval_df:</span>
<span class="sd">                            A Pandas or Spark DataFrame containing ``prediction`` and ``target``</span>
<span class="sd">                            column.  The ``prediction`` column contains the predictions made by the</span>
<span class="sd">                            model.  The ``target`` column contains the corresponding labels to the</span>
<span class="sd">                            predictions made on that row.</span>
<span class="sd">                        builtin_metrics:</span>
<span class="sd">                            A dictionary containing the metrics calculated by the default evaluator.</span>
<span class="sd">                            The keys are the names of the metrics and the values are the scalar</span>
<span class="sd">                            values of the metrics. Refer to the DefaultEvaluator behavior section</span>
<span class="sd">                            for what metrics will be returned based on the type of model (i.e.</span>
<span class="sd">                            classifier or regressor).</span>
<span class="sd">                        artifacts_dir:</span>
<span class="sd">                            A temporary directory path that can be used by the custom artifacts</span>
<span class="sd">                            function to temporarily store produced artifacts. The directory will be</span>
<span class="sd">                            deleted after the artifacts are logged.</span>

<span class="sd">                    Returns:</span>
<span class="sd">                        A dictionary that maps artifact names to artifact objects</span>
<span class="sd">                        (e.g. a Matplotlib Figure) or to artifact paths within ``artifacts_dir``.</span>
<span class="sd">                    &quot;&quot;&quot;</span>
<span class="sd">                    ...</span>

<span class="sd">            Object types that artifacts can be represented as:</span>

<span class="sd">                - A string uri representing the file path to the artifact. MLflow will infer the</span>
<span class="sd">                  type of the artifact based on the file extension.</span>
<span class="sd">                - A string representation of a JSON object. This will be saved as a .json artifact.</span>
<span class="sd">                - Pandas DataFrame. This will be resolved as a CSV artifact.</span>
<span class="sd">                - Numpy array. This will be saved as a .npy artifact.</span>
<span class="sd">                - Matplotlib Figure. This will be saved as an image artifact. Note that</span>
<span class="sd">                  ``matplotlib.pyplot.savefig`` is called behind the scene with default</span>
<span class="sd">                  configurations.</span>
<span class="sd">                  To customize, either save the figure with the desired configurations and return</span>
<span class="sd">                  its file path or define customizations through environment variables in</span>
<span class="sd">                  ``matplotlib.rcParams``.</span>
<span class="sd">                - Other objects will be attempted to be pickled with the default protocol.</span>

<span class="sd">            .. code-block:: python</span>
<span class="sd">                :caption: Example usage of custom artifacts</span>

<span class="sd">                import mlflow</span>
<span class="sd">                import matplotlib.pyplot as plt</span>


<span class="sd">                def scatter_plot(eval_df, builtin_metrics, artifacts_dir):</span>
<span class="sd">                    plt.scatter(eval_df[&quot;prediction&quot;], eval_df[&quot;target&quot;])</span>
<span class="sd">                    plt.xlabel(&quot;Targets&quot;)</span>
<span class="sd">                    plt.ylabel(&quot;Predictions&quot;)</span>
<span class="sd">                    plt.title(&quot;Targets vs. Predictions&quot;)</span>
<span class="sd">                    plt.savefig(os.path.join(artifacts_dir, &quot;example.png&quot;))</span>
<span class="sd">                    plt.close()</span>
<span class="sd">                    return {&quot;pred_target_scatter&quot;: os.path.join(artifacts_dir, &quot;example.png&quot;)}</span>


<span class="sd">                def pred_sample(eval_df, _builtin_metrics, _artifacts_dir):</span>
<span class="sd">                    return {&quot;pred_sample&quot;: pred_sample.head(10)}</span>


<span class="sd">                mlflow.evaluate(..., custom_artifacts=[scatter_plot, pred_sample])</span>

<span class="sd">        validation_thresholds: DEPRECATED. Please use :py:func:`mlflow.validate_evaluation_results`</span>
<span class="sd">            API instead for running model validation against baseline.</span>

<span class="sd">        baseline_model: DEPRECATED. Please use :py:func:`mlflow.validate_evaluation_results`</span>
<span class="sd">            API instead for running model validation against baseline.</span>

<span class="sd">        env_manager: Specify an environment manager to load the candidate ``model`` in</span>
<span class="sd">            isolated Python environments and restore their</span>
<span class="sd">            dependencies. Default value is ``local``, and the following values are</span>
<span class="sd">            supported:</span>

<span class="sd">            - ``virtualenv``: (Recommended) Use virtualenv to restore the python</span>
<span class="sd">              environment that was used to train the model.</span>
<span class="sd">            - ``conda``:  Use Conda to restore the software environment that was used</span>
<span class="sd">              to train the model.</span>
<span class="sd">            - ``local``: Use the current Python environment for model inference, which</span>
<span class="sd">              may differ from the environment used to train the model and may lead to</span>
<span class="sd">              errors or invalid predictions.</span>

<span class="sd">        model_config: the model configuration to use for loading the model with pyfunc. Inspect</span>
<span class="sd">            the model&#39;s pyfunc flavor to know which keys are supported for your</span>
<span class="sd">            specific model. If not indicated, the default model configuration</span>
<span class="sd">            from the model is used (if any).</span>

<span class="sd">        baseline_config: DEPRECATED. Please use :py:func:`mlflow.validate_evaluation_results`</span>
<span class="sd">            API instead for running model validation against baseline.</span>

<span class="sd">        inference_params: (Optional) A dictionary of inference parameters to be passed to the model</span>
<span class="sd">            when making predictions, such as ``{&quot;max_tokens&quot;: 100}``. This is only used when</span>
<span class="sd">            the ``model`` is an MLflow Deployments endpoint URI e.g. ``&quot;endpoints:/my-chat&quot;``</span>

<span class="sd">    Returns:</span>
<span class="sd">        An :py:class:`mlflow.models.EvaluationResult` instance containing</span>
<span class="sd">        metrics of evaluating the model with the given dataset.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="kn">from</span> <span class="nn">mlflow.models.evaluation.evaluator_registry</span> <span class="kn">import</span> <span class="n">_model_evaluation_registry</span>
    <span class="kn">from</span> <span class="nn">mlflow.pyfunc</span> <span class="kn">import</span> <span class="n">PyFuncModel</span><span class="p">,</span> <span class="n">_load_model_or_server</span><span class="p">,</span> <span class="n">_ServedPyFuncModel</span>
    <span class="kn">from</span> <span class="nn">mlflow.utils</span> <span class="kn">import</span> <span class="n">env_manager</span> <span class="k">as</span> <span class="n">_EnvManager</span>

    <span class="c1"># Inference params are currently only supported for passing a deployment endpoint as the model.</span>
    <span class="c1"># TODO: We should support inference_params for other model types</span>

    <span class="k">if</span> <span class="n">inference_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_model_deployment_endpoint_uri</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The inference_params argument can only be specified when the model &quot;</span>
            <span class="s2">&quot;is an MLflow Deployments endpoint URI like `endpoints:/my-chat`&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">evaluator_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">col_mapping</span> <span class="o">=</span> <span class="n">evaluator_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;col_mapping&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">col_mapping</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">col_mapping</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The data argument cannot be None.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">_EnvManager</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">env_manager</span><span class="p">)</span>

    <span class="c1"># If Dataset is provided, the targets can only be specified by the Dataset,</span>
    <span class="c1"># not the targets parameters of the mlflow.evaluate() API.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">)</span> <span class="ow">and</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The top-level targets parameter should not be specified since a Dataset &quot;</span>
            <span class="s2">&quot;is used. Please only specify the targets column name in the Dataset. For example: &quot;</span>
            <span class="s2">&quot;`data = mlflow.data.from_pandas(df=X.assign(y=y), targets=&#39;y&#39;)`. &quot;</span>
            <span class="s2">&quot;Meanwhile, please specify `mlflow.evaluate(..., targets=None, ...)`.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># If Dataset is provided and model is None, then the predictions can only be specified by the</span>
    <span class="c1"># Dataset, not the predictions parameters of the mlflow.evaluate() API.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">)</span> <span class="ow">and</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">predictions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The top-level predictions parameter should not be specified since a Dataset &quot;</span>
            <span class="s2">&quot;is used. Please only specify the predictions column name in the Dataset. For example:&quot;</span>
            <span class="s2">&quot; `data = mlflow.data.from_pandas(df=X.assign(y=y), predictions=&#39;y&#39;)`&quot;</span>
            <span class="s2">&quot;Meanwhile, please specify `mlflow.evaluate(..., predictions=None, ...)`.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># If Dataset is provided and model is specified, then the data.predictions cannot be specified.</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;predictions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The predictions parameter should not be specified in the Dataset since a &quot;</span>
            <span class="s2">&quot;model is specified. Please remove the predictions column from the Dataset.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">_ModelType</span><span class="o">.</span><span class="n">REGRESSOR</span><span class="p">,</span> <span class="n">_ModelType</span><span class="o">.</span><span class="n">CLASSIFIER</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">targets</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">targets</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The targets column name must be specified in the provided Dataset &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;for </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2"> models. For example: &quot;</span>
                    <span class="s2">&quot;`data = mlflow.data.from_pandas(df=X.assign(y=y), targets=&#39;y&#39;)`&quot;</span><span class="p">,</span>
                    <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The targets argument must be specified for </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2"> models.&quot;</span><span class="p">,</span>
                    <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">model_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">extra_metrics</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The extra_metrics argument must be specified model_type is None.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">_is_model_deployment_endpoint_uri</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">_get_model_from_deployment_endpoint_uri</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inference_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">_load_model_or_server</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">env_manager</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">env_manager</span> <span class="o">!=</span> <span class="n">_EnvManager</span><span class="o">.</span><span class="n">LOCAL</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The model argument must be a string URI referring to an MLflow model when a &quot;</span>
            <span class="s2">&quot;non-local env_manager is specified.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">PyFuncModel</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model_config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Indicating ``model_config`` when passing a `PyFuncModel`` object as &quot;</span>
                <span class="s2">&quot;model argument is not allowed. If you need to change the model configuration &quot;</span>
                <span class="s2">&quot;for the evaluation model, use &quot;</span>
                <span class="s2">&quot;``mlflow.pyfunc.load_model(model_uri, model_config=&lt;value&gt;)`` and indicate &quot;</span>
                <span class="s2">&quot;the desired configuration there.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">_get_model_from_function</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="s2">&quot;The model argument must be a string URI referring to an MLflow model, &quot;</span>
            <span class="s2">&quot;an MLflow Deployments endpoint URI, an instance of `mlflow.pyfunc.PyFuncModel`, &quot;</span>
            <span class="s2">&quot;a function, or None.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">evaluators</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">EvaluatorBundle</span><span class="p">]</span> <span class="o">=</span> <span class="n">resolve_evaluators_and_configs</span><span class="p">(</span>
        <span class="n">evaluators</span><span class="p">,</span> <span class="n">evaluator_config</span><span class="p">,</span> <span class="n">model_type</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">_start_run_or_reuse_active_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run_id</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">):</span>
            <span class="c1"># Convert data to `mlflow.data.dataset.Dataset`.</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">convert_data_to_mlflow_dataset</span><span class="p">(</span>
                    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">convert_data_to_mlflow_dataset</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">mlflow.data.pyfunc_dataset_mixin</span> <span class="kn">import</span> <span class="n">PyFuncConvertibleDatasetMixin</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">PyFuncConvertibleDatasetMixin</span><span class="p">):</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_evaluation_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>

            <span class="c1"># Use metrix_prefix configured for builtin evaluators as a dataset tag</span>
            <span class="n">context</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">evaluators</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">_model_evaluation_registry</span><span class="o">.</span><span class="n">is_builtin</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">e</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;metric_prefix&quot;</span><span class="p">):</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;metric_prefix&quot;</span><span class="p">)</span>
                    <span class="k">break</span>

            <span class="n">client</span> <span class="o">=</span> <span class="n">MlflowClient</span><span class="p">()</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">InputTag</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">MLFLOW_DATASET_CONTEXT</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">context</span><span class="p">)]</span> <span class="k">if</span> <span class="n">context</span> <span class="k">else</span> <span class="p">[]</span>
            <span class="n">dataset_input</span> <span class="o">=</span> <span class="n">DatasetInput</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">_to_mlflow_entity</span><span class="p">(),</span> <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">)</span>
            <span class="n">client</span><span class="o">.</span><span class="n">log_inputs</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="p">[</span><span class="n">dataset_input</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="n">EvaluationDataset</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span>
                <span class="n">path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
                <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
                <span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">predictions_expected_in_model_output</span> <span class="o">=</span> <span class="n">predictions</span> <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">evaluate_result</span> <span class="o">=</span> <span class="n">_evaluate</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span>
                <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
                <span class="n">evaluators</span><span class="o">=</span><span class="n">evaluators</span><span class="p">,</span>
                <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
                <span class="n">extra_metrics</span><span class="o">=</span><span class="n">extra_metrics</span><span class="p">,</span>
                <span class="n">custom_artifacts</span><span class="o">=</span><span class="n">custom_artifacts</span><span class="p">,</span>
                <span class="n">predictions</span><span class="o">=</span><span class="n">predictions_expected_in_model_output</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">_ServedPyFuncModel</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">pid</span><span class="p">,</span> <span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">)</span>

    <span class="c1"># TODO: Remove this block in a future release when we</span>
    <span class="c1"># remove the deprecated arguments.</span>
    <span class="k">if</span> <span class="n">baseline_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">validation_thresholds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">mlflow.models.evaluation.validation</span> <span class="kn">import</span> <span class="n">validate_evaluation_results</span>

        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Model validation functionality is moved from `mlflow.evaluate` to the &quot;</span>
            <span class="s2">&quot;`mlflow.validate_evaluation_results()` API. The &quot;</span>
            <span class="s2">&quot;`baseline_model` argument will be removed in a future release.&quot;</span><span class="p">,</span>
            <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">baseline_model</span> <span class="o">=</span> <span class="n">_load_model_or_server</span><span class="p">(</span>
                <span class="n">baseline_model</span><span class="p">,</span> <span class="n">env_manager</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="n">baseline_config</span>
            <span class="p">)</span>

        <span class="n">baseline_result</span> <span class="o">=</span> <span class="n">_evaluate</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">baseline_model</span><span class="p">,</span>
            <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
            <span class="n">evaluators</span><span class="o">=</span><span class="n">evaluators</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">extra_metrics</span><span class="o">=</span><span class="n">extra_metrics</span><span class="p">,</span>
            <span class="n">custom_artifacts</span><span class="o">=</span><span class="n">custom_artifacts</span><span class="p">,</span>
            <span class="n">predictions</span><span class="o">=</span><span class="n">predictions_expected_in_model_output</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">validate_evaluation_results</span><span class="p">(</span>
            <span class="n">validation_thresholds</span><span class="o">=</span><span class="n">validation_thresholds</span><span class="p">,</span>
            <span class="n">candidate_result</span><span class="o">=</span><span class="n">evaluate_result</span><span class="p">,</span>
            <span class="n">baseline_result</span><span class="o">=</span><span class="n">baseline_result</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">evaluate_result</span></div>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../../',
      VERSION:'2.17.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>