

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/transformers -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.transformers</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/transformers.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.1.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/tabs.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.transformers</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/transformers" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.transformers</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;MLflow module for HuggingFace/transformer support.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">ast</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">binascii</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">importlib</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">MappingProxyType</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">urlparse</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">Version</span>

<span class="kn">from</span> <span class="nn">mlflow</span> <span class="kn">import</span> <span class="n">pyfunc</span>
<span class="kn">from</span> <span class="nn">mlflow.environment_variables</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MLFLOW_DEFAULT_PREDICTION_DEVICE</span><span class="p">,</span>
    <span class="n">MLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY</span><span class="p">,</span>
    <span class="n">MLFLOW_HUGGINGFACE_USE_DEVICE_MAP</span><span class="p">,</span>
    <span class="n">MLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE</span><span class="p">,</span>
    <span class="n">MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.exceptions</span> <span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Model</span><span class="p">,</span>
    <span class="n">ModelInputExample</span><span class="p">,</span>
    <span class="n">ModelSignature</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.models.model</span> <span class="kn">import</span> <span class="n">MLMODEL_FILE_NAME</span>
<span class="kn">from</span> <span class="nn">mlflow.models.utils</span> <span class="kn">import</span> <span class="n">_save_example</span>
<span class="kn">from</span> <span class="nn">mlflow.protos.databricks_pb2</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BAD_REQUEST</span><span class="p">,</span>
    <span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
    <span class="n">RESOURCE_DOES_NOT_EXIST</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.store.artifact.artifact_repository_registry</span> <span class="kn">import</span> <span class="n">get_artifact_repository</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking._model_registry</span> <span class="kn">import</span> <span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking.artifact_utils</span> <span class="kn">import</span> <span class="n">_get_root_uri_and_artifact_path</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.flavor_config</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FlavorKey</span><span class="p">,</span>
    <span class="n">build_flavor_config</span><span class="p">,</span>
    <span class="n">build_flavor_config_from_local_checkpoint</span><span class="p">,</span>
    <span class="n">update_flavor_conf_to_persist_pretrained_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.hub_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_valid_hf_repo_id</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.llm_inference_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_LLM_INFERENCE_TASK_CHAT</span><span class="p">,</span>
    <span class="n">_LLM_INFERENCE_TASK_COMPLETIONS</span><span class="p">,</span>
    <span class="n">_LLM_INFERENCE_TASK_EMBEDDING</span><span class="p">,</span>
    <span class="n">_LLM_INFERENCE_TASK_KEY</span><span class="p">,</span>
    <span class="n">_LLM_INFERENCE_TASK_PREFIX</span><span class="p">,</span>
    <span class="n">_METADATA_LLM_INFERENCE_TASK_KEY</span><span class="p">,</span>
    <span class="n">_SUPPORTED_LLM_INFERENCE_TASK_TYPES_BY_PIPELINE_TASK</span><span class="p">,</span>
    <span class="n">_get_default_task_for_llm_inference_task</span><span class="p">,</span>
    <span class="n">convert_messages_to_prompt</span><span class="p">,</span>
    <span class="n">infer_signature_from_llm_inference_task</span><span class="p">,</span>
    <span class="n">postprocess_output_for_llm_inference_task</span><span class="p">,</span>
    <span class="n">postprocess_output_for_llm_v1_embedding_task</span><span class="p">,</span>
    <span class="n">preprocess_llm_embedding_params</span><span class="p">,</span>
    <span class="n">preprocess_llm_inference_input</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.model_io</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_COMPONENTS_BINARY_DIR_NAME</span><span class="p">,</span>
    <span class="n">_MODEL_BINARY_FILE_NAME</span><span class="p">,</span>
    <span class="n">load_model_and_components_from_huggingface_hub</span><span class="p">,</span>
    <span class="n">load_model_and_components_from_local</span><span class="p">,</span>
    <span class="n">save_local_checkpoint</span><span class="p">,</span>
    <span class="n">save_pipeline_pretrained_weights</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.peft</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_PEFT_ADAPTOR_DIR_NAME</span><span class="p">,</span>
    <span class="n">get_model_with_peft_adapter</span><span class="p">,</span>
    <span class="n">get_peft_base_model</span><span class="p">,</span>
    <span class="n">is_peft_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.signature</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">format_input_example_for_special_cases</span><span class="p">,</span>
    <span class="n">infer_or_get_default_signature</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers.torch_utils</span> <span class="kn">import</span> <span class="n">_TORCH_DTYPE_KEY</span><span class="p">,</span> <span class="n">_deserialize_torch_dtype</span>
<span class="kn">from</span> <span class="nn">mlflow.types.utils</span> <span class="kn">import</span> <span class="n">_validate_input_dictionary_contains_only_strings_and_lists_of_strings</span>
<span class="kn">from</span> <span class="nn">mlflow.utils</span> <span class="kn">import</span> <span class="n">_truncate_and_ellipsize</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.annotations</span> <span class="kn">import</span> <span class="n">experimental</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.autologging_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">autologging_integration</span><span class="p">,</span>
    <span class="n">disable_discrete_autologging</span><span class="p">,</span>
    <span class="n">safe_patch</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.docstring_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LOG_MODEL_PARAM_DOCS</span><span class="p">,</span>
    <span class="n">docstring_version_compatibility_warning</span><span class="p">,</span>
    <span class="n">format_docstring</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.environment</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_mlflow_conda_env</span><span class="p">,</span>
    <span class="n">_process_conda_env</span><span class="p">,</span>
    <span class="n">_process_pip_requirements</span><span class="p">,</span>
    <span class="n">_PythonEnv</span><span class="p">,</span>
    <span class="n">_validate_env_arguments</span><span class="p">,</span>
    <span class="n">infer_pip_requirements</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.file_utils</span> <span class="kn">import</span> <span class="n">TempDir</span><span class="p">,</span> <span class="n">get_total_file_size</span><span class="p">,</span> <span class="n">write_to</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.logging_utils</span> <span class="kn">import</span> <span class="n">suppress_logs</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.model_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">,</span>
    <span class="n">_download_artifact_from_uri</span><span class="p">,</span>
    <span class="n">_get_flavor_configuration</span><span class="p">,</span>
    <span class="n">_get_flavor_configuration_from_uri</span><span class="p">,</span>
    <span class="n">_validate_and_copy_code_paths</span><span class="p">,</span>
    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.requirements_utils</span> <span class="kn">import</span> <span class="n">_get_pinned_requirement</span>

<span class="c1"># The following import is only used for type hinting</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Transformers pipeline complains that PeftModel is not supported for any task type, even</span>
<span class="c1"># when the wrapped model is supported. As MLflow require users to use pipeline for logging,</span>
<span class="c1"># we should suppress that confusing error message.</span>
<span class="n">_PEFT_PIPELINE_ERROR_MSG</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;The model &#39;PeftModel[^&#39;]*&#39; is not supported for&quot;</span><span class="p">)</span>

<span class="n">FLAVOR_NAME</span> <span class="o">=</span> <span class="s2">&quot;transformers&quot;</span>

<span class="n">_CARD_TEXT_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;model_card.md&quot;</span>
<span class="n">_CARD_DATA_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;model_card_data.yaml&quot;</span>
<span class="n">_INFERENCE_CONFIG_BINARY_KEY</span> <span class="o">=</span> <span class="s2">&quot;inference_config.txt&quot;</span>
<span class="n">_LICENSE_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;LICENSE.txt&quot;</span>
<span class="n">_LICENSE_FILE_PATTERN</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;license(\.[a-z]+|$)&quot;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">IGNORECASE</span><span class="p">)</span>

<span class="n">_SUPPORTED_RETURN_TYPES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;pipeline&quot;</span><span class="p">,</span> <span class="s2">&quot;components&quot;</span><span class="p">}</span>
<span class="c1"># The default device id for CPU is -1 and GPU IDs are ordinal starting at 0, as documented here:</span>
<span class="c1"># https://huggingface.co/transformers/v4.7.0/main_classes/pipelines.html</span>
<span class="n">_TRANSFORMERS_DEFAULT_CPU_DEVICE_ID</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">_TRANSFORMERS_DEFAULT_GPU_DEVICE_ID</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_SUPPORTED_SAVE_KEYS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span><span class="p">,</span>
    <span class="n">FlavorKey</span><span class="o">.</span><span class="n">TOKENIZER</span><span class="p">,</span>
    <span class="n">FlavorKey</span><span class="o">.</span><span class="n">FEATURE_EXTRACTOR</span><span class="p">,</span>
    <span class="n">FlavorKey</span><span class="o">.</span><span class="n">IMAGE_PROCESSOR</span><span class="p">,</span>
    <span class="n">FlavorKey</span><span class="o">.</span><span class="n">TORCH_DTYPE</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_SUPPORTED_PROMPT_TEMPLATING_TASK_TYPES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;feature-extraction&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;summarization&quot;</span><span class="p">,</span>
    <span class="s2">&quot;text2text-generation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_PROMPT_TEMPLATE_RETURN_FULL_TEXT_INFO</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;text-generation pipelines saved with prompt templates have the `return_full_text` &quot;</span>
    <span class="s2">&quot;pipeline kwarg set to False by default. To override this behavior, provide a &quot;</span>
    <span class="s2">&quot;`model_config` dict with `return_full_text` set to `True` when saving the model.&quot;</span>
<span class="p">)</span>


<span class="c1"># Alias for the audio data types that Transformers pipeline (e.g. Whisper) expects.</span>
<span class="c1"># It can be one of:</span>
<span class="c1">#  1. A string representing the path or URL to an audio file.</span>
<span class="c1">#  2. A bytes object representing the raw audio data.</span>
<span class="c1">#  3. A float numpy array representing the audio time series.</span>
<span class="n">AudioInput</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="get_default_pip_requirements"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.get_default_pip_requirements">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">get_default_pip_requirements</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        model: The model instance to be saved in order to provide the required underlying</span>
<span class="sd">            deep learning execution framework dependency requirements. Note that this must</span>
<span class="sd">            be the actual model instance and not a Pipeline.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of default pip requirements for MLflow Models that have been produced with the</span>
<span class="sd">        ``transformers`` flavor. Calls to :py:func:`save_model()` and :py:func:`log_model()`</span>
<span class="sd">        produce a pip environment that contain these requirements at a minimum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">packages</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformers&quot;</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">_get_engine_type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">packages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">engine</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">packages</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="s2">&quot;tensorflow&quot;</span><span class="p">]</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Could not infer model execution engine type due to huggingface_hub not &quot;</span>
            <span class="s2">&quot;being installed or unable to connect in online mode. Adding both Pytorch&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;and Tensorflow to requirements.</span><span class="se">\n</span><span class="s2">Failure cause: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;torch&quot;</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">:</span>
        <span class="n">packages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;torchvision&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;accelerate&quot;</span><span class="p">):</span>
            <span class="n">packages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;accelerate&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">packages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;peft&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">_get_pinned_requirement</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_validate_transformers_model_dict</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Validator for a submitted save dictionary for the transformers model. If any additional keys</span>
<span class="sd">    are provided, raise to indicate which invalid keys were submitted.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">invalid_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_SAVE_KEYS</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">invalid_keys</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Invalid dictionary submitted for &#39;transformers_model&#39;. The &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;key(s) </span><span class="si">{</span><span class="n">invalid_keys</span><span class="si">}</span><span class="s2"> are not permitted. Must be one of: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_SUPPORTED_SAVE_KEYS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">transformers_model</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The &#39;transformers_model&#39; dictionary must have an entry for </span><span class="si">{</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">model</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;name_or_path&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The submitted model type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not inherit &quot;</span>
            <span class="s2">&quot;from a transformers pre-trained model. It is missing the attribute &quot;</span>
            <span class="s2">&quot;&#39;name_or_path&#39;. Please verify that the model is a supported &quot;</span>
            <span class="s2">&quot;transformers model.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>


<div class="viewcode-block" id="get_default_conda_env"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.get_default_conda_env">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">get_default_conda_env</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        The default Conda environment for MLflow Models produced with the ``transformers``</span>
<span class="sd">        flavor, based on the model instance framework type of the model to be logged.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_mlflow_conda_env</span><span class="p">(</span><span class="n">additional_pip_deps</span><span class="o">=</span><span class="n">get_default_pip_requirements</span><span class="p">(</span><span class="n">model</span><span class="p">))</span></div>


<div class="viewcode-block" id="save_model"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.save_model">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@docstring_version_compatibility_warning</span><span class="p">(</span><span class="n">integration_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="nd">@format_docstring</span><span class="p">(</span><span class="n">LOG_MODEL_PARAM_DOCS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">package_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span>
    <span class="n">transformers_model</span><span class="p">,</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_card</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">inference_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlflow_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Model</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelSignature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelInputExample</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">example_no_conversion</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># pylint: disable=unused-argument</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save a trained transformers model to a path on the local file system. Note that</span>
<span class="sd">    saving transformers models with custom code (i.e. models that require</span>
<span class="sd">    ``trust_remote_code=True``) requires ``transformers &gt;= 4.26.0``.</span>

<span class="sd">    Args:</span>
<span class="sd">        transformers_model:</span>
<span class="sd">            The transformers model to save. This can be one of the following format:</span>

<span class="sd">                1. A transformers `Pipeline` instance.</span>
<span class="sd">                2. A dictionary that maps required components of a pipeline to the named keys</span>
<span class="sd">                    of [&quot;model&quot;, &quot;image_processor&quot;, &quot;tokenizer&quot;, &quot;feature_extractor&quot;].</span>
<span class="sd">                    The `model` key in the dictionary must map to a value that inherits from</span>
<span class="sd">                    `PreTrainedModel`, `TFPreTrainedModel`, or `FlaxPreTrainedModel`.</span>
<span class="sd">                    All other component entries in the dictionary must support the defined task</span>
<span class="sd">                    type that is associated with the base model type configuration.</span>
<span class="sd">                3. A string that represents a path to a local/DBFS directory containing a model</span>
<span class="sd">                    checkpoint. The directory must contain a `config.json` file that is required</span>
<span class="sd">                    for loading the transformers model. This is particularly useful when logging</span>
<span class="sd">                    a model that cannot be loaded into memory for serialization.</span>

<span class="sd">            An example of specifying a `Pipeline` from a default pipeline instantiation:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from transformers import pipeline</span>

<span class="sd">                qa_pipe = pipeline(&quot;question-answering&quot;, &quot;csarron/mobilebert-uncased-squad-v2&quot;)</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    mlflow.transformers.save_model(</span>
<span class="sd">                        transformers_model=qa_pipe,</span>
<span class="sd">                        path=&quot;path/to/save/model&quot;,</span>
<span class="sd">                    )</span>

<span class="sd">            An example of specifying component-level parts of a transformers model is shown below:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from transformers import MobileBertForQuestionAnswering, AutoTokenizer</span>

<span class="sd">                architecture = &quot;csarron/mobilebert-uncased-squad-v2&quot;</span>
<span class="sd">                tokenizer = AutoTokenizer.from_pretrained(architecture)</span>
<span class="sd">                model = MobileBertForQuestionAnswering.from_pretrained(architecture)</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    components = {</span>
<span class="sd">                        &quot;model&quot;: model,</span>
<span class="sd">                        &quot;tokenizer&quot;: tokenizer,</span>
<span class="sd">                    }</span>
<span class="sd">                    mlflow.transformers.save_model(</span>
<span class="sd">                        transformers_model=components,</span>
<span class="sd">                        path=&quot;path/to/save/model&quot;,</span>
<span class="sd">                    )</span>

<span class="sd">            An example of specifying a local checkpoint path is shown below:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    mlflow.transformers.save_model(</span>
<span class="sd">                        transformers_model=&quot;path/to/local/checkpoint&quot;,</span>
<span class="sd">                        path=&quot;path/to/save/model&quot;,</span>
<span class="sd">                    )</span>

<span class="sd">        path: Local path destination for the serialized model to be saved.</span>
<span class="sd">        processor: An optional ``Processor`` subclass object. Some model architectures,</span>
<span class="sd">            particularly multi-modal types, utilize Processors to combine text</span>
<span class="sd">            encoding and image or audio encoding in a single entrypoint.</span>

<span class="sd">            .. Note:: If a processor is supplied when saving a model, the</span>
<span class="sd">                        model will be unavailable for loading as a ``Pipeline`` or for</span>
<span class="sd">                        usage with pyfunc inference.</span>
<span class="sd">        task: The transformers-specific task type of the model, or MLflow inference task type.</span>
<span class="sd">            If provided a transformers-specific task type, these strings are utilized so</span>
<span class="sd">            that a pipeline can be created with the appropriate internal call architecture</span>
<span class="sd">            to meet the needs of a given model.</span>
<span class="sd">            If this argument is provided as a inference task type or not specified, the</span>
<span class="sd">            pipeline utilities within the transformers library will be used to infer the</span>
<span class="sd">            correct task type. If the value specified is not a supported type,</span>
<span class="sd">            an Exception will be thrown.</span>
<span class="sd">        torch_dtype: The Pytorch dtype applied to the model when loading back. This is useful</span>
<span class="sd">            when you want to save the model with a specific dtype that is different from the</span>
<span class="sd">            dtype of the model when it was trained. If not specified, the current dtype of the</span>
<span class="sd">            model instance will be used.</span>
<span class="sd">        model_card: An Optional `ModelCard` instance from `huggingface-hub`. If provided, the</span>
<span class="sd">            contents of the model card will be saved along with the provided</span>
<span class="sd">            `transformers_model`. If not provided, an attempt will be made to fetch</span>
<span class="sd">            the card from the base pretrained model that is provided (or the one that is</span>
<span class="sd">            included within a provided `Pipeline`).</span>

<span class="sd">            .. Note:: In order for a ModelCard to be fetched (if not provided),</span>
<span class="sd">                        the huggingface_hub package must be installed and the version</span>
<span class="sd">                        must be &gt;=0.10.0</span>
<span class="sd">        inference_config:</span>

<span class="sd">            .. Warning:: Deprecated. `inference_config` is deprecated in favor of `model_config`.</span>

<span class="sd">        code_paths: {{ code_paths }}</span>
<span class="sd">        mlflow_model: An MLflow model object that specifies the flavor that this model is being</span>
<span class="sd">            added to.</span>
<span class="sd">        signature: A Model Signature object that describes the input and output Schema of the</span>
<span class="sd">            model. The model signature can be inferred using `infer_signature` function</span>
<span class="sd">            of `mlflow.models.signature`.</span>

<span class="sd">            .. code-block:: python</span>
<span class="sd">                :caption: Example</span>

<span class="sd">                from mlflow.models import infer_signature</span>
<span class="sd">                from mlflow.transformers import generate_signature_output</span>
<span class="sd">                from transformers import pipeline</span>

<span class="sd">                en_to_de = pipeline(&quot;translation_en_to_de&quot;)</span>

<span class="sd">                data = &quot;MLflow is great!&quot;</span>
<span class="sd">                output = generate_signature_output(en_to_de, data)</span>
<span class="sd">                signature = infer_signature(data, output)</span>

<span class="sd">                mlflow.transformers.save_model(</span>
<span class="sd">                    transformers_model=en_to_de,</span>
<span class="sd">                    path=&quot;/path/to/save/model&quot;,</span>
<span class="sd">                    signature=signature,</span>
<span class="sd">                    input_example=data,</span>
<span class="sd">                )</span>

<span class="sd">                loaded = mlflow.pyfunc.load_model(&quot;/path/to/save/model&quot;)</span>
<span class="sd">                print(loaded.predict(data))</span>
<span class="sd">                # MLflow ist gro√üartig!</span>

<span class="sd">            If an input_example is provided and the signature is not, a signature will</span>
<span class="sd">            be inferred automatically and applied to the MLmodel file iff the</span>
<span class="sd">            pipeline type is a text-based model (NLP). If the pipeline type is not</span>
<span class="sd">            a supported type, this inference functionality will not function correctly</span>
<span class="sd">            and a warning will be issued. In order to ensure that a precise signature</span>
<span class="sd">            is logged, it is recommended to explicitly provide one.</span>
<span class="sd">        input_example: {{ input_example }}</span>
<span class="sd">        pip_requirements: {{ pip_requirements }}</span>
<span class="sd">        extra_pip_requirements: {{ extra_pip_requirements }}</span>
<span class="sd">        conda_env: {{ conda_env }}</span>
<span class="sd">        metadata: {{ metadata }}</span>
<span class="sd">        model_config:</span>
<span class="sd">            A dict of valid overrides that can be applied to a pipeline instance during inference.</span>
<span class="sd">            These arguments are used exclusively for the case of loading the model as a ``pyfunc``</span>
<span class="sd">            Model or for use in Spark.</span>
<span class="sd">            These values are not applied to a returned Pipeline from a call to</span>
<span class="sd">            ``mlflow.transformers.load_model()``</span>

<span class="sd">            .. Warning:: If the key provided is not compatible with either the</span>
<span class="sd">                    Pipeline instance for the task provided or is not a valid</span>
<span class="sd">                    override to any arguments available in the Model, an</span>
<span class="sd">                    Exception will be raised at runtime. It is very important</span>
<span class="sd">                    to validate the entries in this dictionary to ensure</span>
<span class="sd">                    that they are valid prior to saving or logging.</span>

<span class="sd">            An example of providing overrides for a question generation model:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from transformers import pipeline, AutoTokenizer</span>

<span class="sd">                task = &quot;text-generation&quot;</span>
<span class="sd">                architecture = &quot;gpt2&quot;</span>

<span class="sd">                sentence_pipeline = pipeline(</span>
<span class="sd">                    task=task,</span>
<span class="sd">                    tokenizer=AutoTokenizer.from_pretrained(architecture),</span>
<span class="sd">                    model=architecture,</span>
<span class="sd">                )</span>

<span class="sd">                # Validate that the overrides function</span>
<span class="sd">                prompts = [&quot;Generative models are&quot;, &quot;I&#39;d like a coconut so that I can&quot;]</span>

<span class="sd">                # validation of config prior to save or log</span>
<span class="sd">                model_config = {</span>
<span class="sd">                    &quot;top_k&quot;: 2,</span>
<span class="sd">                    &quot;num_beams&quot;: 5,</span>
<span class="sd">                    &quot;max_length&quot;: 30,</span>
<span class="sd">                    &quot;temperature&quot;: 0.62,</span>
<span class="sd">                    &quot;top_p&quot;: 0.85,</span>
<span class="sd">                    &quot;repetition_penalty&quot;: 1.15,</span>
<span class="sd">                }</span>

<span class="sd">                # Verify that no exceptions are thrown</span>
<span class="sd">                sentence_pipeline(prompts, **model_config)</span>

<span class="sd">                mlflow.transformers.save_model(</span>
<span class="sd">                    transformers_model=sentence_pipeline,</span>
<span class="sd">                    path=&quot;/path/for/model&quot;,</span>
<span class="sd">                    task=task,</span>
<span class="sd">                    model_config=model_config,</span>
<span class="sd">                )</span>
<span class="sd">        example_no_conversion: {{ example_no_conversion }}</span>
<span class="sd">        prompt_template: {{ prompt_template }}</span>
<span class="sd">        save_pretrained: {{ save_pretrained }}</span>
<span class="sd">        kwargs: Optional additional configurations for transformers serialization.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">transformers</span>

    <span class="n">_validate_env_arguments</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span><span class="p">)</span>

    <span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>

    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>

    <span class="n">code_dir_subpath</span> <span class="o">=</span> <span class="n">_validate_and_copy_code_paths</span><span class="p">(</span><span class="n">code_paths</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">):</span>
        <span class="n">_validate_transformers_model_dict</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">)</span>
        <span class="n">built_pipeline</span> <span class="o">=</span> <span class="n">transformers_model</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">_validate_transformers_model_dict</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">)</span>
        <span class="n">built_pipeline</span> <span class="o">=</span> <span class="n">_build_pipeline_from_model_input</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="c1"># When a string is passed, it should be a path to model checkpoint in local storage or DBFS</span>
        <span class="k">if</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;dbfs:&quot;</span><span class="p">):</span>
            <span class="c1"># Replace the DBFS URI to the actual mount point</span>
            <span class="n">transformers_model</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dbfs:&quot;</span><span class="p">,</span> <span class="s2">&quot;/dbfs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;The `task` argument must be specified when logging a model from a local &quot;</span>
                <span class="s2">&quot;checkpoint. Please provide the task type of the pipeline.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">save_pretrained</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;The `save_pretrained` argument must be set to True when logging a model from a &quot;</span>
                <span class="s2">&quot;local checkpoint. Please set `save_pretrained=True`.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Create a dummy pipeline object to be used for saving the model</span>
        <span class="n">DummyModel</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;DummyModel&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;name_or_path&quot;</span><span class="p">])</span>
        <span class="n">DummyPipeline</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;DummyPipeline&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;task&quot;</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">])</span>
        <span class="n">built_pipeline</span> <span class="o">=</span> <span class="n">DummyPipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">DummyModel</span><span class="p">(</span><span class="n">name_or_path</span><span class="o">=</span><span class="n">transformers_model</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;The `transformers_model` must be one of the following types: </span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot; (1) a transformers Pipeline</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot; (2) a dictionary of components for a transformers Pipeline</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot; (3) a path to a local/DBFS directory containing a transformers model checkpoint.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;received: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Verify that the model has not been loaded to distributed memory</span>
    <span class="c1"># NB: transformers does not correctly save a model whose weights have been loaded</span>
    <span class="c1"># using accelerate iff the model weights have been loaded using a device_map that is</span>
    <span class="c1"># heterogeneous. There is a distinct possibility for a partial write to occur, causing an</span>
    <span class="c1"># invalid state of the model&#39;s weights in this scenario. Hence, we raise.</span>
    <span class="c1"># We might be able to remove this check once this PR is merged to transformers:</span>
    <span class="c1"># https://github.com/huggingface/transformers/issues/20072</span>
    <span class="k">if</span> <span class="n">_is_model_distributed_in_memory</span><span class="p">(</span><span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;The model that is attempting to be saved has been loaded into memory &quot;</span>
            <span class="s2">&quot;with an incompatible configuration. If you are using the accelerate &quot;</span>
            <span class="s2">&quot;library to load your model, please ensure that it is saved only after &quot;</span>
            <span class="s2">&quot;loading with the default device mapping. Do not specify `device_map` &quot;</span>
            <span class="s2">&quot;and please try again.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">mlflow_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">task</span> <span class="ow">and</span> <span class="n">task</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_LLM_INFERENCE_TASK_PREFIX</span><span class="p">):</span>
        <span class="n">llm_inference_task</span> <span class="o">=</span> <span class="n">task</span>

        <span class="c1"># For local checkpoint saving, we set built_pipeline.task to the original `task`</span>
        <span class="c1"># argument value earlier, which is LLM v1 task. Thereby here we update it to the</span>
        <span class="c1"># corresponding Transformers task type.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">default_task</span> <span class="o">=</span> <span class="n">_get_default_task_for_llm_inference_task</span><span class="p">(</span><span class="n">llm_inference_task</span><span class="p">)</span>
            <span class="n">built_pipeline</span> <span class="o">=</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">default_task</span><span class="p">)</span>

        <span class="n">_validate_llm_inference_task_type</span><span class="p">(</span><span class="n">llm_inference_task</span><span class="p">,</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">task</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">llm_inference_task</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">llm_inference_task</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature_from_llm_inference_task</span><span class="p">(</span>
            <span class="n">llm_inference_task</span><span class="p">,</span> <span class="n">signature</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>

    <span class="k">if</span> <span class="n">input_example</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_example</span> <span class="o">=</span> <span class="n">format_input_example_for_special_cases</span><span class="p">(</span><span class="n">input_example</span><span class="p">,</span> <span class="n">built_pipeline</span><span class="p">)</span>
        <span class="n">_save_example</span><span class="p">(</span><span class="n">mlflow_model</span><span class="p">,</span> <span class="n">input_example</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="n">example_no_conversion</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="n">metadata</span>

    <span class="c1"># Check task consistency between model metadata and task argument</span>
    <span class="c1">#  NB: Using mlflow_model.metadata instead of passed metadata argument directly, because</span>
    <span class="c1">#  metadata argument is not directly propagated from log_model() to save_model(), instead</span>
    <span class="c1">#  via the mlflow_model object attribute.</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">metadata_task</span> <span class="o">:=</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_METADATA_LLM_INFERENCE_TASK_KEY</span><span class="p">))</span>
        <span class="ow">and</span> <span class="n">metadata_task</span> <span class="o">!=</span> <span class="n">task</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;LLM v1 task type &#39;</span><span class="si">{</span><span class="n">metadata_task</span><span class="si">}</span><span class="s2">&#39; is specified in &quot;</span>
            <span class="s2">&quot;metadata, but it doesn&#39;t match the task type provided in the `task` argument: &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">task</span><span class="si">}</span><span class="s2">&#39;. The mismatched task type may cause incorrect model inference behavior. &quot;</span>
            <span class="s2">&quot;Please provide the correct LLM v1 task type in the `task` argument. E.g. &quot;</span>
            <span class="sa">f</span><span class="s1">&#39;`mlflow.transformers.save_model(task=&quot;</span><span class="si">{</span><span class="n">metadata_task</span><span class="si">}</span><span class="s1">&quot;, ...)`&#39;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">prompt_template</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># prevent saving prompt templates for unsupported pipeline types</span>
        <span class="k">if</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">task</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_PROMPT_TEMPLATING_TASK_TYPES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Prompt templating is not supported for the `</span><span class="si">{</span><span class="n">built_pipeline</span><span class="o">.</span><span class="n">task</span><span class="si">}</span><span class="s2">` task type. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Supported task types are: </span><span class="si">{</span><span class="n">_SUPPORTED_PROMPT_TEMPLATING_TASK_TYPES</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="n">_validate_prompt_template</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span><span class="p">:</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">PROMPT_TEMPLATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">prompt_template</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">PROMPT_TEMPLATE</span><span class="p">:</span> <span class="n">prompt_template</span><span class="p">}</span>

    <span class="k">if</span> <span class="n">is_peft_model</span><span class="p">(</span><span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">):</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Overriding save_pretrained to False for PEFT models, following the Transformers &quot;</span>
            <span class="s2">&quot;behavior. The PEFT adaptor and config will be saved, but the base model weights &quot;</span>
            <span class="s2">&quot;will not and reference to the HuggingFace Hub repository will be logged instead.&quot;</span>
        <span class="p">)</span>
        <span class="c1"># This will only save PEFT adaptor weights and config, not the base model weights</span>
        <span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_PEFT_ADAPTOR_DIR_NAME</span><span class="p">))</span>
        <span class="n">save_pretrained</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">save_pretrained</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_valid_hf_repo_id</span><span class="p">(</span><span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">name_or_path</span><span class="p">):</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;The save_pretrained parameter is set to False, but the specified model does not &quot;</span>
            <span class="s2">&quot;have a valid HuggingFace Hub repository identifier. Therefore, the weights will &quot;</span>
            <span class="s2">&quot;be saved to disk anyway.&quot;</span>
        <span class="p">)</span>
        <span class="n">save_pretrained</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Create the flavor configuration</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">flavor_conf</span> <span class="o">=</span> <span class="n">build_flavor_config_from_local_checkpoint</span><span class="p">(</span>
            <span class="n">transformers_model</span><span class="p">,</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">task</span><span class="p">,</span> <span class="n">processor</span><span class="p">,</span> <span class="n">torch_dtype</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flavor_conf</span> <span class="o">=</span> <span class="n">build_flavor_config</span><span class="p">(</span><span class="n">built_pipeline</span><span class="p">,</span> <span class="n">processor</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="p">,</span> <span class="n">save_pretrained</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">llm_inference_task</span><span class="p">:</span>
        <span class="n">flavor_conf</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">_LLM_INFERENCE_TASK_KEY</span><span class="p">:</span> <span class="n">llm_inference_task</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span><span class="p">:</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="n">_METADATA_LLM_INFERENCE_TASK_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">llm_inference_task</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="n">_METADATA_LLM_INFERENCE_TASK_KEY</span><span class="p">:</span> <span class="n">llm_inference_task</span><span class="p">}</span>

    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">add_flavor</span><span class="p">(</span>
        <span class="n">FLAVOR_NAME</span><span class="p">,</span>
        <span class="n">transformers_version</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
        <span class="o">**</span><span class="n">flavor_conf</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Flavor config should not be mutated after being added to MLModel</span>
    <span class="n">flavor_conf</span> <span class="o">=</span> <span class="n">MappingProxyType</span><span class="p">(</span><span class="n">flavor_conf</span><span class="p">)</span>

    <span class="c1"># Save pipeline model and components weights</span>
    <span class="k">if</span> <span class="n">save_pretrained</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">save_local_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">transformers_model</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">save_pipeline_pretrained_weights</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">built_pipeline</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">repo</span> <span class="o">=</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">name_or_path</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Skipping saving pretrained model weights to disk as the save_pretrained argument&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;is set to False. The reference to the HuggingFace Hub repository </span><span class="si">{</span><span class="n">repo</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;will be logged instead.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">inference_config</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Indicating `inference_config` is deprecated and will be removed in a future version &quot;</span>
            <span class="s2">&quot;of MLflow. Use `model_config` instead.&quot;</span>
        <span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_INFERENCE_CONFIG_BINARY_KEY</span><span class="p">)</span><span class="o">.</span><span class="n">write_text</span><span class="p">(</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">inference_config</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">name_or_path</span>

    <span class="c1"># Get the model card from either the argument or the HuggingFace marketplace</span>
    <span class="n">card_data</span> <span class="o">=</span> <span class="n">model_card</span> <span class="ow">or</span> <span class="n">_fetch_model_card</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="c1"># If the card data can be acquired, save the text and the data separately</span>
    <span class="n">_write_card_data</span><span class="p">(</span><span class="n">card_data</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="c1"># Write the license information (or guidance) along with the model</span>
    <span class="n">_write_license_information</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">card_data</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="c1"># Only allow a subset of task types to have a pyfunc definition.</span>
    <span class="c1"># Currently supported types are NLP-based language tasks which have a pipeline definition</span>
    <span class="c1"># consisting exclusively of a Model and a Tokenizer.</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="c1"># TODO: when a local checkpoint path is provided as a model, we assume it is eligible</span>
        <span class="c1"># for pyfunc prediction. This may not be true for all cases, so we should revisit this.</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_should_add_pyfunc_to_model</span><span class="p">(</span><span class="n">built_pipeline</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">infer_or_get_default_signature</span><span class="p">(</span>
                <span class="n">pipeline</span><span class="o">=</span><span class="n">built_pipeline</span><span class="p">,</span>
                <span class="n">example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
                <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span> <span class="ow">or</span> <span class="n">inference_config</span><span class="p">,</span>
                <span class="n">flavor_config</span><span class="o">=</span><span class="n">flavor_conf</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># if pipeline is text-generation and a prompt template is specified,</span>
        <span class="c1"># provide the return_full_text=False config by default to avoid confusing</span>
        <span class="c1"># extra text for end-users</span>
        <span class="k">if</span> <span class="n">prompt_template</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">built_pipeline</span><span class="o">.</span><span class="n">task</span> <span class="o">==</span> <span class="s2">&quot;text-generation&quot;</span><span class="p">:</span>
            <span class="n">return_full_text_key</span> <span class="o">=</span> <span class="s2">&quot;return_full_text&quot;</span>
            <span class="n">model_config</span> <span class="o">=</span> <span class="n">model_config</span> <span class="ow">or</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">return_full_text_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_config</span><span class="p">:</span>
                <span class="n">model_config</span><span class="p">[</span><span class="n">return_full_text_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">_PROMPT_TEMPLATE_RETURN_FULL_TEXT_INFO</span><span class="p">)</span>

        <span class="n">pyfunc</span><span class="o">.</span><span class="n">add_to_model</span><span class="p">(</span>
            <span class="n">mlflow_model</span><span class="p">,</span>
            <span class="n">loader_module</span><span class="o">=</span><span class="s2">&quot;mlflow.transformers&quot;</span><span class="p">,</span>
            <span class="n">conda_env</span><span class="o">=</span><span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
            <span class="n">python_env</span><span class="o">=</span><span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
            <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
            <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">processor</span><span class="p">:</span>
            <span class="n">reason</span> <span class="o">=</span> <span class="s2">&quot;the model has been saved with a &#39;processor&#39; argument supplied.&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reason</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;the model is not a language-based model and requires a complex input type &quot;</span>
                <span class="s2">&quot;that is currently not supported.&quot;</span>
            <span class="p">)</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;This model is unable to be used for pyfunc prediction because </span><span class="si">{</span><span class="n">reason</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;The pyfunc flavor will not be added to the Model.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">size</span> <span class="o">:=</span> <span class="n">get_total_file_size</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">model_size_bytes</span> <span class="o">=</span> <span class="n">size</span>

    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">MLMODEL_FILE_NAME</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">conda_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pip_requirements</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="n">get_default_pip_requirements</span><span class="p">(</span><span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_peft_model</span><span class="p">(</span><span class="n">built_pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">):</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;A local checkpoint path or PEFT model is given as the `transformers_model`. &quot;</span>
                    <span class="s2">&quot;To avoid loading the full model into memory, we don&#39;t infer the pip &quot;</span>
                    <span class="s2">&quot;requirement for the model. Instead, we will use the default requirements, &quot;</span>
                    <span class="s2">&quot;but it may not capture all required pip libraries for the model. Consider &quot;</span>
                    <span class="s2">&quot;providing the pip requirements explicitly.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Infer the pip requirements with a timeout to avoid hanging at prediction</span>
                <span class="n">inferred_reqs</span> <span class="o">=</span> <span class="n">infer_pip_requirements</span><span class="p">(</span>
                    <span class="n">model_uri</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">),</span>
                    <span class="n">flavor</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">,</span>
                    <span class="n">fallback</span><span class="o">=</span><span class="n">default_reqs</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="p">)</span>
                <span class="n">default_reqs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inferred_reqs</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">default_reqs</span><span class="p">)</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">default_reqs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_pip_requirements</span><span class="p">(</span>
            <span class="n">default_reqs</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_conda_env</span><span class="p">(</span><span class="n">conda_env</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">)</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">safe_dump</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pip_constraints</span><span class="p">:</span>
        <span class="n">write_to</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">)),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_constraints</span><span class="p">))</span>

    <span class="n">write_to</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">)),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_requirements</span><span class="p">))</span>

    <span class="n">_PythonEnv</span><span class="o">.</span><span class="n">current</span><span class="p">()</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">)))</span></div>


<div class="viewcode-block" id="log_model"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.log_model">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@docstring_version_compatibility_warning</span><span class="p">(</span><span class="n">integration_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="nd">@format_docstring</span><span class="p">(</span><span class="n">LOG_MODEL_PARAM_DOCS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">package_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">log_model</span><span class="p">(</span>
    <span class="n">transformers_model</span><span class="p">,</span>
    <span class="n">artifact_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_card</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">inference_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelSignature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelInputExample</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">await_registration_for</span><span class="o">=</span><span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">example_no_conversion</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log a ``transformers`` object as an MLflow artifact for the current run. Note that</span>
<span class="sd">    logging transformers models with custom code (i.e. models that require</span>
<span class="sd">    ``trust_remote_code=True``) requires ``transformers &gt;= 4.26.0``.</span>

<span class="sd">    Args:</span>
<span class="sd">        transformers_model:</span>
<span class="sd">            The transformers model to save. This can be one of the following format:</span>

<span class="sd">                1. A transformers `Pipeline` instance.</span>
<span class="sd">                2. A dictionary that maps required components of a pipeline to the named keys</span>
<span class="sd">                    of [&quot;model&quot;, &quot;image_processor&quot;, &quot;tokenizer&quot;, &quot;feature_extractor&quot;].</span>
<span class="sd">                    The `model` key in the dictionary must map to a value that inherits from</span>
<span class="sd">                    `PreTrainedModel`, `TFPreTrainedModel`, or `FlaxPreTrainedModel`.</span>
<span class="sd">                    All other component entries in the dictionary must support the defined task</span>
<span class="sd">                    type that is associated with the base model type configuration.</span>
<span class="sd">                3. A string that represents a path to a local/DBFS directory containing a model</span>
<span class="sd">                    checkpoint. The directory must contain a `config.json` file that is required</span>
<span class="sd">                    for loading the transformers model. This is particularly useful when logging</span>
<span class="sd">                    a model that cannot be loaded into memory for serialization.</span>

<span class="sd">            An example of specifying a `Pipeline` from a default pipeline instantiation:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from transformers import pipeline</span>

<span class="sd">                qa_pipe = pipeline(&quot;question-answering&quot;, &quot;csarron/mobilebert-uncased-squad-v2&quot;)</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    mlflow.transformers.log_model(</span>
<span class="sd">                        transformers_model=qa_pipe,</span>
<span class="sd">                        artifact_path=&quot;model&quot;,</span>
<span class="sd">                    )</span>

<span class="sd">            An example of specifying component-level parts of a transformers model is shown below:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from transformers import MobileBertForQuestionAnswering, AutoTokenizer</span>

<span class="sd">                architecture = &quot;csarron/mobilebert-uncased-squad-v2&quot;</span>
<span class="sd">                tokenizer = AutoTokenizer.from_pretrained(architecture)</span>
<span class="sd">                model = MobileBertForQuestionAnswering.from_pretrained(architecture)</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    components = {</span>
<span class="sd">                        &quot;model&quot;: model,</span>
<span class="sd">                        &quot;tokenizer&quot;: tokenizer,</span>
<span class="sd">                    }</span>
<span class="sd">                    mlflow.transformers.log_model(</span>
<span class="sd">                        transformers_model=components,</span>
<span class="sd">                        artifact_path=&quot;model&quot;,</span>
<span class="sd">                    )</span>

<span class="sd">            An example of specifying a local checkpoint path is shown below:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    mlflow.transformers.log_model(</span>
<span class="sd">                        transformers_model=&quot;path/to/local/checkpoint&quot;,</span>
<span class="sd">                        artifact_path=&quot;model&quot;,</span>
<span class="sd">                    )</span>

<span class="sd">        artifact_path: Local path destination for the serialized model to be saved.</span>
<span class="sd">        processor: An optional ``Processor`` subclass object. Some model architectures,</span>
<span class="sd">            particularly multi-modal types, utilize Processors to combine text</span>
<span class="sd">            encoding and image or audio encoding in a single entrypoint.</span>

<span class="sd">                .. Note:: If a processor is supplied when logging a model, the</span>
<span class="sd">                    model will be unavailable for loading as a ``Pipeline`` or for usage</span>
<span class="sd">                    with pyfunc inference.</span>
<span class="sd">        task: The transformers-specific task type of the model. These strings are utilized so</span>
<span class="sd">            that a pipeline can be created with the appropriate internal call architecture</span>
<span class="sd">            to meet the needs of a given model. If this argument is not specified, the</span>
<span class="sd">            pipeline utilities within the transformers library will be used to infer the</span>
<span class="sd">            correct task type. If the value specified is not a supported type within the</span>
<span class="sd">            version of transformers that is currently installed, an Exception will be thrown.</span>
<span class="sd">        torch_dtype: The Pytorch dtype applied to the model when loading back. This is useful</span>
<span class="sd">            when you want to save the model with a specific dtype that is different from the</span>
<span class="sd">            dtype of the model when it was trained. If not specified, the current dtype of the</span>
<span class="sd">            model instance will be used.</span>
<span class="sd">        model_card: An Optional `ModelCard` instance from `huggingface-hub`. If provided, the</span>
<span class="sd">            contents of the model card will be saved along with the provided</span>
<span class="sd">            `transformers_model`. If not provided, an attempt will be made to fetch</span>
<span class="sd">            the card from the base pretrained model that is provided (or the one that is</span>
<span class="sd">            included within a provided `Pipeline`).</span>

<span class="sd">                .. Note:: In order for a ModelCard to be fetched (if not provided),</span>
<span class="sd">                    the huggingface_hub package must be installed and the version</span>
<span class="sd">                    must be &gt;=0.10.0</span>
<span class="sd">        inference_config:</span>

<span class="sd">            .. Warning:: Deprecated. `inference_config` is deprecated in favor of `model_config`.</span>

<span class="sd">        code_paths: {{ code_paths }}</span>
<span class="sd">        registered_model_name: This argument may change or be removed in a</span>
<span class="sd">            future release without warning. If given, create a model</span>
<span class="sd">            version under ``registered_model_name``, also creating a</span>
<span class="sd">            registered model if one with the given name does not exist.</span>
<span class="sd">        signature: A Model Signature object that describes the input and output Schema of the</span>
<span class="sd">            model. The model signature can be inferred using `infer_signature` function</span>
<span class="sd">            of `mlflow.models.signature`.</span>

<span class="sd">            .. code-block:: python</span>
<span class="sd">                :caption: Example</span>

<span class="sd">                from mlflow.models import infer_signature</span>
<span class="sd">                from mlflow.transformers import generate_signature_output</span>
<span class="sd">                from transformers import pipeline</span>

<span class="sd">                en_to_de = pipeline(&quot;translation_en_to_de&quot;)</span>

<span class="sd">                data = &quot;MLflow is great!&quot;</span>
<span class="sd">                output = generate_signature_output(en_to_de, data)</span>
<span class="sd">                signature = infer_signature(data, output)</span>

<span class="sd">                with mlflow.start_run() as run:</span>
<span class="sd">                    mlflow.transformers.log_model(</span>
<span class="sd">                        transformers_model=en_to_de,</span>
<span class="sd">                        artifact_path=&quot;english_to_german_translator&quot;,</span>
<span class="sd">                        signature=signature,</span>
<span class="sd">                        input_example=data,</span>
<span class="sd">                    )</span>

<span class="sd">                model_uri = f&quot;runs:/{run.info.run_id}/english_to_german_translator&quot;</span>
<span class="sd">                loaded = mlflow.pyfunc.load_model(model_uri)</span>

<span class="sd">                print(loaded.predict(data))</span>
<span class="sd">                # MLflow ist gro√üartig!</span>

<span class="sd">            If an input_example is provided and the signature is not, a signature will</span>
<span class="sd">            be inferred automatically and applied to the MLmodel file iff the</span>
<span class="sd">            pipeline type is a text-based model (NLP). If the pipeline type is not</span>
<span class="sd">            a supported type, this inference functionality will not function correctly</span>
<span class="sd">            and a warning will be issued. In order to ensure that a precise signature</span>
<span class="sd">            is logged, it is recommended to explicitly provide one.</span>
<span class="sd">        input_example: {{ input_example }}</span>
<span class="sd">        await_registration_for: Number of seconds to wait for the model version</span>
<span class="sd">            to finish being created and is in ``READY`` status.</span>
<span class="sd">            By default, the function waits for five minutes.</span>
<span class="sd">            Specify 0 or None to skip waiting.</span>
<span class="sd">        pip_requirements: {{ pip_requirements }}</span>
<span class="sd">        extra_pip_requirements: {{ extra_pip_requirements }}</span>
<span class="sd">        conda_env: {{ conda_env }}</span>
<span class="sd">        metadata: {{ metadata }}</span>
<span class="sd">        model_config:</span>
<span class="sd">            A dict of valid overrides that can be applied to a pipeline instance during inference.</span>
<span class="sd">            These arguments are used exclusively for the case of loading the model as a ``pyfunc``</span>
<span class="sd">            Model or for use in Spark. These values are not applied to a returned Pipeline from a</span>
<span class="sd">            call to ``mlflow.transformers.load_model()``</span>

<span class="sd">            .. Warning:: If the key provided is not compatible with either the</span>
<span class="sd">                         Pipeline instance for the task provided or is not a valid</span>
<span class="sd">                         override to any arguments available in the Model, an</span>
<span class="sd">                         Exception will be raised at runtime. It is very important</span>
<span class="sd">                         to validate the entries in this dictionary to ensure</span>
<span class="sd">                         that they are valid prior to saving or logging.</span>

<span class="sd">            An example of providing overrides for a question generation model:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from transformers import pipeline, AutoTokenizer</span>

<span class="sd">                task = &quot;text-generation&quot;</span>
<span class="sd">                architecture = &quot;gpt2&quot;</span>

<span class="sd">                sentence_pipeline = pipeline(</span>
<span class="sd">                    task=task,</span>
<span class="sd">                    tokenizer=AutoTokenizer.from_pretrained(architecture),</span>
<span class="sd">                    model=architecture,</span>
<span class="sd">                )</span>

<span class="sd">                # Validate that the overrides function</span>
<span class="sd">                prompts = [&quot;Generative models are&quot;, &quot;I&#39;d like a coconut so that I can&quot;]</span>

<span class="sd">                # validation of config prior to save or log</span>
<span class="sd">                model_config = {</span>
<span class="sd">                    &quot;top_k&quot;: 2,</span>
<span class="sd">                    &quot;num_beams&quot;: 5,</span>
<span class="sd">                    &quot;max_length&quot;: 30,</span>
<span class="sd">                    &quot;temperature&quot;: 0.62,</span>
<span class="sd">                    &quot;top_p&quot;: 0.85,</span>
<span class="sd">                    &quot;repetition_penalty&quot;: 1.15,</span>
<span class="sd">                }</span>

<span class="sd">                # Verify that no exceptions are thrown</span>
<span class="sd">                sentence_pipeline(prompts, **model_config)</span>

<span class="sd">                with mlflow.start_run():</span>
<span class="sd">                    mlflow.transformers.log_model(</span>
<span class="sd">                        transformers_model=sentence_pipeline,</span>
<span class="sd">                        artifact_path=&quot;my_sentence_generator&quot;,</span>
<span class="sd">                        task=task,</span>
<span class="sd">                        model_config=model_config,</span>
<span class="sd">                    )</span>
<span class="sd">        example_no_conversion: {{ example_no_conversion }}</span>
<span class="sd">        prompt_template: {{ prompt_template }}</span>
<span class="sd">        save_pretrained: {{ save_pretrained }}</span>
<span class="sd">        kwargs: Additional arguments for :py:class:`mlflow.models.model.Model`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="n">artifact_path</span><span class="p">,</span>
        <span class="n">flavor</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">],</span>  <span class="c1"># Get the current module.</span>
        <span class="n">registered_model_name</span><span class="o">=</span><span class="n">registered_model_name</span><span class="p">,</span>
        <span class="n">await_registration_for</span><span class="o">=</span><span class="n">await_registration_for</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">transformers_model</span><span class="p">,</span>
        <span class="n">processor</span><span class="o">=</span><span class="n">processor</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch_dtype</span><span class="p">,</span>
        <span class="n">model_card</span><span class="o">=</span><span class="n">model_card</span><span class="p">,</span>
        <span class="n">inference_config</span><span class="o">=</span><span class="n">inference_config</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">conda_env</span><span class="p">,</span>
        <span class="n">code_paths</span><span class="o">=</span><span class="n">code_paths</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
        <span class="c1"># NB: We don&#39;t validate the serving input if the provided model is a path</span>
        <span class="c1"># to a local checkpoint. This is because the purpose of supporting that</span>
        <span class="c1"># input format is to avoid loading large model into memory. Serving input</span>
        <span class="c1"># validation loads the model into memory and make prediction, which is</span>
        <span class="c1"># expensive and can cause OOM errors.</span>
        <span class="n">validate_serving_input</span><span class="o">=</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span>
        <span class="n">pip_requirements</span><span class="o">=</span><span class="n">pip_requirements</span><span class="p">,</span>
        <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="n">extra_pip_requirements</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
        <span class="n">example_no_conversion</span><span class="o">=</span><span class="n">example_no_conversion</span><span class="p">,</span>
        <span class="n">prompt_template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="n">save_pretrained</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="load_model"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.load_model">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@docstring_version_compatibility_warning</span><span class="p">(</span><span class="n">integration_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span>
    <span class="n">model_uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dst_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;pipeline&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load a ``transformers`` object from a local file or a run.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_uri: The location, in URI format, of the MLflow model. For example:</span>

<span class="sd">            - ``/Users/me/path/to/local/model``</span>
<span class="sd">            - ``relative/path/to/local/model``</span>
<span class="sd">            - ``s3://my_bucket/path/to/model``</span>
<span class="sd">            - ``runs:/&lt;mlflow_run_id&gt;/run-relative/path/to/model``</span>
<span class="sd">            - ``mlflow-artifacts:/path/to/model``</span>

<span class="sd">            For more information about supported URI schemes, see</span>
<span class="sd">            `Referencing Artifacts &lt;https://www.mlflow.org/docs/latest/tracking.html#</span>
<span class="sd">            artifact-locations&gt;`_.</span>
<span class="sd">        dst_path: The local filesystem path to utilize for downloading the model artifact.</span>
<span class="sd">            This directory must already exist if provided. If unspecified, a local output</span>
<span class="sd">            path will be created.</span>
<span class="sd">        return_type: A return type modifier for the stored ``transformers`` object.</span>
<span class="sd">            If set as &quot;components&quot;, the return type will be a dictionary of the saved</span>
<span class="sd">            individual components of either the ``Pipeline`` or the pre-trained model.</span>
<span class="sd">            The components for NLP-focused models will typically consist of a</span>
<span class="sd">            return representation as shown below with a text-classification example:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                {&quot;model&quot;: BertForSequenceClassification, &quot;tokenizer&quot;: BertTokenizerFast}</span>

<span class="sd">            Vision models will return an ``ImageProcessor`` instance of the appropriate</span>
<span class="sd">            type, while multi-modal models will return both a ``FeatureExtractor`` and</span>
<span class="sd">            a ``Tokenizer`` along with the model.</span>
<span class="sd">            Returning &quot;components&quot; can be useful for certain model types that do not</span>
<span class="sd">            have the desired pipeline return types for certain use cases.</span>
<span class="sd">            If set as &quot;pipeline&quot;, the model, along with any and all required</span>
<span class="sd">            ``Tokenizer``, ``FeatureExtractor``, ``Processor``, or ``ImageProcessor``</span>
<span class="sd">            objects will be returned within a ``Pipeline`` object of the appropriate</span>
<span class="sd">            type defined by the ``task`` set by the model instance type. To override</span>
<span class="sd">            this behavior, supply a valid ``task`` argument during model logging or</span>
<span class="sd">            saving. Default is &quot;pipeline&quot;.</span>
<span class="sd">        device: The device on which to load the model. Default is None. Use 0 to</span>
<span class="sd">            load to the default GPU.</span>
<span class="sd">        kwargs: Optional configuration options for loading of a ``transformers`` object.</span>
<span class="sd">            For information on parameters and their usage, see</span>
<span class="sd">            `transformers documentation &lt;https://huggingface.co/docs/transformers/index&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A ``transformers`` model instance or a dictionary of components</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">return_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_RETURN_TYPES</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The specified return_type mode &#39;</span><span class="si">{</span><span class="n">return_type</span><span class="si">}</span><span class="s2">&#39; is unsupported. &quot;</span>
            <span class="s2">&quot;Please select one of: &#39;pipeline&#39; or &#39;components&#39;.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">model_uri</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>

    <span class="n">local_model_path</span> <span class="o">=</span> <span class="n">_download_artifact_from_uri</span><span class="p">(</span><span class="n">artifact_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">dst_path</span><span class="p">)</span>

    <span class="n">flavor_config</span> <span class="o">=</span> <span class="n">_get_flavor_configuration_from_uri</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="n">_logger</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;pipeline&quot;</span> <span class="ow">and</span> <span class="n">FlavorKey</span><span class="o">.</span><span class="n">PROCESSOR_TYPE</span> <span class="ow">in</span> <span class="n">flavor_config</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;This model has been saved with a processor. Processor objects are &quot;</span>
            <span class="s2">&quot;not compatible with Pipelines. Please load this model by specifying &quot;</span>
            <span class="s2">&quot;the &#39;return_type&#39;=&#39;components&#39;.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">BAD_REQUEST</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_config</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_load_model</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_config</span><span class="p">,</span> <span class="n">return_type</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="persist_pretrained_model"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.persist_pretrained_model">[docs]</a><span class="k">def</span> <span class="nf">persist_pretrained_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Persist Transformers pretrained model weights to the artifacts directory of the specified</span>
<span class="sd">    model_uri. This API is primary used for updating an MLflow Model that was logged or saved</span>
<span class="sd">    with setting save_pretrained=False. Such models cannot be registered to Databricks Workspace</span>
<span class="sd">    Model Registry, due to the full pretrained model weights being absent in the artifacts.</span>
<span class="sd">    Transformers models saved in this mode store only the reference to the HuggingFace Hub</span>
<span class="sd">    repository. This API will download the model weights from the HuggingFace Hub repository</span>
<span class="sd">    and save them in the artifacts of the given model_uri so that the model can be registered</span>
<span class="sd">    to Databricks Workspace Model Registry.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_uri: The URI of the existing MLflow Model of the Transformers flavor.</span>
<span class="sd">            It must be logged/saved with save_pretrained=False.</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import mlflow</span>

<span class="sd">        # Saving a model with save_pretrained=False</span>
<span class="sd">        with mlflow.start_run() as run:</span>
<span class="sd">            model = pipeline(&quot;question-answering&quot;, &quot;csarron/mobilebert-uncased-squad-v2&quot;)</span>
<span class="sd">            mlflow.transformers.log_model(</span>
<span class="sd">                transformers_model=model, artifact_path=&quot;pipeline&quot;, save_pretrained=False</span>
<span class="sd">            )</span>

<span class="sd">        # The model cannot be registered to the Model Registry as it is</span>
<span class="sd">        try:</span>
<span class="sd">            mlflow.register_model(f&quot;runs:/{run.info.run_id}/pipeline&quot;, &quot;qa_pipeline&quot;)</span>
<span class="sd">        except MlflowException as e:</span>
<span class="sd">            print(e.message)</span>

<span class="sd">        # Use this API to persist the pretrained model weights</span>
<span class="sd">        mlflow.transformers.persist_pretrained_model(f&quot;runs:/{run.info.run_id}/pipeline&quot;)</span>

<span class="sd">        # Now the model can be registered to the Model Registry</span>
<span class="sd">        mlflow.register_model(f&quot;runs:/{run.info.run_id}/pipeline&quot;, &quot;qa_pipeline&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check if the model weight already exists in the model artifact before downloading</span>
    <span class="n">root_uri</span><span class="p">,</span> <span class="n">artifact_path</span> <span class="o">=</span> <span class="n">_get_root_uri_and_artifact_path</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>
    <span class="n">artifact_repo</span> <span class="o">=</span> <span class="n">get_artifact_repository</span><span class="p">(</span><span class="n">root_uri</span><span class="p">)</span>

    <span class="n">file_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">path</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">artifact_repo</span><span class="o">.</span><span class="n">list_artifacts</span><span class="p">(</span><span class="n">artifact_path</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">MLMODEL_FILE_NAME</span> <span class="ow">in</span> <span class="n">file_names</span> <span class="ow">and</span> <span class="n">_MODEL_BINARY_FILE_NAME</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;The full pretrained model weight already exists in the artifact directory of the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;specified model_uri: </span><span class="si">{</span><span class="n">model_uri</span><span class="si">}</span><span class="s2">. No action is needed.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">with</span> <span class="n">TempDir</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmp_dir</span><span class="p">:</span>
        <span class="n">local_model_path</span> <span class="o">=</span> <span class="n">artifact_repo</span><span class="o">.</span><span class="n">download_artifacts</span><span class="p">(</span><span class="n">artifact_path</span><span class="p">,</span> <span class="n">dst_path</span><span class="o">=</span><span class="n">tmp_dir</span><span class="o">.</span><span class="n">path</span><span class="p">())</span>
        <span class="n">pipeline</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;pipeline&quot;</span><span class="p">)</span>

        <span class="c1"># Update MLModel flavor config</span>
        <span class="n">mlmodel_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">)</span>
        <span class="n">model_conf</span> <span class="o">=</span> <span class="n">Model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">mlmodel_path</span><span class="p">)</span>
        <span class="n">updated_flavor_conf</span> <span class="o">=</span> <span class="n">update_flavor_conf_to_persist_pretrained_model</span><span class="p">(</span>
            <span class="n">model_conf</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="n">FLAVOR_NAME</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">model_conf</span><span class="o">.</span><span class="n">add_flavor</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="o">**</span><span class="n">updated_flavor_conf</span><span class="p">)</span>
        <span class="n">model_conf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">mlmodel_path</span><span class="p">)</span>

        <span class="c1"># Save pretrained weights</span>
        <span class="n">save_pipeline_pretrained_weights</span><span class="p">(</span>
            <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">),</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">updated_flavor_conf</span>
        <span class="p">)</span>

        <span class="c1"># Upload updated local artifacts to MLflow</span>
        <span class="k">for</span> <span class="n">dir_to_upload</span> <span class="ow">in</span> <span class="p">(</span><span class="n">_MODEL_BINARY_FILE_NAME</span><span class="p">,</span> <span class="n">_COMPONENTS_BINARY_DIR_NAME</span><span class="p">):</span>
            <span class="n">local_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">dir_to_upload</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">local_dir</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="n">artifact_repo</span><span class="o">.</span><span class="n">log_artifacts</span><span class="p">(</span><span class="n">local_dir</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">artifact_path</span><span class="p">,</span> <span class="n">dir_to_upload</span><span class="p">))</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1"># NB: log_artifacts method doesn&#39;t support rollback for partial uploads,</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Failed to upload </span><span class="si">{</span><span class="n">local_dir</span><span class="si">}</span><span class="s2"> to the existing model_uri due to </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="s2">&quot;Some other files may have been uploaded.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="c1"># Upload MLModel file</span>
        <span class="n">artifact_repo</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">mlmodel_path</span><span class="p">,</span> <span class="n">artifact_path</span><span class="p">)</span>

    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The pretrained model has been successfully persisted in </span><span class="si">{</span><span class="n">model_uri</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_is_model_distributed_in_memory</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if the model is distributed across multiple devices in memory.&quot;&quot;&quot;</span>

    <span class="c1"># Check if the model attribute exists. If not, accelerate was not used and the model can</span>
    <span class="c1"># be safely saved</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="s2">&quot;hf_device_map&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="c1"># If the device map has more than one unique value entry, then the weights are not within</span>
    <span class="c1"># a contiguous memory system (VRAM, SYS, or DISK) and thus cannot be safely saved.</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">transformers_model</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">1</span>


<span class="c1"># This function attempts to determine if a GPU is available for the PyTorch and TensorFlow libraries</span>
<div class="viewcode-block" id="is_gpu_available"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.is_gpu_available">[docs]</a><span class="k">def</span> <span class="nf">is_gpu_available</span><span class="p">():</span>
    <span class="c1"># try pytorch and if it fails, try tf</span>
    <span class="n">is_gpu</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">torch</span>

        <span class="n">is_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">if</span> <span class="n">is_gpu</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

            <span class="n">is_gpu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">is_gpu_available</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">if</span> <span class="n">is_gpu</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_gpu</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">is_gpu</span></div>


<span class="k">def</span> <span class="nf">_load_model</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">flavor_config</span><span class="p">,</span> <span class="n">return_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads components from a locally serialized ``Pipeline`` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">transformers</span>

    <span class="n">conf</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="n">flavor_config</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">TASK</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">framework</span> <span class="o">:=</span> <span class="n">flavor_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">FRAMEWORK</span><span class="p">):</span>
        <span class="n">conf</span><span class="p">[</span><span class="s2">&quot;framework&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">framework</span>

    <span class="c1"># Note that we don&#39;t set the device in the conf yet because device is</span>
    <span class="c1"># incompatible with device_map.</span>
    <span class="n">accelerate_model_conf</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">MLFLOW_HUGGINGFACE_USE_DEVICE_MAP</span><span class="o">.</span><span class="n">get</span><span class="p">():</span>
        <span class="n">device_map_strategy</span> <span class="o">=</span> <span class="n">MLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">conf</span><span class="p">[</span><span class="s2">&quot;device_map&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_map_strategy</span>
        <span class="n">accelerate_model_conf</span><span class="p">[</span><span class="s2">&quot;device_map&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_map_strategy</span>
        <span class="c1"># Cannot use device with device_map</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                <span class="s2">&quot;The environment variable MLFLOW_HUGGINGFACE_USE_DEVICE_MAP is set to True, but &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;the `device` argument is provided with value </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">. The device_map and &quot;</span>
                <span class="s2">&quot;`device` argument cannot be used together. Set MLFLOW_HUGGINGFACE_USE_DEVICE_MAP &quot;</span>
                <span class="s2">&quot;to False to specify a particular device ID, or pass None for the `device` &quot;</span>
                <span class="s2">&quot;argument to use device_map.&quot;</span>
            <span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">device_value</span> <span class="o">:=</span> <span class="n">MLFLOW_DEFAULT_PREDICTION_DEVICE</span><span class="o">.</span><span class="n">get</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">device_value</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Invalid value for </span><span class="si">{</span><span class="n">MLFLOW_DEFAULT_PREDICTION_DEVICE</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">device_value</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MLFLOW_DEFAULT_PREDICTION_DEVICE</span><span class="si">}</span><span class="s2"> value must be an integer. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Setting to: </span><span class="si">{</span><span class="n">_TRANSFORMERS_DEFAULT_CPU_DEVICE_ID</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
                <span class="n">device</span> <span class="o">=</span> <span class="n">_TRANSFORMERS_DEFAULT_CPU_DEVICE_ID</span>
        <span class="k">elif</span> <span class="n">is_gpu_available</span><span class="p">():</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">_TRANSFORMERS_DEFAULT_GPU_DEVICE_ID</span>

    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">conf</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">accelerate_model_conf</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">device</span>

    <span class="k">if</span> <span class="n">dtype_val</span> <span class="o">:=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_TORCH_DTYPE_KEY</span><span class="p">)</span> <span class="ow">or</span> <span class="n">flavor_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">TORCH_DTYPE</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype_val</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">dtype_val</span> <span class="o">=</span> <span class="n">_deserialize_torch_dtype</span><span class="p">(</span><span class="n">dtype_val</span><span class="p">)</span>
        <span class="n">conf</span><span class="p">[</span><span class="n">_TORCH_DTYPE_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtype_val</span>
        <span class="n">flavor_config</span><span class="p">[</span><span class="n">_TORCH_DTYPE_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtype_val</span>
        <span class="n">accelerate_model_conf</span><span class="p">[</span><span class="n">_TORCH_DTYPE_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtype_val</span>

    <span class="n">accelerate_model_conf</span><span class="p">[</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

    <span class="c1"># Load model and components either from local or from HuggingFace Hub. We check for the</span>
    <span class="c1"># presence of the model revision (a commit hash of the hub repository) that is only present</span>
    <span class="c1"># in the model logged with `save_pretrained=False</span>
    <span class="k">if</span> <span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL_REVISION</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">flavor_config</span><span class="p">:</span>
        <span class="n">model_and_components</span> <span class="o">=</span> <span class="n">load_model_and_components_from_local</span><span class="p">(</span>
            <span class="n">path</span><span class="o">=</span><span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">),</span>
            <span class="n">flavor_conf</span><span class="o">=</span><span class="n">flavor_config</span><span class="p">,</span>
            <span class="n">accelerate_conf</span><span class="o">=</span><span class="n">accelerate_model_conf</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_and_components</span> <span class="o">=</span> <span class="n">load_model_and_components_from_huggingface_hub</span><span class="p">(</span>
            <span class="n">flavor_conf</span><span class="o">=</span><span class="n">flavor_config</span><span class="p">,</span> <span class="n">accelerate_conf</span><span class="o">=</span><span class="n">accelerate_model_conf</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="c1"># Load and apply PEFT adaptor if saved</span>
    <span class="k">if</span> <span class="n">peft_adapter_dir</span> <span class="o">:=</span> <span class="n">flavor_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">PEFT</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">model_and_components</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_model_with_peft_adapter</span><span class="p">(</span>
            <span class="n">base_model</span><span class="o">=</span><span class="n">model_and_components</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span><span class="p">],</span>
            <span class="n">peft_adapter_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">peft_adapter_dir</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="n">conf</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">conf</span><span class="p">,</span> <span class="o">**</span><span class="n">model_and_components</span><span class="p">}</span>

    <span class="k">if</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">:</span>
        <span class="n">conf</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">suppress_logs</span><span class="p">(</span><span class="s2">&quot;transformers.pipelines.base&quot;</span><span class="p">,</span> <span class="n">filter_regex</span><span class="o">=</span><span class="n">_PEFT_PIPELINE_ERROR_MSG</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="o">**</span><span class="n">conf</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;components&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">conf</span>


<span class="k">def</span> <span class="nf">_fetch_model_card</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attempts to retrieve the model card for the specified model architecture iff the</span>
<span class="sd">    `huggingface_hub` library is installed. If a card cannot be found in the registry or</span>
<span class="sd">    the library is not installed, returns None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">huggingface_hub</span> <span class="k">as</span> <span class="nn">hub</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Unable to store ModelCard data with the saved artifact. In order to &quot;</span>
            <span class="s2">&quot;preserve this information, please install the huggingface_hub package &quot;</span>
            <span class="s2">&quot;by running &#39;pip install huggingingface_hub&gt;0.10.0&#39;&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">hub</span><span class="p">,</span> <span class="s2">&quot;ModelCard&quot;</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hub</span><span class="o">.</span><span class="n">ModelCard</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The model card could not be retrieved from the hub due to </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;The version of huggingface_hub that is installed does not provide &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;ModelCard functionality. You have version </span><span class="si">{</span><span class="n">hub</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2"> installed. &quot;</span>
            <span class="s2">&quot;Update huggingface_hub to &gt;= &#39;0.10.0&#39; to retrieve the ModelCard data.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_write_card_data</span><span class="p">(</span><span class="n">card_data</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Writes the card data, if specified or available, to the provided path in two separate files</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">card_data</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_CARD_TEXT_FILE_NAME</span><span class="p">)</span><span class="o">.</span><span class="n">write_text</span><span class="p">(</span><span class="n">card_data</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">UnicodeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unable to save the model card text due to: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_CARD_DATA_FILE_NAME</span><span class="p">)</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">yaml</span><span class="o">.</span><span class="n">safe_dump</span><span class="p">(</span>
                <span class="n">card_data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span> <span class="n">stream</span><span class="o">=</span><span class="n">file</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span>
            <span class="p">)</span>


<span class="k">def</span> <span class="nf">_extract_license_file_from_repository</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the top-level file inventory of `RepoFile` objects from the huggingface hub&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">huggingface_hub</span> <span class="k">as</span> <span class="nn">hub</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Unable to list repository contents for the model repo </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">. In order &quot;</span>
            <span class="s2">&quot;to enable repository listing functionality, please install the huggingface_hub &quot;</span>
            <span class="s2">&quot;package by running `pip install huggingface_hub&gt;0.10.0&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">files</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">list_repo_files</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="n">file</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span> <span class="k">if</span> <span class="n">_LICENSE_FILE_PATTERN</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Failed to retrieve repository file listing data for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> due to </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_write_license_information</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">card_data</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Writes the license file or instructions to retrieve license information.&quot;&quot;&quot;</span>

    <span class="n">fallback</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;A license file could not be found for the &#39;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&#39; repository. </span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;To ensure that you are in compliance with the license requirements for this &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;model, please visit the model repository here: https://huggingface.co/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">license_file</span> <span class="o">:=</span> <span class="n">_extract_license_file_from_repository</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">huggingface_hub</span> <span class="k">as</span> <span class="nn">hub</span>

            <span class="n">license_location</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="n">license_file</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to download the license file due to: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">local_license_path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">license_location</span><span class="p">)</span>
            <span class="n">target_path</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">local_license_path</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">local_license_path</span><span class="p">,</span> <span class="n">target_path</span><span class="p">)</span>
                <span class="k">return</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The license file could not be copied due to: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Fallback or card data license info</span>
    <span class="k">if</span> <span class="n">card_data</span> <span class="ow">and</span> <span class="n">card_data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">license</span> <span class="o">!=</span> <span class="s2">&quot;other&quot;</span><span class="p">:</span>
        <span class="n">fallback</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fallback</span><span class="si">}</span><span class="se">\n</span><span class="s2">The declared license type is: &#39;</span><span class="si">{</span><span class="n">card_data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">license</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Unable to find license information for this model. Please verify &quot;</span>
            <span class="s2">&quot;permissible usage for the model you are storing prior to use.&quot;</span>
        <span class="p">)</span>
    <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_LICENSE_FILE_NAME</span><span class="p">)</span><span class="o">.</span><span class="n">write_text</span><span class="p">(</span><span class="n">fallback</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_supported_pretrained_model_types</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Users might not have all the necessary libraries installed to determine the supported model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">supported_model_types</span> <span class="o">=</span> <span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">FlaxPreTrainedModel</span>

        <span class="n">supported_model_types</span> <span class="o">+=</span> <span class="p">(</span><span class="n">FlaxPreTrainedModel</span><span class="p">,)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">PreTrainedModel</span>

        <span class="n">supported_model_types</span> <span class="o">+=</span> <span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">,)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFPreTrainedModel</span>

        <span class="n">supported_model_types</span> <span class="o">+=</span> <span class="p">(</span><span class="n">TFPreTrainedModel</span><span class="p">,)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">return</span> <span class="n">supported_model_types</span>


<span class="k">def</span> <span class="nf">_build_pipeline_from_model_input</span><span class="p">(</span><span class="n">model_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">task</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Pipeline</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility for generating a pipeline from component parts. If required components are not</span>
<span class="sd">    specified, use the transformers library pipeline component validation to force raising an</span>
<span class="sd">    exception. The underlying Exception thrown in transformers is verbose enough for diagnosis.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model_dict</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">MODEL</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">_get_supported_pretrained_model_types</span><span class="p">())</span> <span class="ow">or</span> <span class="n">is_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;The supplied model type is unsupported. The model must be one of: &quot;</span>
            <span class="s2">&quot;PreTrainedModel, TFPreTrainedModel, FlaxPreTrainedModel, or PeftModel&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">task</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">task</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_LLM_INFERENCE_TASK_PREFIX</span><span class="p">):</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="n">_get_default_task_for_llm_inference_task</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>
        <span class="n">task</span> <span class="o">=</span> <span class="n">_get_task_for_model</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">name_or_path</span><span class="p">,</span> <span class="n">default_task</span><span class="o">=</span><span class="n">default_task</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">suppress_logs</span><span class="p">(</span><span class="s2">&quot;transformers.pipelines.base&quot;</span><span class="p">,</span> <span class="n">filter_regex</span><span class="o">=</span><span class="n">_PEFT_PIPELINE_ERROR_MSG</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="o">**</span><span class="n">model_dict</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;The provided model configuration cannot be created as a Pipeline. &quot;</span>
            <span class="s2">&quot;Please verify that all required and compatible components are &quot;</span>
            <span class="s2">&quot;specified with the correct keys.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>


<span class="k">def</span> <span class="nf">_get_task_for_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">default_task</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the Transformers pipeline task type fro the model instance.</span>

<span class="sd">    NB: The get_task() function only works for remote models available in the Hugging</span>
<span class="sd">    Face hub, so the default task should be supplied when using a custom local model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">transformers.pipelines</span> <span class="kn">import</span> <span class="n">get_task</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_task</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">default_task</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">default_task</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;The task could not be inferred from the model. If you are saving a custom &quot;</span>
            <span class="s2">&quot;local model that is not available in the Hugging Face hub, please provide &quot;</span>
            <span class="s2">&quot;the `task` argument to the `log_model` or `save_model` function.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>


<span class="k">def</span> <span class="nf">_validate_llm_inference_task_type</span><span class="p">(</span><span class="n">llm_inference_task</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pipeline_task</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Validates that an ``inference_task`` type is supported by ``transformers`` pipeline type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">supported_llm_inference_tasks</span> <span class="o">=</span> <span class="n">_SUPPORTED_LLM_INFERENCE_TASK_TYPES_BY_PIPELINE_TASK</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="n">pipeline_task</span><span class="p">,</span> <span class="p">[]</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">llm_inference_task</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_llm_inference_tasks</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The task provided is invalid. &#39;</span><span class="si">{</span><span class="n">llm_inference_task</span><span class="si">}</span><span class="s2">&#39; is not a supported task for &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;the </span><span class="si">{</span><span class="n">pipeline_task</span><span class="si">}</span><span class="s2"> pipeline. Must be one of </span><span class="si">{</span><span class="n">supported_llm_inference_tasks</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_engine_type</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines the underlying execution engine for the model based on the 3 currently supported</span>
<span class="sd">    deep learning framework backends: ``tensorflow``, ``torch``, or ``flax``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">FlaxPreTrainedModel</span><span class="p">,</span> <span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">TFPreTrainedModel</span>
    <span class="kn">from</span> <span class="nn">transformers.utils</span> <span class="kn">import</span> <span class="n">is_torch_available</span>

    <span class="k">if</span> <span class="n">is_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_base_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__mro__</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">TFPreTrainedModel</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;tensorflow&quot;</span>
        <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">PreTrainedModel</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;torch&quot;</span>
        <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">FlaxPreTrainedModel</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;flax&quot;</span>

    <span class="c1"># As a fallback, we check current environment to determine the engine type</span>
    <span class="k">return</span> <span class="s2">&quot;torch&quot;</span> <span class="k">if</span> <span class="n">is_torch_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;tensorflow&quot;</span>


<span class="k">def</span> <span class="nf">_should_add_pyfunc_to_model</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Discriminator for determining whether a particular task type and model instance from within</span>
<span class="sd">    a ``Pipeline`` is currently supported for the pyfunc flavor.</span>

<span class="sd">    Image and Video pipelines can still be logged and used, but are not available for</span>
<span class="sd">    loading as pyfunc.</span>
<span class="sd">    Similarly, esoteric model types (Graph Models, Timeseries Models, and Reinforcement Learning</span>
<span class="sd">    Models) are not permitted for loading as pyfunc due to the complex input types that, in</span>
<span class="sd">    order to support, will require significant modifications (breaking changes) to the pyfunc</span>
<span class="sd">    contract.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">transformers</span>

    <span class="n">exclusion_model_types</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;GraphormerPreTrainedModel&quot;</span><span class="p">,</span>
        <span class="s2">&quot;InformerPreTrainedModel&quot;</span><span class="p">,</span>
        <span class="s2">&quot;TimeSeriesTransformerPreTrainedModel&quot;</span><span class="p">,</span>
        <span class="s2">&quot;DecisionTransformerPreTrainedModel&quot;</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># NB: When pyfunc functionality is added for these pipeline types over time, remove the</span>
    <span class="c1"># entries from the following list.</span>
    <span class="n">exclusion_pipeline_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;DocumentQuestionAnsweringPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ImageToTextPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;VisualQuestionAnsweringPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ImageSegmentationPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;DepthEstimationPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ObjectDetectionPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;VideoClassificationPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ZeroShotImageClassificationPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ZeroShotObjectDetectionPipeline&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ZeroShotAudioClassificationPipeline&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">for</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="n">exclusion_model_types</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformers</span><span class="p">,</span> <span class="n">model_type</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformers</span><span class="p">,</span> <span class="n">model_type</span><span class="p">)):</span>
                <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">exclusion_pipeline_types</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_get_model_config</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">pyfunc_config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load the model configuration if it was provided for use in the `_TransformersWrapper` pyfunc</span>
<span class="sd">    Model wrapper.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_path</span> <span class="o">=</span> <span class="n">local_path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s2">&quot;inference_config.txt&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config_path</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Inference config stored in file ``inference_config.txt`` is deprecated. New logged &quot;</span>
            <span class="s2">&quot;models will store the model configuration in the ``pyfunc`` flavor configuration.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">config_path</span><span class="o">.</span><span class="n">read_text</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pyfunc_config</span> <span class="ow">or</span> <span class="p">{}</span>


<span class="k">def</span> <span class="nf">_load_pyfunc</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads the model as pyfunc model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">local_path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">flavor_configuration</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">FLAVOR_NAME</span><span class="p">)</span>
    <span class="n">model_config</span> <span class="o">=</span> <span class="n">_get_model_config</span><span class="p">(</span><span class="n">local_path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">_COMPONENTS_BINARY_DIR_NAME</span><span class="p">),</span> <span class="n">model_config</span><span class="p">)</span>
    <span class="n">prompt_template</span> <span class="o">=</span> <span class="n">_get_prompt_template</span><span class="p">(</span><span class="n">local_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_TransformersWrapper</span><span class="p">(</span>
        <span class="n">_load_model</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">local_path</span><span class="p">),</span> <span class="n">flavor_configuration</span><span class="p">,</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">),</span>
        <span class="n">flavor_configuration</span><span class="p">,</span>
        <span class="n">model_config</span><span class="p">,</span>
        <span class="n">prompt_template</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_conversational_pipeline</span><span class="p">(</span><span class="n">pipeline</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the pipeline is a ConversationalPipeline.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">cp</span> <span class="o">:=</span> <span class="n">_try_import_conversational_pipeline</span><span class="p">():</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">cp</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_try_import_conversational_pipeline</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Try importing ConversationalPipeline because for version &gt; 4.41.2</span>
<span class="sd">    it is removed from the transformers package.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ConversationalPipeline</span>

        <span class="k">return</span> <span class="n">ConversationalPipeline</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="k">return</span>


<div class="viewcode-block" id="generate_signature_output"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.generate_signature_output">[docs]</a><span class="nd">@experimental</span>
<span class="k">def</span> <span class="nf">generate_signature_output</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">flavor_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility for generating the response output for the purposes of extracting an output signature</span>
<span class="sd">    for model saving and logging. This function simulates loading of a saved model or pipeline</span>
<span class="sd">    as a ``pyfunc`` model without having to incur a write to disk.</span>

<span class="sd">    Args:</span>
<span class="sd">        pipeline: A ``transformers`` pipeline object. Note that component-level or model-level</span>
<span class="sd">            inputs are not permitted for extracting an output example.</span>
<span class="sd">        data: An example input that is compatible with the given pipeline</span>
<span class="sd">        model_config: Any additional model configuration, provided as kwargs, to inform</span>
<span class="sd">            the format of the output type from a pipeline inference call.</span>
<span class="sd">        params: A dictionary of additional parameters to pass to the pipeline for inference.</span>
<span class="sd">        flavor_config: The flavor configuration for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The output from the ``pyfunc`` pipeline wrapper&#39;s ``predict`` method</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">transformers</span>

    <span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">signature</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The pipeline type submitted is not a valid transformers Pipeline. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;The type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not supported.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">signature</span><span class="o">.</span><span class="n">generate_signature_output</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_config</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_TransformersWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">flavor_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prompt_template</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">pipeline</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span> <span class="o">=</span> <span class="n">flavor_config</span>
        <span class="c1"># The predict method updates the model_config several times. This should be done over a</span>
        <span class="c1"># deep copy of the original model_config that was specified by the user, otherwise the</span>
        <span class="c1"># prediction won&#39;t be idempotent. Hence we creates an immutable dictionary of the original</span>
        <span class="c1"># model config here and enforce creating a deep copy at every predict call.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span> <span class="o">=</span> <span class="n">MappingProxyType</span><span class="p">(</span><span class="n">model_config</span> <span class="ow">or</span> <span class="p">{})</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prompt_template</span> <span class="o">=</span> <span class="n">prompt_template</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_conversation</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># NB: Current special-case custom pipeline types that have not been added to</span>
        <span class="c1"># the native-supported transformers package but require custom parsing:</span>
        <span class="c1"># InstructionTextGenerationPipeline [Dolly] https://huggingface.co/databricks/dolly-v2-12b</span>
        <span class="c1">#   (and all variants)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_supported_custom_generator_types</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;InstructionTextGenerationPipeline&quot;</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_LLM_INFERENCE_TASK_KEY</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_raw_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the underlying model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span>

    <span class="k">def</span> <span class="nf">_convert_pandas_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">ZeroShotClassificationPipeline</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># NB: The ZeroShotClassificationPipeline requires an input in the form of</span>
            <span class="c1"># Dict[str, Union[str, List[str]]] and will throw if an additional nested</span>
            <span class="c1"># List is present within the List value (which is what the duplicated values</span>
            <span class="c1"># within the orient=&quot;list&quot; conversion in Pandas will do. This parser will</span>
            <span class="c1"># deduplicate label lists to a single list.</span>
            <span class="n">unpacked</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;list&quot;</span><span class="p">)</span>
            <span class="n">parsed</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">unpacked</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="n">contents</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
                        <span class="c1"># Deduplication logic</span>
                        <span class="k">if</span> <span class="n">item</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">contents</span><span class="p">:</span>
                            <span class="n">contents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
                    <span class="c1"># Collapse nested lists to return the correct data structure for the</span>
                    <span class="c1"># ZeroShotClassificationPipeline input structure</span>
                    <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">contents</span>
                        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">contents</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
                        <span class="k">else</span> <span class="n">contents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="p">)</span>
            <span class="k">return</span> <span class="n">parsed</span>

    <span class="k">def</span> <span class="nf">_merge_model_config_with_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;params provided to the `predict` method will override the inference &quot;</span>
                <span class="s2">&quot;configuration saved with the model. If the params provided are not &quot;</span>
                <span class="s2">&quot;valid for the pipeline, MlflowException will be raised.&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Override the inference configuration with any additional kwargs provided by the user.</span>
            <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">model_config</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_config</span>

    <span class="k">def</span> <span class="nf">_validate_model_config_and_return_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_config</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="k">if</span> <span class="n">return_tensors</span><span class="p">:</span>
            <span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;return_tensors&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">model_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_full_text&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The `return_full_text` parameter is mutually exclusive with the &quot;</span>
                    <span class="s2">&quot;`return_tensors` parameter set when a MLflow inference task is provided. &quot;</span>
                    <span class="s2">&quot;The `return_full_text` parameter will be ignored.&quot;</span>
                <span class="p">)</span>
                <span class="c1"># `return_full_text` is mutually exclusive with `return_tensors`</span>
                <span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;return_full_text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">,</span> <span class="o">**</span><span class="n">model_config</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">**</span><span class="n">model_config</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;The following `model_kwargs` are not used by the model&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                    <span class="s2">&quot;The params provided to the `predict` method are not valid &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;for pipeline </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">AutomaticSpeechRecognitionPipeline</span><span class="p">,</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">AudioClassificationPipeline</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="c1"># transformers &lt;= 4.33.3</span>
                <span class="s2">&quot;Malformed soundfile&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                <span class="c1"># transformers &gt; 4.33.3</span>
                <span class="ow">or</span> <span class="s2">&quot;Soundfile is either not in the correct format or is malformed&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                    <span class="s2">&quot;Failed to process the input audio data. Either the audio file is &quot;</span>
                    <span class="s2">&quot;corrupted or a uri was passed in without overriding the default model &quot;</span>
                    <span class="s2">&quot;signature. If submitting a string uri, please ensure that the model has &quot;</span>
                    <span class="s2">&quot;been saved with a signature that defines a string input type.&quot;</span><span class="p">,</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
            <span class="k">raise</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            data: Model input data.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># NB: This `predict` method updates the model_config several times. To make the predict</span>
        <span class="c1"># call idempotent, we keep the original self.model_config immutable and creates a deep</span>
        <span class="c1"># copy of it at every predict call.</span>
        <span class="n">model_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_model_config_with_params</span><span class="p">(</span><span class="n">model_config</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span> <span class="o">==</span> <span class="n">_LLM_INFERENCE_TASK_CHAT</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">preprocess_llm_inference_input</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">convert_messages_to_prompt</span><span class="p">(</span><span class="n">msgs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span> <span class="k">for</span> <span class="n">msgs</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span> <span class="o">==</span> <span class="n">_LLM_INFERENCE_TASK_COMPLETIONS</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">preprocess_llm_inference_input</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span> <span class="o">==</span> <span class="n">_LLM_INFERENCE_TASK_EMBEDDING</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">preprocess_llm_embedding_params</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_pandas_to_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="s2">&quot;Invalid data submission. Ensure all elements in the list are strings &quot;</span>
                    <span class="s2">&quot;or dictionaries. If dictionaries are supplied, all keys in the &quot;</span>
                    <span class="s2">&quot;dictionaries must be strings and values must be either str or List[str].&quot;</span><span class="p">,</span>
                    <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Input data must be either a pandas.DataFrame, a string, bytes, List[str], &quot;</span>
                <span class="s2">&quot;List[Dict[str, str]], List[Dict[str, Union[str, List[str]]]], &quot;</span>
                <span class="s2">&quot;or Dict[str, Union[str, List[str]]].&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_raw_pipeline_input</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="c1"># Validate resolved or input dict types</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">_validate_input_dictionary_contains_only_strings_and_lists_of_strings</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">):</span>
            <span class="c1"># Validate each dict inside an input List[Dict]</span>
            <span class="nb">all</span><span class="p">(</span>
                <span class="n">_validate_input_dictionary_contains_only_strings_and_lists_of_strings</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_data</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="c1"># NB: the ordering of these conditional statements matters. TranslationPipeline and</span>
        <span class="c1"># SummarizationPipeline both inherit from TextGenerationPipeline (they are subclasses)</span>
        <span class="c1"># in which the return data structure from their __call__ implementation is modified.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TranslationPipeline</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_or_list_str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;translation_text&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">SummarizationPipeline</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_or_list_str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_template</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;summary_text&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Text2TextGenerationPipeline</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_text2text_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_template</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;generated_text&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TextGenerationPipeline</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_or_list_str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_template</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;generated_text&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">QuestionAnsweringPipeline</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_question_answer_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;answer&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">FillMaskPipeline</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_or_list_str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_template</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;token_str&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TextClassificationPipeline</span><span class="p">):</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">ImageClassificationPipeline</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_image_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">ZeroShotClassificationPipeline</span><span class="p">):</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;labels&quot;</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_json_encoded_list</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;candidate_labels&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TableQuestionAnsweringPipeline</span><span class="p">):</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;answer&quot;</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_json_encoded_dict_payload_to_dict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;table&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TokenClassificationPipeline</span><span class="p">):</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;entity_group&quot;</span><span class="p">,</span> <span class="s2">&quot;entity&quot;</span><span class="p">}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">FeatureExtractionPipeline</span><span class="p">):</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_feature_extraction_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_template</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_is_conversational_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">):</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conversation</span><span class="p">:</span>
                <span class="c1"># this import is valid if conversational_pipeline is not None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_conversation</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Conversation</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_conversation</span><span class="o">.</span><span class="n">add_user_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supported_custom_generator_types</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_or_list_str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;generated_text&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutomaticSpeechRecognitionPipeline</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">model_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_timestamps&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;word&quot;</span><span class="p">,</span> <span class="s2">&quot;char&quot;</span><span class="p">]:</span>
                <span class="n">output_key</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;text&quot;</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_audio_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AudioClassificationPipeline</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_audio_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">output_key</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The loaded pipeline type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is &quot;</span>
                <span class="s2">&quot;not enabled for pyfunc predict functionality.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">BAD_REQUEST</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Optional input preservation for specific pipeline types. This is True (include raw</span>
        <span class="c1"># formatting output), but if `include_prompt` is set to False in the `model_config`</span>
        <span class="c1"># option during model saving, excess newline characters and the fed-in prompt will be</span>
        <span class="c1"># trimmed out from the start of the response.</span>
        <span class="n">include_prompt</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include_prompt&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Optional stripping out of `\n` for specific generator pipelines.</span>
        <span class="n">collapse_whitespace</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;collapse_whitespace&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_cast_lists_from_np_back_to_list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c1"># Generate inference data with the pipeline object</span>
        <span class="k">if</span> <span class="n">_is_conversational_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">):</span>
            <span class="n">conversation_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_conversation</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">conversation_output</span><span class="o">.</span><span class="n">generated_responses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If inference task is defined, return tensors internally to get usage information</span>
            <span class="n">return_tensors</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span><span class="p">:</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">output_key</span> <span class="o">=</span> <span class="s2">&quot;generated_token_ids&quot;</span>

            <span class="n">raw_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_config_and_return_output</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span>
            <span class="p">)</span>

        <span class="c1"># Handle the pipeline outputs</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supported_custom_generator_types</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TextGenerationPipeline</span>
        <span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strip_input_from_response_in_instruction_pipelines</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="n">raw_output</span><span class="p">,</span>
                <span class="n">output_key</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span><span class="p">,</span>
                <span class="n">include_prompt</span><span class="p">,</span>
                <span class="n">collapse_whitespace</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">postprocess_output_for_llm_inference_task</span><span class="p">(</span>
                    <span class="n">data</span><span class="p">,</span>
                    <span class="n">output</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">flavor_config</span><span class="p">,</span>
                    <span class="n">model_config</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">FeatureExtractionPipeline</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_inference_task</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">raw_output</span><span class="p">]</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">postprocess_output_for_llm_v1_embedding_task</span><span class="p">(</span>
                    <span class="n">data</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">tokenizer</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_feature_extraction_output</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">FillMaskPipeline</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_list_of_multiple_dicts</span><span class="p">(</span><span class="n">raw_output</span><span class="p">,</span> <span class="n">output_key</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">ZeroShotClassificationPipeline</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_zero_shot_text_classifier_output_to_df</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TokenClassificationPipeline</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_tokenizer_output</span><span class="p">(</span><span class="n">raw_output</span><span class="p">,</span> <span class="n">output_key</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutomaticSpeechRecognitionPipeline</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="n">model_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_timestamps&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;word&quot;</span><span class="p">,</span> <span class="s2">&quot;char&quot;</span><span class="p">]:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">transformers</span><span class="o">.</span><span class="n">AudioClassificationPipeline</span><span class="p">,</span>
                <span class="n">transformers</span><span class="o">.</span><span class="n">TextClassificationPipeline</span><span class="p">,</span>
                <span class="n">transformers</span><span class="o">.</span><span class="n">ImageClassificationPipeline</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_lists_of_dict_to_list_of_str</span><span class="p">(</span><span class="n">raw_output</span><span class="p">,</span> <span class="n">output_key</span><span class="p">)</span>

        <span class="n">sanitized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sanitize_output</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_strings_as_list_if_scalar</span><span class="p">(</span><span class="n">sanitized</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_parse_raw_pipeline_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts inputs to the expected types for specific Pipeline types.</span>
<span class="sd">        Specific logic for individual pipeline types are called via their respective methods if</span>
<span class="sd">        the input isn&#39;t a basic str or List[str] input type of Pipeline.</span>
<span class="sd">        These parsers are required due to the conversion that occurs within schema validation to</span>
<span class="sd">        a Pandas DataFrame encapsulation, a format which is unsupported for the `transformers`</span>
<span class="sd">        library.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TableQuestionAnsweringPipeline</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_coerce_exploded_dict_to_single_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_input_for_table_question_answering</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_is_conversational_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_conversation_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>  <span class="c1"># noqa: SIM114</span>
            <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">FillMaskPipeline</span><span class="p">,</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">TextGenerationPipeline</span><span class="p">,</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">TranslationPipeline</span><span class="p">,</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">SummarizationPipeline</span><span class="p">,</span>
                    <span class="n">transformers</span><span class="o">.</span><span class="n">TokenClassificationPipeline</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">entry</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="c1"># NB: For Text2TextGenerationPipeline, we need more complex handling for dictionary,</span>
        <span class="c1"># as we allow both single string input and dictionary input (or list of them). Both</span>
        <span class="c1"># are once wrapped to Pandas DataFrame during schema enforcement and convert back to</span>
        <span class="c1"># dictionary. The difference between two is columns of the DataFrame, where the first</span>
        <span class="c1"># case (string) will have auto-generated columns like 0, 1, ... while the latter (dict)</span>
        <span class="c1"># will have the original keys to be the columns. When converting back to dictionary,</span>
        <span class="c1"># those columns will becomes the key of dictionary.</span>
        <span class="c1">#</span>
        <span class="c1"># E.g.</span>
        <span class="c1">#  1. If user&#39;s input is string like model.predict(&quot;foo&quot;)</span>
        <span class="c1">#    -&gt; Raw input: &quot;foo&quot;</span>
        <span class="c1">#    -&gt; Pandas dataframe has column 0, with single row &quot;foo&quot;</span>
        <span class="c1">#    -&gt; Derived dictionary will be {0: &quot;foo&quot;}</span>
        <span class="c1">#  2. If user&#39;s input is dictionary like model.predict({&quot;text&quot;: &quot;foo&quot;})</span>
        <span class="c1">#    -&gt; Raw input: {&quot;text&quot;: &quot;foo&quot;}</span>
        <span class="c1">#    -&gt; Pandas dataframe has column &quot;text&quot;, with single row &quot;foo&quot;</span>
        <span class="c1">#    -&gt; Derived dictionary will be {&quot;text&quot;: &quot;foo&quot;}</span>
        <span class="c1">#</span>
        <span class="c1"># Then for the first case, we want to extract values only, similar to other pipelines.</span>
        <span class="c1"># However, for the second case, we want to keep the key-value pair as it is.</span>
        <span class="c1"># In long-term, we should definitely change the upstream handling to avoid this</span>
        <span class="c1"># complexity, but here we just try to make it work by checking if the key is auto-generated.</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Text2TextGenerationPipeline</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">)</span>
            <span class="c1"># Pandas Dataframe derived dictionary will have integer key (row index)</span>
            <span class="ow">and</span> <span class="mi">0</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">entry</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TextClassificationPipeline</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_text_classification_input</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_validate_text_classification_input</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform input type validation for TextClassification pipelines and casting of data</span>
<span class="sd">        that is manipulated internally by the MLflow model server back to a structure that</span>
<span class="sd">        can be used for pipeline inference.</span>

<span class="sd">        To illustrate the input and outputs of this function, for the following inputs to</span>
<span class="sd">        the pyfunc.predict() call for this pipeline type:</span>

<span class="sd">        &quot;text to classify&quot;</span>
<span class="sd">        [&quot;text to classify&quot;, &quot;other text to classify&quot;]</span>
<span class="sd">        {&quot;text&quot;: &quot;text to classify&quot;, &quot;text_pair&quot;: &quot;pair text&quot;}</span>
<span class="sd">        [{&quot;text&quot;: &quot;text&quot;, &quot;text_pair&quot;: &quot;pair&quot;}, {&quot;text&quot;: &quot;t&quot;, &quot;text_pair&quot;: &quot;tp&quot; }]</span>

<span class="sd">        Pyfunc processing will convert these to the following structures:</span>

<span class="sd">        [{0: &quot;text to classify&quot;}]</span>
<span class="sd">        [{0: &quot;text to classify&quot;}, {0: &quot;other text to classify&quot;}]</span>
<span class="sd">        [{&quot;text&quot;: &quot;text to classify&quot;, &quot;text_pair&quot;: &quot;pair text&quot;}]</span>
<span class="sd">        [{&quot;text&quot;: &quot;text&quot;, &quot;text_pair&quot;: &quot;pair&quot;}, {&quot;text&quot;: &quot;t&quot;, &quot;text_pair&quot;: &quot;tp&quot; }]</span>

<span class="sd">        The purpose of this function is to convert them into the correct format for input</span>
<span class="sd">        to the pipeline (wrapping as a list has no bearing on the correctness of the</span>
<span class="sd">        inferred classifications):</span>

<span class="sd">        [&quot;text to classify&quot;]</span>
<span class="sd">        [&quot;text to classify&quot;, &quot;other text to classify&quot;]</span>
<span class="sd">        [{&quot;text&quot;: &quot;text to classify&quot;, &quot;text_pair&quot;: &quot;pair text&quot;}]</span>
<span class="sd">        [{&quot;text&quot;: &quot;text&quot;, &quot;text_pair&quot;: &quot;pair&quot;}, {&quot;text&quot;: &quot;t&quot;, &quot;text_pair&quot;: &quot;tp&quot; }]</span>

<span class="sd">        Additionally, for dict input types (the &#39;text&#39; &amp; &#39;text_pair&#39; input example), the dict</span>
<span class="sd">        input will be JSON stringified within MLflow model serving. In order to reconvert this</span>
<span class="sd">        structure back into the appropriate type, we use ast.literal_eval() to convert back</span>
<span class="sd">        to a dict. We avoid using JSON.loads() due to pandas DataFrame conversions that invert</span>
<span class="sd">        single and double quotes with escape sequences that are not consistent if the string</span>
<span class="sd">        contains escaped quotes.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_check_keys</span><span class="p">(</span><span class="n">payload</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Check if a dictionary contains only allowable keys.&quot;&quot;&quot;</span>
            <span class="n">allowable_str_keys</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text_pair&quot;</span><span class="p">}</span>
            <span class="k">if</span> <span class="nb">set</span><span class="p">(</span><span class="n">payload</span><span class="p">)</span> <span class="o">-</span> <span class="n">allowable_str_keys</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">payload</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="s2">&quot;Text Classification pipelines may only define dictionary inputs with keys &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;defined as </span><span class="si">{</span><span class="n">allowable_str_keys</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">_check_keys</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">data</span>
            <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">payload</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                    <span class="n">_check_keys</span><span class="p">(</span><span class="n">payload</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># NB: To support MLflow serving signature validation, the value within dict</span>
                    <span class="c1"># inputs is JSON encoded. In order for the proper data structure input support</span>
                    <span class="c1"># for a {&quot;text&quot;: &quot;a&quot;, &quot;text_pair&quot;: &quot;b&quot;} (or the list of such a structure) as</span>
                    <span class="c1"># an input, we have to convert the string encoded dict back to a dict.</span>
                    <span class="c1"># Due to how unescaped characters (such as &quot;&#39;&quot;) are encoded, using an explicit</span>
                    <span class="c1"># json.loads() attempted cast can result in invalid input data to the pipeline.</span>
                    <span class="c1"># ast.literal_eval() shows correct conversion, as validated in unit tests.</span>
                    <span class="k">return</span> <span class="p">[</span><span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
                <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">SyntaxError</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="s2">&quot;An unsupported data type has been passed for Text Classification inference. &quot;</span>
                    <span class="s2">&quot;Only str, list of str, dict, and list of dict are supported.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;An unsupported data type has been passed for Text Classification inference. &quot;</span>
                <span class="s2">&quot;Only str, list of str, dict, and list of dict are supported.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_parse_conversation_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># The conversation pipeline can only accept a single string at a time</span>
            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

    <span class="k">def</span> <span class="nf">_parse_input_for_table_question_answering</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;table&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;The input dictionary must have the &#39;table&#39; key.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;table&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">data</span><span class="p">[</span><span class="s2">&quot;table&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;table&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">_coerce_exploded_dict_to_single_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses the result of Pandas DataFrame.to_dict(orient=&quot;records&quot;) from pyfunc</span>
<span class="sd">        signature validation to coerce the output to the required format for a</span>
<span class="sd">        Pipeline that requires a single dict with list elements such as</span>
<span class="sd">        TableQuestionAnsweringPipeline.</span>
<span class="sd">        Example input:</span>

<span class="sd">        [</span>
<span class="sd">          {&quot;answer&quot;: &quot;We should order more pizzas to meet the demand.&quot;},</span>
<span class="sd">          {&quot;answer&quot;: &quot;The venue size should be updated to handle the number of guests.&quot;},</span>
<span class="sd">        ]</span>

<span class="sd">        Output:</span>

<span class="sd">        {</span>
<span class="sd">          &quot;answer&quot;: [</span>
<span class="sd">              &quot;We should order more pizzas to meet the demand.&quot;,</span>
<span class="sd">              &quot;The venue size should be updated to handle the number of guests.&quot;,</span>
<span class="sd">          ]</span>
<span class="sd">        }</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="n">collection</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">parsed</span> <span class="o">=</span> <span class="n">collection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">coll</span> <span class="ow">in</span> <span class="n">collection</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">coll</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parsed</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                            <span class="s2">&quot;Unable to parse the input. The keys within each &quot;</span>
                            <span class="s2">&quot;dictionary of the parsed input are not consistent&quot;</span>
                            <span class="s2">&quot;among the dictionaries.&quot;</span><span class="p">,</span>
                            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">value</span> <span class="o">!=</span> <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]:</span>
                        <span class="n">value_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
                        <span class="k">if</span> <span class="n">value_type</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
                            <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">value</span><span class="p">]</span>
                        <span class="k">elif</span> <span class="n">value_type</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>
                            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">value</span><span class="p">):</span>
                                <span class="c1"># This conversion is required solely for model serving.</span>
                                <span class="c1"># In the parsing logic that occurs internally, strings that</span>
                                <span class="c1"># contain single quotes `&#39;` result in casting to a List[char]</span>
                                <span class="c1"># instead of a str type. Attempting to append a List[char]</span>
                                <span class="c1"># to a List[str] as would happen in the `else` block here</span>
                                <span class="c1"># results in the entire List being overwritten as `None` without</span>
                                <span class="c1"># an Exception being raised. By checking for single value entries</span>
                                <span class="c1"># and subsequently converting to list and extracting the first</span>
                                <span class="c1"># element reconstructs the original input string.</span>
                                <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)][</span><span class="mi">0</span><span class="p">])</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">parsed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">return</span> <span class="n">parsed</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">_flatten_zero_shot_text_classifier_output_to_df</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts the output of sequences, labels, and scores to a Pandas DataFrame output.</span>

<span class="sd">        Example input:</span>

<span class="sd">        [{&#39;sequence&#39;: &#39;My dog loves to eat spaghetti&#39;,</span>
<span class="sd">          &#39;labels&#39;: [&#39;happy&#39;, &#39;sad&#39;],</span>
<span class="sd">          &#39;scores&#39;: [0.9896970987319946, 0.010302911512553692]},</span>
<span class="sd">         {&#39;sequence&#39;: &#39;My dog hates going to the vet&#39;,</span>
<span class="sd">          &#39;labels&#39;: [&#39;sad&#39;, &#39;happy&#39;],</span>
<span class="sd">          &#39;scores&#39;: [0.957074761390686, 0.042925238609313965]}]</span>

<span class="sd">        Output:</span>

<span class="sd">        pd.DataFrame in a fully normalized (flattened) format with each sequence, label, and score</span>
<span class="sd">        having a row entry.</span>
<span class="sd">        For example, here is the DataFrame output:</span>

<span class="sd">                                sequence labels    scores</span>
<span class="sd">        0  My dog loves to eat spaghetti  happy  0.989697</span>
<span class="sd">        1  My dog loves to eat spaghetti    sad  0.010303</span>
<span class="sd">        2  My dog hates going to the vet    sad  0.957075</span>
<span class="sd">        3  My dog hates going to the vet  happy  0.042925</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Encountered an unknown return type from the pipeline type &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">. Expecting a List[Dict]&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">BAD_REQUEST</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">]</span>

        <span class="n">flattened_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">entry</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span> <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;scores&quot;</span><span class="p">]):</span>
                <span class="n">flattened_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;sequence&quot;</span><span class="p">:</span> <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;sequence&quot;</span><span class="p">],</span> <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">label</span><span class="p">,</span> <span class="s2">&quot;scores&quot;</span><span class="p">:</span> <span class="n">score</span><span class="p">}</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flattened_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_strip_input_from_response_in_instruction_pipelines</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_data</span><span class="p">,</span>
        <span class="n">output</span><span class="p">,</span>
        <span class="n">output_key</span><span class="p">,</span>
        <span class="n">flavor_config</span><span class="p">,</span>
        <span class="n">include_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">collapse_whitespace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parse the output from instruction pipelines to conform with other text generator</span>
<span class="sd">        pipeline types and remove line feed characters and other confusing outputs</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">extract_response_data</span><span class="p">(</span><span class="n">data_out</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data_out</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">elem</span><span class="p">[</span><span class="n">output_key</span><span class="p">]</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">data_out</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data_out</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">elem</span><span class="p">[</span><span class="n">output_key</span><span class="p">]</span> <span class="k">for</span> <span class="n">coll</span> <span class="ow">in</span> <span class="n">data_out</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">coll</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to parse the pipeline output. Expected List[Dict[str,str]] or &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;List[List[Dict[str,str]]] but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data_out</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
                <span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">extract_response_data</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">trim_input</span><span class="p">(</span><span class="n">data_in</span><span class="p">,</span> <span class="n">data_out</span><span class="p">):</span>
            <span class="c1"># NB: the &#39;\n\n&#39; pattern is exclusive to specific InstructionalTextGenerationPipeline</span>
            <span class="c1"># types that have been loaded as a plain TextGenerator. The structure of these</span>
            <span class="c1"># pipelines will precisely repeat the input question immediately followed by 2 carriage</span>
            <span class="c1"># return statements, followed by the start of the response to the prompt. We only</span>
            <span class="c1"># want to left-trim these types of pipelines output values if the user has indicated</span>
            <span class="c1"># the removal action of the input prompt in the returned str or List[str] by applying</span>
            <span class="c1"># the optional model_config entry of `{&quot;include_prompt&quot;: False}`.</span>
            <span class="c1"># By default, the prompt is included in the response.</span>
            <span class="c1"># Stripping out additional carriage returns (\n) is another additional optional flag</span>
            <span class="c1"># that can be set for these generator pipelines. It is off by default (False).</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="n">include_prompt</span>
                <span class="ow">and</span> <span class="n">flavor_config</span><span class="p">[</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">INSTANCE_TYPE</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supported_custom_generator_types</span>
                <span class="ow">and</span> <span class="n">data_out</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">data_in</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="c1"># If the user has indicated to not preserve the prompt input in the response,</span>
                <span class="c1"># split the response output and trim the input prompt from the response.</span>
                <span class="n">data_out</span> <span class="o">=</span> <span class="n">data_out</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">data_in</span><span class="p">)</span> <span class="p">:]</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">data_out</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;A:&quot;</span><span class="p">):</span>
                    <span class="n">data_out</span> <span class="o">=</span> <span class="n">data_out</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>

            <span class="c1"># If the user has indicated to remove newlines and extra spaces from the generated</span>
            <span class="c1"># text, replace them with a single space.</span>
            <span class="k">if</span> <span class="n">collapse_whitespace</span><span class="p">:</span>
                <span class="n">data_out</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">data_out</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">data_out</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">trim_input</span><span class="p">(</span><span class="n">data_in</span><span class="p">,</span> <span class="n">data_out</span><span class="p">)</span> <span class="k">for</span> <span class="n">data_in</span><span class="p">,</span> <span class="n">data_out</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">output</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">trim_input</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Unknown data structure after parsing output. Expected str or List[str]. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sanitize_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">input_data</span><span class="p">):</span>
        <span class="c1"># Some pipelines and their underlying models leave leading or trailing whitespace.</span>
        <span class="c1"># This method removes that whitespace.</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TokenClassificationPipeline</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="c1"># Retrieve the first output for return types that are List[str] of only a single</span>
            <span class="c1"># element.</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">output</span><span class="p">):</span>
                <span class="n">cleaned</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
                <span class="c1"># If the list has only a single string, return as string.</span>
                <span class="k">return</span> <span class="n">cleaned</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cleaned</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">cleaned</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_sanitize_output</span><span class="p">(</span><span class="n">coll</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span> <span class="k">for</span> <span class="n">coll</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_wrap_strings_as_list_if_scalar</span><span class="p">(</span><span class="n">output_data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps single string outputs in a list to support batch processing logic in serving.</span>
<span class="sd">        Scalar values are not supported for processing in batch logic as they cannot be coerced</span>
<span class="sd">        to DataFrame representations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">output_data</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output_data</span>

    <span class="k">def</span> <span class="nf">_parse_lists_of_dict_to_list_of_str</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_data</span><span class="p">,</span> <span class="n">target_dict_key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses the output results from select Pipeline types to extract specific values from a</span>
<span class="sd">        target key.</span>
<span class="sd">        Examples (with &quot;a&quot; as the `target_dict_key`):</span>

<span class="sd">        Input: [{&quot;a&quot;: &quot;valid&quot;, &quot;b&quot;: &quot;invalid&quot;}, {&quot;a&quot;: &quot;another valid&quot;, &quot;c&quot;: invalid&quot;}]</span>
<span class="sd">        Output: [&quot;valid&quot;, &quot;another_valid&quot;]</span>

<span class="sd">        Input: [{&quot;a&quot;: &quot;valid&quot;, &quot;b&quot;: [{&quot;a&quot;: &quot;another valid&quot;}, {&quot;b&quot;: &quot;invalid&quot;}]},</span>
<span class="sd">                {&quot;a&quot;: &quot;valid 2&quot;, &quot;b&quot;: [{&quot;a&quot;: &quot;another valid 2&quot;}, {&quot;c&quot;: &quot;invalid&quot;}]}]</span>
<span class="sd">        Output: [&quot;valid&quot;, &quot;another valid&quot;, &quot;valid 2&quot;, &quot;another valid 2&quot;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">output_coll</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">output_data</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="n">target_dict_key</span><span class="p">:</span>
                            <span class="n">output_coll</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">target_dict_key</span><span class="p">])</span>
                        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                            <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">value</span>
                        <span class="p">):</span>
                            <span class="n">output_coll</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">_parse_lists_of_dict_to_list_of_str</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">target_dict_key</span><span class="p">)</span>
                            <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="n">output_coll</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_parse_lists_of_dict_to_list_of_str</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_dict_key</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">return</span> <span class="n">output_coll</span>
        <span class="k">elif</span> <span class="n">target_dict_key</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output_data</span><span class="p">[</span><span class="n">target_dict_key</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output_data</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_parse_feature_extraction_input</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_data</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_parse_feature_extraction_output</span><span class="p">(</span><span class="n">output_data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parse the return type from a FeatureExtractionPipeline output. The mixed types for</span>
<span class="sd">        input are present depending on how the pyfunc is instantiated. For model serving usage,</span>
<span class="sd">        the returned type from MLServer will be a numpy.ndarray type, otherwise, the return</span>
<span class="sd">        within a manually executed pyfunc (i.e., for udf usage), the return will be a collection</span>
<span class="sd">        of nested lists.</span>

<span class="sd">        Examples:</span>

<span class="sd">        Input: [[[0.11, 0.98, 0.76]]] or np.array([0.11, 0.98, 0.76])</span>
<span class="sd">        Output: np.array([0.11, 0.98, 0.76])</span>

<span class="sd">        Input: [[[[0.1, 0.2], [0.3, 0.4]]]] or</span>
<span class="sd">            np.array([np.array([0.1, 0.2]), np.array([0.3, 0.4])])</span>
<span class="sd">        Output: np.array([np.array([0.1, 0.2]), np.array([0.3, 0.4])])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">output_data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_parse_tokenizer_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_data</span><span class="p">,</span> <span class="n">target_set</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses the tokenizer pipeline output.</span>

<span class="sd">        Examples:</span>

<span class="sd">        Input: [{&quot;entity&quot;: &quot;PRON&quot;, &quot;score&quot;: 0.95}, {&quot;entity&quot;: &quot;NOUN&quot;, &quot;score&quot;: 0.998}]</span>
<span class="sd">        Output: &quot;PRON,NOUN&quot;</span>

<span class="sd">        Input: [[{&quot;entity&quot;: &quot;PRON&quot;, &quot;score&quot;: 0.95}, {&quot;entity&quot;: &quot;NOUN&quot;, &quot;score&quot;: 0.998}],</span>
<span class="sd">                [{&quot;entity&quot;: &quot;PRON&quot;, &quot;score&quot;: 0.95}, {&quot;entity&quot;: &quot;NOUN&quot;, &quot;score&quot;: 0.998}]]</span>
<span class="sd">        Output: [&quot;PRON,NOUN&quot;, &quot;PRON,NOUN&quot;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># NB: We&#39;re collapsing the results here to a comma separated string for each inference</span>
        <span class="c1"># input string. This is to simplify having to otherwise make extensive changes to</span>
        <span class="c1"># ColSpec in order to support schema enforcement of List[List[str]]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_parse_tokenizer_output</span><span class="p">(</span><span class="n">coll</span><span class="p">,</span> <span class="n">target_set</span><span class="p">)</span> <span class="k">for</span> <span class="n">coll</span> <span class="ow">in</span> <span class="n">output_data</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># NB: Since there are no attributes accessible from the pipeline object that determine</span>
            <span class="c1"># what the characteristics of the return structure names are within the dictionaries,</span>
            <span class="c1"># Determine which one is present in the output to extract the correct entries.</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target_set</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">output_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="k">return</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">coll</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="k">for</span> <span class="n">coll</span> <span class="ow">in</span> <span class="n">output_data</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_parse_list_of_multiple_dicts</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">target_dict_key</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the first value of the `target_dict_key` that matches in the first dictionary in a</span>
<span class="sd">        list of dictionaries.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">fetch_target_key_value</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">key</span><span class="p">]</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">fetch_target_key_value</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">target_dict_key</span><span class="p">)</span> <span class="k">for</span> <span class="n">collection</span> <span class="ow">in</span> <span class="n">output_data</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">output_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">target_dict_key</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">_parse_question_answer_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses the single string input representation for a question answer pipeline into the</span>
<span class="sd">        required dict format for a `question-answering` pipeline.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_parse_question_answer_input</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">expected_keys</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="s2">&quot;context&quot;</span><span class="p">}</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">expected_keys</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span> <span class="o">==</span> <span class="n">expected_keys</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Invalid keys were submitted. Keys must be exclusively </span><span class="si">{</span><span class="n">expected_keys</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;An invalid type has been supplied. Must be either List[Dict[str, str]] or &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Dict[str, str]. </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not supported.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_parse_text2text_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses the mixed input types that can be submitted into a text2text Pipeline.</span>
<span class="sd">        Valid examples:</span>

<span class="sd">        Input:</span>
<span class="sd">            {&quot;context&quot;: &quot;abc&quot;, &quot;answer&quot;: &quot;def&quot;}</span>
<span class="sd">        Output:</span>
<span class="sd">            &quot;context: abc answer: def&quot;</span>
<span class="sd">        Input:</span>
<span class="sd">            [{&quot;context&quot;: &quot;abc&quot;, &quot;answer&quot;: &quot;def&quot;}, {&quot;context&quot;: &quot;ghi&quot;, &quot;answer&quot;: &quot;jkl&quot;}]</span>
<span class="sd">        Output:</span>
<span class="sd">            [&quot;context: abc answer: def&quot;, &quot;context: ghi answer: jkl&quot;]</span>
<span class="sd">        Input:</span>
<span class="sd">            &quot;abc&quot;</span>
<span class="sd">        Output:</span>
<span class="sd">            &quot;abc&quot;</span>
<span class="sd">        Input:</span>
<span class="sd">            [&quot;abc&quot;, &quot;def&quot;]</span>
<span class="sd">        Output:</span>
<span class="sd">            [&quot;abc&quot;, &quot;def&quot;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;inputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                <span class="c1"># NB: Text2Text Pipelines require submission of text in a pseudo-string based dict</span>
                <span class="c1"># formatting.</span>
                <span class="c1"># As an example, for the input of:</span>
                <span class="c1"># data = {&quot;context&quot;: &quot;The sky is blue&quot;, &quot;answer&quot;: &quot;blue&quot;}</span>
                <span class="c1"># This method will return the Pipeline-required format of:</span>
                <span class="c1"># &quot;context: The sky is blue. answer: blue&quot;</span>
                <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_parse_text2text_input</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;An invalid type has been supplied: </span><span class="si">{</span><span class="n">_truncate_and_ellipsize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">). Please supply a Dict[str, str], str, List[str], &quot;</span>
                <span class="s2">&quot;or a List[Dict[str, str]] for a Text2Text Pipeline.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_parse_json_encoded_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">key_to_unpack</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses the complex input types for pipelines such as ZeroShotClassification in which</span>
<span class="sd">        the required input type is Dict[str, Union[str, List[str]]] wherein the list</span>
<span class="sd">        provided is encoded as JSON. This method unpacks that string to the required</span>
<span class="sd">        elements.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_parse_json_encoded_list</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span> <span class="n">key_to_unpack</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">key_to_unpack</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="s2">&quot;Invalid key in inference payload. The expected inference data key &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;is: </span><span class="si">{</span><span class="n">key_to_unpack</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">key_to_unpack</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">key_to_unpack</span> <span class="k">else</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="k">except</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">data</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">key_to_unpack</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">data</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_parse_json_encoded_dict_payload_to_dict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">key_to_unpack</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parses complex dict input types that have been json encoded. Pipelines like</span>
<span class="sd">        TableQuestionAnswering require such input types.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="n">key</span><span class="p">:</span> <span class="p">(</span>
                        <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="n">key_to_unpack</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                        <span class="k">else</span> <span class="n">value</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">entry</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># This is to handle serving use cases as the DataFrame encapsulation converts</span>
            <span class="c1"># collections within rows to np.array type. In order to process this data through</span>
            <span class="c1"># the transformers.Pipeline API, we need to cast these arrays back to lists</span>
            <span class="c1"># and replace the single quotes with double quotes after extracting the</span>
            <span class="c1"># json-encoded `table` (a pandas DF) in order to convert it to a dict that</span>
            <span class="c1"># the TableQuestionAnsweringPipeline can accept and cast to a Pandas DataFrame.</span>
            <span class="c1">#</span>
            <span class="c1"># An example casting that occurs for this case when input to model serving is the</span>
            <span class="c1"># conversion of a user input of:</span>
            <span class="c1">#   &#39;{&quot;inputs&quot;: {&quot;query&quot;: &quot;What is the longest distance?&quot;,</span>
            <span class="c1">#                &quot;table&quot;: {&quot;Distance&quot;: [&quot;1000&quot;, &quot;10&quot;, &quot;1&quot;]}}}&#39;</span>
            <span class="c1"># is converted to:</span>
            <span class="c1">#   [{&#39;query&#39;: array(&#39;What is the longest distance?&#39;, dtype=&#39;&lt;U29&#39;),</span>
            <span class="c1">#     &#39;table&#39;: array(&#39;{\&#39;Distance\&#39;: [\&#39;1000\&#39;, \&#39;10\&#39;, \&#39;1\&#39;]}&#39;, dtype=&#39;U&lt;204&#39;)}]</span>
            <span class="c1"># which is an invalid input to the pipeline.</span>
            <span class="c1"># this method converts the input to:</span>
            <span class="c1">#   {&#39;query&#39;: &#39;What is the longest distance?&#39;,</span>
            <span class="c1">#    &#39;table&#39;: {&#39;Distance&#39;: [&#39;1000&#39;, &#39;10&#39;, &#39;1&#39;]}}</span>
            <span class="c1"># which is a valid input to the TableQuestionAnsweringPipeline.</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="n">key_to_unpack</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                        <span class="n">output</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">output</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                        <span class="c1"># This cast to np.ndarray occurs when more than one question is asked.</span>
                        <span class="n">output</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Otherwise, the entry does not need casting from a np.ndarray type to</span>
                        <span class="c1"># list as it is already a scalar string.</span>
                        <span class="n">output</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="n">key</span><span class="p">:</span> <span class="p">(</span>
                    <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="n">key_to_unpack</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">value</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_validate_str_or_list_str</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The input data is of an incorrect type. </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2"> is invalid. &quot;</span>
                <span class="s2">&quot;Must be either string or List[str]&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;If supplying a list, all values must be of string type.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_convert_cast_lists_from_np_back_to_list</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This handles the casting of dicts within lists from Pandas DF conversion within model</span>
<span class="sd">        serving back into the required Dict[str, List[str]] if this type matching occurs.</span>
<span class="sd">        Otherwise, it&#39;s a noop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># NB: applying a short-circuit return here to not incur runtime overhead with</span>
            <span class="c1"># type validation if the input is not a list</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">parsed_data</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">entry</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                    <span class="n">parsed_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">entry</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">parsed_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">parsed_data</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_base64_image</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check whether input image is a base64 encoded&quot;&quot;&quot;</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">b64_decoded_image</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">b64_decoded_image</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">image</span>
                <span class="ow">or</span> <span class="n">base64</span><span class="o">.</span><span class="n">encodebytes</span><span class="p">(</span><span class="n">b64_decoded_image</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">image</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="n">binascii</span><span class="o">.</span><span class="n">Error</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_convert_image_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Conversion utility for decoding the base64 encoded bytes data of a raw image file when</span>
<span class="sd">        parsed through model serving, if applicable. Direct usage of the pyfunc implementation</span>
<span class="sd">        outside of model serving will treat this utility as a noop.</span>

<span class="sd">        For reference, the expected encoding for input to Model Serving will be:</span>

<span class="sd">        import requests</span>
<span class="sd">        import base64</span>

<span class="sd">        response = requests.get(&quot;https://www.my.images/a/sound/file.jpg&quot;)</span>
<span class="sd">        encoded_image = base64.b64encode(response.content).decode(&quot;utf-8&quot;)</span>

<span class="sd">        inference_data = json.dumps({&quot;inputs&quot;: [encoded_image]})</span>

<span class="sd">        or</span>

<span class="sd">        inference_df = pd.DataFrame(</span>
<span class="sd">        pd.Series([encoded_image], name=&quot;image_file&quot;)</span>
<span class="sd">        )</span>
<span class="sd">        split_dict = {&quot;dataframe_split&quot;: inference_df.to_dict(orient=&quot;split&quot;)}</span>
<span class="sd">        split_json = json.dumps(split_dict)</span>

<span class="sd">        or</span>

<span class="sd">        records_dict = {&quot;dataframe_records&quot;: inference_df.to_dict(orient=&quot;records&quot;)}</span>
<span class="sd">        records_json = json.dumps(records_dict)</span>

<span class="sd">        This utility will convert this JSON encoded, base64 encoded text back into bytes for</span>
<span class="sd">        input into the Image pipelines for inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">process_input_element</span><span class="p">(</span><span class="n">input_element</span><span class="p">):</span>
            <span class="n">input_value</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">input_element</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_base64_image</span><span class="p">(</span><span class="n">input_value</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_input_uri_or_file</span><span class="p">(</span><span class="n">input_value</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">input_value</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">input_data</span>
        <span class="p">):</span>
            <span class="c1"># Use a list comprehension for readability</span>
            <span class="c1"># the elimination of empty collection declarations</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">process_input_element</span><span class="p">(</span><span class="n">element</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_base64_image</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_input_uri_or_file</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_data</span>

    <span class="k">def</span> <span class="nf">_convert_audio_input</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AudioInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">AudioInput</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">AudioInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">AudioInput</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert the input data into the format that the Transformers pipeline expects.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: The input data to be converted. This can be one of the following:</span>
<span class="sd">                1. A single input audio data (bytes, numpy array, or a path or URI to an audio file)</span>
<span class="sd">                2. List of dictionaries, derived from Pandas DataFrame with `orient=&quot;records&quot;`.</span>
<span class="sd">                   This is the outcome of the pyfunc signature validation for the audio input.</span>
<span class="sd">                   E.g. [{[0]: &lt;audio data&gt;}, {[1]: &lt;audio data&gt;}]</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single or list of audio data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">element</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_decode_audio</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span> <span class="k">for</span> <span class="n">audio</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
            <span class="c1"># Signature validation converts a single audio data into a list (via Pandas Series).</span>
            <span class="c1"># We have to unwrap it back not to confuse with batch processing.</span>
            <span class="k">return</span> <span class="n">decoded</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">decoded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode_audio</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_decode_audio</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">:</span> <span class="n">AudioInput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AudioInput</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decode the audio data if it is base64 encoded bytes, otherwise no-op.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># Input is an URI to the audio file to be processed.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_str_input_uri_or_file</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">audio</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="c1"># Input is a numpy array that contains floating point time series of the audio.</span>
            <span class="k">return</span> <span class="n">audio</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">):</span>
            <span class="c1"># Input is a bytes object. In model serving, the input audio data is b64encoded.</span>
            <span class="c1"># They are typically decoded before reaching here, but iff the inference payload</span>
            <span class="c1"># contains raw bytes in the key &#39;inputs&#39;, the upstream code will not decode the</span>
            <span class="c1"># bytes. Therefore, we need to decode the bytes here. For other cases like</span>
            <span class="c1"># &#39;dataframe_records&#39; or &#39;dataframe_split&#39;, the bytes should be already decoded.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_base64_audio</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">audio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Invalid audio data. Must be either bytes, str, or np.ndarray.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_base64_audio</span><span class="p">(</span><span class="n">audio</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check whether input audio is a base64 encoded&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">audio</span><span class="p">))</span> <span class="o">==</span> <span class="n">audio</span>
        <span class="k">except</span> <span class="n">binascii</span><span class="o">.</span><span class="n">Error</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_validate_str_input_uri_or_file</span><span class="p">(</span><span class="n">input_str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validation of blob references to either audio or image files,</span>
<span class="sd">        if a string is input to the ``predict``</span>
<span class="sd">        method, perform validation of the string contents by checking for a valid uri or</span>
<span class="sd">        filesystem reference instead of surfacing the cryptic stack trace that is otherwise raised</span>
<span class="sd">        for an invalid uri input.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">is_uri</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">all</span><span class="p">([</span><span class="n">result</span><span class="o">.</span><span class="n">scheme</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">netloc</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

        <span class="n">valid_uri</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_uri</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_uri</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">20</span><span class="p">:</span>
                <span class="n">data_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Received: </span><span class="si">{</span><span class="n">input_str</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Received (truncated): </span><span class="si">{</span><span class="n">input_str</span><span class="p">[:</span><span class="mi">20</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;An invalid string input was provided. String inputs to &quot;</span>
                <span class="s2">&quot;audio or image files must be either a file location or a uri.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;audio files must be either a file location or a uri. </span><span class="si">{</span><span class="n">data_str</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">BAD_REQUEST</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_prompt_template</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps the input data in the specified prompt template. If no template is</span>
<span class="sd">        specified, or if the pipeline is an unsupported type, or if the input type</span>
<span class="sd">        is not a string or list of strings, then the input data is returned unchanged.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_template</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_data</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">task</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_PROMPT_TEMPLATING_TASK_TYPES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;_format_prompt_template called on an unexpected pipeline type. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Expected one of: </span><span class="si">{</span><span class="n">_SUPPORTED_PROMPT_TEMPLATING_TASK_TYPES</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Received: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">task</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># if every item is a string, then apply formatting to every item</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">]</span>

        <span class="c1"># throw for unsupported types</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
            <span class="s2">&quot;Prompt templating is only supported for data of type str or List[str]. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="autolog"><a class="viewcode-back" href="../../python_api/mlflow.transformers.html#mlflow.transformers.autolog">[docs]</a><span class="nd">@experimental</span>
<span class="nd">@autologging_integration</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">autolog</span><span class="p">(</span>
    <span class="n">log_input_examples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_model_signatures</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_datasets</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">disable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">disable_for_unsupported_versions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">extra_tags</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This autologging integration is solely used for disabling spurious autologging of irrelevant</span>
<span class="sd">    sub-models that are created during the training and evaluation of transformers-based models.</span>
<span class="sd">    Autologging functionality is not implemented fully for the transformers flavor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># A list of other flavors whose base autologging config would be automatically logged due to</span>
    <span class="c1"># training a model that would otherwise create a run and be logged internally within the</span>
    <span class="c1"># transformers-supported trainer calls.</span>
    <span class="n">DISABLED_ANCILLARY_FLAVOR_AUTOLOGGING</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sklearn&quot;</span><span class="p">,</span> <span class="s2">&quot;tensorflow&quot;</span><span class="p">,</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">disable_discrete_autologging</span><span class="p">(</span><span class="n">DISABLED_ANCILLARY_FLAVOR_AUTOLOGGING</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">original</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">setfit</span>

        <span class="n">safe_patch</span><span class="p">(</span>
            <span class="n">FLAVOR_NAME</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">setfit</span><span class="o">.</span><span class="n">SetFitTrainer</span>
                <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">setfit</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.0.0&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">setfit</span><span class="o">.</span><span class="n">Trainer</span>
            <span class="p">),</span>
            <span class="s2">&quot;train&quot;</span><span class="p">,</span>
            <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">train</span><span class="p">),</span>
            <span class="n">manage_run</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Seq2SeqTrainer</span><span class="p">]</span>
        <span class="n">methods</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">clazz</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">methods</span><span class="p">:</span>
                <span class="n">safe_patch</span><span class="p">(</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="n">clazz</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">train</span><span class="p">),</span> <span class="n">manage_run</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_get_prompt_template</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;Could not find an &quot;</span><span class="si">{</span><span class="n">MLMODEL_FILE_NAME</span><span class="si">}</span><span class="s1">&quot; configuration file at &quot;</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">,</span>
            <span class="n">RESOURCE_DOES_NOT_EXIST</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">model_conf</span> <span class="o">=</span> <span class="n">Model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_conf</span><span class="o">.</span><span class="n">metadata</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_conf</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FlavorKey</span><span class="o">.</span><span class="n">PROMPT_TEMPLATE</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_validate_prompt_template</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">prompt_template</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Argument `prompt_template` must be a string, received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">format_args</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">Formatter</span><span class="p">()</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">)</span> <span class="k">if</span> <span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">]</span>

    <span class="c1"># expect there to only be one format arg, and for that arg to be &quot;prompt&quot;</span>
    <span class="k">if</span> <span class="n">format_args</span> <span class="o">!=</span> <span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
            <span class="s2">&quot;Argument `prompt_template` must be a string with a single format arg, &#39;prompt&#39;. &quot;</span>
            <span class="s2">&quot;For example: &#39;Answer the following question in a friendly tone. Q: </span><span class="si">{prompt}</span><span class="s2">. A:&#39;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Received </span><span class="si">{</span><span class="n">prompt_template</span><span class="si">}</span><span class="s2">. &quot;</span>
        <span class="p">)</span>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>