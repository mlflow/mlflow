name: Pipeline tests

on:
  pull_request:
  push:
    branches:
      - master
      - branch-[0-9]+.[0-9]+
  workflow_dispatch:
    inputs:
      repository:
        description: >
          [Optional] Repository name with owner. For example, mlflow/mlflow.
           Defaults to the repository that triggered a workflow.
        required: false
        default: ""
      ref:
        description: >
          [Optional] The branch, tag or SHA to checkout. When checking out the repository that
           triggered a workflow, this defaults to the reference or SHA for that event. Otherwise,
           uses the default branch.
        required: false
        default: ""

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

# Use `bash --noprofile --norc -exo pipefail` by default for all `run` steps in this workflow:
# https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#defaultsrun
defaults:
  run:
    shell: bash --noprofile --norc -exo pipefail {0}

env:
  # Note miniconda is pre-installed in the virtual environments for GitHub Actions:
  # https://github.com/actions/virtual-environments/blob/main/images/linux/scripts/installers/miniconda.sh
  MLFLOW_CONDA_HOME: /usr/share/miniconda
  SPARK_LOCAL_IP: localhost

jobs:
  pipelines:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - uses: ./.github/actions/setup-python
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps.sh
          pip install -e .
          # TODO: Unpin once Delta supports Spark 3.3 (https://github.com/delta-io/delta/issues/1217)
          pip install 'pyspark<3.3'
      - name: Check package versions
        run: |
          python dev/show_package_release_dates.py
      - name: Run tests
        run: |
          export MLFLOW_HOME=$(pwd)
          pytest tests/pipelines

  pipelines-windows:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - uses: ./.github/actions/setup-python
      - name: Install python dependencies
        run: |
          pip install -r requirements/test-requirements.txt
          pip install --no-dependencies tests/resources/mlflow-test-plugin
          pip install -e .[extras]
          # TODO: Unpin once Delta supports Spark 3.3 (https://github.com/delta-io/delta/issues/1217)
          pip install 'pyspark<3.3'
      - name: Check package versions
        run: |
          python dev/show_package_release_dates.py
      - name: Download Hadoop winutils for Spark
        run: |
          git clone https://github.com/cdarlint/winutils
      - name: Run tests
        run: |
          # Set Hadoop environment variables required for testing Spark integrations on Windows
          export HADOOP_HOME=`realpath winutils/hadoop-3.2.2`
          export PATH=$PATH:$HADOOP_HOME/bin
          # Run pipelines tests
          export MLFLOW_HOME=$(pwd)
          python examples/spark_udf/spark_udf.py || true
          pip uninstall -y waitress
          python examples/spark_udf/spark_udf.py || true
          pip install -U flask==2.0.0
          python examples/spark_udf/spark_udf.py || true
          pip install -U flask==2.1.3
          python examples/spark_udf/spark_udf.py || true
          pytest tests/pipelines
