name: codex

on:
  pull_request:

jobs:
  codex:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      models: read
    timeout-minutes: 10
    steps:
      - name: Install codex
        run: |
          npm install -g @openai/codex

      - name: Set up config
        run: |
          # https://github.com/openai/codex/blob/main/codex-rs/config.md
          mkdir -p ~/.codex
          cat <<EOF > ~/.codex/config.toml
          model = "openai/gpt-4.1"
          model_provider = "github-models"

          [model_providers.github-models]
          name = "GitHub Models"
          base_url = "https://models.github.ai/inference"
          env_key = "GITHUB_TOKEN"
          wire_api = "chat"

          [mcp_servers.github]
          command = "docker"
          args = [
            "run",
            "-i",
            "--rm",
            "-e",
            "GITHUB_PERSONAL_ACCESS_TOKEN",
            "-e",
            "GITHUB_READ_ONLY=1",
            "ghcr.io/github/github-mcp-server",
          ]
          env = { "GITHUB_PERSONAL_ACCESS_TOKEN" = "${{ secrets.GITHUB_TOKEN }}" }
          EOF

      - name: Run codex
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          codex --full-auto exec "Can you fetch the most recent issue and summarize it?"
