name: MLflow tests

on:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened
      - ready_for_review
  push:
    branches:
      - master
      - branch-[0-9]+.[0-9]+

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name }}-${{ github.ref }}
  cancel-in-progress: true

# Use `bash` by default for all `run` steps in this workflow:
# https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#defaultsrun
defaults:
  run:
    shell: bash

env:
  MLFLOW_HOME: ${{ github.workspace }}
  # Note miniconda is pre-installed in the virtual environments for GitHub Actions:
  # https://github.com/actions/virtual-environments/blob/main/images/linux/scripts/installers/miniconda.sh
  MLFLOW_CONDA_HOME: /usr/share/miniconda
  SPARK_LOCAL_IP: localhost
  PIP_EXTRA_INDEX_URL: https://download.pytorch.org/whl/cpu
  PIP_CONSTRAINT: ${{ github.workspace }}/requirements/constraints.txt
  PYTHONUTF8: "1"
  _MLFLOW_TESTING_TELEMETRY: "true"

jobs:
  # python-skinny tests cover a subset of mlflow functionality
  # that is meant to be supported with a smaller dependency footprint.
  # The python skinny tests cover the subset of mlflow functionality
  # while also verifying certain dependencies are omitted.
  python-skinny:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh --skinny
      - uses: ./.github/actions/show-versions
      - name: Run tests
        run: |
          ./dev/run-python-skinny-tests-uv.sh

  python:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        include:
          - splits: 10
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh --ml
          # transformers doesn't support Keras 3 yet. tf-keras needs to be installed as a workaround.
          uv pip install -c requirements/constraints.txt tf-keras
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - name: Import check
        run: |
          # `-I` is used to avoid importing modules from user-specific site-packages
          # that might conflict with the built-in modules (e.g. `types`).
          uv run python -I tests/check_mlflow_lazily_imports_ml_packages.py
      - name: Setup SSH and run tests
        run: |
          source dev/setup-ssh.sh
      - uses: ./.github/actions/manage-test-durations
        with:
          job_name: "python"
          test_command: "uv run pytest --quiet --requires-ssh --ignore-flavors --ignore=tests/examples --ignore=tests/evaluate --ignore tests/genai tests || true"

  py310:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5]
        include:
          - splits: 5
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          ./dev/install-common-deps-uv.sh
          # test telemetry events
          uv pip install -c requirements/constraints.txt openai
          uv pip install -c requirements/constraints.txt .
      - uses: ./.github/actions/show-versions
      - name: Run tests
        run: |
          uv run python -c "import sys; assert sys.version_info[:2] == (3, 10), sys.version_info"
          # Copy consolidated duration file for reading (gets overwritten with this group's durations)
          cp .github/workflows/test_durations/python-skinny-tests.test_duration /tmp/skinny_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/skinny_group_${{ matrix.group }}_durations.json
          uv run pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/skinny_group_${{ matrix.group }}_durations.json --store-durations \
            tests/types/test_type_hints.py tests/pyfunc/test_pyfunc_model_with_type_hints.py tests/telemetry || [ $? -eq 5 ]
          # Move to group-specific file for artifact upload
          mv /tmp/skinny_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json
      - name: Run databricks-connect related tests
        run: |
          # this needs to be run in a separate job because installing databricks-connect could break other 
          # tests that uses normal SparkSession instead of remote SparkSession
          uv pip install -c requirements/constraints.txt databricks-agents
          # Note: These tests append to the existing durations file
          cp group_${{ matrix.group }}_durations.json /tmp/skinny_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/skinny_group_${{ matrix.group }}_durations.json
          uv run pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/skinny_group_${{ matrix.group }}_durations.json --store-durations \
            tests/utils/test_requirements_utils.py::test_infer_pip_requirements_on_databricks_agents || [ $? -eq 5 ]
          # Move back to group-specific file
          mv /tmp/skinny_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json

      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: test-durations-python-skinny-tests-group-${{ matrix.group }}
          path: group_${{ matrix.group }}_durations.json
          retention-days: 7

  database:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 90
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - name: Build
        run: |
          ./tests/db/compose.sh pull -q postgresql mysql mssql
          docker images
          ./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)"
      - name: Run tests
        run: |
          set +e
          err=0
          trap 'err=1' ERR
          RESULTS=""
          for service in $(./tests/db/compose.sh config --services | grep '^mlflow-' | sort)
          do
            # Set `--no-TTY` to show container logs on GitHub Actions:
            # https://github.com/actions/virtual-environments/issues/5022
            ./tests/db/compose.sh run --rm --no-TTY $service pytest \
              tests/store/tracking/test_sqlalchemy_store.py \
              tests/store/model_registry/test_sqlalchemy_store.py \
              tests/db
              RESULTS="$RESULTS\n$service: $(if [ $? -eq 0 ]; then echo "✅"; else echo "❌"; fi)"
          done

          echo -e "$RESULTS"
          test $err = 0

      - name: Run migration check
        run: |
          set +e
          err=0
          trap 'err=1' ERR

          ./tests/db/compose.sh down --volumes --remove-orphans
          for service in $(./tests/db/compose.sh config --services | grep '^migration-')
          do
            ./tests/db/compose.sh run --rm --no-TTY $service
          done

          test $err = 0

      - name: Clean up
        run: |
          ./tests/db/compose.sh down --volumes --remove-orphans --rmi all

  java:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - name: Run tests
        run: |
          # Add uv's virtual environment bin to PATH so Java can find mlflow executable
          export PATH="$(pwd)/.venv/bin:$PATH"
          cd mlflow/java
          mvn clean package -q

  flavors:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        include:
          - splits: 10
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh --ml
          # transformers doesn't support Keras 3 yet. tf-keras needs to be installed as a workaround.
          # Required by mlflow.litellm. Temporarily update the version here due to version conflict with dspy.
          uv pip install -c requirements/constraints.txt tf-keras uvicorn "litellm>=1.52.9"
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - uses: ./.github/actions/manage-test-durations
        with:
          job_name: "flavors"
          test_command: "uv run pytest tests/utils/test_model_utils.py tests/tracking/fluent/test_fluent_autolog.py tests/autologging tests/server/auth"

  # It takes 9 ~ 10 minutes to run tests in `tests/models`. To make CI finish faster,
  # run these tests in a separate job.
  models:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8]
        include:
          - splits: 8
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh
          uv pip install -c requirements/constraints.txt pyspark langchain langchain-community '.[mlserver]'
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - uses: ./.github/actions/manage-test-durations
        with:
          job_name: "models"
          test_command: "uv run pytest tests/models || [ $? -eq 5 ]"

  # NOTE: numpy is pinned in this suite due to its heavy reliance on shap, which internally uses
  # references to the now fully deprecated (as of 1.24.x) numpy types (i.e., np.bool).
  # When the shap cross version tests are passing in a new release version of shap, this pin should
  # be removed.
  evaluate:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 90
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5]
        include:
          - splits: 5
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh
          uv pip install -c requirements/constraints.txt pyspark torch transformers langchain langchain-experimental '.[genai]' 'shap<0.47.0' lightgbm xgboost
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - uses: ./.github/actions/manage-test-durations
        with:
          job_name: "evaluate"
          test_command: "uv run pytest tests/evaluate --ignore=tests/evaluate/test_default_evaluator_delta.py || [ $? -eq 5 ]"
      - name: Run tests with delta
        run: |
          uv run pytest tests/evaluate/test_default_evaluator_delta.py

  genai:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4]
        include:
          - splits: 4
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh
          uv pip install -c requirements/constraints.txt openai dspy
      - uses: ./.github/actions/show-versions
      - name: Run GenAI Tests (OSS)
        run: |
          # Copy consolidated duration file for reading (gets overwritten with this group's durations)
          cp .github/workflows/test_durations/genai.test_duration /tmp/genai_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/genai_group_${{ matrix.group }}_durations.json
          uv run pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/genai_group_${{ matrix.group }}_durations.json --store-durations \
            tests/genai
          # Move to group-specific file for artifact upload
          mv /tmp/genai_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json

      - name: Run GenAI Tests (Databricks)
        run: |
          uv pip install -c requirements/constraints.txt databricks-agents
          # Copy existing group durations file to append to it
          cp group_${{ matrix.group }}_durations.json /tmp/genai_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/genai_group_${{ matrix.group }}_durations.json
          uv run pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/genai_group_${{ matrix.group }}_durations.json --store-durations \
            tests/genai --ignore tests/genai/test_genai_import_without_agent_sdk.py \
            --ignore tests/genai/optimize --ignore tests/genai/prompts
          # Move back to group file
          mv /tmp/genai_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json

      - name: Run Tests with Local Spark Session
        run: |
          # databricks-agents installs databricks-connect that blocks us from running spark-related
          # tests with local spark session. To work around this, we run skipped tests after
          # uninstalling databricks-connect.
          uv pip uninstall databricks-connect pyspark
          uv pip install -c requirements/constraints.txt pyspark
          # Copy existing group durations file to append to it
          cp group_${{ matrix.group }}_durations.json /tmp/genai_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/genai_group_${{ matrix.group }}_durations.json
          uv run pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/genai_group_${{ matrix.group }}_durations.json --store-durations \
            tests/genai/evaluate/test_utils.py
          # Move back to group file
          mv /tmp/genai_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json

      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: test-durations-genai-group-${{ matrix.group }}
          path: group_${{ matrix.group }}_durations.json
          retention-days: 7

  optuna:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh
          uv pip install -c requirements/constraints.txt 'pyspark>=3.5' 'optuna>=4' .
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - name: Run tests
        run: |
          uv run pytest tests/optuna tests/pyspark/optuna
  pyfunc:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        include:
          - splits: 10
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Configure virtualenv to use uv Python
        run: |
          # Use uv's Python (which has all modules including _bz2) instead of pyenv's
          export VIRTUALENV_PYTHON=$(which python3)
          echo "VIRTUALENV_PYTHON=$VIRTUALENV_PYTHON" >> $GITHUB_ENV
      - name: Install dependencies
        run: |
          ./dev/install-common-deps-uv.sh
          uv pip install -c requirements/constraints.txt tensorflow 'pyspark[connect]' typing_extensions -U
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - name: Run tests
        run: |
          # Copy consolidated duration file for reading (gets overwritten with this group's durations)
          cp .github/workflows/test_durations/pyfunc.test_duration /tmp/pyfunc_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/pyfunc_group_${{ matrix.group }}_durations.json
          uv run --no-project pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/pyfunc_group_${{ matrix.group }}_durations.json --store-durations --durations=30 \
            tests/pyfunc tests/types --ignore tests/pyfunc/test_spark_connect.py
          # Move to group-specific file for artifact upload
          mv /tmp/pyfunc_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json

          # test_spark_connect.py fails if it's run with other tests, so run it separately.
          uv run --no-project pytest tests/pyfunc/test_spark_connect.py

      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: test-durations-pyfunc-group-${{ matrix.group }}
          path: group_${{ matrix.group }}_durations.json
          retention-days: 7

  pyfunc-pydanticv1:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4]
        include:
          - splits: 4
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/free-disk-space
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Configure virtualenv to use uv Python
        run: |
          # Use uv's Python (which has all modules including _bz2) instead of pyenv's
          export VIRTUALENV_PYTHON=$(which python3)
          echo "VIRTUALENV_PYTHON=$VIRTUALENV_PYTHON" >> $GITHUB_ENV
      - name: Install dependencies
        run: |
          ./dev/install-common-deps-uv.sh
          uv pip install -c requirements/constraints.txt tensorflow 'pyspark[connect]' typing_extensions -U
          # Install pydantic v1 for this job
          uv pip install -c requirements/constraints.txt 'pydantic<2'
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - name: Run tests with pydantic v1
        run: |
          # Copy consolidated duration file for reading (gets overwritten with this group's durations)
          cp .github/workflows/test_durations/pyfunc-pydanticv1.test_duration /tmp/pydantic_group_${{ matrix.group }}_durations.json 2>/dev/null || echo "{}" > /tmp/pydantic_group_${{ matrix.group }}_durations.json
          uv run pytest --splits=${{ matrix.splits }} --group=${{ matrix.group }} \
            --durations-path=/tmp/pydantic_group_${{ matrix.group }}_durations.json --store-durations --durations=30 \
            tests/types tests/pyfunc/test_pyfunc_model_with_type_hints.py tests/utils/test_pydantic_utils.py
          # Move to group-specific file for artifact upload
          mv /tmp/pydantic_group_${{ matrix.group }}_durations.json group_${{ matrix.group }}_durations.json

      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: test-durations-pyfunc-pydanticv1-group-${{ matrix.group }}
          path: group_${{ matrix.group }}_durations.json
          retention-days: 7

  sagemaker:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4]
        include:
          - splits: 4
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps-uv.sh --ml
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - uses: ./.github/actions/manage-test-durations
        with:
          job_name: "sagemaker"
          test_command: "uv run pytest tests/sagemaker"

  windows:
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    runs-on: windows-latest
    timeout-minutes: 120
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        include:
          - splits: 10
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: ./.github/actions/untracked
      - name: Install uv
        uses: astral-sh/setup-uv@557e51de59eb14aaaba2ed9621916900a91d50c6 # v6
      - name: Fix uv PATH (Windows)
        shell: bash
        run: |
          if ! command -v uv &> /dev/null; then
            echo "uv not found in PATH, attempting to locate and add to PATH..."
            # On Windows, uv is typically installed in a different location
            UV_PATH=$(find /c/hostedtoolcache/uv -name "uv.exe" -type f 2>/dev/null | head -1)
            if [ -z "$UV_PATH" ]; then
              # Try alternative location
              UV_PATH=$(find "$RUNNER_TOOL_CACHE/uv" -name "uv.exe" -type f 2>/dev/null | head -1)
            fi
            if [ -n "$UV_PATH" ]; then
              UV_DIR=$(dirname "$UV_PATH")
              echo "$UV_DIR" >> $GITHUB_PATH
              echo "Added $UV_DIR to PATH"
            else
              echo "uv binary not found"
              exit 1
            fi
          else
            echo "uv already available in PATH"
          fi
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: ./.github/actions/setup-java
      - uses: ./.github/actions/cache-pip
      - name: Install python dependencies
        run: |
          python -m venv .venv
          source .venv/Scripts/activate
          # Install mlflow-test-plugin without dependencies (must be separate)
          uv pip install --no-deps tests/resources/mlflow-test-plugin
          # Install all other packages in parallel with uv (with constraints)
          uv pip install -c requirements/constraints.txt -U pip setuptools wheel \
            '.[extras,genai]' \
            pyspark \
            datasets \
            tensorflow \
            torch \
            transformers \
            tf-keras \
            openai \
            -r requirements/test-requirements.txt
      - uses: ./.github/actions/show-versions
      - uses: ./.github/actions/pipdeptree
      - name: Download Hadoop winutils for Spark
        run: |
          git clone https://github.com/cdarlint/winutils /tmp/winutils
      - name: Setup Windows environment
        env:
          # Starting from SQLAlchemy version 2.0, `QueuePool` is the default connection pool
          # when creating an `Engine`. `QueuePool` prevents the removal of temporary database
          # files created during tests on Windows as it keeps the DB connection open until
          # it's explicitly disposed.
          MLFLOW_SQLALCHEMYSTORE_POOLCLASS: "NullPool"
        run: |
          source .venv/Scripts/activate
          # Set Hadoop environment variables required for testing Spark integrations on Windows
          export HADOOP_HOME=/tmp/winutils/hadoop-3.2.2
          export PATH=$PATH:$HADOOP_HOME/bin
      - uses: ./.github/actions/manage-test-durations
        env:
          MLFLOW_SQLALCHEMYSTORE_POOLCLASS: "NullPool"
        with:
          job_name: "windows"
          test_command: "uv run pytest --ignore-flavors --ignore=tests/projects --ignore=tests/examples --ignore=tests/evaluate --ignore=tests/optuna --ignore=tests/pyspark/optuna --ignore=tests/genai tests || true"
