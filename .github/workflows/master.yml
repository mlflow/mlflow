name: MLflow tests

on:
  push:
    branches:
      - master
      - branch-[0-9]+.[0-9]+
  pull_request:
    branches:
      - master
      - branch-[0-9]+.[0-9]+

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

# Use `bash` by default for all `run` steps in this workflow:
# https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#defaultsrun
defaults:
  run:
    # GitHub Actions runner executes commands specified in a run step via:
    # $ bash --noprofile --norc -eo pipefail < commands >
    #
    # https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#using-a-specific-shell
    shell: bash

env:
  CONDA_DIR: /usr/share/miniconda

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Install dependencies
      env:
        INSTALL_LARGE_PYTHON_DEPS: true
        INSTALL_SMALL_PYTHON_DEPS: true
      run: |
        source ./dev/install-common-deps.sh
        pip install -r ./dev/lint-requirements.txt
    - name: Run tests
      run: |
        export PATH="$CONDA_DIR/bin:$PATH"
        source activate test-environment
        ./lint.sh
  r:
    runs-on: ubuntu-latest
    steps:
    - name: Uninstall default-jdk and adoptopenjdk-11-hotspot if present
      run: |
        # deleting other version(s) of JDK because they are not needed and they might interfere with JNI linker configuration in the 'setup-r' step
        sudo apt-get -y remove --purge default-jdk adoptopenjdk-11-hotspot || :
    - uses: actions/checkout@master
    - uses: actions/setup-java@v2
      with:
        # GitHub Actions' Ubuntu 20.04 image uses Java 11 (which is incompatible with Spark 2.4.x) by default:
        # https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-README.md#java
        java-version: 8
        distribution: 'adopt'
    - name: Re-configure dynamic linker run-time bindings for adoptopenjdk-8-hotspot-amd64
      run: |
        sudo mkdir -p /etc/ld.so.conf.d
        sudo bash -c "cat > /etc/ld.so.conf.d/jre.conf <<< '/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/lib/amd64/server'"
        sudo ldconfig -v
    - uses: r-lib/actions/setup-r@v1
    # This step dumps the current set of R dependencies and R version into files to be used
    # as a cache key when caching/restoring R dependencies.
    - name: Query dependencies
      run: |
        print(R.version)
        install.packages('remotes')
        saveRDS(remotes::dev_package_deps("mlflow/R/mlflow", dependencies = TRUE), ".github/depends.Rds", version = 2)
        writeLines(sprintf("R-%i.%i", getRversion()$major, getRversion()$minor), ".github/R-version")
      shell: Rscript {0}

    - name: Get OS name
      id: os-name
      run: |
        # `os_name` will be like "Ubuntu-20.04.1-LTS"
        os_name=$(lsb_release -ds | sed 's/\s/-/g')
        echo "::set-output name=os-name::$os_name"
    - name: Cache R packages
      if: runner.os != 'Windows'
      uses: actions/cache@v2
      with:
        path: ${{ env.R_LIBS_USER }}
        # We cache R dependencies based on a tuple of the current OS, the R version, and the list of
        # R dependencies
        key: ${{ steps.os-name.outputs.os-name }}-${{ hashFiles('.github/R-version') }}-1-${{ hashFiles('.github/depends.Rds') }}
        restore-keys: ${{ steps.os-name.outputs.os-name }}-${{ hashFiles('.github/R-version') }}-1-
    # Cache spark archive downloaded in `mlflow/R/mlflow/.create-test-env.R`
    - name: Cache spark archive
      uses: actions/cache@v2
      with:
        path: ~/spark/spark-2.4.5-bin-hadoop2.7
        key: ${{ hashFiles('mlflow/R/mlflow/.spark-version') }}
    - name: Install system dependencies
      run: |
        sudo apt-get install -y libcurl4-openssl-dev
        sudo R CMD javareconf
    - name: Install dependencies
      run: |
        install.packages("devtools")
        remotes::install_deps('mlflow/R/mlflow', dependencies = TRUE, upgrade = FALSE)
      shell: Rscript {0}
    - name: Create test environment
      run: |
        source ./dev/install-common-deps.sh
        cd mlflow/R/mlflow
        R CMD build .
        cd tests
        Rscript ../.create-test-env.R
    - name: Run tests
      env:
        # Hack to get around this issue:
        # https://stat.ethz.ch/pipermail/r-package-devel/2020q3/005930.html
        #
        # The system clock check during `R CMD check` relies on two external web APIs and fails
        # when they are unavailable. By setting `_R_CHECK_SYSTEM_CLOCK_` to FALSE, we can skip it:
        # https://github.com/wch/r-source/blob/59a1965239143ca6242b9cc948d8834e1194e84a/src/library/tools/R/check.R#L511
        _R_CHECK_SYSTEM_CLOCK_: FALSE
      run: |
        export LINTR_COMMENT_BOT=false
        cd mlflow/R/mlflow/tests
        Rscript ../.run-tests.R
    - name: Calculate code coverage
      if: ${{ success() }}
      run: |
        export MLFLOW_HOME=$(pwd)
        cd mlflow/R/mlflow/tests
        Rscript -e 'covr::codecov()' || :
      env:
        COVR_RUNNING: true
    - name: Show 00check.log on failure
      if: ${{ failure() }}
      run: |
        LOG_FILE="${HOME}/build/mlflow/mlflow/mlflow/R/mlflow/mlflow.Rcheck/00check.log"
        [ -r "${LOG_FILE}" ] && cat "${LOG_FILE}"
        cp "${LOG_FILE}" /tmp
    - uses: actions/upload-artifact@v1
      if: failure()
      with:
        name: 00check.log
        path: /tmp/00check.log

  # python-skinny tests cover a subset of mlflow functionality
  # that is meant to be supported with a smaller dependency footprint.
  # The python skinny tests cover the subset of mlflow functionality
  # while also verifying certain dependencies are omitted.
  python-skinny:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Install dependencies
      env:
        INSTALL_SKINNY_PYTHON_DEPS: true
        MLFLOW_SKINNY: true
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      run: |
        export PATH="$CONDA_DIR/bin:$PATH"
        source activate test-environment
        ./dev/run-python-skinny-tests.sh

  python-small:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Install dependencies
      env:
        INSTALL_SMALL_PYTHON_DEPS: true
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      run: |
        export PATH="$CONDA_DIR/bin:$PATH"
        source activate test-environment
        ./dev/run-small-python-tests.sh

  python-large:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Increase available disk space
      run: |
        # Increase available disk space by removing unnecessary tool chains:
        # https://github.com/actions/virtual-environments/issues/709#issuecomment-612569242
        rm -rf "$AGENT_TOOLSDIRECTORY"
    - uses: actions/setup-java@v2
      with:
        # GitHub Actions' Ubuntu 20.04 image uses Java 11 (which is incompatible with Spark 2.4.x) by default:
        # https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-README.md#java
        java-version: 8
        distribution: 'adopt'
    - name: Install dependencies
      env:
        INSTALL_LARGE_PYTHON_DEPS: true
        INSTALL_SMALL_PYTHON_DEPS: true
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      env:
        # Fix for https://github.com/mlflow/mlflow/issues/4229
        SPARK_LOCAL_IP: 127.0.0.1
      run: |
        export PATH="$CONDA_DIR/bin:$PATH"
        source activate test-environment
        ./dev/run-large-python-tests.sh
    - name: Run database initialization tests
      run: |
        set -x
        # Build wheel and copy it under tests/db
        python setup.py bdist_wheel
        cp -r dist tests/db

        # Run tests
        cd tests/db
        docker-compose build
        docker-compose run mlflow-postgres python log.py
        docker-compose run mlflow-mysql python log.py

        # Clean up
        docker-compose down --rmi all --volumes
    - name: Run anaconda compatibility tests
      run: |
        ./dev/test-anaconda-compatibility.sh "anaconda3:2020.11"
        ./dev/test-anaconda-compatibility.sh "anaconda3:2019.03"

  java:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Java
      uses: actions/setup-java@v2
      with:
        java-version: 8
        distribution: 'adopt'
    - name: Install dependencies
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      env:
        SPARK_LOCAL_IP: 127.0.0.1
      run: |
        export PATH="$CONDA_DIR/bin:$PATH"
        source activate test-environment
        cd mlflow/java
        mvn clean package -q

  js:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: mlflow/server/js
    steps:
    - uses: actions/checkout@v2
    - name: Set up Node
      uses: actions/setup-node@v1
      with:
        node-version: 10.x
    - name: Install dependencies
      run: |
        npm i
    - name: Run lint
      run: |
        npm run lint
    - name: Run tests
      run: |
        npm run test

  protos:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Test building Docker image
      working-directory: dev
      env:
        DOCKER_BUILDKIT: 1
      run: |
        docker build -t gen-protos -f Dockerfile.protos .
        docker run --rm gen-protos protoc --version
    - name: Install dependencies
      run: |
        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.0/protoc-3.6.0-linux-x86_64.zip -O $HOME/protoc.zip
        sudo unzip $HOME/protoc.zip -d /usr
    - name: Run tests
      run: |
        ./test-generate-protos.sh

  flavors:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Increase available disk space
        run: |
          # Increase available disk space by removing unnecessary tool chains:
          # https://github.com/actions/virtual-environments/issues/709#issuecomment-612569242
          rm -rf "$AGENT_TOOLSDIRECTORY"
      - uses: actions/setup-java@v2
        with:
          java-version: 8
          distribution: 'adopt'
      - name: Install dependencies
        env:
          INSTALL_LARGE_PYTHON_DEPS: true
          INSTALL_SMALL_PYTHON_DEPS: true
        run: |
          source ./dev/install-common-deps.sh
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          export PATH="$CONDA_DIR/bin:$PATH"
          source activate test-environment
          # Install libopenblas-dev for mxnet 1.8.0.post0
          sudo apt-get install libopenblas-dev
          ./dev/run-python-flavor-tests.sh;

  # It takes 9 ~ 10 minutes to run tests in `tests/models`. To make CI finish faster,
  # run these tests in a separate job.
  models:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-java@v2
        with:
          java-version: 8
          distribution: 'adopt'
      - name: Install dependencies
        env:
          INSTALL_SMALL_PYTHON_DEPS: true
          INSTALL_LARGE_PYTHON_DEPS: false
        run: |
          source ./dev/install-common-deps.sh
          pip install pyspark
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          export PATH="$CONDA_DIR/bin:$PATH"
          source activate test-environment
          export MLFLOW_HOME=$(pwd)
          pytest tests/models --large

  import:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: '3.6'
      - name: Install mlflow
        run: |
          pip install -e .
      - name: Verify mlflow can be imported without errors
        run: |
          python -c "import mlflow"

          # Just importing mlflow should not create `mlruns` directory
          # See: https://github.com/mlflow/mlflow/issues/3400
          if [ -d "./mlruns" ]; then
            exit 1
          fi

  sagemaker:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-java@v2
        with:
          java-version: 8
          distribution: 'adopt'
      - name: Install dependencies
        env:
          INSTALL_LARGE_PYTHON_DEPS: true
          INSTALL_SMALL_PYTHON_DEPS: true
        run: |
          source ./dev/install-common-deps.sh
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          export PATH="$CONDA_DIR/bin:$PATH"
          source activate test-environment
          ./dev/run-python-sagemaker-tests.sh;

  windows:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.7
      - name: Install dependecies
        run: |
          pip install -r dev-requirements.txt
          pip install -r dev/small-requirements.txt
          pip install --no-dependencies tests/resources/mlflow-test-plugin
          pip install -e .[extras]
      - name: Run tests
        run: |
          pytest --ignore-flavors --ignore=tests/projects tests
