name: MLflow tests

on:
  push:
    branches:
      - master
      - branch-[0-9]+.[0-9]+
  pull_request:
    branches:
      - master
      - branch-[0-9]+.[0-9]+
  workflow_dispatch:
    inputs:
      repository:
        description: >
          [Optional] Repository name with owner. For example, mlflow/mlflow.
           Defaults to the repository that triggered a workflow.
        required: false
        default: ""
      ref:
        description: >
          [Optional] The branch, tag or SHA to checkout. When checking out the repository that
           triggered a workflow, this defaults to the reference or SHA for that event. Otherwise,
           uses the default branch.
        required: false
        default: ""

concurrency:
  group: ${{ github.workflow }}-${{ github.ref == 'refs/heads/master' && github.run_number || github.ref }}
  cancel-in-progress: true

# Use `bash --noprofile --norc -exo pipefail` by default for all `run` steps in this workflow:
# https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#defaultsrun
defaults:
  run:
    shell: bash --noprofile --norc -exo pipefail {0}

env:
  # Note miniconda is pre-installed in the virtual environments for GitHub Actions:
  # https://github.com/actions/virtual-environments/blob/main/images/linux/scripts/installers/miniconda.sh
  MLFLOW_CONDA_HOME: /usr/share/miniconda

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        repository: ${{ github.event.inputs.repository }}
        ref: ${{ github.event.inputs.ref }}
    - uses: ./.github/actions/setup-python
    - uses: ./.github/actions/cache-pip
    - name: Add problem matchers
      run: |
        echo "::add-matcher::.github/workflows/matchers/pylint.json"
        echo "::add-matcher::.github/workflows/matchers/black.json"
    - name: Install dependencies
      env:
        INSTALL_ML_DEPENDENCIES: true
      run: |
        source ./dev/install-common-deps.sh
        pip install -r requirements/lint-requirements.txt
    - name: Test custom pylint-plugins
      run : |
        pytest tests/pylint_plugins
    - name: Run lint checks
      run: |
        ./dev/lint.sh

  # python-skinny tests cover a subset of mlflow functionality
  # that is meant to be supported with a smaller dependency footprint.
  # The python skinny tests cover the subset of mlflow functionality
  # while also verifying certain dependencies are omitted.
  python-skinny:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        submodules: recursive
        repository: ${{ github.event.inputs.repository }}
        ref: ${{ github.event.inputs.ref }}
    - uses: ./.github/actions/setup-python
    - name: Install dependencies
      env:
        INSTALL_SKINNY_PYTHON_DEPS: true
        MLFLOW_SKINNY: true
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      run: |
        ./dev/run-python-skinny-tests.sh

  python:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        repository: ${{ github.event.inputs.repository }}
        ref: ${{ github.event.inputs.ref }}
        submodules: recursive
    - name: Increase available disk space
      run: |
        # Increase available disk space by removing unnecessary tool chains:
        # https://github.com/actions/virtual-environments/issues/709#issuecomment-612569242
        rm -rf "$AGENT_TOOLSDIRECTORY"
    - uses: ./.github/actions/setup-python
    - uses: ./.github/actions/setup-pyenv
    - uses: actions/setup-java@v3
      with:
        java-version: 11
        distribution: 'adopt'
    - uses: ./.github/actions/cache-pip
    - name: Install dependencies
      env:
        INSTALL_ML_DEPENDENCIES: true
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      env:
        # Fix for https://github.com/mlflow/mlflow/issues/4229
        SPARK_LOCAL_IP: 127.0.0.1
      run: |
        # Establish SSH to localhost for test_sftp_artifact_repo.py
        ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa

        cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
        ssh-keyscan -H localhost >> ~/.ssh/known_hosts
        ssh $(whoami)@localhost exit
        export LOGNAME=$(whoami)

        ./dev/run-python-tests.sh

  database:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        repository: ${{ github.event.inputs.repository }}
        ref: ${{ github.event.inputs.ref }}
        submodules: recursive
    - name: Build
      run: |
        ./tests/db/compose.sh pull -q postgresql mysql mssql
        ./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python setup.py -q dependencies)"
    - name: Run tests
      run: |
        for service in $(./tests/db/compose.sh config --services | grep '^mlflow-')
        do
          # Set `--no-TTY` to show container logs on GitHub Actions:
          # https://github.com/actions/virtual-environments/issues/5022
          ./tests/db/compose.sh run --rm --no-TTY $service pytest tests/store/tracking/test_sqlalchemy_store.py tests/db
        done
    - name: Clean up
      run: |
        ./tests/db/compose.sh down --volumes --remove-orphans --rmi all

  java:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        submodules: recursive
        repository: ${{ github.event.inputs.repository }}
        ref: ${{ github.event.inputs.ref }}
    - uses: ./.github/actions/setup-python
    - name: Set up Java
      uses: actions/setup-java@v3
      with:
        java-version: 11
        distribution: 'adopt'
    - name: Install dependencies
      run: |
        source ./dev/install-common-deps.sh
    - name: Run tests
      env:
        SPARK_LOCAL_IP: 127.0.0.1
      run: |
        cd mlflow/java
        mvn clean package -q

  protos:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        repository: ${{ github.event.inputs.repository }}
        ref: ${{ github.event.inputs.ref }}
        submodules: recursive
    - name: Test building Docker image
      working-directory: dev
      env:
        DOCKER_BUILDKIT: 1
      run: |
        docker build -t gen-protos -f Dockerfile.protos .
        docker run --rm gen-protos protoc --version
    - name: Install dependencies
      run: |
        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-linux-x86_64.zip -O $HOME/protoc.zip
        sudo unzip $HOME/protoc.zip -d /tmp/protoc
        sudo chmod -R 777 /tmp/protoc
        echo "/tmp/protoc/bin" >> $GITHUB_PATH
    - name: Run tests
      run: |
        ./dev/test-generate-protos.sh

  flavors:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          repository: ${{ github.event.inputs.repository }}
          ref: ${{ github.event.inputs.ref }}
          submodules: recursive
      - name: Increase available disk space
        run: |
          # Increase available disk space by removing unnecessary tool chains:
          # https://github.com/actions/virtual-environments/issues/709#issuecomment-612569242
          rm -rf "$AGENT_TOOLSDIRECTORY"
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'adopt'
      - uses: ./.github/actions/cache-pip
      - name: Install dependencies
        env:
          INSTALL_ML_DEPENDENCIES: true
        run: |
          source ./dev/install-common-deps.sh
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          ./dev/run-python-flavor-tests.sh;

  # It takes 9 ~ 10 minutes to run tests in `tests/models`. To make CI finish faster,
  # run these tests in a separate job.
  models:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
          repository: ${{ github.event.inputs.repository }}
          ref: ${{ github.event.inputs.ref }}
      - uses: ./.github/actions/setup-python
      - uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'adopt'
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps.sh
          pip install pyspark
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          export MLFLOW_HOME=$(pwd)
          pytest tests/models

  pipelines:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - uses: ./.github/actions/setup-python
      - name: Install dependencies
        env:
          INSTALL_SMALL_PYTHON_DEPS: true
          INSTALL_LARGE_PYTHON_DEPS: false
        run: |
          source ./dev/install-common-deps.sh
          pip install -e .[pipelines]
          # TODO: Unpin once Delta supports Spark 3.3 (https://github.com/delta-io/delta/issues/1217)
          pip install 'pyspark<3.3'
      - name: Run tests
        run: |
          export MLFLOW_HOME=$(pwd)
          pytest tests/pipelines

  pipelines-windows:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - uses: ./.github/actions/setup-python
      - name: Install python dependencies
        run: |
          pip install -r requirements/test-requirements.txt
          pip install --no-dependencies tests/resources/mlflow-test-plugin
          pip install -e .[extras]
          pip install -e .[pipelines]
          # TODO: Unpin once Delta supports Spark 3.3 (https://github.com/delta-io/delta/issues/1217)
          pip install 'pyspark<3.3'
      - name: Download Hadoop winutils for Spark
        run: |
          git clone https://github.com/cdarlint/winutils
      - name: Run tests
        run: |
          # Set Hadoop environment variables required for testing Spark integrations on Windows
          export HADOOP_HOME=`realpath winutils/hadoop-3.2.2`
          export PATH=$PATH:$HADOOP_HOME/bin
          # Run pipelines tests
          export MLFLOW_HOME=$(pwd)
          pytest tests/pipelines

  pyfunc:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          repository: ${{ github.event.inputs.repository }}
          ref: ${{ github.event.inputs.ref }}
          submodules: recursive
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'adopt'
      - name: Install dependencies
        run: |
          source ./dev/install-common-deps.sh
          pip install keras tensorflow pyspark
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          export MLFLOW_HOME=$(pwd)
          pytest --durations=30 tests/pyfunc

  sagemaker:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
          repository: ${{ github.event.inputs.repository }}
          ref: ${{ github.event.inputs.ref }}
      - uses: ./.github/actions/setup-python
      - uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'adopt'
      - name: Install dependencies
        env:
          INSTALL_ML_DEPENDENCIES: true
        run: |
          source ./dev/install-common-deps.sh
      - name: Run tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          ./dev/run-python-sagemaker-tests.sh;

  windows:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
          repository: ${{ github.event.inputs.repository }}
          ref: ${{ github.event.inputs.ref }}
      - uses: ./.github/actions/setup-python
      - uses: ./.github/actions/setup-pyenv
      - uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'adopt'
      - name: Install python dependencies
        run: |
          pip install -r requirements/test-requirements.txt
          pip install --no-dependencies tests/resources/mlflow-test-plugin
          pip install -e .[extras]
          pip install pyspark
      - name: Run python tests
        run: |
          pytest --ignore-flavors --ignore=tests/projects --ignore=tests/examples tests --ignore=tests/pipelines
