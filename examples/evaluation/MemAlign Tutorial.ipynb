{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8b3ba5-03cc-40d8-bcb6-a1beefaab8f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup & Install Dependencies"
    }
   },
   "outputs": [],
   "source": "%pip install --upgrade mlflow>=3.9.0 openai datasets huggingface_hub fsspec litellm dspy"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb576ff4-0c87-45a1-b9a1-6ebb2c981c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MemAlign Tutorial: Aligning LLM Judges with Human Feedback\n",
    "\n",
    "This tutorial demonstrates how to improve LLM judge accuracy using MLflow's **MemAlign** optimizer.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to evaluate LLM outputs using **judges** (scorers)\n",
    "2. How to provide **human feedback** on judge assessments\n",
    "3. How to use **MemAlign** to align judges with human preferences\n",
    "4. How to compare judge performance before and after alignment\n",
    "\n",
    "## Key Concepts\n",
    "- **Judge/Scorer**: An LLM that evaluates another LLM's outputs (e.g., \"Is this classification correct?\")\n",
    "- **Assessment**: The judge's evaluation result (e.g., \"Yes\" or \"No\")\n",
    "- **Trace**: A recorded LLM interaction including inputs, outputs, and assessments\n",
    "- **Alignment**: Improving a judge's instructions based on human feedback so it better matches human judgment\n",
    "\n",
    "## Data Source\n",
    "We'll use the [PubMed text classification dataset](https://huggingface.co/datasets/ml4pubmed/pubmed-text-classification-cased) where sentences from medical abstracts are classified into categories: BACKGROUND, OBJECTIVE, METHODS, RESULTS, or CONCLUSIONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61c8e83-ca9b-4cf5-a310-895d07f448ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "mlflow.set_tracking_uri(\"your MLflow server\")\n",
    "mlflow.set_experiment(<\"refer to your experiment\">)\n",
    "mlflow.openai.autolog()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 0: Set your Variables!"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your OpenAI API key if you're using OpenAI\"\n",
    "\n",
    "client = \"your LLM client and provider call\"\n",
    "\n",
    "# OpenAI Client Example\n",
    "LLM_NAME = \"gpt-5-2\"\n",
    "client = OpenAI(model=LLM_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c784da6-4f84-4125-b177-3acb374058ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "g# Step 1: Load Evaluation Dataset\n",
    "\n",
    "First, we load a sample of sentences from the PubMed dataset. Each sentence has a ground truth label that we'll use to measure judge accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663188c6-be19-428b-94bc-6f18a92e23d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "ds = load_dataset(\"ml4pubmed/pubmed-text-classification-cased\", cache_dir='/tmp/huggingface')\n",
    "split = ds[\"train\"] if \"train\" in ds else ds[list(ds.keys())[0]]\n",
    "\n",
    "idxs = random.sample(range(len(split)), 50)\n",
    "\n",
    "samples_list = [\n",
    "    {\"id\": str(i), \"text\": split[idx][\"description_cln\"], \"answer\": split[idx][\"target\"]}\n",
    "    for i, idx in enumerate(idxs)\n",
    "]\n",
    "\n",
    "ground_truth = {s[\"id\"]: {\"text\": s[\"text\"], \"answer\": s[\"answer\"]} for s in samples_list}\n",
    "\n",
    "print(f\"Loaded {len(samples_list)} evaluation samples\")\n",
    "print(f\"Labels: {set(s['answer'] for s in samples_list)}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  ID: {samples_list[0]['id']}\")\n",
    "print(f\"  Text: {samples_list[0]['text'][:100]}...\")\n",
    "print(f\"  Label: {samples_list[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fffb8ab-d300-4d32-bef0-1ca97df1bf06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "# Step 2: Format Dataset for Evaluation\n\nMLflow's `evaluate()` function expects data in a specific format. We format each sample with:\n- `inputs`: The conversation history (user message with the sentence to classify) and the sample ID for tracking\n- `expectations`: Ground truth information used by the custom judge to evaluate correctness"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8421df35-68f3-4cd4-b27e-8cce30d3d9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset_records = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [{\"role\": \"user\", \"content\": s[\"text\"]}],\n",
    "            \"sample_id\": s[\"id\"],\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_facts\": [\n",
    "                f\"The correct classification for this sentence is: {s['answer']}\",\n",
    "                \"Classification label must be one of: CONCLUSIONS, RESULTS, METHODS, OBJECTIVE, or BACKGROUND\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    for s in samples_list\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(eval_dataset_records)} evaluation records\")\n",
    "print(f\"\\nSample record structure:\")\n",
    "print(f\"  inputs: {{input: [{{role: 'user', content: '<sentence>'}}], sample_id: '<id>'}}\")\n",
    "print(f\"  expectations: {{expected_facts: ['The correct classification...', '...']}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdec474e-a682-4c7f-9156-477e2271008f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: Define the Prediction Function\n",
    "\n",
    "This is the LLM we're evaluating. It takes a sentence and classifies it into one of the 5 PubMed categories.\n",
    "The judges will evaluate whether this LLM's classifications are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1e5d4c-30b4-4df1-85e3-501406ad529f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@mlflow.trace\n",
    "def predict_fn(input: list[dict], sample_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Classify a sentence into one of 5 PubMed categories.\n",
    "    \"\"\"\n",
    "\n",
    "    mlflow.update_current_trace(metadata={\"sample_id\": sample_id})\n",
    "\n",
    "    user_text = input[0][\"content\"]\n",
    "\n",
    "    prompt = (\n",
    "        \"Classify the sentence into exactly one label from: \"\n",
    "        \"METHODS, RESULTS, CONCLUSIONS, BACKGROUND, OBJECTIVE.\\n\\n\"\n",
    "        f\"Sentence: {user_text}\\n\\n\"\n",
    "        \"Respond with json containing a single key 'label' with your classification.\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=LLM_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44473b0e-89cb-4a23-af55-c389d0e6dbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Define Reusable Helper Functions\n",
    "\n",
    "These helper functions will be used throughout the tutorial to:\n",
    "- Analyze judge accuracy by comparing assessments to ground truth\n",
    "- Log human feedback for judge alignment\n",
    "- Print summaries and comparisons\n",
    "- Automatically add human feedback with another LLM (but manual option is provided)\n",
    "\n",
    "**Key insight**: A judge is \"correct\" when it says \"Yes\" for correct LLM answers and \"No\" for incorrect ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c725a4e2-5621-4663-9f40-0d30b448c246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def normalize_label(x):\n",
    "    if not x:\n",
    "        return None\n",
    "    return str(x).strip().upper()\n",
    "\n",
    "\n",
    "def extract_classification_label(response) -> str | None:\n",
    "    try:\n",
    "        if isinstance(response, str):\n",
    "            content = response\n",
    "        elif isinstance(response, dict):\n",
    "            if \"choices\" in response:\n",
    "                content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            else:\n",
    "                content = response\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if isinstance(content, str):\n",
    "            parsed = json.loads(content)\n",
    "        else:\n",
    "            parsed = content\n",
    "\n",
    "        label = parsed.get(\"label\") or parsed.get(\"classification\") or parsed.get(\"answer\")\n",
    "        if label:\n",
    "            return normalize_label(label)\n",
    "\n",
    "        for value in parsed.values():\n",
    "            if isinstance(value, str) and value.upper() in {\"METHODS\", \"RESULTS\", \"CONCLUSIONS\", \"BACKGROUND\", \"OBJECTIVE\"}:\n",
    "                return normalize_label(value)\n",
    "        return None\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def judge_approved(feedback_value) -> bool:\n",
    "    if feedback_value is None:\n",
    "        return False\n",
    "    if isinstance(feedback_value, bool):\n",
    "        return feedback_value\n",
    "    val = str(feedback_value).strip().lower()\n",
    "    return val in {\"yes\", \"true\", \"pass\", \"correct\", \"1\"}\n",
    "\n",
    "\n",
    "def analyze_judge_accuracy(assessment_name: str, ground_truth: dict) -> dict:\n",
    "    traces = mlflow.search_traces(max_results=100, return_type=\"list\")\n",
    "\n",
    "    matches, mismatches = [], []\n",
    "\n",
    "    for trace in traces:\n",
    "        # Get sample_id from trace metadata\n",
    "        trace_metadata = getattr(trace.info, \"trace_metadata\", None) or {}\n",
    "        sample_id = trace_metadata.get(\"sample_id\")\n",
    "        if not sample_id:\n",
    "            continue\n",
    "\n",
    "        # Look up ground truth by ID\n",
    "        sample_data = ground_truth.get(sample_id)\n",
    "        if not sample_data:\n",
    "            continue\n",
    "\n",
    "        sentence = sample_data[\"text\"]\n",
    "        true_label = normalize_label(sample_data[\"answer\"])\n",
    "\n",
    "        # Parse trace response to get LLM's classification\n",
    "        # Note: trace.data.response might be a JSON string or already parsed\n",
    "        response = trace.data.response\n",
    "        if isinstance(response, str):\n",
    "            try:\n",
    "                response = json.loads(response)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        llm_label = extract_classification_label(response)\n",
    "        if not llm_label:\n",
    "            continue\n",
    "\n",
    "        # Get judge's assessment for this trace\n",
    "        assessments = trace.search_assessments(name=assessment_name)\n",
    "        judge_assessments = [a for a in assessments if a.source.source_type == \"LLM_JUDGE\"]\n",
    "        if not judge_assessments:\n",
    "            continue\n",
    "\n",
    "        assessment = judge_assessments[0]\n",
    "        judge_feedback_value = assessment.feedback.value\n",
    "        judge_rationale = assessment.rationale\n",
    "\n",
    "        # Determine if LLM was actually correct (compared to ground truth)\n",
    "        llm_was_correct = llm_label == true_label\n",
    "\n",
    "        # Determine if judge said the LLM was correct\n",
    "        judge_said_correct = judge_approved(judge_feedback_value)\n",
    "\n",
    "        # Judge is right if its assessment matches reality\n",
    "        judge_was_right = llm_was_correct == judge_said_correct\n",
    "\n",
    "        record = {\n",
    "            \"trace_id\": trace.info.trace_id,\n",
    "            \"sample_id\": sample_id,\n",
    "            \"sentence\": sentence,\n",
    "            \"llm_label\": llm_label,\n",
    "            \"true_label\": true_label,\n",
    "            \"llm_was_correct\": llm_was_correct,\n",
    "            \"judge_feedback\": judge_feedback_value,\n",
    "            \"judge_said_correct\": judge_said_correct,\n",
    "            \"judge_rationale\": judge_rationale,\n",
    "            \"judge_was_right\": judge_was_right,\n",
    "        }\n",
    "\n",
    "        if judge_was_right:\n",
    "            matches.append(record)\n",
    "        else:\n",
    "            mismatches.append(record)\n",
    "\n",
    "    total = len(matches) + len(mismatches)\n",
    "    accuracy = len(matches) / total if total > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"total\": total,\n",
    "        \"correct\": len(matches),\n",
    "        \"incorrect\": len(mismatches),\n",
    "        \"matches\": matches,\n",
    "        \"mismatches\": mismatches,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_accuracy_summary(name: str, results: dict):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Judge: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.1%} ({results['correct']}/{results['total']})\")\n",
    "    print(f\"  - Correct assessments: {results['correct']}\")\n",
    "    print(f\"  - Incorrect assessments: {results['incorrect']}\")\n",
    "\n",
    "    if results['mismatches']:\n",
    "        print(f\"\\nExamples where judge was WRONG:\")\n",
    "        for m in results['mismatches'][:3]:\n",
    "            llm_status = \"correct\" if m['llm_was_correct'] else \"wrong\"\n",
    "            print(f\"  - LLM: '{m['llm_label']}' vs Truth: '{m['true_label']}' (LLM was {llm_status})\")\n",
    "            print(f\"    Judge said: '{m['judge_feedback']}' (approved: {m['judge_said_correct']})\")\n",
    "\n",
    "\n",
    "def log_feedback_interactive(records: list, judge_name: str, reviewer: AssessmentSource):\n",
    "    for i, record in enumerate(records, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Record {i}/{len(records)}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Sample ID: {record['sample_id']}\")\n",
    "        print(f\"Sentence: {record['sentence']}\")\n",
    "        print(f\"LLM Label: {record['llm_label']}\")\n",
    "        print(f\"True Label: {record['true_label']}\")\n",
    "        print(f\"LLM was: {'CORRECT' if record['llm_was_correct'] else 'WRONG'}\")\n",
    "        print(f\"Judge feedback: '{record['judge_feedback']}'\")\n",
    "        print(f\"Judge was: {'CORRECT' if record['judge_was_right'] else 'WRONG'}\")\n",
    "\n",
    "        suggested_value = \"Yes\" if record[\"llm_was_correct\"] else \"No\"\n",
    "        print(f\"\\nSuggested feedback: '{suggested_value}'\")\n",
    "\n",
    "        user_value = input(f\"Your feedback (Yes/No, or press Enter for '{suggested_value}'): \").strip()\n",
    "        if not user_value:\n",
    "            user_value = suggested_value\n",
    "        elif user_value.lower() in [\"yes\", \"y\"]:\n",
    "            user_value = \"Yes\"\n",
    "        elif user_value.lower() in [\"no\", \"n\"]:\n",
    "            user_value = \"No\"\n",
    "\n",
    "        comment = input(\"Comment (optional, press Enter to skip): \").strip()\n",
    "\n",
    "        mlflow.log_feedback(\n",
    "            trace_id=record[\"trace_id\"],\n",
    "            name=judge_name,\n",
    "            value=user_value,\n",
    "            rationale=comment if comment else f\"Human feedback: {user_value}\",\n",
    "            source=reviewer,\n",
    "        )\n",
    "        print(f\"Logged: {user_value}\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Logged feedback for {len(records)} traces\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "def log_feedback_automatic(records: list, judge_name: str, reviewer: AssessmentSource):\n",
    "    print(f\"Automatically generating feedback for {len(records)} traces...\")\n",
    "\n",
    "    for i, record in enumerate(records, 1):\n",
    "        # Determine correct feedback based on ground truth\n",
    "        feedback_value = \"Yes\" if record[\"llm_was_correct\"] else \"No\"\n",
    "\n",
    "        # Generate rationale using LLM\n",
    "        rationale_prompt = f\"\"\"You are reviewing an LLM classification task. Generate a brief rationale (1-2 sentences) explaining why the feedback should be \"{feedback_value}\".\n",
    "\n",
    "Sentence to classify: {record['sentence'][:200]}...\n",
    "LLM's classification: {record['llm_label']}\n",
    "Ground truth label: {record['true_label']}\n",
    "LLM was {'CORRECT' if record['llm_was_correct'] else 'INCORRECT'}\n",
    "\n",
    "Provide a concise rationale for why the feedback is \"{feedback_value}\".\"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": rationale_prompt}],\n",
    "                max_tokens=100,\n",
    "            )\n",
    "            rationale = completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            # Fallback rationale if LLM call fails\n",
    "            if record[\"llm_was_correct\"]:\n",
    "                rationale = f\"Correct: LLM classified as '{record['llm_label']}' which matches ground truth '{record['true_label']}'.\"\n",
    "            else:\n",
    "                rationale = f\"Incorrect: LLM classified as '{record['llm_label']}' but ground truth is '{record['true_label']}'.\"\n",
    "\n",
    "        # Log the feedback\n",
    "        mlflow.log_feedback(\n",
    "            trace_id=record[\"trace_id\"],\n",
    "            name=judge_name,\n",
    "            value=feedback_value,\n",
    "            rationale=rationale,\n",
    "            source=reviewer,\n",
    "        )\n",
    "\n",
    "        if i % 10 == 0 or i == len(records):\n",
    "            print(f\"  Processed {i}/{len(records)} traces\")\n",
    "\n",
    "    print(f\"\\nAutomatically logged feedback for {len(records)} traces\")\n",
    "    print(f\"  - Correct (Yes): {sum(1 for r in records if r['llm_was_correct'])}\")\n",
    "    print(f\"  - Incorrect (No): {sum(1 for r in records if not r['llm_was_correct'])}\")\n",
    "\n",
    "judge_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff6dc120-46ac-4439-b8ec-8eeab1c02e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "# Step 5: Evaluate with Built-in Guidelines Judge\n\nMLflow provides a built-in `Guidelines` judge that evaluates whether responses follow specific constraints or instructions.\nLet's see how well it performs on our classification task when we give it guidelines about valid classification labels.\nWe will use this as a comparison with a general-purpose judge vs a custom judge."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd5a668-c4c1-4e55-b264-f159c9bbdbaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines\n",
    "from mlflow.genai import evaluate\n",
    "\n",
    "classification_guidelines = Guidelines(\n",
    "    name=\"classification_guidelines\",\n",
    "    guidelines=[\n",
    "        \"The response must be exactly one of: CONCLUSIONS, RESULTS, METHODS, OBJECTIVE, or BACKGROUND\",\n",
    "        \"The classification must be appropriate for the medical/scientific sentence provided\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Running evaluation with Guidelines judge...\")\n",
    "print(\"This will classify each sentence and have the judge evaluate the results.\\n\")\n",
    "\n",
    "guidelines_eval_results = evaluate(\n",
    "    data=eval_dataset_records,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[classification_guidelines],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2a07ef-4ee9-409d-9da2-cc80eabb1677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Analyze Guidelines Judge Results\n\nNow let's see how accurate the Guidelines judge was. Remember:\n- The judge is \"correct\" if it says \"Yes\" when the LLM was right, or \"No\" when wrong\n- The judge is \"wrong\" if it says \"Yes\" when the LLM was actually wrong, or vice versa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ebb2ea-49a2-41de-b696-70cbc79d10ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "guidelines_results = analyze_judge_accuracy(\"classification_guidelines\", ground_truth)\n",
    "print_accuracy_summary(\"Guidelines (built-in)\", guidelines_results)\n",
    "judge_results[\"Guidelines (built-in)\"] = guidelines_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32754171-28e0-406a-b800-6f080425ad9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 6: Create a Custom Judge\n",
    "\n",
    "Now let's create our own judge specifically designed for this PubMed classification task.\n",
    "We'll use `make_judge` to define custom evaluation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4003ec7-6489-493f-b3e6-4c74f2ecc819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "from mlflow.genai.scorers import get_scorer\n",
    "from mlflow.genai.scorers import ScorerSamplingConfig\n",
    "\n",
    "JUDGE_NAME = \"pubmed_classifier\"\n",
    "\n",
    "pubmed_classification_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"Evaluate if the response in {{ outputs }} appropriately classifies the sentence \"\n",
    "        \"in {{ inputs }}. The expectations are: {{ expectations }}. \"\n",
    "        \"Your grading criteria should be: \"\n",
    "        \"Yes: This is the correct classification. \"\n",
    "        \"No: This is the wrong classification. \"\n",
    "        \"Explain your rationale.\"\n",
    "    ),\n",
    "    feedback_value_type=str,\n",
    ")\n",
    "\n",
    "try:\n",
    "    registered_judge = get_scorer(name=JUDGE_NAME)\n",
    "    registered_judge = registered_judge.update(\n",
    "        sampling_config=ScorerSamplingConfig(sample_rate=1.0)\n",
    "    )\n",
    "    print(f\"Updated existing judge: {JUDGE_NAME}\")\n",
    "except Exception:\n",
    "    registered_judge = pubmed_classification_judge.register(name=JUDGE_NAME)\n",
    "    print(f\"Registered new judge: {JUDGE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19dd47f9-130e-4724-b125-613b6567935b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate with Custom Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2894488-0aaa-4350-aff6-e7e378c5e31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running evaluation with custom '{JUDGE_NAME}' judge...\")\n",
    "\n",
    "custom_eval_results = evaluate(\n",
    "    data=eval_dataset_records,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[pubmed_classification_judge],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92392bc6-8fa1-495f-9364-b390845bfb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Custom Judge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9183f7f-9f09-4674-b368-95027ca37437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_results = analyze_judge_accuracy(JUDGE_NAME, ground_truth)\n",
    "print_accuracy_summary(f\"Custom ({JUDGE_NAME})\", custom_results)\n",
    "judge_results[f\"Custom ({JUDGE_NAME})\"] = custom_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70efd1de-b5e7-45e5-9e18-23a372f8e5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mandatory: Provide Human Feedback on Custom Judge\n",
    "\n",
    "We need to provide some feedback on the incorrect assessments. This is used by our judge alignment optimizers to improve the judge\n",
    "\n",
    "**Important**: The feedback `name` must match the judge name so that the optimizers know what and where the base judge is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a12dab3e-edc7-4531-8a3b-6ab74dcfff42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "from mlflow.entities import AssessmentSource, AssessmentSourceType\n\nreviewer = AssessmentSource(\n    source_type=AssessmentSourceType.HUMAN,\n    source_id=\"auto_feedback_generator\",\n)\n\n# Combine all traces for feedback\nall_custom_traces = custom_results[\"matches\"] + custom_results[\"mismatches\"]\nprint(f\"Found {len(all_custom_traces)} traces to provide feedback on\")\nprint(f\"  - {len(custom_results['matches'])} where judge was correct\")\nprint(f\"  - {len(custom_results['mismatches'])} where judge was wrong\")\n\n# Use automatic feedback generation (uses LLM to generate rationales)\nlog_feedback_automatic(all_custom_traces, JUDGE_NAME, reviewer)\n\n# Alternative: Use interactive feedback (uncomment to manually review each trace)\n# log_feedback_interactive(all_custom_traces, JUDGE_NAME, reviewer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e31f077-4abc-4161-be73-dc0f5ee540b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 7: Align the Judge with MemAlign\n",
    "\n",
    "**MemAlign** is an optimizer that improves judge instructions based on human feedback.\n",
    "It analyzes cases where the judge disagreed with humans and refines the instructions\n",
    "to better capture human judgment criteria.\n",
    "\n",
    "For alignment to work, we need traces that have both:\n",
    "- LLM_JUDGE assessments (from running evaluate)\n",
    "- HUMAN assessments (from our feedback above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7291e384-701f-4bf0-b87e-f264e68f591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find Traces with Both Judge and Human Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1389e6-f6d3-4a31-b902-4534d8c04a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "traces_for_alignment = mlflow.search_traces(return_type=\"list\", max_results=100)\n\nJUDGE_NAME = \"pubmed_classifier\"\n\ncandidate_trace_ids = []\nfor trace in traces_for_alignment:\n    assessments = trace.search_assessments(name=JUDGE_NAME)\n    has_judge = any(a.source.source_type == \"LLM_JUDGE\" for a in assessments)\n    has_human = any(a.source.source_type == \"HUMAN\" for a in assessments)\n    if has_judge and has_human:\n        candidate_trace_ids.append(trace.info.trace_id)\n\nprint(f\"Found {len(candidate_trace_ids)} traces with both judge + human assessments\")\n\nvalid_traces = []\nfor trace_id in candidate_trace_ids:\n    full_trace = mlflow.get_trace(trace_id)\n    if full_trace and full_trace.info.assessments:\n        valid_traces.append(full_trace)\n\nprint(f\"Traces ready for alignment: {len(valid_traces)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd6543b-b371-4cec-801a-27f3b9d7c42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run MemAlign Optimization\n",
    "\n",
    "MemAlign will:\n",
    "1. Analyze disagreements between judge and human feedback\n",
    "2. Identify patterns in what the judge got wrong\n",
    "3. Generate improved instructions that better match human judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96c962d-d393-4d58-b4e6-dbf6a0ae4624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.judges.optimizers import MemAlignOptimizer\n",
    "\n",
    "print(\"Original judge instructions:\")\n",
    "print(pubmed_classification_judge.instructions)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running MemAlign optimization...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run alignment\n",
    "aligned_judge = pubmed_classification_judge.align(\n",
    "    traces=valid_traces,\n",
    "    optimizer=MemAlignOptimizer(\n",
    "        reflection_lm=\"databricks:/databricks-gpt-5-2\",\n",
    "        embedding_model=\"openai/text-embedding-3-large\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "121a9402-f8cb-4935-a4c2-67d9c8a58f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compare Original vs Aligned Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3508c2-664d-4b48-8c32-142c3256f7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ORIGINAL INSTRUCTIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(pubmed_classification_judge.instructions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALIGNED INSTRUCTIONS (after MemAlign):\")\n",
    "print(\"=\"*60)\n",
    "print(aligned_judge.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "368db877-f8a9-4a26-b533-71270b584f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 8: Evaluate with Aligned Judge\n",
    "\n",
    "Now let's see if the aligned judge performs better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fd64a8-121b-41dc-999e-9c31c0ed2243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the aligned judge as a new scorer\n",
    "ALIGNED_JUDGE_NAME = f\"{JUDGE_NAME}_MemAligned\"\n",
    "\n",
    "aligned_judge_scorer = make_judge(\n",
    "    name=ALIGNED_JUDGE_NAME,\n",
    "    instructions=f\"{aligned_judge.instructions}\",\n",
    "    feedback_value_type=str,\n",
    ")\n",
    "\n",
    "try:\n",
    "    registered_judge = get_scorer(name=ALIGNED_JUDGE_NAME)\n",
    "    registered_judge = registered_judge.update(\n",
    "        sampling_config=ScorerSamplingConfig(sample_rate=1.0)\n",
    "    )\n",
    "    print(f\"Updated existing judge: {ALIGNED_JUDGE_NAME}\")\n",
    "except Exception:\n",
    "    registered_judge = pubmed_classification_judge.register(name=ALIGNED_JUDGE_NAME)\n",
    "    print(f\"Registered new judge: {ALIGNED_JUDGE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1adc5c22-6f9b-49e9-a52a-8ba99a5960f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Evaluation with Aligned Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfe74c2-5d43-4707-8536-68d602325f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running evaluation with aligned '{ALIGNED_JUDGE_NAME}' judge...\")\n",
    "\n",
    "aligned_eval_results = evaluate(\n",
    "    data=eval_dataset_records,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[aligned_judge_scorer],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256db4be-01bb-4773-ac21-71f1cc7b46c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Aligned Judge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd0ed94-24db-4bbf-91f9-11f1fe7b213a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aligned_results = analyze_judge_accuracy(ALIGNED_JUDGE_NAME, ground_truth)\n",
    "print_accuracy_summary(f\"MemAligned ({ALIGNED_JUDGE_NAME})\", aligned_results)\n",
    "\n",
    "# Store for final comparison\n",
    "judge_results[f\"MemAligned ({JUDGE_NAME})\"] = aligned_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f031e1-8897-462c-acea-6090347ea9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 9: Final Comparison\n",
    "\n",
    "Let's compare all three judges side-by-side to see the impact of MemAlign!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108ec926-abec-485a-88cd-772654a98218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = [\n",
    "    {\"Judge\": name, \"Accuracy\": f\"{acc:.1%}\", \"Accuracy (raw)\": acc}\n",
    "    for name, acc in judge_results.items()\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON: Judge Accuracy\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nHow often did each judge correctly identify right/wrong LLM answers?\\n\")\n",
    "\n",
    "display(comparison_df[[\"Judge\", \"Accuracy\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e353ab3-163f-4864-923a-423c8c474e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. **Evaluated an LLM** on a PubMed text classification task\n",
    "2. **Used judges** to assess whether classifications were correct\n",
    "3. **Compared judge accuracy** against ground truth labels\n",
    "4. **Provided human feedback** to teach judges what they got wrong\n",
    "5. **Applied MemAlign** to improve judge instructions based on feedback\n",
    "6. **Measured improvement** in judge accuracy after alignment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Judges aren't perfect**: Even well-designed judges can make mistakes\n",
    "- **Human feedback is valuable**: It captures nuances that initial instructions miss\n",
    "- **MemAlign automates improvement**: It learns from disagreements to refine instructions\n",
    "- **Iteration helps**: Multiple rounds of feedback and alignment can continue improving judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec79ff87-848b-49da-80dd-0d82544aa901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MemAlign Tutorial (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
