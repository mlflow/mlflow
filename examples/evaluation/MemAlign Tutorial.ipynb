{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8b3ba5-03cc-40d8-bcb6-a1beefaab8f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup & Install Dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade mlflow==3.9.0 openai datasets huggingface_hub fsspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb576ff4-0c87-45a1-b9a1-6ebb2c981c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MemAlign Tutorial: Aligning LLM Judges with Human Feedback\n",
    "\n",
    "This tutorial demonstrates how to improve LLM judge accuracy using MLflow's **MemAlign** optimizer.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to evaluate LLM outputs using **judges** (scorers)\n",
    "2. How to provide **human feedback** on judge assessments\n",
    "3. How to use **MemAlign** to align judges with human preferences\n",
    "4. How to compare judge performance before and after alignment\n",
    "\n",
    "## Key Concepts\n",
    "- **Judge/Scorer**: An LLM that evaluates another LLM's outputs (e.g., \"Is this classification correct?\")\n",
    "- **Assessment**: The judge's evaluation result (e.g., \"Yes\" or \"No\")\n",
    "- **Trace**: A recorded LLM interaction including inputs, outputs, and assessments\n",
    "- **Alignment**: Improving a judge's instructions based on human feedback so it better matches human judgment\n",
    "\n",
    "## Data Source\n",
    "We'll use the [PubMed text classification dataset](https://huggingface.co/datasets/ml4pubmed/pubmed-text-classification-cased) where sentences from medical abstracts are classified into categories: BACKGROUND, OBJECTIVE, METHODS, RESULTS, or CONCLUSIONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61c8e83-ca9b-4cf5-a310-895d07f448ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "mlflow.set_registry_uri(\"your MLflow server\")\n",
    "mlflow.set_experiment(<\"refer to your experiment\">)\n",
    "mlflow.openai.autolog()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c784da6-4f84-4125-b177-3acb374058ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Load Evaluation Dataset\n",
    "\n",
    "First, we load a sample of sentences from the PubMed dataset. Each sentence has a ground truth label that we'll use to measure judge accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663188c6-be19-428b-94bc-6f18a92e23d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load PubMed text classification dataset\n",
    "ds = load_dataset(\"ml4pubmed/pubmed-text-classification-cased\", cache_dir='/tmp/huggingface')\n",
    "split = ds[\"train\"] if \"train\" in ds else ds[list(ds.keys())[0]]\n",
    "\n",
    "# Sample 30 examples for evaluation\n",
    "# random.seed(42)  # For reproducibility\n",
    "idxs = random.sample(range(len(split)), 50)\n",
    "\n",
    "samples_list = [\n",
    "    {\"text\": split[i][\"description_cln\"], \"answer\": split[i][\"target\"]}\n",
    "    for i in idxs\n",
    "]\n",
    "\n",
    "# Create ground truth lookup: sentence text -> correct label\n",
    "ground_truth = {s[\"text\"].strip(): s[\"answer\"] for s in samples_list}\n",
    "\n",
    "print(f\"Loaded {len(samples_list)} evaluation samples\")\n",
    "print(f\"Labels: {set(s['answer'] for s in samples_list)}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Text: {samples_list[0]['text'][:100]}...\")\n",
    "print(f\"  Label: {samples_list[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fffb8ab-d300-4d32-bef0-1ca97df1bf06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2: Format Dataset for Evaluation\n",
    "\n",
    "MLflow's `evaluate()` function expects data in a specific format. For this dataset, we are following the `Correctness()` judge's schema. We format each sample with:\n",
    "- `inputs`: The conversation history (user message with the sentence to classify)\n",
    "- `expectations`: What we expect from a correct response (used by the Correctness judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8421df35-68f3-4cd4-b27e-8cce30d3d9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Format for MLflow evaluate() - compatible with built-in Correctness judge\n",
    "eval_dataset_records = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [{\"role\": \"user\", \"content\": [\"text\"]}]\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_facts\": [\n",
    "                \"Classification label must be 'CONCLUSIONS', 'RESULTS', 'METHODS', 'OBJECTIVE', or 'BACKGROUND'\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    for s in samples_list\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(eval_dataset_records)} evaluation records\")\n",
    "print(f\"\\nSample record structure:\")\n",
    "print(f\"  inputs: {{input: [{{role: 'user', content: '<sentence>'}}]}}\")\n",
    "print(f\"  expectations: {{expected_facts: ['<constraint>']}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdec474e-a682-4c7f-9156-477e2271008f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: Define the Prediction Function\n",
    "\n",
    "This is the LLM we're evaluating. It takes a sentence and classifies it into one of the 5 PubMed categories.\n",
    "The judges will evaluate whether this LLM's classifications are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1e5d4c-30b4-4df1-85e3-501406ad529f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "client = \"your LLM client and provider call\"\n",
    "\n",
    "\n",
    "def predict_fn(input: list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Classify a sentence into one of 5 PubMed categories.\n",
    "\n",
    "    Args:\n",
    "        input: Conversation history, e.g., [{\"role\": \"user\", \"content\": \"...\"}]\n",
    "\n",
    "    Returns:\n",
    "        Dict with \"response\" key containing the classification\n",
    "    \"\"\"\n",
    "    user_text = input[0][\"content\"]\n",
    "\n",
    "    prompt = (\n",
    "        \"Classify the sentence into exactly one label from: \"\n",
    "        \"METHODS, RESULTS, CONCLUSIONS, BACKGROUND, OBJECTIVE.\\n\\n\"\n",
    "        f\"Sentence: {user_text}\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"your model\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44473b0e-89cb-4a23-af55-c389d0e6dbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Define Reusable Helper Functions\n",
    "\n",
    "These helper functions will be used throughout the tutorial to:\n",
    "- Analyze judge accuracy by comparing assessments to ground truth\n",
    "- Log human feedback for judge alignment\n",
    "- Print summaries and comparisons\n",
    "\n",
    "**Key insight**: A judge is \"correct\" when it says \"Yes\" for correct LLM answers and \"No\" for incorrect ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c725a4e2-5621-4663-9f40-0d30b448c246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "\n",
    "def normalize_label(x):\n",
    "    \"\"\"Normalize a label for comparison (uppercase, stripped).\"\"\"\n",
    "    return str(x).strip().upper() if x else None\n",
    "\n",
    "\n",
    "def extract_sentence_from_prompt(request: dict) -> str:\n",
    "    \"\"\"\n",
    "    Extract the original sentence from a trace request.\n",
    "\n",
    "    The prediction function formats prompts as:\n",
    "    \"Classify the sentence... Sentence: <TEXT>\"\n",
    "\n",
    "    This extracts <TEXT> for matching against ground truth.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        messages = request.get(\"messages\", [])\n",
    "        if not messages:\n",
    "            return \"\"\n",
    "        content = messages[0].get(\"content\", \"\")\n",
    "        if \"Sentence:\" in content:\n",
    "            return content.split(\"Sentence:\", 1)[1].strip()\n",
    "        return content.strip()\n",
    "    except (IndexError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_llm_response(response: dict) -> str:\n",
    "    \"\"\"Extract the LLM's classification from a trace response.\"\"\"\n",
    "    try:\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_judge_accuracy(assessment_name: str, ground_truth: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze how well a judge's assessments match ground truth.\n",
    "\n",
    "    A judge is \"correct\" when:\n",
    "    - It says \"Yes\" and the LLM's answer matches ground truth\n",
    "    - It says \"No\" and the LLM's answer doesn't match ground truth\n",
    "\n",
    "    Args:\n",
    "        assessment_name: Name of the judge/assessment to analyze (e.g., \"correctness\")\n",
    "        ground_truth: Dict mapping sentence text -> correct label\n",
    "\n",
    "    Returns:\n",
    "        Dict containing:\n",
    "        - accuracy: Fraction of correct judge assessments\n",
    "        - total: Number of traces analyzed\n",
    "        - correct: Number of correct assessments\n",
    "        - incorrect: Number of incorrect assessments\n",
    "        - matches: List of traces where judge was correct\n",
    "        - mismatches: List of traces where judge was wrong\n",
    "    \"\"\"\n",
    "    traces = mlflow.search_traces(max_results=30, return_type=\"list\")\n",
    "\n",
    "    matches, mismatches = [], []\n",
    "\n",
    "    for trace in traces:\n",
    "        # Parse trace data\n",
    "        request = json.loads(trace.data.request) if trace.data.request else {}\n",
    "        response = json.loads(trace.data.response) if trace.data.response else {}\n",
    "\n",
    "        # Extract components\n",
    "        sentence = extract_sentence_from_prompt(request)\n",
    "        llm_answer = extract_llm_response(response)\n",
    "        true_answer = ground_truth.get(sentence)\n",
    "\n",
    "        # Skip if we can't match to ground truth\n",
    "        if not true_answer or not llm_answer:\n",
    "            continue\n",
    "\n",
    "        # Get judge's assessment for this trace\n",
    "        assessments = trace.search_assessments(name=assessment_name)\n",
    "        judge_assessments = [a for a in assessments if a.source.source_type == \"LLM_JUDGE\"]\n",
    "        if not judge_assessments:\n",
    "            continue\n",
    "\n",
    "        judge_feedback = str(judge_assessments[0].feedback.value).strip().lower()\n",
    "        judge_rationale = judge_assessments[0].rationale\n",
    "\n",
    "        # Determine correctness\n",
    "        llm_correct = normalize_label(llm_answer) == normalize_label(true_answer)\n",
    "        judge_said_correct = judge_feedback == \"yes\"\n",
    "        judge_was_right = llm_correct == judge_said_correct\n",
    "\n",
    "        record = {\n",
    "            \"trace_id\": trace.info.trace_id,\n",
    "            \"sentence\": sentence,\n",
    "            \"llm_answer\": llm_answer,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"llm_correct\": llm_correct,\n",
    "            \"judge_feedback\": judge_feedback,\n",
    "            \"judge_rationale\": judge_rationale,\n",
    "            \"judge_correct\": judge_was_right,\n",
    "        }\n",
    "\n",
    "        if judge_was_right:\n",
    "            matches.append(record)\n",
    "        else:\n",
    "            mismatches.append(record)\n",
    "\n",
    "    total = len(matches) + len(mismatches)\n",
    "    accuracy = len(matches) / total if total > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"total\": total,\n",
    "        \"correct\": len(matches),\n",
    "        \"incorrect\": len(mismatches),\n",
    "        \"matches\": matches,\n",
    "        \"mismatches\": mismatches,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_accuracy_summary(name: str, results: dict):\n",
    "    \"\"\"Print a formatted summary of judge accuracy.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Judge: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.1%} ({results['correct']}/{results['total']})\")\n",
    "    print(f\"  - Correct assessments: {results['correct']}\")\n",
    "    print(f\"  - Incorrect assessments: {results['incorrect']}\")\n",
    "\n",
    "    if results['mismatches']:\n",
    "        print(f\"\\nExamples where judge was WRONG:\")\n",
    "        for m in results['mismatches'][:3]:  # Show first 3\n",
    "            status = \"correct\" if m['llm_correct'] else \"wrong\"\n",
    "            print(f\"  - LLM said '{m['llm_answer']}' (actually {status}), judge said '{m['judge_feedback']}'\")\n",
    "\n",
    "\n",
    "def log_feedback_interactive(records: list, judge_name: str, reviewer: AssessmentSource):\n",
    "    \"\"\"\n",
    "    Interactively collect human feedback for all traces.\n",
    "\n",
    "    For each trace, shows the context and asks for:\n",
    "    - Feedback value (Yes/No)\n",
    "    - Optional comment explaining the reasoning\n",
    "\n",
    "    This feedback is used by MemAlign to improve judge instructions.\n",
    "    \"\"\"\n",
    "    for i, record in enumerate(records, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Record {i}/{len(records)}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Sentence: {record['sentence']}\")\n",
    "        print(f\"LLM Answer: {record['llm_answer']}\")\n",
    "        print(f\"True Answer: {record['true_answer']}\")\n",
    "        print(f\"LLM was: {'CORRECT' if record['llm_correct'] else 'WRONG'}\")\n",
    "        print(f\"Judge said: '{record['judge_feedback']}'\")\n",
    "        print(f\"Judge was: {'CORRECT' if record['judge_correct'] else 'WRONG'}\")\n",
    "\n",
    "        suggested_value = \"Yes\" if record[\"llm_correct\"] else \"No\"\n",
    "        print(f\"\\nSuggested feedback: '{suggested_value}'\")\n",
    "\n",
    "        # Ask for feedback value\n",
    "        user_value = input(f\"Your feedback (Yes/No, or press Enter for '{suggested_value}'): \").strip()\n",
    "        if not user_value:\n",
    "            user_value = suggested_value\n",
    "        elif user_value.lower() in [\"yes\", \"y\"]:\n",
    "            user_value = \"Yes\"\n",
    "        elif user_value.lower() in [\"no\", \"n\"]:\n",
    "            user_value = \"No\"\n",
    "\n",
    "        # Ask for comment\n",
    "        comment = input(\"Comment (optional, press Enter to skip): \").strip()\n",
    "\n",
    "        mlflow.log_feedback(\n",
    "            trace_id=record[\"trace_id\"],\n",
    "            name=judge_name,\n",
    "            value=user_value,\n",
    "            rationale=comment if comment else f\"Human feedback: {user_value}\",\n",
    "            source=reviewer,\n",
    "        )\n",
    "        print(f\"Logged: {user_value}\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Logged feedback for {len(records)} traces\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "\n",
    "# Dictionary to store results for final comparison\n",
    "judge_results = {}\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff6dc120-46ac-4439-b8ec-8eeab1c02e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 5: Evaluate with Built-in Correctness Judge\n",
    "\n",
    "MLflow provides a built-in `Correctness` judge that evaluates whether responses meet expected criteria.\n",
    "Let's see how well it performs on our classification task. \n",
    "We will use this as comparison with a general purpose judge vs a custom judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd5a668-c4c1-4e55-b264-f159c9bbdbaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Correctness\n",
    "from mlflow.genai import evaluate\n",
    "\n",
    "print(\"Running evaluation with Correctness judge...\")\n",
    "print(\"This will classify each sentence and have the judge evaluate the results.\\n\")\n",
    "\n",
    "correctness_eval_results = evaluate(\n",
    "    data=eval_dataset_records,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[Correctness()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2a07ef-4ee9-409d-9da2-cc80eabb1677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Correctness Judge Results\n",
    "\n",
    "Now let's see how accurate the Correctness judge was. Remember:\n",
    "- The judge is \"correct\" if it says \"Yes\" when the LLM was right, or \"No\" when wrong\n",
    "- The judge is \"wrong\" if it says \"Yes\" when the LLM was actually wrong, or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ebb2ea-49a2-41de-b696-70cbc79d10ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "correctness_results = analyze_judge_accuracy(\"correctness\", ground_truth)\n",
    "print_accuracy_summary(\"Correctness (built-in)\", correctness_results)\n",
    "\n",
    "# Store for final comparison\n",
    "judge_results[\"Correctness (built-in)\"] = correctness_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32754171-28e0-406a-b800-6f080425ad9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 6: Create a Custom Judge\n",
    "\n",
    "Now let's create our own judge specifically designed for this PubMed classification task.\n",
    "We'll use `make_judge` to define custom evaluation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4003ec7-6489-493f-b3e6-4c74f2ecc819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "from mlflow.genai.scorers import ScorerSamplingConfig\n",
    "\n",
    "JUDGE_NAME = \"pubmed_classifier\"\n",
    "\n",
    "pubmed_classification_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"Evaluate if the response in {{ outputs }} appropriately classifies the sentence \"\n",
    "        \"in {{ inputs }}. The expectations are: {{ expectations }}. \"\n",
    "        \"Your grading criteria should be: \"\n",
    "        \"Yes: This is the correct classification. \"\n",
    "        \"No: This is the wrong classification. \"\n",
    "        \"Explain your rationale.\"\n",
    "    ),\n",
    "    feedback_value_type=str,\n",
    ")\n",
    "\n",
    "# Register the judge (or update if already exists)\n",
    "try:\n",
    "    registered_judge = pubmed_classification_judge.register()\n",
    "    print(f\"Registered new judge: {JUDGE_NAME}\")\n",
    "except ValueError as e:\n",
    "    if \"has already been registered\" in str(e):\n",
    "        registered_judge = pubmed_classification_judge.update(\n",
    "            sampling_config=ScorerSamplingConfig(sample_rate=1)\n",
    "        )\n",
    "        print(f\"Updated existing judge: {JUDGE_NAME}\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19dd47f9-130e-4724-b125-613b6567935b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate with Custom Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2894488-0aaa-4350-aff6-e7e378c5e31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running evaluation with custom '{JUDGE_NAME}' judge...\")\n",
    "\n",
    "custom_eval_results = evaluate(\n",
    "    data=eval_dataset_records,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[pubmed_classification_judge],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92392bc6-8fa1-495f-9364-b390845bfb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Custom Judge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9183f7f-9f09-4674-b368-95027ca37437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_results = analyze_judge_accuracy(JUDGE_NAME, ground_truth)\n",
    "print_accuracy_summary(f\"Custom ({JUDGE_NAME})\", custom_results)\n",
    "\n",
    "# Store for final comparison\n",
    "judge_results[f\"Custom ({JUDGE_NAME})\"] = custom_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70efd1de-b5e7-45e5-9e18-23a372f8e5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Mandatory: Provide Human Feedback on Custom Judge\n",
    "\n",
    "We need to provide some feedback on the incorrect assessments. This is used by our judge alignment optimizers to improve the judge\n",
    "\n",
    "**Important**: The feedback `name` must match the judge name so that the optimizers know what and where the base judge is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a12dab3e-edc7-4531-8a3b-6ab74dcfff42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "reviewer = AssessmentSource(\n",
    "    source_type=AssessmentSourceType.HUMAN,\n",
    "    source_id=\"your name or identifier\",\n",
    ")\n",
    "\n",
    "# Combine all traces for interactive feedback\n",
    "all_custom_traces = custom_results[\"matches\"] + custom_results[\"mismatches\"]\n",
    "print(f\"Found {len(all_custom_traces)} traces to review\")\n",
    "print(f\"  - {len(custom_results['matches'])} where judge was correct\")\n",
    "print(f\"  - {len(custom_results['mismatches'])} where judge was wrong\")\n",
    "\n",
    "log_feedback_interactive(all_custom_traces, JUDGE_NAME, reviewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e31f077-4abc-4161-be73-dc0f5ee540b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 7: Align the Judge with MemAlign\n",
    "\n",
    "**MemAlign** is an optimizer that improves judge instructions based on human feedback.\n",
    "It analyzes cases where the judge disagreed with humans and refines the instructions\n",
    "to better capture human judgment criteria.\n",
    "\n",
    "For alignment to work, we need traces that have both:\n",
    "- LLM_JUDGE assessments (from running evaluate)\n",
    "- HUMAN assessments (from our feedback above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7291e384-701f-4bf0-b87e-f264e68f591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find Traces with Both Judge and Human Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1389e6-f6d3-4a31-b902-4534d8c04a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traces_for_alignment = mlflow.search_traces(return_type=\"list\", max_results=100)\n",
    "\n",
    "JUDGE_NAME = \"pubmed_classifier\"\n",
    "\n",
    "valid_traces = []\n",
    "for trace in traces_for_alignment:\n",
    "    feedbacks = trace.search_assessments(name=JUDGE_NAME)\n",
    "    has_judge = any(f.source.source_type == \"LLM_JUDGE\" for f in feedbacks)\n",
    "    has_human = any(f.source.source_type == \"HUMAN\" for f in feedbacks)\n",
    "    if has_judge and has_human:\n",
    "        valid_traces.append(trace)\n",
    "\n",
    "print(f\"Total traces found: {len(traces_for_alignment)}\")\n",
    "print(f\"Traces with both judge + human feedback: {len(valid_traces)}\")\n",
    "\n",
    "if len(valid_traces) < 5:\n",
    "    print(\"\\nWarning: Few traces available for alignment. Results may be limited.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd6543b-b371-4cec-801a-27f3b9d7c42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run MemAlign Optimization\n",
    "\n",
    "MemAlign will:\n",
    "1. Analyze disagreements between judge and human feedback\n",
    "2. Identify patterns in what the judge got wrong\n",
    "3. Generate improved instructions that better match human judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96c962d-d393-4d58-b4e6-dbf6a0ae4624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mlflow.genai.judges.optimizers import MemAlignOptimizer\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set if using OpenAI, otherwise set for the provider you're using\n",
    "\n",
    "print(\"Original judge instructions:\")\n",
    "print(pubmed_classification_judge.instructions)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running MemAlign optimization...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run alignment\n",
    "aligned_judge = pubmed_classification_judge.align(\n",
    "    traces=valid_traces,\n",
    "    optimizer=MemAlignOptimizer(\n",
    "        reflection_lm=\"openai:/gpt-5-2\",\n",
    "        embedding_model=\"openai/text-embedding-3-large\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "121a9402-f8cb-4935-a4c2-67d9c8a58f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compare Original vs Aligned Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3508c2-664d-4b48-8c32-142c3256f7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ORIGINAL INSTRUCTIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(pubmed_classification_judge.instructions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALIGNED INSTRUCTIONS (after MemAlign):\")\n",
    "print(\"=\"*60)\n",
    "print(aligned_judge.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "368db877-f8a9-4a26-b533-71270b584f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 8: Evaluate with Aligned Judge\n",
    "\n",
    "Now let's see if the aligned judge performs better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fd64a8-121b-41dc-999e-9c31c0ed2243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the aligned judge as a new scorer\n",
    "ALIGNED_JUDGE_NAME = f\"{JUDGE_NAME}_MemAligned\"\n",
    "\n",
    "aligned_judge_scorer = make_judge(\n",
    "    name=ALIGNED_JUDGE_NAME,\n",
    "    instructions=f\"{aligned_judge.instructions}\",\n",
    "    feedback_value_type=str,\n",
    ")\n",
    "\n",
    "try:\n",
    "    aligned_judge_scorer.register()\n",
    "    print(f\"Registered aligned judge: {ALIGNED_JUDGE_NAME}\")\n",
    "except ValueError as e:\n",
    "    if \"has already been registered\" in str(e):\n",
    "        aligned_judge_scorer.update(sampling_config=ScorerSamplingConfig(sample_rate=1))\n",
    "        print(f\"Updated existing aligned judge: {ALIGNED_JUDGE_NAME}\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1adc5c22-6f9b-49e9-a52a-8ba99a5960f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Evaluation with Aligned Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfe74c2-5d43-4707-8536-68d602325f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running evaluation with aligned '{ALIGNED_JUDGE_NAME}' judge...\")\n",
    "\n",
    "aligned_eval_results = evaluate(\n",
    "    data=eval_dataset_records,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[aligned_judge_scorer],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256db4be-01bb-4773-ac21-71f1cc7b46c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Aligned Judge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd0ed94-24db-4bbf-91f9-11f1fe7b213a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aligned_results = analyze_judge_accuracy(ALIGNED_JUDGE_NAME, ground_truth)\n",
    "print_accuracy_summary(f\"MemAligned ({ALIGNED_JUDGE_NAME})\", aligned_results)\n",
    "\n",
    "# Store for final comparison\n",
    "judge_results[f\"MemAligned ({JUDGE_NAME})\"] = aligned_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f031e1-8897-462c-acea-6090347ea9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 9: Final Comparison\n",
    "\n",
    "Let's compare all three judges side-by-side to see the impact of MemAlign!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108ec926-abec-485a-88cd-772654a98218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = [\n",
    "    {\"Judge\": name, \"Accuracy\": f\"{acc:.1%}\", \"Accuracy (raw)\": acc}\n",
    "    for name, acc in judge_results.items()\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON: Judge Accuracy\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nHow often did each judge correctly identify right/wrong LLM answers?\\n\")\n",
    "\n",
    "display(comparison_df[[\"Judge\", \"Accuracy\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e353ab3-163f-4864-923a-423c8c474e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. **Evaluated an LLM** on a PubMed text classification task\n",
    "2. **Used judges** to assess whether classifications were correct\n",
    "3. **Compared judge accuracy** against ground truth labels\n",
    "4. **Provided human feedback** to teach judges what they got wrong\n",
    "5. **Applied MemAlign** to improve judge instructions based on feedback\n",
    "6. **Measured improvement** in judge accuracy after alignment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Judges aren't perfect**: Even well-designed judges can make mistakes\n",
    "- **Human feedback is valuable**: It captures nuances that initial instructions miss\n",
    "- **MemAlign automates improvement**: It learns from disagreements to refine instructions\n",
    "- **Iteration helps**: Multiple rounds of feedback and alignment can continue improving judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec79ff87-848b-49da-80dd-0d82544aa901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MemAlign Tutorial (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}