{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "from queue import Queue, Empty\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from cuml.metrics.accuracy import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from cuml.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull sample airline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -N https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/airline_small.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loader, using cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by CPU/GPU models.\n",
    "\n",
    "    :param fpath: Path to the data to be ingested\n",
    "    :return: DataFrame wrapping the data at [fpath]. Data will be in either a Pandas or RAPIDS (cuDF) DataFrame\n",
    "    \"\"\"\n",
    "    import cudf\n",
    "\n",
    "    df = cudf.read_parquet(fpath)\n",
    "    X = df.drop([\"ArrDelayBinary\"], axis=1)\n",
    "    y = df[\"ArrDelayBinary\"].astype('int32')\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fpath, max_detph, max_features, n_estimators):\n",
    "    \"\"\"\n",
    "    :param fpath: Path or URL for the training data used with the model.\n",
    "    :max_detph: int Max tree depth\n",
    "    :max_features: float percentage of features to use in classification\n",
    "    :n_estimators: int number of trees to create\n",
    "    :return: Trained Model\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = load_data(fpath)\n",
    "    mod = RandomForestClassifier(max_depth=max_depth, max_features=max_features, n_estimators=n_estimators)\n",
    "    acc_scorer = accuracy_score\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    preds = mod.predict(X_test)\n",
    "    acc = acc_scorer(y_test, preds)\n",
    "\n",
    "    mlparams = {\"max_depth\": str(max_depth),\n",
    "                \"max_features\": str(max_features),\n",
    "                \"n_estimators\": str(n_estimators),\n",
    "                }\n",
    "    mlflow.log_params(mlparams)\n",
    "\n",
    "    mlmetrics = {\"accuracy\": acc}\n",
    "    mlflow.log_metrics(mlmetrics)\n",
    "\n",
    "    return mod, infer_signature(X_train.to_pandas(), y_train.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement our MLFlow training loop, and save our best model to the tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = f'conda.yaml'\n",
    "fpath     = f'airline_small.parquet'\n",
    "\n",
    "max_depth = 10\n",
    "max_features = 0.75\n",
    "n_estimators = 500\n",
    "\n",
    "artifact_path = \"Airline-Demo\"\n",
    "artifact_uri = None\n",
    "experiment_name = \"RAPIDS-Notebook\"\n",
    "experiment_id = None\n",
    "\n",
    "mlflow.set_tracking_uri(uri='sqlite:////tmp/mlflow-db.sqlite')\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"mlflow.runName\", \"(Notebook) RAPIDS-MLFlow\")\n",
    "    \n",
    "    model, signature = train(fpath, max_depth, max_features, n_estimators)\n",
    "        \n",
    "    mlflow.sklearn.log_model(model,\n",
    "                             signature=signature,\n",
    "                             artifact_path=artifact_path,\n",
    "                             registered_model_name=\"rapids-mlflow-notebook\",\n",
    "                             conda_env='conda.yaml')\n",
    "    \n",
    "    artifact_uri = mlflow.get_artifact_uri(artifact_path=artifact_path)\n",
    "print(artifact_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to track our server output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def queue_descriptor_output(out, queue):\n",
    "    for line in iter(out.readline, b''):\n",
    "        queue.put(line)\n",
    "    out.close()\n",
    "\n",
    "def follow_subprocess(cmd, timeout=1000, line_timeout=60.00):\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    q = Queue()\n",
    "    t = threading.Thread(target=queue_descriptor_output, args=(p.stdout, q))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "    elapsed = 0\n",
    "    line_elapsed = 0\n",
    "    last_line_time = time.perf_counter()\n",
    "    while (p.poll() is None and elapsed < timeout and line_elapsed < line_timeout):\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            elapsed += 2\n",
    "            while (True):\n",
    "                line = q.get(timeout=0.1)\n",
    "                line_elapsed = 0\n",
    "                last_line_time = time.perf_counter()\n",
    "                sys.stdout.write(line.decode())\n",
    "\n",
    "        except Empty:\n",
    "            line_elapsed = (time.perf_counter() - last_line_time)\n",
    "        except KeyboardInterrupt:\n",
    "            sys.stderr.write(\"\\nCaught ctrl+c, killing subprocess ({})\\n\".format(' '.join(cmd)))\n",
    "            p.kill()\n",
    "            raise\n",
    "\n",
    "    try:\n",
    "        p.kill()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    t.join(2)\n",
    "\n",
    "    ## Drain any remaining text\n",
    "    try:\n",
    "        while (True):\n",
    "            line = q.get(timeout=0.1)\n",
    "            sys.stdout.write(line)\n",
    "\n",
    "    except Empty:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin serving our trained model using MLFlow\n",
    "**Note:** The serving thread will continue to run after cell execution. Select the cell and click 'interrupt the kernel' to stop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 55755\n",
    "host = 'localhost'\n",
    "\n",
    "command = f\"mlflow models serve -m {artifact_uri} -p {port} -h {host}\".split()\n",
    "kwargs = { \"cmd\": command, \"timeout\":float('Inf'), \"line_timeout\": float('Inf') }\n",
    "\n",
    "threading.Thread(target=follow_subprocess, kwargs=kwargs).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make requests against the deployed model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "mlflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}