{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0082b221-e28f-4d0b-8377-f0cc328dff91",
   "metadata": {},
   "source": [
    "# Building Advanced RAG with MLflow and LlamaIndex Workflow\n",
    "\n",
    "Augmenting LLMs with various data sources is a strong strategy to build LLM applications. However, as the system grows more complex, it is challenging to xxx and running quick development cycle for improvement.\n",
    "\n",
    "LlamaIndex Workflow is a great framework to build such compound system, and combined with MLflow brings efficiency and robustness in the development cycle, featuring debugging, tracking experiment, and evaluation for continuous improvement.\n",
    "\n",
    "In this notebook, we will go through the journey of building an sophisticated chatbot with LlamaIndex Workflow and MLflow. By the end of this tutorial you will have:\n",
    "\n",
    "- Set up a **LlamaIndex Workflow** with multiple retrieval strategies, including vector search, BM25, and web search.\n",
    "- Logged and tracked the workflow in **MLflow Experiment** and track parameters.\n",
    "- Evaluated the workflowâ€™s performance using **MLflow Evaluate**, with metrics such as latency and answer correctness.\n",
    "- Explored how to use **MLflow UI** to compare model performance across multiple configurations.\n",
    "- Gained insights into workflow execution with **MLflow Tracing** to identify quality issues and optimize retriever strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93af38f",
   "metadata": {},
   "source": [
    "## Strategy: Hybrid Approach Using Multiple Retrieval Methods\n",
    "\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a powerful framework, but the retrieval step can often become a bottleneck, because embedding-based retrieval may not always capture the most relevant context. While many techniques exist to improve retrieval quality, no single solution works universally. Therefore, an effective strategy is to combine multiple retrieval approaches.\n",
    "\n",
    "The concept we will explore here is to run several retrieval methods in parallel: (1) standard vector search, (2) keyword-based search (BM25), and (3) web search. The retrieved contexts are then merged, with irrelevant data filtered out to enhance the overall quality.\n",
    "\n",
    "![Hybrid RAG Concept](static/images/llama_index_workflow_hybrid_rag_concept.png)\n",
    "\n",
    "How do we bring this concept to life? Letâ€™s dive in and build this hybrid RAG using LlamaIndex Workflow and MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10434134",
   "metadata": {},
   "source": [
    "## What is LlamaIndex Workflow?\n",
    "\n",
    "[LlamaIndex Workflow](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) is an event-driven orchestration framework for designing dynamic AI applications. The core of LlamaIndex Workflow consists of:\n",
    "\n",
    "* `Steps` are units of execution, representing distinct actions in the workflow.\n",
    "\n",
    "* `Events` trigger these steps, acting as signals that control the workflowâ€™s flow.\n",
    "\n",
    "* `Workflow` connects these two as a Python class. Each step is implemented as a method of the workflow class, defined with input and output events.\n",
    "\n",
    "This simple yet powerful abstraction allows you to break down complex tasks into manageable steps, enabling greater flexibility and scalability. As a nature of event-driven design, it is super easy to design parallel/asynchronous execution flow, which significantly enhances efficiency involve long-running tasks and provides production-ready scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5ada4-db4c-4dfa-bb3e-34cb4e82a2da",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "\n",
    "Follow the instructions on `README.md` to set up environment if you haven't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec266b4a-b679-4cbb-b309-db25dc627f00",
   "metadata": {},
   "source": [
    "## 2. Start an MLflow Experiment\n",
    "\n",
    "An **MLflow Experiment** is where you track all aspects of model development, including model definitions, configurations, parameters, dependency versions, and more. Letâ€™s start by creating a new MLflow experiment called \"LlamaIndex Workflow RAG\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe067408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"LlamaIndex Workflow RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d35e-2061-4307-af75-4fec5b85ede6",
   "metadata": {},
   "source": [
    "## 3. Choose your LLM and Embeddings\n",
    "\n",
    "\n",
    "Now, set up your preferred LLM and embeddings models to LlamaIndex's Settings object. These models will be used throughout the LlamaIndex components.\n",
    "\n",
    "ðŸ’¡ *MLflow will automatically log the `Settings` configuration into your MLflow Experiment when logging models, ensuring reproducibility and reducing the risk of discrepancies between environments.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab2584-cca2-4215-aac1-7d8502c43323",
   "metadata": {},
   "source": [
    "### Option 1: OpenAI (default)\n",
    "\n",
    "LlamaIndex by default uses OpenAI APIs for LLMs and embeddings models. You can use the default model (`gpt-3.5-turbo` and `text-embeddings-ada-002` as of Oct 2024), but we recommend setting them to the latest efficient models for getting better results with lower cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf61f1-15cd-4c7b-bc35-f987a3ba12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b534f-035e-4021-8a8b-e5fdf64ca2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# LlamaIndex by default uses OpenAI APIs for LLMs and embeddings models. You can use the default\n",
    "# model (`gpt-3.5-turbo` and `text-embeddings-ada-002` as of Oct 2024), but we recommend using the\n",
    "# latest efficient models instead for getting better results with lower cost.\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40717988-458a-4b3b-b9f2-ce46ff9299e3",
   "metadata": {},
   "source": [
    "### Option 2: Other Hosted Models\n",
    "\n",
    "If you want to use other hosted LLMs,\n",
    "\n",
    "1. Download the integration package for the model provider of your choice.\n",
    "2. Set up required environment variables as specified in the integration documentation.\n",
    "3. Instantiate the LLM and Embeddings instances and set them to the global Settings object.\n",
    "\n",
    "The following cells show an example for using Databricks hosted LLMs (Llama3.1 70B instruct).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea86c74-faac-44d3-9e93-33b22b9e2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-databricks -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c04d7-d692-4353-8457-2aaac660845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"https://YOUR_DATABRICKS_HOST/serving-endpoints/\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass(\"Enter Databricks API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83e674-903e-41c1-8480-7d95cfef0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.databricks import DatabricksEmbedding\n",
    "from llama_index.llms.databricks import Databricks\n",
    "\n",
    "Settings.embed_model = DatabricksEmbedding(model=\"databricks-gte-large-en\")\n",
    "Settings.llm = Databricks(model=\"databricks-meta-llama-3-1-70b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1fbb8-e3e3-46a9-9fbb-2244cc818d03",
   "metadata": {},
   "source": [
    "### Option 3: Local Models\n",
    "\n",
    "LlamaIndex also support locally hosted LLMs. Please refer to the [Starter Tutorial (Local Models)](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/) for how to set them up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e1c89-f046-4fb4-a7f3-c2460c2013a7",
   "metadata": {},
   "source": [
    "## 4. Set Up Web Search API\n",
    "\n",
    "Later in this notebook, we will add web search capability to the QA bot. Tavily AI provides a search API\n",
    "optimized for LLM application and natively integrated with LlamaIndex. Visit [their website](https://tavily.com/) to\n",
    "get an API key for free-tier use, or use different search engine integrated with LlamaIndex, e.g. [GoogleSearchToolSpec](https://docs.llamaindex.ai/en/stable/api_reference/tools/google/#llama_index.tools.google.GoogleSearchToolSpec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed9c06-0501-4e80-91a3-3cc38402a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_AI_API_KEY\"] = getpass.getpass(\"Enter your Tavily AI APi Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c673bf",
   "metadata": {},
   "source": [
    "## 5. Set Up Document Indices for Retrieval\n",
    "\n",
    "The next step is to build a document index for retrieval from MLflow documentation. The `urls.txt` file in the `data` directory contains a list of MLflow documentation pages. These pages can be loaded as document objects using the web page reader utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7442b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "with open(\"data/urls.txt\") as file:\n",
    "    urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48419c9f-5d3e-4ba4-a1b5-34b26d01c3e0",
   "metadata": {},
   "source": [
    "### Vector Index\n",
    "Next, ingest these documents into a vector database. In this tutorial, weâ€™ll use the [Qdrant](https://qdrant.tech/) vector store, which is free if self-hosted. If Docker is installed on your machine, you can start the Qdrant database by running the official Docker container:\n",
    "\n",
    "\n",
    "```shell\n",
    "$ docker pull qdrant/qdrant\n",
    "$ docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/.qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant\n",
    "```\n",
    "\n",
    "Once the container is running, you can create an index object that connects to the Qdrant database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"mlflow_doc\")\n",
    "\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents=documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7411d",
   "metadata": {},
   "source": [
    "Of course, you can use your preferred vector store here. LlamaIndex supports a variety of vector databases, such as [FAISS](https://docs.llamaindex.ai/en/stable/examples/vector_stores/FaissIndexDemo/), [Chroma](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo/), and [Databricks Vector Search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/DatabricksVectorSearchDemo/). If you choose an alternative, follow the relevant LlamaIndex documentation and update the `workflow/workflow.py` file accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db489188-9998-4b81-b0af-6aba4b989470",
   "metadata": {},
   "source": [
    "### Keyword-based Retrieval\n",
    "\n",
    "In addition to evaluating the vector search retrieval, we will assess the keyword-based retriever (BM25) later.\n",
    "Let's set up local document storage to enable BM25 retrieval in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b8b1f-bc61-4152-9e04-b93b0d34bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes)\n",
    "bm25_retriever.persist(\".bm25_retriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d061ff-1abc-4ce3-88f8-8c7d4fe19d63",
   "metadata": {},
   "source": [
    "## 6. Define a Workflow\n",
    "\n",
    "Now that the environment and data sources are ready, we can build the workflow and experiment with it. The complete workflow code is defined in the `workflow` directory. Let's explore some key components of the implementation.\n",
    "\n",
    "### Events\n",
    "\n",
    "The `workflow/events.py` file defines all the events used within the workflow. These are simple Pydantic models that carry information between workflow steps. For example, the `VectorSearchRetrieveEvent` triggers the vector search step by passing the user's query.\n",
    "\n",
    "```python\n",
    "class VectorSearchRetrieveEvent(Event):\n",
    "    \"\"\"Event for triggering VectorStore index retrieval step.\"\"\"\n",
    "    query: str\n",
    "```\n",
    "\n",
    "### Prompts\n",
    "\n",
    "Throughout the workflow execution, we call LLMs multiple times. The prompt templates for these LLM calls are defined in the `workflow/prompts.py` file.\n",
    "\n",
    "\n",
    "### Workflow Class\n",
    "\n",
    "The main workflow class is defined in `workflow/workflow.py`. Let's break down how it works.\n",
    "\n",
    "The constructor accepts a retrievers argument, which specifies the retrieval methods to be used in the workflow. For instance, if `[\"vector_search\", \"bm25\"]` is passed, the workflow performs vector search and keyword-based search, skipping web search.\n",
    "\n",
    "ðŸ’¡ Deciding retrievers dynamically allows us to experiment different retrieval strategies without replicating model code with almost same definition.\n",
    "\n",
    "```python\n",
    "class HybridRAGWorkflow(Workflow):\n",
    "\n",
    "    VALID_RETRIEVERS = {\"vector_search\", \"bm25\", \"web_search\"}\n",
    "\n",
    "    def __init__(self, retrievers=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm = Settings.llm\n",
    "        self.retrievers = retrievers or []\n",
    "\n",
    "        if invalid_retrievers := set(self.retrievers) - self.VALID_RETRIEVERS:\n",
    "            raise ValueError(f\"Invalid retrievers specified: {invalid_retrievers}\")\n",
    "\n",
    "        self._use_vs_retriever = \"vector_search\" in self.retrievers\n",
    "        self._use_bm25_retriever = \"bm25\" in self.retrievers\n",
    "        self._use_web_search = \"web_search\" in self.retrievers\n",
    "\n",
    "        if self._use_vs_retriever:\n",
    "            qd_client = qdrant_client.QdrantClient(host=_QDRANT_HOST, port=_QDRANT_PORT)\n",
    "            vector_store = QdrantVectorStore(client=qd_client, collection_name=_QDRANT_COLLECTION_NAME)\n",
    "            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "            self.vs_retriever = index.as_retriever()\n",
    "\n",
    "        if self._use_bm25_retriever:\n",
    "            self.bm25_retriever = BM25Retriever.from_persist_dir(_BM25_PERSIST_DIR)\n",
    "\n",
    "        if self._use_web_search:\n",
    "            self.tavily_tool = TavilyToolSpec(api_key=os.environ.get(\"TAVILY_AI_API_KEY\"))\n",
    "```\n",
    "\n",
    "The workflow begins by executing a step that takes the `StartEvent` as input, which is the `route_retrieval` step in this case. This step inspects the retrievers parameter and triggers the necessary retrieval steps. By using the `send_event()` method of the context object, multiple events can be dispatched in parallel from this single step.\n",
    "\n",
    "\n",
    "```python\n",
    "    # If no retriever is specified, proceed directly to the final query step with an empty context\n",
    "    if len(self.retrievers) == 0:\n",
    "        return QueryEvent(context=\"\")\n",
    "\n",
    "    # Trigger the retrieval steps based on the configuration\n",
    "    if self._use_vs_retriever:\n",
    "        ctx.send_event(VectorSearchRetrieveEvent(query=query))\n",
    "    if self._use_bm25_retriever:\n",
    "        ctx.send_event(BM25RetrieveEvent(query=query))\n",
    "    if self._use_web_search:\n",
    "        ctx.send_event(TransformQueryEvent(query=query))\n",
    "```\n",
    "\n",
    "The retrieval steps are straightforward. However, the web search step is more advanced as it includes an additional step to transform the user's question into a search-friendly query using an LLM.\n",
    "\n",
    "The results from all the retrieval steps are aggregated in the `gather_retrieval_results` step. Here, the `ctx.collect_events()` method is used to poll for the results of the asynchronously executed steps.\n",
    "\n",
    "```python\n",
    "    results = ctx.collect_events(ev, [RetrievalResultEvent] * len(self.retrievers))\n",
    "```\n",
    "\n",
    "Passing all results from multiple retrievers often leads to a large context with unrelated or duplicate content. To address this, we need to filter and select the most relevant results. While a score-based approach is common, web search results do not return similarity scores. Therefore, we use an LLM to sort and filter out irrelevant results. The rerank step achieves this by leveraging the built-in reranker integration with [RankGPT](https://github.com/sunnweiwei/RankGPT).\n",
    "\n",
    "```python\n",
    "    reranker = RankGPTRerank(llm=self.llm, top_n=5)\n",
    "    reranked_nodes = reranker.postprocess_nodes(ev.nodes, query_str=query)\n",
    "    reranked_context = \"\\n\".join(node.text for node in reranked_nodes)\n",
    "```\n",
    "\n",
    "Finally, the reranked context is passed to the LLM along with the user query to generate the final answer. The result is returned as a `StopEvent` with the `result` key.\n",
    "\n",
    "```python\n",
    "    @step\n",
    "    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        \"\"\"Get result with relevant text.\"\"\"\n",
    "        query = await ctx.get(\"query\")\n",
    "\n",
    "        prompt = FINAL_QUERY_TEMPLATE.format(context=ev.context, query=query)\n",
    "        response = self.llm.complete(prompt).text\n",
    "        return StopEvent(result=response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8f614-cc00-4602-b283-0564ce6d1f14",
   "metadata": {},
   "source": [
    "Now, let's instantiate the workflow and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c91200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow with VS + BM25 retrieval\n",
    "from workflow.workflow import HybridRAGWorkflow\n",
    "\n",
    "workflow = HybridRAGWorkflow(retrievers=[\"vector_search\", \"bm25\"], timeout=60)\n",
    "response = await workflow.run(query=\"Why use MLflow with LlamaIndex?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4f09d-98a7-4298-b44d-6ed8eb0d212c",
   "metadata": {},
   "source": [
    "## 7. Log the Workflow in MLflow Experiment\n",
    "\n",
    "Now we want to run the workflow with various different retrieval strategy and evaluate the performance. However, before running the evaluation, we'll log the model in MLflow to track both the model and its performance within an **MLflow Experiment**.\n",
    "\n",
    "For the LlamaIndex Workflow, we use the new [Model-from-code](https://mlflow.org/docs/latest/models.html#models-from-code) method, which logs models as standalone Python scripts. This approach avoids the risks and instability associated with serialization methods like pickle, relying instead on code as the single source of truth for the model definition. When combined with MLflow's environment-freezing capability, it provides a reliable way to persist models. For more details, refer to the [MLflow documentation](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "ðŸ’¡ In the `workflow` directory, there's a `model.py` script that imports the `HybridRAGWorkflow` and instantiates it with dynamic configurations passed via the `model_config` parameter during logging. This design allows you to track models with different configurations without duplicating the model definition.\n",
    "\n",
    "We'll start an MLflow Run and log the model script `model.py` with different configurations using the [mlflow.llama_index.log_model()](https://mlflow.org/docs/latest/python_api/mlflow.llama_index.html#mlflow.llama_index.log_model) API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164450c-f81c-4916-803c-5b5588642bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different configurations we will evaluate. We don't run evaluation for all permutation\n",
    "# for demonstration purpose, but you can add as many patterns as you want.\n",
    "run_name_to_retrievers = {\n",
    "    # 1. No retrievers (prior knowledge in LLM).\n",
    "    \"none\": [],\n",
    "    # 2. Vector search retrieval only.\n",
    "    \"vs\": [\"vector_search\"],\n",
    "    # 3. Vector search and keyword search (BM25)\n",
    "    \"vs + bm25\": [\"vector_search\", \"bm25\"],\n",
    "    # 4. All retrieval methods including web search.\n",
    "    \"vs + bm25 + web\": [\"vector_search\", \"bm25\", \"web_search\"],\n",
    "}\n",
    "\n",
    "# Create an MLflow Run and log model with each configuration.\n",
    "models = []\n",
    "for run_name, retrievers in run_name_to_retrievers.items():\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        model_info = mlflow.llama_index.log_model(\n",
    "            # Specify the model Python script.\n",
    "            llama_index_model=\"workflow/model.py\",\n",
    "            # Specify retrievers to use.\n",
    "            model_config={\"retrievers\": retrievers},\n",
    "            # Define dependency files to save along with the model\n",
    "            code_paths=[\"workflow\"],\n",
    "            # Subdirectory to save artifacts (not important)\n",
    "            artifact_path=\"model\",\n",
    "        )\n",
    "        models.append(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6e827-1d8b-4611-9053-f1ff70497364",
   "metadata": {},
   "source": [
    "Now open the MLflow UI again, and this time it should show 4 MLflow Runs are recorded with different `retrievers` parameter values. By clicking each Run name and navigate to the \"Artifacts\" tab, you can see MLflow records the model and various metadata, such as dependency versions and settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca55e2-d4b7-45b5-96d8-60a247ddb69f",
   "metadata": {},
   "source": [
    "## 8. Enable MLflow Tracing\n",
    "\n",
    "Before running the evaluation, thereâ€™s one final step: enabling **MLflow Tracing**. We'll dive into this feature and why we do this here later, but for now, you can enable it with a simple one-line command. MLflow will automatically trace every LlamaIndex execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be3720-2562-422c-b1c0-44e1b19961d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.llama_index.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4110c71",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Workflow with Different Retriever Strategies\n",
    "\n",
    "The example repository includes a sample evaluation dataset, `mlflow_qa_dataset.csv`, containing 30 question-answer pairs related to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv(\"data/mlflow_qa_dataset.csv\")\n",
    "display(eval_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14703e34",
   "metadata": {},
   "source": [
    "To evaluate the workflow, use the [mlflow.evaluate()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) API, which requires (1) your dataset, (2) the logged model, and (3) the metrics you want to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics import latency\n",
    "from mlflow.metrics.genai import answer_correctness\n",
    "\n",
    "for model_info in models:\n",
    "    with mlflow.start_run(run_id=model_info.run_id):\n",
    "        result = mlflow.evaluate(\n",
    "            # Pass the URI of the logged model above\n",
    "            model=model_info.model_uri,\n",
    "            data=eval_df,\n",
    "            # Specify the column for ground truth answers.\n",
    "            targets=\"ground_truth\",\n",
    "            # Define the metrics to compute.\n",
    "            extra_metrics=[\n",
    "                latency(),\n",
    "                answer_correctness(\"openai:/gpt-4o-mini\"),\n",
    "            ],\n",
    "            # The answer_correctness metric requires \"inputs\" column to be\n",
    "            # present in the dataset. We have \"query\" instead so need to\n",
    "            # specify the mapping in `evaluator_config` parameter.\n",
    "            evaluator_config={\"col_mapping\": {\"inputs\": \"query\"}},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811568f0-811e-4b01-b1ab-bebf2c0a0707",
   "metadata": {},
   "source": [
    "In this example, we evaluate the model with two metrics:\n",
    "\n",
    "1. **Latency**: Measures the time taken to execute a workflow for a single query.\n",
    "2. **Answer Correctness**: Evaluates the accuracy of answers based on the ground truth, scored by the OpenAI GPT-4o model on a 1â€“5 scale.\n",
    "\n",
    "These metrics are just for demonstration purposesâ€”you can add additional metrics like toxicity or faithfulness, or even create your own.\n",
    "\n",
    "The evaluation process will take a few minutes. Once completed, you can view the results in the MLflow UI. Open the Experiment page and click on the chart icon ðŸ“ˆ above the Run list.\n",
    "\n",
    "*ðŸ’¡ The evaluation results can be different depending on model set up and some randomness.\n",
    "\n",
    "![Evaluation Result](static/images/llama_index_workflow_result_chart.png)\n",
    "\n",
    "The first row shows bar charts for the answer correctness metrics, while the second row displays latency results. The best-performing combination is \"Vector Search + BM25\". Interestingly, adding web search not only increases latency significantly but also decreases answer correctness.\n",
    "\n",
    "Why does this happen? It appears some answers from the web-search-enabled model are off-topic. For example, in response to a question about starting Model Registry, the web-search model provides an unrelated answer about model deployment, while the \"vs + bm25\" model offers a correct response.\n",
    "\n",
    "![Answer Comparison](static/images/llama_index_workflow_answer_comparison.png)\n",
    "\n",
    "Where did this incorrect answer come from? This seems to be a retriever issue, as we only changed the retrieval strategy. However, it's difficult to see what each retriever returned from the final result. To gain deeper insights into what's happening behind the scenes, MLflow Tracing is the perfect solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388dd7c-5479-4d5f-838b-73cf33d073f1",
   "metadata": {},
   "source": [
    "## 10. Inspecting Quality Issues with MLflow Trace\n",
    "\n",
    "[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) is a new feature that brings observability to LLM applications. It integrates seamlessly with LlamaIndex, recording all inputs, outputs, and metadata about intermediate steps during workflow execution. Since we called `mlflow.llama_index.autolog()` at the start, every LlamaIndex operation has been traced and recorded in the MLflow Experiment.\n",
    "\n",
    "To inspect the trace for a specific question from the evaluation, navigate to the \"Traces\" tab on the experiment page. Look for the row with the particular question in the request column and the run name \"vs + bm25 + web.\" Clicking the request ID link opens the Trace UI, where you can view detailed information about each step in the execution, including inputs, outputs, metadata, and latency.\n",
    "\n",
    "![Trace](static/images/llama_index_workflow_trace.png)\n",
    "\n",
    "In this case, we identified the issue by examining the reranker step. The web search retriever returned irrelevant context related to model serving, and the reranker incorrectly ranked it as the most relevant. With this insight, we can determine potential improvements, such as refining the reranker to better understand MLflow topics, improving web search precision, or even removing the web search retriever altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ebbd4-f394-425b-97fc-a5709e70233c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored how the combination of LlamaIndex and MLflow can elevate the development of Retrieval-Augmented Generation (RAG) workflows, bringing together powerful model management and observability capabilities. By integrating multiple retrieval strategiesâ€”such as vector search, BM25, and web searchâ€”we demonstrated how flexible retrieval can enhance the performance of LLM-driven applications.\n",
    "\n",
    "- **Experiment Tracking** allowed us to organize and log different workflow configurations, ensuring reproducibility and enabling us to track model performance across multiple runs.\n",
    "- **MLflow Evaluate** enabled us to easily log and evaluate the workflow with different retriever strategies, using key metrics like latency and answer correctness to compare performance.\n",
    "- **MLflow UI** gave us a clear visualization of how various retrieval strategies impacted both accuracy and latency, helping us identify the most effective configurations.\n",
    "- **MLflow Tracing**, integrated with LlamaIndex, provided detailed observability into each step of the workflow for diagnosing quality issues, such as incorrect reranking of search results.\n",
    "\n",
    "With these tools, you have a complete framework for building, logging, and optimizing RAG workflows. As LLM technology continues to evolve, the ability to track, evaluate, and fine-tune every aspect of model performance will be essential. We highly encourage you to experiment further and see how these tools can be tailored to your own applications.\n",
    "\n",
    "To continue learning, explore the following resources:\n",
    "\n",
    "* Learn more about the [MLflow LlamaIndex integration](https://mlflow.org/docs/latest/llms/llama-index/index.html).\n",
    "* Discover additional MLflow LLM features at [LLMs in MLflow](https://mlflow.org/docs/latest/llms/index.html).\n",
    "* Deploy your workflow to a serving endpoint with [MLflow Deployment](https://mlflow.org/docs/latest/deployment/index.html).\n",
    "* Check out more [Workflow examples](https://docs.llamaindex.ai/en/stable/module_guides/workflow/#examples) from LlamaIndex.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
