from unittest import mock

import httpx
import openai
import pytest

import mlflow
from mlflow.genai.utils.trace_utils import is_model_traced

from tests.tracing.helper import get_traces


def httpx_send_patch(request, *args, **kwargs):
    return httpx.Response(
        status_code=200,
        request=request,
        json={
            "id": "chatcmpl-Ax4UAd5xf32KjgLkS1SEEY9oorI9m",
            "object": "chat.completion",
            "created": 1738641958,
            "model": "gpt-4o-2024-08-06",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": '{"name":"Angelo","age":42}',
                        "refusal": None,
                    },
                    "logprobs": None,
                    "finish_reason": "stop",
                }
            ],
        },
    )


def get_openai_predict_fn(with_tracing=False):
    if with_tracing:
        mlflow.openai.autolog()

    def predict_fn(request):
        with mock.patch("httpx.Client.send", side_effect=httpx_send_patch):
            response = openai.OpenAI().chat.completions.create(
                messages=request["messages"],
                model="gpt-4o-mini",
            )
            return response.choices[0].message.content

    return predict_fn


def get_dummy_predict_fn(with_tracing=False):
    def predict_fn(request):
        return request

    if with_tracing:
        return mlflow.trace(predict_fn)

    return predict_fn


@pytest.fixture
def mock_openai_env(monkeypatch):
    monkeypatch.setenv("OPENAI_API_KEY", "fake_api_key")


@pytest.mark.usefixtures("mock_openai_env")
@pytest.mark.parametrize(
    ("predict_fn_generator", "with_tracing", "expected_traced"),
    [
        (get_dummy_predict_fn, False, False),
        (get_dummy_predict_fn, True, True),
        (get_openai_predict_fn, False, False),
        (get_openai_predict_fn, True, True),
    ],
    ids=[
        "dummy predict_fn without tracing",
        "dummy predict_fn with tracing",
        "openai predict_fn without tracing",
        "openai predict_fn with tracing",
    ],
)
def test_is_traced(predict_fn_generator, with_tracing, expected_traced):
    predict_fn = predict_fn_generator(with_tracing=with_tracing)
    sample_input = {"request": {"messages": [{"role": "user", "content": "test"}]}}
    is_actually_traced = is_model_traced(predict_fn, sample_input)
    assert is_actually_traced == expected_traced

    # No traces should be logged to backend during the check
    assert len(get_traces()) == 0

    # Make a prediction normally
    predict_fn(**sample_input)
    assert len(get_traces()) == (1 if expected_traced else 0)
